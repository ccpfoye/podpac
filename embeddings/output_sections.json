[
    {
        "file": "/home/cfoye/podpac/setup.py",
        "comments": [
            "//podpac.org\","
        ],
        "docstrings": [
            "\"\"\"\npodpac module\n\"\"\""
        ],
        "code_snippets": [
            "class PostDevelopCommand(develop):",
            "def run(self):\n        try:\n            subprocess.check_call([\"pre-commit\", \"install\"])\n        except subprocess.CalledProcessError as e:\n            print(\"Failed to install pre-commit hook\")\n\n        develop.run(self)\n\n\nsetup(\n    # ext_modules=None,\n    name=\"podpac\",\n    version=__version__,\n    description=\"Pipeline for Observational Data Processing, Analysis, and Collaboration\",\n    author=\"Creare\",\n    url=\"https:"
        ]
    },
    {
        "file": "/home/cfoye/podpac/doc/source/conf.py",
        "comments": [
            "//github.com/creare-com/podpac\"",
            "//www.sphinx-doc.org/en/master/ext/autosummary.html",
            "//github.com/sphinx-doc/sphinx/pull/4029",
            "//www.sphinx-doc.org/en/master/ext/extlinks.html",
            "//podpac.org\", \"logo_only\": True}",
            "//github.com/creare-com/podpac-examples/blob/develop/notebooks\""
        ],
        "docstrings": [
            "\"\"\"generates links to example notebooks dynamically\"\"\"",
            "\"\"\"copy the changelog from the root of the repository\"\"\""
        ],
        "code_snippets": [
            "def generate_example_links():",
            "def copy_changelog():",
            "def setup(app):\n    app.add_stylesheet(\"style.css\")  # may also be an URL\n    generate_example_links()\n    copy_changelog()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/embeddings/pre_process_repo.py",
        "comments": [
            "//.*\", file_content)",
            "//.*|/\\*[\\s\\S]*?\\*/|\"\"\".*?\"\"\"', file_content, re.DOTALL)"
        ],
        "docstrings": [
            "\"\"\".*?\"\"\"",
            "\"\"\".*?\"\"\""
        ],
        "code_snippets": [
            "def extract_comments(file_content):\n    single_line_comments = re.findall(r\"",
            "def extract_docstrings(file_content):\n    docstrings = re.findall(r'",
            "def extract_python_sections(file_content):\n    # Define regex patterns for classes and functions\n    class_pattern = r\"(class\\s+\\w+\\s*(?:\\([^\\)]*\\))?\\s*:\\s*(?:(?:(?:\\n(?:\\t| {4})).+)+)?)\"\n    function_pattern = r\"(def\\s+\\w+\\s*\\([^\\)]*\\)\\s*:\\s*(?:(?:(?:\\n(?:\\t| {4})).+)+)?)\"\n\n    # Combine the patterns\n    pattern = r\"|\".join([class_pattern, function_pattern])\n\n    # Find the start indices of matched sections\n    start_indices = [m.start() for m in re.finditer(pattern, file_content)]\n\n    # Find the end indices of matched sections by checking the indentation level\n    end_indices = []\n    for i, start_index in enumerate(start_indices[:-1]):\n        for line in file_content[start_indices[i+1]:].split(\"\\n\"):\n            if not re.match(r\"\\s*\\S\", line):\n                end_indices.append(start_indices[i+1] + line.rfind(\"\\n\"))\n                break\n        else:\n            end_indices.append(len(file_content))\n    end_indices.append(len(file_content))\n\n    sections = [file_content[start:end].strip() for start, end in zip(start_indices, end_indices)]\n\n    return sections",
            "def extract_related_sections(file_content):\n    # Assuming the code is written in Python, adjust the function for other languages\n    return extract_python_sections(file_content)\n\ndef extract_code_snippets(file_content):\n    code_snippets = re.split(r'",
            "def extract_code_snippets(file_content):\n    code_snippets = re.split(r'",
            "def process_file(file_path):\n    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        content = f.read()\n\n    comments = extract_comments(content)\n    docstrings = extract_docstrings(content)\n    code_snippets = extract_code_snippets(content)\n\n    return {\n        \"file\": file_path,\n        \"comments\": comments,\n        \"docstrings\": docstrings,\n        \"code_snippets\": code_snippets,\n    }",
            "def process_directory(root_path):\n    sections = []\n    for dirpath, _, filenames in os.walk(root_path):\n        for filename in filenames:\n            if filename.endswith((\".py\", \".js\", \".java\", \".c\", \".cpp\", \".h\", \".hpp\")):\n                file_path = os.path.join(dirpath, filename)\n                sections.append(process_file(file_path))\n    return sections\n\nif __name__ == \"__main__\":\n    repo_path = \"/home/cfoye/podpac\"\n    sections = process_directory(repo_path)\n\n    with open(\"output_sections.json\", \"w\") as f:\n        json.dump(sections, f, indent=4)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/embeddings/generate_embeddings.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/style.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nStyle Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/compositor.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nCompositor Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/algorithm.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nAlgorithm Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/__init__.py",
        "comments": [
            "//podpac.org/developer/contributing.html#public-api"
        ],
        "docstrings": [
            "\"\"\"\nPodpac Module\n\nPublic API\nSee https://podpac.org/developer/contributing.html#public-api\nfor more information about import conventions\n\nAttributes\n----------\nversion_info : OrderedDict\n    Dict with keys MAJOR, MINOR, HOTFIX depicting version\n\"\"\""
        ],
        "code_snippets": [
            "def makedirs(name, mode=511, exist_ok=False):\n    try:\n        _osmakedirs(name, mode)\n    except Exception as e:\n        if exist_ok:\n            pass\n        else:\n            raise e\n\n\nif sys.version_info.major == 2:\n    makedirs.__doc__ = os.makedirs.__doc__\n    os.makedirs = makedirs\nelse:\n    del _osmakedirs\ndel os\ndel sys\ndel makedirs\n\n# Public API\nfrom podpac.core.settings import settings\nfrom podpac.core.coordinates import Coordinates, crange, clinspace\nfrom podpac.core.node import Node, NodeException\nfrom podpac.core.utils import cached_property\nfrom podpac.core.units import ureg as units, UnitsDataArray\n\n# Organized submodules\n# These files are simply wrappers to create a curated namespace of podpac modules\nfrom podpac import algorithm\nfrom podpac import authentication\nfrom podpac import data\nfrom podpac import interpolators\nfrom podpac import coordinates\nfrom podpac import compositor\nfrom podpac import managers\nfrom podpac import utils\nfrom podpac import style\n\n## Developer API\nfrom podpac import core\n\n# version handling\nfrom podpac import version\n\n__version__ = version.version()\nversion_info = version.VERSION_INFO"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/managers.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nManagers Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/authentication.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nAuthentication Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/utils.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nUtils Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/coordinates.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nCoordinate Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/conftest.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nTest Setup\n\"\"\"",
            "\"\"\"Add command line option to pytest\n    Note you MUST invoke test as `pytest podpac --ci` to use these options.\n    Using only `pytest --ci` will result in an error\n\n    Parameters\n    ----------\n    parser : TYPE\n\n    \"\"\"",
            "\"\"\"Configuration before all tests are run\n\n    Parameters\n    ----------\n    config : TYPE\n\n    \"\"\"",
            "\"\"\"Configuration after all tests are run\n\n    Parameters\n    ----------\n    config : TYPE\n\n    \"\"\""
        ],
        "code_snippets": [
            "def pytest_addoption(parser):\n    \"\"\"Add command line option to pytest\n    Note you MUST invoke test as `pytest podpac --ci` to use these options.\n    Using only `pytest --ci` will result in an error\n\n    Parameters\n    ----------\n    parser : TYPE\n\n    \"\"\"\n    # config option for when we're running tests on ci\n    parser.addoption(\"--ci\", action=\"store_true\", default=False)",
            "def pytest_runtest_setup(item):\n    markers = [marker.name for marker in item.iter_markers()]\n    if item.config.getoption(\"--ci\") and \"aws\" in markers:\n        pytest.skip(\"Skip aws tests during CI\")",
            "def pytest_configure(config):\n    \"\"\"Configuration before all tests are run\n\n    Parameters\n    ----------\n    config : TYPE\n\n    \"\"\"\n\n    config.addinivalue_line(\"markers\", \"aws: mark test as an aws test\")\n    config.addinivalue_line(\"markers\", \"integration: mark test as integration test\")",
            "def pytest_unconfigure(config):\n    \"\"\"Configuration after all tests are run\n\n    Parameters\n    ----------\n    config : TYPE\n\n    \"\"\"\n    pass\n\n\noriginal_settings = {}",
            "def pytest_sessionstart(session):\n    # save settings\n    global original_settings\n    original_settings = copy.copy(settings)\n\n    # set the default cache\n    settings[\"DEFAULT_CACHE\"] = []",
            "def pytest_sessionfinish(session, exitstatus):\n    # restore settings\n    keys = list(settings.keys())\n    for key in keys:\n        if key in original_settings:\n            settings[key] = original_settings[key]\n        else:\n            del settings[key]"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/version.py",
        "comments": [],
        "docstrings": [
            "\"\"\"PODPAC Version\n\nAttributes\n----------\nVERSION : OrderedDict\n    dict of podpac version {('MAJOR': int), ('MINOR': int), ('HOTFIX': int)}\nVERSION_INFO : tuple of int\n    (MAJOR, MINOR, HOTFIX)\n\"\"\"",
            "\"\"\"Return semantic version of current PODPAC installation\n\n    Returns\n    -------\n    str\n        Semantic version of the current PODPAC installation\n    \"\"\"",
            "\"\"\"Retrieve PODPAC semantic version as string\n\n    Returns\n    -------\n    str\n        Semantic version if outside git repository\n        Returns `git describe --always` if inside the git repository\n    \"\"\""
        ],
        "code_snippets": [
            "def semver():\n    \"\"\"Return semantic version of current PODPAC installation\n\n    Returns\n    -------\n    str\n        Semantic version of the current PODPAC installation\n    \"\"\"\n    return \".\".join([str(v) for v in VERSION])",
            "def version():\n    \"\"\"Retrieve PODPAC semantic version as string\n\n    Returns\n    -------\n    str\n        Semantic version if outside git repository\n        Returns `git describe --always` if inside the git repository\n    \"\"\"\n\n    version_full = semver()\n    CWD = os.path.dirname(__file__)\n    got_git = os.path.exists(os.path.join(os.path.dirname(__file__), \"..\", \".git\"))\n    if not got_git:\n        return version_full\n    try:\n        # determine git binary\n        git = \"git\"\n        try:\n            subprocess.check_output([git, \"--version\"])\n        except Exception:\n            git = \"/usr/bin/git\"\n            try:\n                subprocess.check_output([git, \"--version\"])\n            except Exception as e:\n                return version_full\n\n        version_full = subprocess.check_output([git, \"describe\", \"--always\", \"--tags\"], cwd=CWD).strip().decode(\"ascii\")\n        version_full = version_full.replace(\"-\", \"+\", 1).replace(\"-\", \".\")  # Make this consistent with PEP440\n\n    except Exception as e:\n        print(\"Could not determine PODPAC version from git repo.\\n\" + str(e))\n\n    return version_full"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/interpolators.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nInterpolators Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/data.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nData Public Module\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/soilgrids.py",
        "comments": [
            "//maps.isric.org/",
            "//maps.isric.org/mapserv?map=/map/wrb.map\"",
            "//maps.isric.org/mapserv?map=/map/bdod.map\"",
            "//maps.isric.org/mapserv?map=/map/cec.map\"",
            "//maps.isric.org/mapserv?map=/map/cfvo.map\"",
            "//maps.isric.org/mapserv?map=/map/clay.map\"",
            "//maps.isric.org/mapserv?map=/map/nitrogen.map\"",
            "//maps.isric.org/mapserv?map=/map/phh2o.map\"",
            "//maps.isric.org/mapserv?map=/map/sand.map\"",
            "//maps.isric.org/mapserv?map=/map/silt.map\"",
            "//maps.isric.org/mapserv?map=/map/soc.map\"",
            "//maps.isric.org/mapserv?map=/map/ocs.map\"",
            "//maps.isric.org/mapserv?map=/map/ocd.map\""
        ],
        "docstrings": [
            "\"\"\"\nSoilGrids\n\nSee: https://maps.isric.org/\n\"\"\"",
            "\"\"\"Base SoilGrids WCS datasource.\"\"\"",
            "\"\"\"SoilGrids: WRB classes and probabilities (WCS)\"\"\"",
            "\"\"\"SoilGrids: Bulk density (WCS)\"\"\"",
            "\"\"\"SoilGrids: Cation exchange capacity and ph 7 (WCS)\"\"\"",
            "\"\"\"SoilGrids: Coarse fragments volumetric (WCS)\"\"\"",
            "\"\"\"SoilGrids: Clay content (WCS)\"\"\"",
            "\"\"\"SoilGrids: Nitrogen (WCS)\"\"\"",
            "\"\"\"SoilGrids: Soil pH in H2O (WCS)\"\"\"",
            "\"\"\"SoilGrids: Sand content (WCS)\"\"\"",
            "\"\"\"SoilGrids: Silt content (WCS)\"\"\"",
            "\"\"\"SoilGrids: Soil organic carbon content (WCS)\"\"\"",
            "\"\"\"SoilGrids: Soil organic carbon stock (WCS)\"\"\"",
            "\"\"\"SoilGrids: Organic carbon densities (WCS)\"\"\""
        ],
        "code_snippets": [
            "class SoilGridsBase(WCS):",
            "class SoilGridsWRB(SoilGridsBase):",
            "class SoilGridsBDOD(SoilGridsBase):",
            "class SoilGridsCEC(SoilGridsBase):",
            "class SoilGridsCFVO(SoilGridsBase):",
            "class SoilGridsClay(SoilGridsBase):",
            "class SoilGridsNitrogen(SoilGridsBase):",
            "class SoilGridsPHH2O(SoilGridsBase):",
            "class SoilGridsSand(SoilGridsBase):\n    \"\"\"SoilGrids: Sand content (WCS)\"\"\"\n\n    source = \"https://maps.isric.org/mapserv?map=/map/sand.map\"",
            "class SoilGridsSilt(SoilGridsBase):\n    \"\"\"SoilGrids: Silt content (WCS)\"\"\"\n\n    source = \"https://maps.isric.org/mapserv?map=/map/silt.map\"",
            "class SoilGridsSOC(SoilGridsBase):\n    \"\"\"SoilGrids: Soil organic carbon content (WCS)\"\"\"\n\n    source = \"https://maps.isric.org/mapserv?map=/map/soc.map\"",
            "class SoilGridsOCS(SoilGridsBase):\n    \"\"\"SoilGrids: Soil organic carbon stock (WCS)\"\"\"\n\n    source = \"https://maps.isric.org/mapserv?map=/map/ocs.map\"",
            "class SoilGridsOCD(SoilGridsBase):\n    \"\"\"SoilGrids: Organic carbon densities (WCS)\"\"\"\n\n    source = \"https://maps.isric.org/mapserv?map=/map/ocd.map\"\n\n\nif __name__ == \"__main__\":\n    import podpac\n\n    c = podpac.Coordinates(\n        [podpac.clinspace(-132.9023, -53.6051, 346, name=\"lon\"), podpac.clinspace(23.6293, 53.7588, 131, name=\"lat\")]\n    )\n\n    print(\"layers\")\n    print(SoilGridsSand.get_layers())\n\n    node = SoilGridsSand(layer=\"sand_0-5cm_mean\")\n    print(\"node\")\n    print(node)\n\n    output = node.eval(c)\n    print(\"eval\")\n    print(output)\n\n    node_chunked = SoilGridsSand(layer=\"sand_0-5cm_mean\", max_size=10000)\n    output_chunked = node_chunked.eval(c)\n\n    from matplotlib import pyplot\n\n    pyplot.figure()\n    pyplot.subplot(211)\n    output.plot()\n    pyplot.subplot(212)\n    output_chunked.plot()\n    pyplot.show(block=False)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/satutils.py",
        "comments": [
            "//github.com/sat-utils) developed by Development Seed",
            "//registry.opendata.aws/landsat-8/",
            "// urls",
            "//landsat-pds.s3.us-west-2.amazonaws.com/c1/L8/034/033/LC08_L1TP_034033_20201209_20201218_01_T1/LC08_L1TP_034033_20201209_20201218_01_T1_B2.TIF",
            "//landsat-pds/c1/L8/034/033/LC08_L1TP_034033_20201209_20201218_01_T1/LC08_L1TP_034033_20201209_20201218_01_T1_B2.TIF",
            "//\"):",
            "//\"):",
            "//%s/%s\" % (bucket, key)",
            "//github.com/sat-utils",
            "//github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb",
            "//github.com/radiantearth/stac-spec/tree/master/extensions/eo",
            "//github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb",
            "//registry.opendata.aws/landsat-8/",
            "//github.com/sat-utils) developed by Development Seed",
            "//github.com/radiantearth/stac-spec/tree/master/extensions/eo",
            "//github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb",
            "//github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb",
            "//registry.opendata.aws/sentinel-2/",
            "//github.com/sat-utils) developed by Development Seed.",
            "//github.com/radiantearth/stac-spec/tree/master/extensions/eo",
            "//github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb",
            "//github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb"
        ],
        "docstrings": [
            "\"\"\"\nSatellite data access using sat-utils (https://github.com/sat-utils) developed by Development Seed\n\nSupports access to:\n\n- Landsat 8 on AWS OpenData: https://registry.opendata.aws/landsat-8/\n- Sentinel 2\n\"\"\"",
            "\"\"\"for forwards/backwards compatibility, convert B0x to/from Bx as needed\"\"\"",
            "\"\"\"convert to s3:// urls\n    href: https://landsat-pds.s3.us-west-2.amazonaws.com/c1/L8/034/033/LC08_L1TP_034033_20201209_20201218_01_T1/LC08_L1TP_034033_20201209_20201218_01_T1_B2.TIF\n    url:  s3://landsat-pds/c1/L8/034/033/LC08_L1TP_034033_20201209_20201218_01_T1/LC08_L1TP_034033_20201209_20201218_01_T1_B2.TIF\n    \"\"\"",
            "\"\"\"\n    PODPAC DataSource node to access the data using sat-utils developed by Development Seed\n    See https://github.com/sat-utils\n\n    See :class:`podpac.compositor.OrderedCompositor` for more information.\n\n    Parameters\n    ----------\n    collection : str, optional\n        Specifies the collection for satellite data.\n        Options include \"landsat-8-l1\", \"sentinel-2-l1c\".\n        Defaults to all collections.\n    query : dict, optional\n        Dictionary of properties to query on, supports eq, lt, gt, lte, gte\n        Passed through to the sat-search module.\n        See https://github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb\n        Defaults to None\n    asset : str, optional\n        Asset to download from the satellite image.\n        The asset must be a band name or a common extension name, see https://github.com/radiantearth/stac-spec/tree/master/extensions/eo\n        See also the Assets section of this tutorial: https://github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb\n        Defaults to \"B3\" (green)\n    min_bounds_span : dict, optional\n        Default is {}. When specified, gives the minimum bounds that will be used for a coordinate in the query, so\n        it works properly. If a user specified a lat, lon point, the query may fail since the min/max values for\n        lat/lon are the same. When specified, these bounds will be padded by the following for latitude (as an example):\n        [lat - min_bounds_span['lat'] / 2, lat + min_bounds_span['lat'] / 2]\n    \"\"\"",
            "\"\"\"\n        Query data from sat-utils interface within PODPAC coordinates\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            PODPAC coordinates specifying spatial and temporal bounds\n\n        Raises\n        ------\n        ValueError\n            Error raised when no spatial or temporal bounds are provided\n\n        Returns\n        -------\n        search : :class:`satsearch.search.Search`\n            Results form sat-search\n        \"\"\"",
            "\"\"\"\n    Landsat 8 on AWS OpenData\n    https://registry.opendata.aws/landsat-8/\n\n    Leverages sat-utils (https://github.com/sat-utils) developed by Development Seed\n\n    Parameters\n    ----------\n    asset : str, optional\n        Asset to download from the satellite image.\n        For Landsat8, this includes: 'B01','B02','B03','B04','B05','B06','B07','B08','B09','B10','B11','B12'\n        The asset must be a band name or a common extension name, see https://github.com/radiantearth/stac-spec/tree/master/extensions/eo\n        See also the Assets section of this tutorial: https://github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb\n    query : dict, optional\n        Dictionary of properties to query on, supports eq, lt, gt, lte, gte\n        Passed through to the sat-search module.\n        See https://github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb\n        Defaults to None\n    min_bounds_span : dict, optional\n        Default is {}. When specified, gives the minimum bounds that will be used for a coordinate in the query, so\n        it works properly. If a user specified a lat, lon point, the query may fail since the min/max values for\n        lat/lon are the same. When specified, these bounds will be padded by the following for latitude (as an example):\n        [lat - min_bounds_span['lat'] / 2, lat + min_bounds_span['lat'] / 2]\n    \"\"\"",
            "\"\"\"\n    Sentinel 2 on AWS OpenData\n    https://registry.opendata.aws/sentinel-2/\n\n    Leverages sat-utils (https://github.com/sat-utils) developed by Development Seed.\n\n    Note this data source requires the requester to pay, so you must set podpac settings[\"AWS_REQUESTER_PAYS\"] = True\n\n    Parameters\n    ----------\n    asset : str, optional\n        Asset to download from the satellite image.\n        For Sentinel2, this includes: 'tki','B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B10','B11','B12\n        The asset must be a band name or a common extension name, see https://github.com/radiantearth/stac-spec/tree/master/extensions/eo\n        See also the Assets section of this tutorial: https://github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb\n    query : dict, optional\n        Dictionary of properties to query on, supports eq, lt, gt, lte, gte\n        Passed through to the sat-search module.\n        See https://github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb\n        Defaults to None\n    min_bounds_span : dict, optional\n        Default is {}. When specified, gives the minimum bounds that will be used for a coordinate in the query, so\n        it works properly. If a user specified a lat, lon point, the query may fail since the min/max values for\n        lat/lon are the same. When specified, these bounds will be padded by the following for latitude (as an example):\n        [lat - min_bounds_span['lat'] / 2, lat + min_bounds_span['lat'] / 2]\n    \"\"\""
        ],
        "code_snippets": [
            "def _get_asset_info(item, name):",
            "def _get_s3_url(item, asset_name):\n    \"\"\"convert to s3:",
            "class SatUtilsSource(RasterioRaw):\n    date = tl.Unicode(help=\"item.properties.datetime from sat-utils item\").tag(attr=True)",
            "def get_coordinates(self):\n        # get spatial coordinates from rasterio over s3\n        spatial_coordinates = super(SatUtilsSource, self).get_coordinates()\n        time = podpac.Coordinates([self.date], dims=[\"time\"], crs=spatial_coordinates.crs)\n        return podpac.coordinates.merge_dims([spatial_coordinates, time])\n\n\nclass SatUtils(S3Mixin, TileCompositor):\n    \"\"\"\n    PODPAC DataSource node to access the data using sat-utils developed by Development Seed\n    See https:",
            "class SatUtils(S3Mixin, TileCompositor):\n    \"\"\"\n    PODPAC DataSource node to access the data using sat-utils developed by Development Seed\n    See https:",
            "def _default_interpolation(self):\n        # this default interpolation enables NN interpolation without having to expand past the bounds of the query\n        # we're relying on satutils to give us the nearest neighboring tile here.\n        return {\"method\": \"nearest\", \"params\": {\"respect_bounds\": False}}\n\n    @tl.default(\"stac_api_url\")",
            "def _get_stac_api_url_from_env(self):\n        if \"STAC_API_URL\" not in os.environ:\n            raise TypeError(\n                \"STAC endpoint required. Please define the SatUtils 'stac_api_url' or 'STAC_API_URL' environmental variable\"\n            )\n\n        return os.environ",
            "def select_sources(self, coordinates, _selector=None):\n        result = self.search(coordinates)\n\n        if result.found() == 0:\n            _logger.warning(\n                \"Sat Utils did not find any items for collection {}. Ensure that sat-stac is installed, or try with a different set of coordinates (self.search(coordinates)).\".format(\n                    self.collection\n                )\n            )\n            return []\n\n        return [\n            SatUtilsSource(source=_get_s3_url(item, self.asset), date=item.properties[\"datetime\"], anon=self.anon)\n            for item in result.items()\n        ]",
            "def search(self, coordinates):\n        \"\"\"\n        Query data from sat-utils interface within PODPAC coordinates\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            PODPAC coordinates specifying spatial and temporal bounds\n\n        Raises\n        ------\n        ValueError\n            Error raised when no spatial or temporal bounds are provided\n\n        Returns\n        -------\n        search : :class:`satsearch.search.Search`\n            Results form sat-search\n        \"\"\"\n\n        # Ensure Coordinates are in decimal lat-lon\n        coordinates = coordinates.transform(\"epsg:4326\")\n\n        time_bounds = None\n        if \"time\" in coordinates.udims:\n            time_bounds = [\n                str(np.datetime64(bound, \"s\"))\n                for bound in coordinates[\"time\"].bounds\n                if isinstance(bound, np.datetime64)\n            ]\n            if len(time_bounds) < 2:\n                raise ValueError(\"Time coordinates must be of type np.datetime64\")\n\n            if self.min_bounds_span != None and \"time\" in self.min_bounds_span:\n                time_span, time_unit = self.min_bounds_span[\"time\"].split(\",\")\n                time_delta = np.timedelta64(int(time_span), time_unit)\n                time_bounds_dt = [np.datetime64(tb) for tb in time_bounds]\n                timediff = np.diff(time_bounds_dt)\n                if timediff < time_delta:\n                    pad = (time_delta - timediff) / 2\n                    time_bounds = [str((time_bounds_dt[0] - pad)[0]), str((time_bounds_dt[1] + pad)[0])]\n\n        bbox = None\n        if \"lat\" in coordinates.udims or \"lon\" in coordinates.udims:\n            lat = coordinates[\"lat\"].bounds\n            lon = coordinates[\"lon\"].bounds\n            if (self.min_bounds_span != None) and (\"lat\" in self.min_bounds_span) and (\"lon\" in self.min_bounds_span):\n                latdiff = np.diff(lat)\n                londiff = np.diff(lon)\n                if latdiff < self.min_bounds_span[\"lat\"]:\n                    pad = ((self.min_bounds_span[\"lat\"] - latdiff) / 2)[0]\n                    lat = [lat[0] - pad, lat[1] + pad]\n\n                if londiff < self.min_bounds_span[\"lon\"]:\n                    pad = ((self.min_bounds_span[\"lon\"] - londiff) / 2)[0]\n                    lon = [lon[0] - pad, lon[1] + pad]\n\n            bbox = [lon[0], lat[0], lon[1], lat[1]]\n\n        # TODO: do we actually want to limit an open query?\n        if time_bounds is None and bbox is None:\n            raise ValueError(\"No time or spatial coordinates requested\")\n\n        # search dict\n        search_kwargs = {}\n\n        search_kwargs[\"url\"] = self.stac_api_url\n\n        if time_bounds is not None:\n            search_kwargs[\"datetime\"] = \"{start_time}/{end_time}\".format(\n                start_time=time_bounds[0], end_time=time_bounds[1]\n            )\n\n        if bbox is not None:\n            search_kwargs[\"bbox\"] = bbox\n\n        if self.query is not None:\n            search_kwargs[\"query\"] = self.query\n        else:\n            search_kwargs[\"query\"] = {}\n\n        if self.collection is not None:\n            search_kwargs[\"collections\"] = [self.collection]\n\n        # search with sat-search\n        _logger.debug(\"sat-search searching with {}\".format(search_kwargs))\n        search = satsearch.Search(**search_kwargs)\n        _logger.debug(\"sat-search found {} items\".format(search.found()))\n\n        return search\n\n\nclass Landsat8(SatUtils):\n    \"\"\"\n    Landsat 8 on AWS OpenData\n    https:",
            "class Landsat8(SatUtils):\n    \"\"\"\n    Landsat 8 on AWS OpenData\n    https:",
            "class Sentinel2(SatUtils):\n    \"\"\"\n    Sentinel 2 on AWS OpenData\n    https://registry.opendata.aws/sentinel-2/\n\n    Leverages sat-utils (https://github.com/sat-utils) developed by Development Seed.\n\n    Note this data source requires the requester to pay, so you must set podpac settings[\"AWS_REQUESTER_PAYS\"] = True\n\n    Parameters\n    ----------\n    asset : str, optional\n        Asset to download from the satellite image.\n        For Sentinel2, this includes: 'tki','B01','B02','B03','B04','B05','B06','B07','B08','B8A','B09','B10','B11','B12\n        The asset must be a band name or a common extension name, see https://github.com/radiantearth/stac-spec/tree/master/extensions/eo\n        See also the Assets section of this tutorial: https://github.com/sat-utils/sat-stac/blob/master/tutorial-2.ipynb\n    query : dict, optional\n        Dictionary of properties to query on, supports eq, lt, gt, lte, gte\n        Passed through to the sat-search module.\n        See https://github.com/sat-utils/sat-search/blob/master/tutorial-1.ipynb\n        Defaults to None\n    min_bounds_span : dict, optional\n        Default is {}. When specified, gives the minimum bounds that will be used for a coordinate in the query, so\n        it works properly. If a user specified a lat, lon point, the query may fail since the min/max values for\n        lat/lon are the same. When specified, these bounds will be padded by the following for latitude (as an example):\n        [lat - min_bounds_span['lat'] / 2, lat + min_bounds_span['lat'] / 2]\n    \"\"\"\n\n    collection = \"sentinel-s2-l1c\""
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/nasaCMR.py",
        "comments": [
            "//cmr.earthdata.nasa.gov/search/\"",
            "//cmr.earthdata.nasa.gov/search/collections.json?short_name=SPL2SMAP_S",
            "//cmr.earthdata.nasa.gov/search/collections.json?short_name=SPL2SMAP_S"
        ],
        "docstrings": [
            "\"\"\"\nSearch using NASA CMR\n\"\"\"",
            "\"\"\"Uses NASA CMR to retrieve metadata about a collection\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    short_name: str, optional\n        The short name of the dataset\n    keyword: str, optional\n        Any keyword search parameters\n    **kwargs: str, optional\n        Any additional query parameters\n\n    Returns\n    ---------\n    list:\n        A list of collection metadata dictionaries\n\n    Examples:\n    -----------\n    >>> # This make the following request https://cmr.earthdata.nasa.gov/search/collections.json?short_name=SPL2SMAP_S\n    >>> get_collection_id(short_name='SPL2SMAP_S')\n    ['C1522341104-NSIDC_ECS']\n    \"\"\"",
            "\"\"\"Uses NASA CMR to retrieve collection id\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    short_name: str, optional\n        The short name of the dataset\n    keyword: str, optional\n        Any keyword search parameters\n    **kwargs: str, optional\n        Any additional query parameters\n\n    Returns\n    ---------\n    list\n        A list of collection id's (ideally only one)\n\n    Examples:\n    -----------\n    >>> # This make the following request https://cmr.earthdata.nasa.gov/search/collections.json?short_name=SPL2SMAP_S\n    >>> get_collection_id(short_name='SPL2SMAP_S')\n    ['C1522341104-NSIDC_ECS']\n    \"\"\"",
            "\"\"\"Search for specific files from NASA CMR for a particular collection\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    entry_map: function\n        A function applied to each individual entry. Could be used to filter out certain data in an entry\n    **kwargs: dict\n        Additional query string parameters.\n        At minimum the provider, provider_id, concept_id, collection_concept_id, short_name, version, or entry_title\n        need to be provided for a granule search.\n\n    Returns\n    ---------\n    list\n        Entries for each granule in the collection based on the search terms\n    \"\"\"",
            "\"\"\"Helper function for searching through all pages for a collection.\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    url: str\n        URL to website\n    entry_map: function\n        Function for mapping the entries to a desired format\n    max_paging_depth\n    \"\"\""
        ],
        "code_snippets": [
            "def get_collection_entries(session=None, short_name=None, keyword=None, **kwargs):\n    \"\"\"Uses NASA CMR to retrieve metadata about a collection\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    short_name: str, optional\n        The short name of the dataset\n    keyword: str, optional\n        Any keyword search parameters\n    **kwargs: str, optional\n        Any additional query parameters\n\n    Returns\n    ---------\n    list:\n        A list of collection metadata dictionaries\n\n    Examples:\n    -----------\n    >>> # This make the following request https:",
            "def get_collection_id(session=None, short_name=None, keyword=None, **kwargs):\n    \"\"\"Uses NASA CMR to retrieve collection id\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    short_name: str, optional\n        The short name of the dataset\n    keyword: str, optional\n        Any keyword search parameters\n    **kwargs: str, optional\n        Any additional query parameters\n\n    Returns\n    ---------\n    list\n        A list of collection id's (ideally only one)\n\n    Examples:\n    -----------\n    >>> # This make the following request https:",
            "def search_granule_json(session=None, entry_map=None, **kwargs):\n    \"\"\"Search for specific files from NASA CMR for a particular collection\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    entry_map: function\n        A function applied to each individual entry. Could be used to filter out certain data in an entry\n    **kwargs: dict\n        Additional query string parameters.\n        At minimum the provider, provider_id, concept_id, collection_concept_id, short_name, version, or entry_title\n        need to be provided for a granule search.\n\n    Returns\n    ---------\n    list\n        Entries for each granule in the collection based on the search terms\n    \"\"\"\n    base_url = CMR_URL + \"granules.json?\"\n\n    if not np.any(\n        [\n            m not in kwargs\n            for m in [\n                \"provider\",\n                \"provider_id\",\n                \"concept_id\",\n                \"collection_concept_id\",\n                \"short_name\",\n                \"version\",\n                \"entry_title\",\n            ]\n        ]\n    ):\n        raise ValueError(\n            \"Need to provide either\"\n            \" provider, provider_id, concept_id, collection_concept_id, short_name, version or entry_title\"\n            \" for granule search.\"\n        )\n\n    if \"page_size\" not in kwargs:\n        kwargs[\"page_size\"] = \"2000\"\n\n    if entry_map is None:\n        entry_map = lambda x: x\n\n    query_string = \"&\".join([k + \"=\" + str(v) for k, v in kwargs.items()])\n\n    if session is None:\n        session = requests\n\n    url = base_url + query_string\n    if \"page_num\" not in kwargs:\n        entries = _get_all_granule_pages(session, url, entry_map)\n    else:\n        pydict = _get_from_url(url, session).json()\n        entries = list(map(entry_map, pydict[\"feed\"][\"entry\"]))\n\n    return entries",
            "def _get_all_granule_pages(session, url, entry_map, max_paging_depth=1000000):\n    \"\"\"Helper function for searching through all pages for a collection.\n\n    Parameters\n    -----------\n    session: :class:`requets.Session`, optional\n        An authenticated Earthdata login session\n    url: str\n        URL to website\n    entry_map: function\n        Function for mapping the entries to a desired format\n    max_paging_depth\n    \"\"\"\n    page_size = int([q for q in url.split(\"?\")[1].split(\"&\") if \"page_size\" in q][0].split(\"=\")[1])\n    max_pages = int(max_paging_depth / page_size)\n\n    pydict = _get_from_url(url, session).json()\n    entries = list(map(entry_map, pydict[\"feed\"][\"entry\"]))\n\n    for i in range(1, max_pages):\n        page_url = url + \"&page_num=%d\" % (i + 1)\n        page_entries = _get_from_url(page_url, session).json()[\"feed\"][\"entry\"]\n        if not page_entries:\n            break\n        entries.extend(list(map(entry_map, page_entries)))\n    return entries"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/__init__.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nDatalib Public API\n\nThis module gets imported in the root __init__.py\nand exposed its contents to podpac.datalib\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/drought_monitor.py",
        "comments": [
            "//%s/%s\" % (bucket, store)"
        ],
        "docstrings": [],
        "code_snippets": [
            "class DroughtMonitorCategory(Zarr):\n    style = Style(clim=[0, 0.6], colormap=\"gist_earth_r\")",
            "class DroughtCategory(Algorithm):\n    # soil_moisture = NodeTrait().tag(attr=True, required=True)\n    # d0 = NodeTrait().tag(attr=True, required=True)\n    # d1 = NodeTrait().tag(attr=True, required=True)\n    # d2 = NodeTrait().tag(attr=True, required=True)\n    # d3 = NodeTrait().tag(attr=True, required=True)\n    # d4 = NodeTrait().tag(attr=True, required=True)\n    soil_moisture = NodeTrait().tag(attr=True)\n    d0 = NodeTrait().tag(attr=True)\n    d1 = NodeTrait().tag(attr=True)\n    d2 = NodeTrait().tag(attr=True)\n    d3 = NodeTrait().tag(attr=True)\n    d4 = NodeTrait().tag(attr=True)\n\n    style = Style(\n        clim=[0, 6],\n        enumeration_colors={\n            0: (0.45098039, 0.0, 0.0, 1.0),\n            1: (0.90196078, 0.0, 0.0, 1.0),\n            2: (1.0, 0.66666667, 0.0, 1.0),\n            3: (0.98823529, 0.82745098, 0.49803922, 1.0),\n            4: (1.0, 1.0, 0.0, 1.0),\n            5: (1.0, 1.0, 1.0, 0.0),\n        },\n    )",
            "def algorithm(self, inputs, coordinates):\n        sm = inputs[\"soil_moisture\"]\n        d0 = inputs[\"d0\"]\n        d1 = inputs[\"d1\"]\n        d2 = inputs[\"d2\"]\n        d3 = inputs[\"d3\"]\n        d4 = inputs[\"d4\"]\n\n        return (\n            (sm >= 0) * (sm < d4) * ((sm - 0) / (d4 - 0) + 0)\n            + (sm >= d4) * (sm < d3) * ((sm - d4) / (d3 - d4) + 1)\n            + (sm >= d3) * (sm < d2) * ((sm - d3) / (d2 - d3) + 2)\n            + (sm >= d2) * (sm < d1) * ((sm - d2) / (d1 - d2) + 3)\n            + (sm >= d1) * (sm < d0) * ((sm - d1) / (d0 - d1) + 4)\n            + (sm >= d0) * (sm < 0.75) * ((sm - d0) / (0.75 - d1) + 5)\n            + (sm >= 0.75) * 6\n        )\n\n\nif __name__ == \"__main__\":\n    import os\n    import numpy as np\n    import podpac\n\n    c = podpac.Coordinates([46.6, -123.5, \"2018-06-01\"], dims=[\"lat\", \"lon\", \"time\"])\n\n    # local\n    path = \"droughtmonitor/beta_parameters.zarr\"\n    if not os.path.exists(path):\n        print(\"No local drought monitor data found at '%s'\" % path)\n    else:\n        # drought monitor parameters\n        d0 = DroughtMonitorCategory(source=path, data_key=\"d0\")\n        print(d0.coordinates)\n        print(d0.eval(c))\n\n        # drought category\n        mock_sm = podpac.data.Array(data=np.random.random(d0.coordinates.shape), coordinates=d0.coordinates)\n\n        category = DroughtCategory(\n            soil_moisture=mock_sm,\n            d0=DroughtMonitorCategory(source=path, data_key=\"d0\"),\n            d1=DroughtMonitorCategory(source=path, data_key=\"d1\"),\n            d2=DroughtMonitorCategory(source=path, data_key=\"d2\"),\n            d3=DroughtMonitorCategory(source=path, data_key=\"d3\"),\n            d4=DroughtMonitorCategory(source=path, data_key=\"d4\"),\n        )\n        print(category.eval(c))\n\n    # s3\n    bucket = \"podpac-internal-test\"\n    store = \"drought_parameters.zarr\"\n    path = \"s3:"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/egi.py",
        "comments": [
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#overview",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#egiparameters",
            "//n5eil01u.ecs.nsidc.org/egi/request\"",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#egiparameters",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters",
            "//wiki.earthdata.nasa.gov/display/CMR/Creating+a+Token+Common",
            "//urs.earthdata.nasa.gov/)",
            "//urs.earthdata.nasa.gov/)",
            "//n5eil01u.ecs.nsidc.org/egi/request\"",
            "//cmr.earthdata.nasa.gov/legacy-services/rest/tokens\""
        ],
        "docstrings": [
            "\"\"\"\nPODPAC node to access the NASA EGI Programmatic Interface\nhttps://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#overview\n\"\"\"",
            "\"\"\"\n    PODPAC DataSource node to access the NASA EGI Programmatic Interface\n    https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters\n\n    Parameters\n    ----------\n    short_name : str\n        Specifies the short name of the collection used to find granules for the coverage requested. Required.\n        See https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters\n    data_key : str\n        Path to the subset data layer or group for Parameter Subsetting. Required.\n        Equivalent to \"Coverage\" paramter described in\n        https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters\n    lat_key : str\n        Key for latitude data in endpoint HDF-5 file. Required.\n    lon_key : str\n        Key for longitude data in endpoint HDF-5 file. Required.\n    min_bounds_span: dict, optional\n        Default is {}. When specified, gives the minimum bounds that will be used for a coordinate in the EGI query, so\n        it works properly. If a user specified a lat,lon point, the EGI query may fail since the min/max values for\n        lat/lon are the same. When specified, these bounds will be padded by the following for latitude (as an example):\n        [lat - min_bounds_span['lat'] / 2, lat + min_bounds_span['lat'] / 2]\n    base_url : str, optional\n        URL for EGI data endpoint.\n        Defaults to :str:`BASE_URL`\n        See https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#egiparameters\n    page_size : int, optional\n        Number of granules returned from CMR per HTTP call. Defaults to 20.\n        See https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters\n    updated_since : str, optional\n        Can be used to find granules recently updated in CMR. Optional.\n        See https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs#cmrparameters\n    version : str, int, optional\n        Data product version. Optional.\n        Number input will be cast into a 3 character string NNN, i.e. 3 -> \"003\"\n    token : str, optional\n        EGI Token from authentication process.\n        See https://wiki.earthdata.nasa.gov/display/CMR/Creating+a+Token+Common\n        If undefined, the node will look for a token under the setting key \"token@urs.earthdata.nasa.gov\".\n        If this setting is not defined, the node will attempt to generate a token using\n        :attr:`self.username` and :attr:`self.password`\n    username : str, optional\n        Earthdata username (https://urs.earthdata.nasa.gov/)\n        If undefined, node will look for a username under setting key \"username@urs.earthdata.nasa.gov\"\n    password : str, optional\n        Earthdata password (https://urs.earthdata.nasa.gov/)\n        If undefined, node will look for a password under setting key \"password@urs.earthdata.nasa.gov\"\n\n    Attributes\n    ----------\n    data : :class:`podpac.UnitsDataArray`\n        The data array compiled from downloaded EGI data\n    \"\"\"",
            "\"\"\" This needs to be implemented so this node will cache properly. See Datasource.eval.\"\"\"",
            "\"\"\"\n        URL Endpoint built from input parameters\n\n        Returns\n        -------\n        str\n        \"\"\"",
            "\"\"\"Interpret individual file from  EGI zip archive.\n\n        Parameters\n        ----------\n        filelike : filelike\n            Reference to file inside EGI zip archive\n\n        Returns\n        -------\n        podpac.UnitsDataArray\n\n        Raises\n        ------\n        ValueError\n        \"\"\"",
            "\"\"\"Append new data\n\n        Parameters\n        ----------\n        all_data : podpac.UnitsDataArray\n            aggregated data\n        data : podpac.UnitsDataArray\n            new data to append\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\"",
            "\"\"\"\n        Download data from EGI Interface within PODPAC coordinates\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            PODPAC coordinates specifying spatial and temporal bounds\n\n        Raises\n        ------\n        ValueError\n            Error raised when no spatial or temporal bounds are provided\n\n        Returns\n        -------\n        zipfile.ZipFile\n            Returns zip file byte-str to downloaded data\n        \"\"\"",
            "\"\"\"Generator for getting zip files from EGI interface\n\n        Parameters\n        ----------\n        url : str\n            base url without page_num attached\n        page_num : int, optional\n            page_num to query\n\n        Yields\n        ------\n        zipfile.ZipFile\n            ZipFile of results from page\n\n        Raises\n        ------\n        ValueError\n            Raises value error if no granules available from EGI\n        \"\"\"",
            "\"\"\"Shortcut to :func:`podpac.authentication.set_crendentials` using class member :attr:`self.hostname` for the hostname\n\n        Parameters\n        ----------\n        username : str, optional\n            Username to store in settings for `self.hostname`.\n            If no username is provided and the username does not already exist in the settings,\n            the user will be prompted to enter one.\n        password : str, optional\n            Password to store in settings for `self.hostname`\n            If no password is provided and the password does not already exist in the settings,\n            the user will be prompted to enter one.\n        \"\"\"",
            "\"\"\"\n        Validate EGI token set in :attr:`token` attribute of EGI Node\n\n        Returns\n        -------\n        Bool\n            True if token is valid, False if token is invalid\n        \"\"\"",
            "\"\"\"\n        Get token for EGI interface using Earthdata credentials\n\n        Returns\n        -------\n        str\n            Token for access to EGI interface\n\n        Raises\n        ------\n        ValueError\n            Raised if Earthdata username or password is unavailable\n        \"\"\"",
            "\"\"\"\n        <token>\n            <username>{username}</username>\n            <password>{password}</password>\n            <client_id>podpac</client_id>\n            <user_ip_address>{ip}</user_ip_address>\n        </token>\n        \"\"\"",
            "\"\"\"\n        Utility to return a best guess at the IP address of the local machine.\n        Required by EGI authentication to get EGI token.\n        \"\"\""
        ],
        "code_snippets": [
            "class EGI(InterpolationMixin, DataSource):\n    \"\"\"\n    PODPAC DataSource node to access the NASA EGI Programmatic Interface\n    https:",
            "def udims(self):\n        if self.udims_overwrite:\n            return self.udims_overwrite",
            "def _version_to_str(self, proposal):\n        v = proposal[\"value\"]\n        if isinstance(v, int):\n            return \"{:03d}\".format(v)\n\n        if isinstance(v, string_types):\n            return v.zfill(3)\n\n        return None\n\n    updated_since = tl.Unicode(default_value=None, allow_none=True)\n\n    # auth\n    username = tl.Unicode(allow_none=True)\n\n    @tl.default(\"username\")",
            "def _username_default(self):\n        if \"username@urs.earthdata.nasa.gov\" in settings:\n            return settings[\"username@urs.earthdata.nasa.gov\"]\n\n        return None\n\n    password = tl.Unicode(allow_none=True)\n\n    @tl.default(\"password\")",
            "def _password_default(self):\n        if \"password@urs.earthdata.nasa.gov\" in settings:\n            return settings[\"password@urs.earthdata.nasa.gov\"]\n\n        return None\n\n    token = tl.Unicode(allow_none=True)\n\n    @tl.default(\"token\")",
            "def _token_default(self):\n        if \"token@urs.earthdata.nasa.gov\" in settings:\n            return settings[\"token@urs.earthdata.nasa.gov\"]\n\n        return None\n\n    @property",
            "def coverage(self):\n        return (self.data_key, self.lat_key, self.lon_key)\n\n    # attributes\n    data = tl.Any(allow_none=True)\n    _url = tl.Unicode(allow_none=True)\n\n    @cached_property",
            "def source(self):\n        \"\"\"\n        URL Endpoint built from input parameters\n\n        Returns\n        -------\n        str\n        \"\"\"\n        url = copy.copy(self.base_url)\n        url += \"?short_name={}\".format(self.short_name)",
            "def _append(u, key, val):\n            u += \"&{key}={val}\".format(key=key, val=val)\n            return u\n\n        url = _append(url, \"Coverage\", \",\".join(self.coverage))\n\n        # Format could be customized - see response_format above\n        # For now we set to HDF5\n        # url = _append(url, \"format\", self.response_format)\n        url = _append(url, \"format\", \"HDF-EOS\")\n\n        if self.version:\n            url = _append(url, \"version\", self.version)\n\n        if self.updated_since:\n            url = _append(url, \"Updated_since\", self.updated_since)\n\n        # other parameters are included at eval time\n        return url\n\n    @property",
            "def coordinates(self):\n        if self.data is None:\n            _log.warning(\"No coordinates found in EGI source\")\n            return Coordinates([], dims=[])\n\n        return Coordinates.from_xarray(self.data)",
            "def get_data(self, coordinates, coordinates_index):\n        if self.data is not None:\n            da = self.data.data[coordinates_index]\n            return da\n        else:\n            _log.warning(\"No data found in EGI source\")\n            return np.array([])",
            "def _eval(self, coordinates, output=None, _selector=None):\n        # download data for coordinate bounds, then handle that data as an H5PY node\n        zip_files = self._download(coordinates)\n        try:\n            self.data = self._read_zips(zip_files)  # reads each file in zip archive and creates single dataarray\n        except KeyError as e:\n            print(\"This following error may occur if data_key, lat_key, or lon_key is not correct.\")\n            print(\n                \"This error may also occur if the specified area bounds are smaller than the dataset pixel size, in\"\n                \" which case EGI is returning no data.\"\n            )\n            raise e\n\n        # run normal eval once self.data is prepared\n        return super(EGI, self)._eval(coordinates, output=output, _selector=_selector)\n\n    ##########\n    # Data I/O\n    ##########",
            "def read_file(self, filelike):\n        \"\"\"Interpret individual file from  EGI zip archive.\n\n        Parameters\n        ----------\n        filelike : filelike\n            Reference to file inside EGI zip archive\n\n        Returns\n        -------\n        podpac.UnitsDataArray\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n\n        raise NotImplementedError(\"read_file must be implemented for EGI DataSource\")\n\n        ## TODO: implement generic handler based on keys and dimensions\n\n        # # load file\n        # hdf5_file = h5py.File(filelike)\n\n        # # handle data\n        # data = hdf5_file[self.data_key]\n        # lat = hdf5_file[self.lat_key] if self.lat_key in hdf5_file else None\n        # lon = hdf5_file[self.lon_key] if self.lon_key in hdf5_file else None\n        # time = hdf5_file[self.time_key] if self.time_key in hdf5_file else None\n\n        # # stacked coords\n        # if data.ndim == 2:\n        #     c = Coordinates([(lat, lon), time], dims=['lat_lon', 'time'])\n\n        # # gridded coords\n        # elif data.ndim == 3:\n        #     c = Coordinates([lat, lon, time], dims=['lat', 'lon', 'time'])\n        # else:\n        #     raise ValueError('Data must have either 2 or 3 dimensions')",
            "def append_file(self, all_data, data):\n        \"\"\"Append new data\n\n        Parameters\n        ----------\n        all_data : podpac.UnitsDataArray\n            aggregated data\n        data : podpac.UnitsDataArray\n            new data to append\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\"\n        raise NotImplementedError()",
            "def _download(self, coordinates):\n        \"\"\"\n        Download data from EGI Interface within PODPAC coordinates\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            PODPAC coordinates specifying spatial and temporal bounds\n\n        Raises\n        ------\n        ValueError\n            Error raised when no spatial or temporal bounds are provided\n\n        Returns\n        -------\n        zipfile.ZipFile\n            Returns zip file byte-str to downloaded data\n        \"\"\"\n\n        # Ensure Coordinates are in decimal lat-lon\n        coordinates = coordinates.transform(\"epsg:4326\")\n        self._authenticate()\n\n        time_bounds = None\n        bbox = None\n\n        if \"time\" in coordinates.udims:\n            time_bounds = [\n                str(np.datetime64(bound, \"s\"))\n                for bound in coordinates[\"time\"].bounds\n                if isinstance(bound, np.datetime64)\n            ]\n            if len(time_bounds) < 2:\n                raise ValueError(\"Time coordinates must be of type np.datetime64\")\n\n            if self.min_bounds_span != None and \"time\" in self.min_bounds_span:\n                time_span, time_unit = self.min_bounds_span[\"time\"].split(\",\")\n                time_delta = np.timedelta64(int(time_span), time_unit)\n                time_bounds_dt = [np.datetime64(tb) for tb in time_bounds]\n                timediff = np.diff(time_bounds_dt)\n                if timediff < time_delta:\n                    pad = (time_delta - timediff) / 2\n                    time_bounds = [str((time_bounds_dt[0] - pad)[0]), str((time_bounds_dt[1] + pad)[0])]\n\n        if \"lat\" in coordinates.udims or \"lon\" in coordinates.udims:\n            lat = coordinates[\"lat\"].bounds\n            lon = coordinates[\"lon\"].bounds\n            if (self.min_bounds_span != None) and (\"lat\" in self.min_bounds_span) and (\"lon\" in self.min_bounds_span):\n                latdiff = np.diff(lat)\n                londiff = np.diff(lon)\n                if latdiff < self.min_bounds_span[\"lat\"]:\n                    pad = ((self.min_bounds_span[\"lat\"] - latdiff) / 2)[0]\n                    lat = [lat[0] - pad, lat[1] + pad]\n\n                if londiff < self.min_bounds_span[\"lon\"]:\n                    pad = ((self.min_bounds_span[\"lon\"] - londiff) / 2)[0]\n                    lon = [lon[0] - pad, lon[1] + pad]\n\n            bbox = \"{},{},{},{}\".format(lon[0], lat[0], lon[1], lat[1])\n\n        # TODO: do we actually want to limit an open query?\n        if time_bounds is None and bbox is None:\n            raise ValueError(\"No time or spatial coordinates requested\")\n\n        url = self.source\n\n        if time_bounds is not None:\n            url += \"&time={start_time},{end_time}\".format(start_time=time_bounds[0], end_time=time_bounds[1])\n\n        if bbox is not None:\n            url += \"&Bbox={bbox}\".format(bbox=bbox)\n\n        # admin parameters\n        url += \"&token={token}&page_size={page_size}\".format(token=self.token, page_size=self.page_size)\n        self._url = url  # for debugging\n\n        # iterate through pages to build up zipfiles containg data\n        return list(self._query_egi(url))",
            "def _query_egi(self, url, page_num=1):\n        \"\"\"Generator for getting zip files from EGI interface\n\n        Parameters\n        ----------\n        url : str\n            base url without page_num attached\n        page_num : int, optional\n            page_num to query\n\n        Yields\n        ------\n        zipfile.ZipFile\n            ZipFile of results from page\n\n        Raises\n        ------\n        ValueError\n            Raises value error if no granules available from EGI\n        \"\"\"\n        good_result = True\n        while good_result:\n            # create the full url\n            page_url = \"{}&page_num={}\".format(url, page_num)\n            _log.debug(\"Querying EGI url: {}\".format(page_url))\n            r = requests.get(page_url)\n\n            if r.status_code != 200:\n                good_result = False\n\n                # raise exception if the status is not 200 on the first page\n                if page_num == 1:\n                    raise ValueError(\"Failed to download data from EGI Interface. EGI Reponse: {}\".format(r.text))\n\n                # end iteration\n                elif r.status_code == 501 and \"No granules returned by CMR\" in r.text:\n                    _log.debug(\"Last page returned from EGI Interface: {}\".format(page_num - 1))\n\n                # not sure of response, so end iteration\n                else:\n                    _log.warning(\"Page returned from EGI Interface with unknown response: {}\".format(r.text))\n\n            else:\n                good_result = True\n                # most of the time, EGI returns a zip file\n                if \".zip\" in r.headers[\"Content-Disposition\"]:\n                    # load content into file-like object and then read into zip file\n                    f = BytesIO(r.content)\n                    zip_file = zipfile.ZipFile(f)\n\n                # if only one file exists, it will return the single file. This puts the single file in a zip archive\n                else:\n                    filename = r.headers[\"Content-Disposition\"].split('filename=\"')[1].replace('\"', \"\")\n                    f = BytesIO()\n                    zip_file = zipfile.ZipFile(f, \"w\")\n                    zip_file.writestr(filename, r.content)\n\n                # yield the current zip file\n                yield zip_file\n            page_num += 1",
            "def _read_zips(self, zip_files):\n\n        all_data = None\n        _log.debug(\"Processing {} zip files from EGI response\".format(len(zip_files)))\n\n        for zip_file in zip_files:\n            for name in zip_file.namelist():\n                if name.endswith(\"json\"):\n                    _log.debug(\"Ignoring file: {}\".format(name))\n                    continue\n\n                _log.debug(\"Reading file: {}\".format(name))\n\n                # BytesIO\n                try:\n                    bio = BytesIO(zip_file.read(name))\n                except (zipfile.BadZipfile, EOFError) as e:\n                    _log.warning(str(e))\n                    continue\n\n                # read file\n                uda = self.read_file(bio)\n\n                # TODO: this can likely be simpler and automated\n                if uda is not None:\n                    if all_data is None:\n                        all_data = uda.isel(lon=np.isfinite(uda.lon), lat=np.isfinite(uda.lat))\n                    else:\n                        all_data = self.append_file(all_data, uda)\n                else:\n                    _log.warning(\"No data returned from file: {}\".format(name))\n\n        return all_data\n\n    ######################################\n    # Token and Authentication Handling  #\n    ######################################",
            "def set_credentials(self, username=None, password=None):\n        \"\"\"Shortcut to :func:`podpac.authentication.set_crendentials` using",
            "class member :attr:`self.hostname` for the hostname\n\n        Parameters\n        ----------\n        username : str, optional\n            Username to store in settings for `self.hostname`.\n            If no username is provided and the username does not already exist in the settings,\n            the user will be prompted to enter one.\n        password : str, optional\n            Password to store in settings for `self.hostname`\n            If no password is provided and the password does not already exist in the settings,\n            the user will be prompted to enter one.\n        \"\"\"\n        return authentication.set_credentials(\"urs.earthdata.nasa.gov\", username=username, password=password)",
            "def _authenticate(self):\n        if self.token is None:\n            self.get_token()\n\n        # if token's not valid, try getting a new token\n        if not self.token_valid():\n            self.get_token()\n\n        # if token is still not valid, throw error\n        if not self.token_valid():\n            raise ValueError(\n                \"Failed to get a valid token from EGI Interface. \"\n                + \"Try requesting a token manually using `self.get_token()`\"\n            )\n\n        _log.debug(\"EGI Token valid\")",
            "def token_valid(self):\n        \"\"\"\n        Validate EGI token set in :attr:`token` attribute of EGI Node\n\n        Returns\n        -------\n        Bool\n            True if token is valid, False if token is invalid\n        \"\"\"\n        r = requests.get(\"{base_url}?token={token}\".format(base_url=self.base_url, token=self.token))\n\n        return r.status_code != 401",
            "def get_token(self):\n        \"\"\"\n        Get token for EGI interface using Earthdata credentials\n\n        Returns\n        -------\n        str\n            Token for access to EGI interface\n\n        Raises\n        ------\n        ValueError\n            Raised if Earthdata username or password is unavailable\n        \"\"\"\n        # token access URL\n        url = \"https:",
            "def _get_ip(self):\n        \"\"\"\n        Utility to return a best guess at the IP address of the local machine.\n        Required by EGI authentication to get EGI token.\n        \"\"\"\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        try:\n            s.connect((\"8.8.8.8\", 80))\n            ip = s.getsockname()[0]\n        except Exception:\n            ip = \"127.0.0.1\"\n        finally:\n            s.close()\n\n        return ip\n\n    @classmethod",
            "def get_ui_spec(cls, help_as_html=False):\n        spec = super().get_ui_spec(help_as_html=help_as_html)\n        spec[\"attrs\"][\"username\"] = {}\n        spec[\"attrs\"][\"password\"] = {}\n        return spec"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/terraintiles.py",
        "comments": [
            "//registry.opendata.aws/terrain-tiles/",
            "//mapzen.com/documentation/terrain-tiles/",
            "//github.com/tilezen/joerd",
            "//github.com/tilezen/joerd/blob/master/docs/attribution.md",
            "//github.com/racemap/elevation-service/blob/master/tileset.js",
            "//\", \"\"))  # path to file",
            "//registry.opendata.aws/terrain-tiles/",
            "//mapzen.com/documentation/terrain-tiles/",
            "//{}/{}\".format(self.bucket, s) for s in sources]",
            "//registry.opendata.aws/terrain-tiles/",
            "//mapzen.com/documentation/terrain-tiles/",
            "//github.com/tilezen/joerd",
            "//github.com/tilezen/joerd"
        ],
        "docstrings": [
            "\"\"\"\nTerrain Tiles\n\nHosted on AWS S3\nhttps://registry.opendata.aws/terrain-tiles/\n\nDescription\n    Gridded elevation tiles\nResource type\n    S3 Bucket\nAmazon Resource Name (ARN)\n    arn:aws:s3:::elevation-tiles-prod\nAWS Region\n    us-east-1\n\nDocumentation: https://mapzen.com/documentation/terrain-tiles/\n\nAttribution\n-----------\n- Some source adapted from https://github.com/tilezen/joerd\n- See required attribution when using terrain tiles:\n  https://github.com/tilezen/joerd/blob/master/docs/attribution.md\n\nAttributes\n----------\nTILE_FORMATS : list\n    list of support tile formats\n\nNotes\n-----\nSee https://github.com/racemap/elevation-service/blob/master/tileset.js\nfor example skadi implementation\n\"\"\"",
            "\"\"\"DataSource to handle individual TerrainTiles raster files\n\n    Parameters\n    ----------\n    source : str\n        Path to the sourcefile on S3\n\n    Attributes\n    ----------\n    dataset : :class:`rasterio.io.DatasetReader`\n        rasterio dataset\n    \"\"\"",
            "\"\"\"\n        Download the TerrainTile file from S3 to a local file.\n        This is a convience method for users and not used by PODPAC machinery.\n\n        Parameters\n        ----------\n        path : str\n            Subdirectory to put files. Defaults to 'terraintiles'.\n            Within this directory, the tile files will retain the same directory structure as on S3.\n        \"\"\"",
            "\"\"\"Terrain Tiles gridded elevation tiles data library\n\n    Hosted on AWS S3\n    https://registry.opendata.aws/terrain-tiles/\n\n    Description\n        Gridded elevation tiles\n    Resource type\n        S3 Bucket\n    Amazon Resource Name (ARN)\n        arn:aws:s3:::elevation-tiles-prod\n    AWS Region\n        us-east-1\n\n    Documentation: https://mapzen.com/documentation/terrain-tiles/\n\n    Parameters\n    ----------\n    zoom : int\n        Zoom level of tiles, in [0, ..., 14]. Defaults to 7. A value of \"-1\" will automatically determine the zoom level.\n        WARNING: When automatic zoom is used, evaluating points (stacked lat,lon) uses the maximum zoom level (level 14)\n    tile_format : str\n        One of ['geotiff', 'terrarium', 'normal']. Defaults to 'geotiff'\n        PODPAC node can only evaluate 'geotiff' formats.\n        Other tile_formats can be specified for :meth:`download`\n        No support for 'skadi' formats at this time.\n    bucket : str\n        Bucket of the terrain tiles.\n        Defaults to 'elevation-tiles-prod'\n    \"\"\"",
            "\"\"\"\n        Download active terrain tile source files to local directory\n\n        Parameters\n        ----------\n        path : str\n            Subdirectory to put files. Defaults to 'terraintiles'.\n            Within this directory, the tile files will retain the same directory structure as on S3.\n        \"\"\"",
            "\"\"\"Terrain Tiles gridded elevation tiles data library\n\n    Hosted on AWS S3\n    https://registry.opendata.aws/terrain-tiles/\n\n    Description\n        Gridded elevation tiles\n    Resource type\n        S3 Bucket\n    Amazon Resource Name (ARN)\n        arn:aws:s3:::elevation-tiles-prod\n    AWS Region\n        us-east-1\n\n    Documentation: https://mapzen.com/documentation/terrain-tiles/\n\n    Parameters\n    ----------\n    zoom : int\n        Zoom level of tiles. Defaults to 6.\n    tile_format : str\n        One of ['geotiff', 'terrarium', 'normal']. Defaults to 'geotiff'\n        PODPAC node can only evaluate 'geotiff' formats.\n        Other tile_formats can be specified for :meth:`download`\n        No support for 'skadi' formats at this time.\n    bucket : str\n        Bucket of the terrain tiles.\n        Defaults to 'elevation-tiles-prod'\n    \"\"\"",
            "\"\"\"Get tile urls for a specific zoom level and geospatial coordinates\n\n    Parameters\n    ----------\n    tile_format : str\n        format of the tile to get\n    zoom : int\n        zoom level\n    coordinates : :class:`podpac.Coordinates`, optional\n        only return tiles within coordinates\n\n    Returns\n    -------\n    list of str\n        list of tile urls\n    \"\"\"",
            "\"\"\"Query for tiles within podpac coordinates\n\n    This method allows you to get the available tiles in a given spatial area.\n    This will work for all :attr:`TILE_FORMAT` types\n\n    Parameters\n    ----------\n    coordinates : :class:`podpac.coordinates.Coordinates`\n        Find available tiles within coordinates\n    zoom : int, optional\n        zoom level\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    Returns\n    -------\n    list of tuple\n        list of tile tuples (x, y, zoom) for zoom level and coordinates\n    \"\"\"",
            "\"\"\"Build S3 URL prefix\n\n    The S3 bucket is organized {tile_format}/{z}/{x}/{y}.tif\n\n    Parameters\n    ----------\n    tile_format : str\n        One of 'terrarium', 'normal', 'geotiff'\n    zoom : int\n        zoom level\n    x : int\n        x tilespace coordinate\n    y : int\n        x tilespace coordinate\n\n    Returns\n    -------\n    str\n        Bucket prefix\n\n    Raises\n    ------\n    TypeError\n    \"\"\"",
            "\"\"\"\n    Convert geographic bounds into a list of tile coordinates at given zoom.\n    Adapted from https://github.com/tilezen/joerd\n\n    Parameters\n    ----------\n    lat_bounds : :class:`np.array` of float\n        [min, max] bounds from lat (y) coordinates\n    lon_bounds : :class:`np.array` of float\n        [min, max] bounds from lon (x) coordinates\n    zoom : int\n        zoom level\n\n    Returns\n    -------\n    list of tuple\n        list of tuples (x, y, zoom) describing the tiles to cover coordinates\n    \"\"\"",
            "\"\"\"Get tiles at a single point and zoom level\n\n    Parameters\n    ----------\n    lat : float\n        latitude\n    lon : float\n        longitude\n    zoom : int\n        zoom level\n\n    Returns\n    -------\n    tuple\n        (x, y, zoom) tile url\n    \"\"\"",
            "\"\"\"Convert latitude, longitude to x, y mercator coordinate at given zoom\n    Adapted from https://github.com/tilezen/joerd\n\n    Parameters\n    ----------\n    lat : float\n        latitude\n    lon : float\n        longitude\n\n    Returns\n    -------\n    tuple\n        (x, y) float mercator coordinates\n    \"\"\"",
            "\"\"\"Convert mercator to tilespace coordinates\n\n    Parameters\n    ----------\n    x : float\n        mercator x coordinate\n    y : float\n        mercator y coordinate\n    zoom : int\n        zoom level\n\n    Returns\n    -------\n    tuple\n        (x, y) int tile coordinates\n    \"\"\""
        ],
        "code_snippets": [
            "class TerrainTilesSourceRaw(RasterioRaw):\n    \"\"\"DataSource to handle individual TerrainTiles raster files\n\n    Parameters\n    ----------\n    source : str\n        Path to the sourcefile on S3\n\n    Attributes\n    ----------\n    dataset : :class:`rasterio.io.DatasetReader`\n        rasterio dataset\n    \"\"\"\n\n    anon = tl.Bool(True)\n\n    @tl.default(\"crs\")",
            "def _default_crs(self):\n        if \"geotiff\" in self.source:\n            return \"EPSG:3857\"\n        if \"terrarium\" in self.source:\n            return \"EPSG:3857\"\n        if \"normal\" in self.source:\n            return \"EPSG:3857\"",
            "def download(self, path=\"terraintiles\"):\n        \"\"\"\n        Download the TerrainTile file from S3 to a local file.\n        This is a convience method for users and not used by PODPAC machinery.\n\n        Parameters\n        ----------\n        path : str\n            Subdirectory to put files. Defaults to 'terraintiles'.\n            Within this directory, the tile files will retain the same directory structure as on S3.\n        \"\"\"\n\n        filename = os.path.split(self.source)[1]  # get filename off of source\n        joined_path = os.path.join(path, os.path.split(self.source)[0].replace(\"s3:",
            "def get_coordinates(self):\n        coordinates = super(TerrainTilesSourceRaw, self).get_coordinates()\n\n        for dim in coordinates:\n            coordinates[dim] = np.round(coordinates[dim].coordinates, 6)\n\n        return coordinates",
            "class TerrainTilesComposite(TileCompositorRaw):\n    \"\"\"Terrain Tiles gridded elevation tiles data library\n\n    Hosted on AWS S3\n    https:",
            "def _zoom(self, coordinates):\n        if self.zoom >= 0:\n            return self.zoom\n        crds = coordinates.transform(\"EPSG:3857\")\n        if coordinates.is_stacked(\"lat\") or coordinates.is_stacked(\"lon\"):\n            return len(ZOOM_SIZES) - 1\n        steps = []\n        for crd in crds.values():\n            if crd.name not in [\"lat\", \"lon\"]:\n                continue\n            if crd.size == 1:\n                continue\n            if isinstance(crd, podpac.coordinates.UniformCoordinates1d):\n                steps.append(np.abs(crd.step))\n            elif isinstance(crd, podpac.coordinates.ArrayCoordinates1d):\n                steps.append(np.abs(np.diff(crd.coordinates)).min())\n            else:\n                continue\n        if not steps:\n            return len(ZOOM_SIZES) - 1\n\n        step = min(steps) / 2\n        zoom = 0\n        for z, zs in enumerate(ZOOM_SIZES):\n            zoom = z\n            if zs < step:\n                break\n        return zoom\n\n    def select_sources(self, coordinates, _selector=None):\n        # get all the tile sources for the requested zoom level and coordinates\n        sources = get_tile_urls(self.tile_format, self._zoom(coordinates), coordinates)\n        urls = [\"s3:",
            "def select_sources(self, coordinates, _selector=None):\n        # get all the tile sources for the requested zoom level and coordinates\n        sources = get_tile_urls(self.tile_format, self._zoom(coordinates), coordinates)\n        urls = [\"s3:",
            "def find_coordinates(self):\n        return [podpac.coordinates.union([source.coordinates for source in self.sources])]",
            "def download(self, path=\"terraintiles\"):\n        \"\"\"\n        Download active terrain tile source files to local directory\n\n        Parameters\n        ----------\n        path : str\n            Subdirectory to put files. Defaults to 'terraintiles'.\n            Within this directory, the tile files will retain the same directory structure as on S3.\n        \"\"\"\n\n        try:\n            for source in self.sources[0].sources:\n                source.download(path)\n        except tl.TraitError as e:\n            raise ValueError(\"No terrain tile sources selected. Evaluate node at coordinates to select sources.\") from e",
            "def _create_composite(self, urls):\n        # Share the s3 connection\n        sample_source = TerrainTilesSourceRaw(\n            source=urls[0],\n            cache_ctrl=self.cache_ctrl,\n            force_eval=self.force_eval,\n            cache_output=self.cache_output,\n            cache_dataset=True,\n        )\n        return [\n            TerrainTilesSourceRaw(\n                source=url,\n                s3=sample_source.s3,\n                cache_ctrl=self.cache_ctrl,\n                force_eval=self.force_eval,\n                cache_output=self.cache_output,\n                cache_dataset=True,\n            )\n            for url in urls\n        ]",
            "class TerrainTiles(InterpolationMixin, TerrainTilesComposite):\n    \"\"\"Terrain Tiles gridded elevation tiles data library\n\n    Hosted on AWS S3\n    https:",
            "def get_tile_urls(tile_format, zoom, coordinates=None):\n    \"\"\"Get tile urls for a specific zoom level and geospatial coordinates\n\n    Parameters\n    ----------\n    tile_format : str\n        format of the tile to get\n    zoom : int\n        zoom level\n    coordinates : :class:`podpac.Coordinates`, optional\n        only return tiles within coordinates\n\n    Returns\n    -------\n    list of str\n        list of tile urls\n    \"\"\"\n\n    # get all the tile definitions for the requested zoom level\n    tiles = _get_tile_tuples(zoom, coordinates)\n\n    # get source urls\n    return [_tile_url(tile_format, x, y, z) for (x, y, z) in tiles]\n\n\n############\n# Private Utilites\n############",
            "def _get_tile_tuples(zoom, coordinates=None):\n    \"\"\"Query for tiles within podpac coordinates\n\n    This method allows you to get the available tiles in a given spatial area.\n    This will work for all :attr:`TILE_FORMAT` types\n\n    Parameters\n    ----------\n    coordinates : :class:`podpac.coordinates.Coordinates`\n        Find available tiles within coordinates\n    zoom : int, optional\n        zoom level\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    Returns\n    -------\n    list of tuple\n        list of tile tuples (x, y, zoom) for zoom level and coordinates\n    \"\"\"\n\n    # if no coordinates are supplied, get all tiles for zoom level\n    if coordinates is None:\n        # get whole world\n        tiles = _get_tiles_grid([-90, 90], [-180, 180], zoom)\n\n    # down select tiles based on coordinates\n    else:\n        _logger.debug(\"Getting tiles for coordinates {}\".format(coordinates))\n\n        if \"lat\" not in coordinates.udims or \"lon\" not in coordinates.udims:\n            raise TypeError(\"input coordinates must have lat and lon dimensions to get tiles\")\n\n        # transform to WGS84 (epsg:4326) to use the mapzen example for transforming coordinates to tilespace\n        # it doesn't seem to conform to standard google tile indexing\n        c = coordinates.transform(\"epsg:4326\")\n\n        # point coordinates\n        if \"lat_lon\" in c.dims or \"lon_lat\" in c.dims:\n            lat_lon = zip(c[\"lat\"].coordinates, c[\"lon\"].coordinates)\n\n            tiles = []\n            for (lat, lon) in lat_lon:\n                tile = _get_tiles_point(lat, lon, zoom)\n                if tile not in tiles:\n                    tiles.append(tile)\n\n        # gridded coordinates\n        else:\n            lat_bounds = c[\"lat\"].bounds\n            lon_bounds = c[\"lon\"].bounds\n\n            tiles = _get_tiles_grid(lat_bounds, lon_bounds, zoom)\n\n    return tiles",
            "def _tile_url(tile_format, x, y, zoom):\n    \"\"\"Build S3 URL prefix\n\n    The S3 bucket is organized {tile_format}/{z}/{x}/{y}.tif\n\n    Parameters\n    ----------\n    tile_format : str\n        One of 'terrarium', 'normal', 'geotiff'\n    zoom : int\n        zoom level\n    x : int\n        x tilespace coordinate\n    y : int\n        x tilespace coordinate\n\n    Returns\n    -------\n    str\n        Bucket prefix\n\n    Raises\n    ------\n    TypeError\n    \"\"\"\n\n    tile_url = \"{tile_format}/{zoom}/{x}/{y}.{ext}\"\n    ext = {\"geotiff\": \"tif\", \"normal\": \"png\", \"terrarium\": \"png\"}\n\n    return tile_url.format(tile_format=tile_format, zoom=zoom, x=x, y=y, ext=ext[tile_format])\n\n\ndef _get_tiles_grid(lat_bounds, lon_bounds, zoom):\n    \"\"\"\n    Convert geographic bounds into a list of tile coordinates at given zoom.\n    Adapted from https:",
            "def _get_tiles_grid(lat_bounds, lon_bounds, zoom):\n    \"\"\"\n    Convert geographic bounds into a list of tile coordinates at given zoom.\n    Adapted from https:",
            "def _get_tiles_point(lat, lon, zoom):\n    \"\"\"Get tiles at a single point and zoom level\n\n    Parameters\n    ----------\n    lat : float\n        latitude\n    lon : float\n        longitude\n    zoom : int\n        zoom level\n\n    Returns\n    -------\n    tuple\n        (x, y, zoom) tile url\n    \"\"\"\n    xm, ym = _mercator(lat, lon)\n    x, y = _mercator_to_tilespace(xm, ym, zoom)\n\n    return x, y, zoom\n\n\ndef _mercator(lat, lon):\n    \"\"\"Convert latitude, longitude to x, y mercator coordinate at given zoom\n    Adapted from https:",
            "def _mercator(lat, lon):\n    \"\"\"Convert latitude, longitude to x, y mercator coordinate at given zoom\n    Adapted from https:",
            "def _mercator_to_tilespace(xm, ym, zoom):\n    \"\"\"Convert mercator to tilespace coordinates\n\n    Parameters\n    ----------\n    x : float\n        mercator x coordinate\n    y : float\n        mercator y coordinate\n    zoom : int\n        zoom level\n\n    Returns\n    -------\n    tuple\n        (x, y) int tile coordinates\n    \"\"\"\n\n    tiles = 2**zoom\n    diameter = 2 * np.pi\n    x = int(tiles * (xm + np.pi) / diameter)\n    y = int(tiles * (np.pi - ym) / diameter)\n\n    return x, y\n\n\nif __name__ == \"__main__\":\n    from podpac import Coordinates, clinspace\n\n    c = Coordinates([clinspace(40, 43, 1000), clinspace(-76, -72, 1000)], dims=[\"lat\", \"lon\"])\n    c2 = Coordinates(\n        [clinspace(40, 43, 1000), clinspace(-76, -72, 1000), [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"]\n    )\n\n    print(\"TerrainTiles\")\n    node = TerrainTiles(tile_format=\"geotiff\", zoom=8)\n    output = node.eval(c)\n    print(output)\n\n    output = node.eval(c2)\n    print(output)\n\n    print(\"TerrainTiles cached\")\n    node = TerrainTiles(tile_format=\"geotiff\", zoom=8, cache_ctrl=[\"ram\", \"disk\"])\n    output = node.eval(c)\n    print(output)\n\n    # tile urls\n    print(\"get tile urls\")\n    print(np.array(get_tile_urls(\"geotiff\", 1)))\n    print(np.array(get_tile_urls(\"geotiff\", 9, coordinates=c)))\n\n    print(\"done\")"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/soilscape.py",
        "comments": [
            "//thredds.daac.ornl.gov/thredds/fileServer/ornldaac/1339\""
        ],
        "docstrings": [
            "\"\"\"\n    Get SoilSCAPE node location by id.\n\n    Arguments\n    ---------\n    node : int\n        node id\n\n    Returns\n    -------\n    location : tuple\n        (lat, lon) coordinates\n    \"\"\"",
            "\"\"\"\n    Get location coordinates for the given SoilSCAPE site.\n\n    Arguments\n    ---------\n    site : str\n        SoilSCAPE site, e.g. 'Canton_OK'\n    time : array, datetime64, str\n        datetime(s). Default is the current time.\n    depth : float, array\n        depth(s). Default: [4, 13, 30] (all available depths)\n\n    Returns\n    -------\n    coords : Coordinates\n        Coordinates with (lat_lon) for all nodes at the site and the given time and depth\n    \"\"\"",
            "\"\"\"SoilSCAPE 20min soil moisture for a particular node.\n\n    Data is loaded from the THREDDS https fileserver.\n\n    Attributes\n    ----------\n    site : str\n        SoilSCAPE site, e.g. 'Canton_OK'.\n    node : int\n        SoilSCAPE node id.\n    rescale : float\n        Default is 0.01. The soilscape soil moisture is multiplied by this number.\n        Soilscape soil moisture by default is absolute volumetric percentage, so we can rescale that to absolute volumetric fraction.\n    \"\"\"",
            "\"\"\"dataset coordinate dims\"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"Raw SoilSCAPE 20min soil moisture data for an entire site.\n\n    Data is loaded from the THREDDS https fileserver.\n\n    Attributes\n    ----------\n    site : str\n        SoilSCAPE site, e.g. 'Canton_OK'.\n    exclude : list\n        data points with these quality flags will be excluded. Default excludes [1, 2, 3, 4].\n        Flags::\n         * 0 - (G) Good (Standard for all data)\n         * 1 - (D) Dubious (Automatically flagged, spikes etc.,)\n         * 2 - (I) Interpolated / Estimated\n         * 3 - (B) Bad (Manually flagged)\n         * 4 - (M) Missing\n         * 5 - (C) Exceeds field size (Negative SM values, fixed at 0.1 percent)\n    \"\"\"",
            "\"\"\"\n        Make coordinates with the site locations and the given time and depth.\n\n        Arguments\n        ---------\n        time : array, datetime64, str\n            datetime(s). Default is the current time.\n        depth : float, array\n            depth(s). Default: [4, 13, 30] (all available depths)\n\n        Returns\n        -------\n        coords : Coordinates\n            Coordinates with (lat_lon) for all nodes at the site and the given time and depth\n        \"\"\"",
            "\"\"\"SoilSCAPE 20min soil moisture data for an entire site, with interpolation.\"\"\""
        ],
        "code_snippets": [
            "def get_node_location(node):\n    \"\"\"\n    Get SoilSCAPE node location by id.\n\n    Arguments\n    ---------\n    node : int\n        node id\n\n    Returns\n    -------\n    location : tuple\n        (lat, lon) coordinates\n    \"\"\"\n\n    if node not in NODE_LOCATIONS:\n        _logger.info(\"Looking up location for '%s' node %d\" % (NODE2SITE[node], node))\n        source = SoilSCAPENode(site=NODE2SITE[node], node=node)\n        NODE_LOCATIONS[node] = (source.lat, source.lon)\n    return NODE_LOCATIONS[node]",
            "def get_site_coordinates(site, time=None, depth=None):\n    \"\"\"\n    Get location coordinates for the given SoilSCAPE site.\n\n    Arguments\n    ---------\n    site : str\n        SoilSCAPE site, e.g. 'Canton_OK'\n    time : array, datetime64, str\n        datetime(s). Default is the current time.\n    depth : float, array\n        depth(s). Default: [4, 13, 30] (all available depths)\n\n    Returns\n    -------\n    coords : Coordinates\n        Coordinates with (lat_lon) for all nodes at the site and the given time and depth\n    \"\"\"\n\n    if site not in NODES:\n        raise ValueError(\"site '%s' not found\" % site)\n\n    if time is None:\n        time = np.datetime64(datetime.datetime.now())  # now\n\n    if depth is None:\n        depth = [4, 13, 30]  # all\n\n    lats = []\n    lons = []\n    for node in NODES[site]:\n        try:\n            lat, lon = get_node_location(node)\n        except:\n            _logger.exception(\"Could not get coordinates for '%s' node '%s'\" % (NODE2SITE[node], node))\n            continue\n        lats.append(lat)\n        lons.append(lon)\n\n    return podpac.Coordinates([[lats, lons], time, depth], dims=[\"lat_lon\", \"time\", \"alt\"], crs=CRS)",
            "class SoilSCAPENode(podpac.core.data.dataset_source.DatasetRaw):\n    \"\"\"SoilSCAPE 20min soil moisture for a particular node.\n\n    Data is loaded from the THREDDS https fileserver.\n\n    Attributes\n    ----------\n    site : str\n        SoilSCAPE site, e.g. 'Canton_OK'.\n    node : int\n        SoilSCAPE node id.\n    rescale : float\n        Default is 0.01. The soilscape soil moisture is multiplied by this number.\n        Soilscape soil moisture by default is absolute volumetric percentage, so we can rescale that to absolute volumetric fraction.\n    \"\"\"\n\n    alt_key = \"depth\"\n    site = tl.Enum(list(NODES)).tag(attr=True)\n    node = tl.Int().tag(attr=True)\n    cache_dataset = tl.Bool(True)\n    rescale = tl.Float(0.01)\n    coordinate_index_type = \"numpy\"\n\n    _repr_keys = [\"site\", \"node\"]\n\n    @cached_property",
            "def dims(self):",
            "def get_data(self, coordinates, coordinates_index):",
            "def lat(self):\n        return self.dataset.lat.item()\n\n    @property",
            "def lon(self):\n        return self.dataset.lon.item()\n\n    @property",
            "def physicalid(self):\n        # note: this should be the same as the node number\n        return self.dataset.physicalid.item()",
            "def get_coordinates(self):\n        coordinates = super(SoilSCAPENode, self).get_coordinates()\n        coordinates.set_trait(\"crs\", CRS)\n        return coordinates\n\n    @tl.validate(\"node\")",
            "def _validate_node(self, d):\n        if d[\"value\"] not in NODES[self.site]:\n            raise ValueError(\"Site '%s' does not have a node n%d\" % (self.site, d[\"value\"]))\n\n        return d[\"value\"]\n\n    @property",
            "def source(self):\n        return \"{base_url}/{filename}.nc\".format(base_url=SOILSCAPE_FILESERVER_BASE, filename=self.filename)\n\n    @property",
            "def filename(self):\n        return \"soil_moist_20min_{site}_n{node}\".format(site=self.site, node=self.node)",
            "class SoilSCAPE20minRaw(podpac.compositor.TileCompositorRaw):\n    \"\"\"Raw SoilSCAPE 20min soil moisture data for an entire site.\n\n    Data is loaded from the THREDDS https fileserver.\n\n    Attributes\n    ----------\n    site : str\n        SoilSCAPE site, e.g. 'Canton_OK'.\n    exclude : list\n        data points with these quality flags will be excluded. Default excludes [1, 2, 3, 4].\n        Flags::\n         * 0 - (G) Good (Standard for all data)\n         * 1 - (D) Dubious (Automatically flagged, spikes etc.,)\n         * 2 - (I) Interpolated / Estimated\n         * 3 - (B) Bad (Manually flagged)\n         * 4 - (M) Missing\n         * 5 - (C) Exceeds field size (Negative SM values, fixed at 0.1 percent)\n    \"\"\"\n\n    site = tl.Enum(list(NODES), allow_none=True, default_value=None).tag(attr=True)\n    exclude = tl.List([1, 2, 3, 4]).tag(attr=True)\n    dataset_expires = tl.Any()\n    data_key = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n\n    @tl.validate(\"dataset_expires\")",
            "def _validate_dataset_expires(self, d):\n        podpac.core.cache.utils.expiration_timestamp(d[\"value\"])\n        return d[\"value\"]\n\n    @property",
            "def _repr_keys(self):\n        keys = []\n        if self.site is not None:\n            keys.append(\"site\")\n        return keys\n\n    @podpac.cached_property",
            "def nodes(self):\n        if self.site is not None:\n            return [(self.site, node) for node in NODES[self.site]]\n        else:\n            return [(site, node) for site in NODES for node in NODES[site]]\n\n    @podpac.cached_property",
            "def source_coordinates(self):\n        latlons = np.array([NODE_LOCATIONS[node[1]] for node in self.nodes])\n        return podpac.Coordinates([latlons.T.tolist()], [\"lat_lon\"])\n\n    @podpac.cached_property",
            "def sources(self):\n        return [self._make_source(site, node) for site, node in self.nodes]",
            "def _make_source(self, site, node):\n        return SoilSCAPENode(\n            site=site,\n            node=node,\n            cache_ctrl=self.cache_ctrl,\n            dataset_expires=self.dataset_expires,\n            data_key=self.data_key,\n        )",
            "def make_coordinates(self, time=None, depth=None):\n        \"\"\"\n        Make coordinates with the site locations and the given time and depth.\n\n        Arguments\n        ---------\n        time : array, datetime64, str\n            datetime(s). Default is the current time.\n        depth : float, array\n            depth(s). Default: [4, 13, 30] (all available depths)\n\n        Returns\n        -------\n        coords : Coordinates\n            Coordinates with (lat_lon) for all nodes at the site and the given time and depth\n        \"\"\"\n\n        return get_site_coordinates(self.site, time=time, depth=depth)\n\n    @property",
            "def available_sites(self):\n        return list(NODES.keys())",
            "class SoilSCAPE20min(InterpolationMixin, SoilSCAPE20minRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/weathercitizen.py",
        "comments": [
            "//weathercitizen.org)",
            "//weathercitizen.org/docs",
            "//api.weathercitizen.org",
            "//api.weathercitizen.org/\"",
            "//api.weathercitizen.org/static/sensorburst_pb2.py",
            "//api.weathercitizen.org/static/sensorburst_pb2.py.\""
        ],
        "docstrings": [
            "\"\"\"\nWeather Citizen\n\nCrowd sourced environmental observations from mobile devices (https://weathercitizen.org)\n\n- Documentation: https://weathercitizen.org/docs\n- API: https://api.weathercitizen.org\n\nRequires\n\n- requests: `pip install requests`\n- pandas: `pip install pandas`\n\nOptionally:\n\n- read_protobuf: `pip install read-protobuf` - decodes sensor burst media files\n\"\"\"",
            "\"\"\"DataSource to handle WeatherCitizen data\n\n    Attributes\n    ----------\n    source : str\n        Collection (database) to pull data from.\n        Defaults to \"geosensors\" which is the primary data collection\n    data_key : str, int\n        Data key of interest, default \"properties.pressure\"\n    uuid : str, list(str), options\n        String or list of strings to filter data by uuid\n    device : str, list(str), ObjectId, list(ObjectId), optional\n        String or list of strings to filter data by device object id\n    version : string, list(str), optional\n        String or list of strings to filter data to filter data by WeatherCitizen version\n    query : dict, optional\n        Arbitrary pymongo query to apply to data.\n        Note that certain fields in this query may be overriden if other keyword arguments are specified\n    verbose : bool, optional\n        Display log messages or progress\n    \"\"\"",
            "\"\"\"{get_coordinates}\"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"Get documents from the server for devices in a timerange\n\n    Parameters\n    ----------\n    collection : str, list(str)\n        Collection(s) to query\n    start_time : str, datetime, optional\n        String or datetime for start of timerange (>=).\n        Defaults to 1 hour ago.\n        This input must be compatible with pandas `pd.to_datetime(start_time, utc=True)`\n        Input assumes UTC by default, but will recognize timezone string EDT, UTC, etc. For example \"2019-09-01 08:00 EDT\"\n    end_time : str, datetime, optional\n        Same as `start_time` but specifies end of time range (<).\n        Defaults to now.\n    box : list(list(float)), optional\n        Geo bounding box described as 2-d array of bottom-left and top-right corners.\n        If specified, `near` will be ignored.\n        Contents: [[ <lon>, <lat> (bottom left coordinates) ], [  <lon>, <lat> (upper right coordinates) ]]\n        For example: [[-83, 36], [-81, 34]]\n    near : tuple([float, float], int), optional\n        Geo bounding box described as 2-d near with a center point and a radius (km) from center point.\n        This input will be ignored if box is defined.\n        Contents: ([<lon>, <lat>], <radius in km>)\n        For example: ([-72.544655, 40.932559], 16000)\n    uuid : str, list(str), options\n        String or list of strings to filter data by uuid\n    device : str, list(str), ObjectId, list(ObjectId), optional\n        String or list of strings to filter data by device object id\n    version : string, list(str), optional\n        String or list of strings to filter data to filter data by WeatherCitizen version\n    query : dict, optional\n        Arbitrary pymongo query to apply to data.\n        Note that certain fields in this query may be overriden if other keyword arguments are specified\n    projection: dict, optional\n        Specify what fields should or should not be returned.\n        Dict keys are field names.\n        Dict values should be set to 1 to include field (and exclude all others) or set to 0 to exclude field and include all others\n    verbose : bool, optional\n        Display log messages or progress\n    dry_run : bool, optional\n        Return urls of queries instead of the actual query.\n        Returns a list of str with urls for each collections.\n        Defaults to False.\n    return_length : bool, optional\n        Return length of the documents that match the query\n\n    Returns\n    -------\n    list\n        List of items from server matching query.\n        If `dry_run` is True, returns a list or url strings for query.\n    \"\"\"",
            "\"\"\"Get a single record from a collection by obj_id\n\n    Parameters\n    ----------\n    collection : str\n        Collection name\n    obj_id : str\n        Object id\n    \"\"\"",
            "\"\"\"Get media file\n\n    Parameters\n    ----------\n    media : str, dict\n        Media record or media record object id in the media or geomedia collections.\n    save : bool, optional\n        Save to file\n    output_path : None, optional\n        If save is True, output the file to different file path\n\n    Returns\n    -------\n    bytes\n        If output_path is None, returns raw file content as bytes\n\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\"",
            "\"\"\"Download and read sensorburst records.\n\n    Requires:\n    - read-protobuf: `pip install read-protobuf`\n    - sensorburst_pb2: Download from https://api.weathercitizen.org/static/sensorburst_pb2.py\n        - Once downloaded, put this file in the directory as your analysis\n\n    Parameters\n    ----------\n    media : str, dict, list of str, list of dict\n        Media record(s) or media record object id(s) in the media or geomedia collections.\n\n    Returns\n    -------\n    pd.DataFrame\n        Returns pandas dataframe of records\n    \"\"\"",
            "\"\"\"Create normalized dataframe from records\n\n    Parameters\n    ----------\n    items : list of dict\n        Record items returned from `get()`\n    \"\"\"",
            "\"\"\"Convert items to CSV output\n\n    Parameters\n    ----------\n    items : list of dict\n        Record items returned from `get()`\n    \"\"\"",
            "\"\"\"\n    Parameters\n    ----------\n    current : int, float\n        current number\n    total : int, floar\n        total number\n    \"\"\"",
            "\"\"\"Build a query string for a single collection.\n    See :func:`get` for type definitions of each input\n\n    Returns\n    -------\n    string\n        query string\n    \"\"\"",
            "\"\"\"Internal method to query API.\n    See `get` for interface.\n\n    Parameters\n    ----------\n    query : dict, str\n        query dict or string\n        if dict, it will be converted into a string with json.dumps()\n    items : list, optional\n        aggregated items as this method is recursively called. Defaults to [].\n    url : str, optional\n        API url. Defaults to module URL.\n    verbose : bool, optional\n        Display log messages or progress\n    return_length : bool, optional\n        Return length of the documents that match the query\n\n    Returns\n    -------\n    list\n\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\""
        ],
        "code_snippets": [
            "class WeatherCitizen(InterpolationMixin, DataSource):\n    \"\"\"DataSource to handle WeatherCitizen data\n\n    Attributes\n    ----------\n    source : str\n        Collection (database) to pull data from.\n        Defaults to \"geosensors\" which is the primary data collection\n    data_key : str, int\n        Data key of interest, default \"properties.pressure\"\n    uuid : str, list(str), options\n        String or list of strings to filter data by uuid\n    device : str, list(str), ObjectId, list(ObjectId), optional\n        String or list of strings to filter data by device object id\n    version : string, list(str), optional\n        String or list of strings to filter data to filter data by WeatherCitizen version\n    query : dict, optional\n        Arbitrary pymongo query to apply to data.\n        Note that certain fields in this query may be overriden if other keyword arguments are specified\n    verbose : bool, optional\n        Display log messages or progress\n    \"\"\"\n\n    source = tl.Unicode(allow_none=True, default_value=\"geosensors\").tag(attr=True, required=True)\n    data_key = tl.Unicode(allow_none=True, default_value=\"properties.pressure\").tag(attr=True)\n    uuid = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    device = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    version = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    query = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    verbose = tl.Bool(allow_none=True, default_value=True).tag(attr=True)\n    override_limit = tl.Bool(allow_none=True, default_value=False).tag(attr=True)\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_coordinates(self):",
            "def get_data(self, coordinates, coordinates_index):",
            "def get(\n    collection=\"geosensors\",\n    start_time=None,\n    end_time=None,\n    box=None,\n    near=None,\n    uuid=None,\n    device=None,\n    version=None,\n    query=None,\n    projection=None,\n    verbose=False,\n    dry_run=False,\n    return_length=False,\n):\n    \"\"\"Get documents from the server for devices in a timerange\n\n    Parameters\n    ----------\n    collection : str, list(str)\n        Collection(s) to query\n    start_time : str, datetime, optional\n        String or datetime for start of timerange (>=).\n        Defaults to 1 hour ago.\n        This input must be compatible with pandas `pd.to_datetime(start_time, utc=True)`\n        Input assumes UTC by default, but will recognize timezone string EDT, UTC, etc. For example \"2019-09-01 08:00 EDT\"\n    end_time : str, datetime, optional\n        Same as `start_time` but specifies end of time range (<).\n        Defaults to now.\n    box : list(list(float)), optional\n        Geo bounding box described as 2-d array of bottom-left and top-right corners.\n        If specified, `near` will be ignored.\n        Contents: [[ <lon>, <lat> (bottom left coordinates) ], [  <lon>, <lat> (upper right coordinates) ]]\n        For example: [[-83, 36], [-81, 34]]\n    near : tuple([float, float], int), optional\n        Geo bounding box described as 2-d near with a center point and a radius (km) from center point.\n        This input will be ignored if box is defined.\n        Contents: ([<lon>, <lat>], <radius in km>)\n        For example: ([-72.544655, 40.932559], 16000)\n    uuid : str, list(str), options\n        String or list of strings to filter data by uuid\n    device : str, list(str), ObjectId, list(ObjectId), optional\n        String or list of strings to filter data by device object id\n    version : string, list(str), optional\n        String or list of strings to filter data to filter data by WeatherCitizen version\n    query : dict, optional\n        Arbitrary pymongo query to apply to data.\n        Note that certain fields in this query may be overriden if other keyword arguments are specified\n    projection: dict, optional\n        Specify what fields should or should not be returned.\n        Dict keys are field names.\n        Dict values should be set to 1 to include field (and exclude all others) or set to 0 to exclude field and include all others\n    verbose : bool, optional\n        Display log messages or progress\n    dry_run : bool, optional\n        Return urls of queries instead of the actual query.\n        Returns a list of str with urls for each collections.\n        Defaults to False.\n    return_length : bool, optional\n        Return length of the documents that match the query\n\n    Returns\n    -------\n    list\n        List of items from server matching query.\n        If `dry_run` is True, returns a list or url strings for query.\n    \"\"\"\n\n    # always make collection a list\n    if isinstance(collection, str):\n        collection = [collection]\n\n    # get query string for each collection in list\n    query_strs = [\n        _build_query(\n            collection=coll,\n            start_time=start_time,\n            end_time=end_time,\n            box=box,\n            near=near,\n            uuid=uuid,\n            device=device,\n            version=version,\n            query=query,\n            projection=projection,\n        )\n        for coll in collection\n    ]\n\n    # dry run\n    if dry_run:\n        return query_strs\n\n    if verbose:\n        print(\"Querying WeatherCitizen API\")\n\n    # only return the length of the matched documents\n    if return_length:\n        length = 0\n        for query_str in query_strs:\n            length += _get(query_str, verbose=verbose, return_length=return_length)\n\n        if verbose:\n            print(\"Returned {} records\".format(length))\n\n        return length\n\n    # start query at page 0 with no items\n    # iterate through collections aggregating items\n    items = []\n    for query_str in query_strs:\n        items += _get(query_str, verbose=verbose)\n\n    if verbose:\n        print(\"\\r\")\n        print(\"Downloaded {} records\".format(len(items)))\n\n    return items",
            "def get_record(collection, obj_id, url=URL):\n    \"\"\"Get a single record from a collection by obj_id\n\n    Parameters\n    ----------\n    collection : str\n        Collection name\n    obj_id : str\n        Object id\n    \"\"\"\n\n    # check url\n    if url[-1] != \"/\":\n        url = \"{}/\".format(url)\n\n    # query the server\n    r = requests.get(url + collection + \"/\" + obj_id)\n\n    if r.status_code != 200:\n        raise ValueError(\"Failed to query the server with status {}.\\n\\nResponse:\\n {}\".format(r.status_code, r.text))\n\n    return r.json()",
            "def get_file(media, save=False, output_path=None):\n    \"\"\"Get media file\n\n    Parameters\n    ----------\n    media : str, dict\n        Media record or media record object id in the media or geomedia collections.\n    save : bool, optional\n        Save to file\n    output_path : None, optional\n        If save is True, output the file to different file path\n\n    Returns\n    -------\n    bytes\n        If output_path is None, returns raw file content as bytes\n\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\"\n\n    if isinstance(media, str):\n        media_id = media\n    elif isinstance(media, dict):\n        media_id = media[\"_id\"]\n\n    try:\n        record = get_record(\"media\", media_id)\n    except ValueError:\n        try:\n            record = get_record(\"geomedia\", media_id)\n\n        except ValueError:\n            raise ValueError(\"Media id {} not found in the database\".format(media_id))\n\n    # get file\n    r = requests.get(record[\"file\"][\"url\"])\n\n    if r.status_code != 200:\n        raise ValueError(\n            \"Failed to download binary data with status code {}.\\n\\nResponse:\\n {}\".format(r.status_code, r.text)\n        )\n\n    # save to file if output_path is not None\n    if save:\n        if output_path is None:\n            output_path = record[\"properties\"][\"filename\"]\n        with open(output_path, \"wb\") as f:\n            f.write(r.content)\n    else:\n        return r.content",
            "def read_sensorburst(media):\n    \"\"\"Download and read sensorburst records.\n\n    Requires:\n    - read-protobuf: `pip install read-protobuf`\n    - sensorburst_pb2: Download from https:",
            "def to_dataframe(items):\n    \"\"\"Create normalized dataframe from records\n\n    Parameters\n    ----------\n    items : list of dict\n        Record items returned from `get()`\n    \"\"\"\n    df = pd.json_normalize(items)\n\n    # Convert geometry.coordinates to lat and lon\n    df[\"lat\"] = df[\"geometry.coordinates\"].apply(lambda coord: coord[1] if coord and coord is not np.nan else None)\n    df[\"lon\"] = df[\"geometry.coordinates\"].apply(lambda coord: coord[0] if coord and coord is not np.nan else None)\n    df = df.drop([\"geometry.coordinates\"], axis=1)\n\n    # break up all the arrays so the data is easier to use\n    arrays = [\n        \"properties.accelerometer\",\n        \"properties.gravity\",\n        \"properties.gyroscope\",\n        \"properties.linear_acceleration\",\n        \"properties.magnetic_field\",\n        \"properties.orientation\",\n        \"properties.rotation_vector\",\n    ]\n\n    for col in arrays:\n        df[col + \"_0\"] = df[col].apply(lambda val: val[0] if val and val is not np.nan else None)\n        df[col + \"_1\"] = df[col].apply(lambda val: val[1] if val and val is not np.nan else None)\n        df[col + \"_2\"] = df[col].apply(lambda val: val[2] if val and val is not np.nan else None)\n\n        df = df.drop([col], axis=1)\n\n    return df",
            "def to_csv(items, filename=\"weathercitizen-data.csv\"):\n    \"\"\"Convert items to CSV output\n\n    Parameters\n    ----------\n    items : list of dict\n        Record items returned from `get()`\n    \"\"\"\n\n    df = to_dataframe(items)\n\n    df.to_csv(filename)",
            "def update_progress(current, total):\n    \"\"\"\n    Parameters\n    ----------\n    current : int, float\n        current number\n    total : int, floar\n        total number\n    \"\"\"\n\n    if total == 0:\n        return\n\n    progress = float(current / total)\n    bar_length = 20\n    block = int(round(bar_length * progress))\n    text = \"Progress: |{0}| [{1} / {2}]\".format(\"#\" * block + \" \" * (bar_length - block), current, total)\n\n    print(\"\\r\", text, end=\"\")",
            "def _build_query(\n    collection=\"geosensors\",\n    start_time=None,\n    end_time=None,\n    box=None,\n    near=None,\n    uuid=None,\n    device=None,\n    version=None,\n    query=None,\n    projection=None,\n):\n    \"\"\"Build a query string for a single collection.\n    See :func:`get` for type definitions of each input\n\n    Returns\n    -------\n    string\n        query string\n    \"\"\"\n\n    if query is None:\n        query = {}\n\n    # filter by time\n    # default to 1 hour ago\n    one_hour_ago = (datetime.utcnow() - timedelta(hours=1)).strftime(DATE_FORMAT)\n    if start_time is not None:\n        start_time = pd.to_datetime(start_time, utc=True, infer_datetime_format=True).strftime(DATE_FORMAT)\n        query[\"properties.time\"] = {\"$gte\": start_time}\n    else:\n        query[\"properties.time\"] = {\"$gte\": one_hour_ago}\n\n    # default to now\n    if end_time is not None:\n        end_time = pd.to_datetime(end_time, utc=True, infer_datetime_format=True).strftime(DATE_FORMAT)\n        query[\"properties.time\"][\"$lte\"] = end_time\n\n    # geo bounding box\n    if box is not None:\n        if len(box) != 2:\n            raise ValueError(\"box parameter must be a list of length 2\")\n\n        query[\"geometry\"] = {\"$geoWithin\": {\"$box\": box}}\n\n    # geo bounding circle\n    if near is not None:\n        if len(near) != 2 or not isinstance(near, tuple):\n            raise ValueError(\"near parameter must be a tuple of length 2\")\n\n        query[\"geometry\"] = {\"$near\": {\"$geometry\": {\"type\": \"Point\", \"coordinates\": near[0]}, \"$maxDistance\": near[1]}}\n\n    # specify uuid\n    if uuid is not None:\n        if isinstance(uuid, str):\n            query[\"properties.uuid\"] = uuid\n        elif isinstance(uuid, list):\n            query[\"properties.uuid\"] = {\"$in\": uuid}\n\n    # specify device\n    if device is not None:\n        if isinstance(device, str):\n            query[\"properties.device\"] = device\n        elif isinstance(device, list):\n            query[\"properties.device\"] = {\"$in\": device}\n\n    # specify version\n    if version is not None:\n        if isinstance(version, str):\n            query[\"version\"] = version\n        elif isinstance(version, list):\n            query[\"version\"] = {\"$in\": version}\n\n    # add collection to query string and handle projection\n    if projection is not None:\n        query_str = \"{}?where={}&projection={}\".format(collection, json.dumps(query), json.dumps(projection))\n    else:\n        query_str = \"{}?where={}\".format(collection, json.dumps(query))\n\n    return query_str",
            "def _get(query, items=None, url=URL, verbose=False, return_length=False):\n    \"\"\"Internal method to query API.\n    See `get` for interface.\n\n    Parameters\n    ----------\n    query : dict, str\n        query dict or string\n        if dict, it will be converted into a string with json.dumps()\n    items : list, optional\n        aggregated items as this method is recursively called. Defaults to [].\n    url : str, optional\n        API url. Defaults to module URL.\n    verbose : bool, optional\n        Display log messages or progress\n    return_length : bool, optional\n        Return length of the documents that match the query\n\n    Returns\n    -------\n    list\n\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\"\n\n    # if items are none, set to []\n    if items is None:\n        items = []\n\n    # check url\n    if url[-1] != \"/\":\n        url = \"{}/\".format(url)\n\n    # query the server\n    r = requests.get(url + query)\n\n    if r.status_code != 200:\n        raise ValueError(\"Failed to query the server with status {}.\\n\\nResponse:\\n {}\".format(r.status_code, r.text))\n\n    # get json out of response\n    resp = r.json()\n\n    # return length only if requested\n    if return_length:\n        return resp[\"_meta\"][\"total\"]\n\n    # return documents\n    if len(resp[\"_items\"]):\n\n        # show progress\n        if verbose:\n            current_page = resp[\"_meta\"][\"page\"]\n            total_pages = round(resp[\"_meta\"][\"total\"] / resp[\"_meta\"][\"max_results\"])\n            update_progress(current_page, total_pages)\n\n        # append items\n        items += resp[\"_items\"]\n\n        # get next set, if in links\n        if \"_links\" in resp and \"next\" in resp[\"_links\"]:\n            return _get(resp[\"_links\"][\"next\"][\"href\"], items=items)\n        else:\n            return items\n    else:\n        return items"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/modis_pds.py",
        "comments": [
            "//modis-land.gsfc.nasa.gov/MODLAND_grid.html",
            "//%s/%s/%s\" % (BUCKET, self.prefix, self.filename)",
            "//ladsweb.modaps.eosdis.nasa.gov/search/order/1",
            "//modis.gsfc.nasa.gov/about/specifications.php"
        ],
        "docstrings": [
            "\"\"\"\nMODIS on AWS OpenData\n\nMODIS Coordinates Grids: https://modis-land.gsfc.nasa.gov/MODLAND_grid.html\n\"\"\"",
            "\"\"\"use pre-fetched lat and lon bounds to get coordinates for a single tile\"\"\"",
            "\"\"\"\n    Individual MODIS data tile using AWS OpenData, with caching.\n\n    Attributes\n    ----------\n    product : str\n        MODIS product ('MCD43A4.006', 'MOD09GA.006', 'MYD09GA.006', 'MOD09GQ.006', or 'MYD09GQ.006')\n    horizontal : str\n        column in the MODIS Sinusoidal Tiling System, e.g. '21'\n    vertical : str\n        row in the MODIS Sinusoidal Tiling System, e.g. '07'\n    date : str\n        year and three-digit day of year, e.g. '2011260'\n    data_key : str\n        individual object (varies by product)\n    \"\"\"",
            "\"\"\"validation\"\"\"",
            "\"\"\"MODIS whole-world compositor.\n    For documentation about the data, start here: https://ladsweb.modaps.eosdis.nasa.gov/search/order/1\n    For information about the bands, see here: https://modis.gsfc.nasa.gov/about/specifications.php\n\n    Attributes\n    ----------\n    product : str\n        MODIS product ('MCD43A4.006', 'MOD09GA.006', 'MYD09GA.006', 'MOD09GQ.006', or 'MYD09GQ.006')\n    data_key : str\n        individual object (varies by product)\n    \"\"\"",
            "\"\"\"2d select sources filtering\"\"\""
        ],
        "code_snippets": [
            "def _parse_modis_date(date):\n    return datetime.datetime.strptime(date, \"%Y%j\").strftime(\"%Y-%m-%d\")",
            "def _available(s3, *l):\n    prefix = \"/\".join([BUCKET] + list(l))\n    return [obj.replace(prefix + \"/\", \"\") for obj in s3.ls(prefix) if \"_scenes.txt\" not in obj]",
            "def get_tile_coordinates(h, v):",
            "class MODISSource(RasterioRaw):\n    \"\"\"\n    Individual MODIS data tile using AWS OpenData, with caching.\n\n    Attributes\n    ----------\n    product : str\n        MODIS product ('MCD43A4.006', 'MOD09GA.006', 'MYD09GA.006', 'MOD09GQ.006', or 'MYD09GQ.006')\n    horizontal : str\n        column in the MODIS Sinusoidal Tiling System, e.g. '21'\n    vertical : str\n        row in the MODIS Sinusoidal Tiling System, e.g. '07'\n    date : str\n        year and three-digit day of year, e.g. '2011260'\n    data_key : str\n        individual object (varies by product)\n    \"\"\"\n\n    product = tl.Enum(values=PRODUCTS, help=\"MODIS product ID\").tag(attr=True)\n    horizontal = tl.Unicode(help=\"column in the MODIS Sinusoidal Tiling System, e.g. '21'\").tag(attr=True)\n    vertical = tl.Unicode(help=\"row in the MODIS Sinusoidal Tiling System, e.g. '07'\").tag(attr=True)\n    date = tl.Unicode(help=\"year and three-digit day of year, e.g. '2011460'\").tag(attr=True)\n    data_key = tl.Unicode(help=\"data to retrieve (varies by product)\").tag(attr=True)\n    anon = tl.Bool(True)\n    check_exists = tl.Bool(True)\n\n    _repr_keys = [\"prefix\", \"data_key\"]",
            "def init(self):",
            "def filename(self):\n        _logger.info(\n            \"Looking up source filename (product=%s, h=%s, v=%s, date=%s, data_key=%s)...\"\n            % (self.product, self.horizontal, self.vertical, self.date, self.data_key)\n        )\n        prefix = \"/\".join([BUCKET, self.product, self.horizontal, self.vertical, self.date])\n        objs = [obj.replace(prefix + \"/\", \"\") for obj in self.s3.ls(prefix) if obj.endswith(\"%s.TIF\" % self.data_key)]\n        if len(objs) == 0:\n            raise RuntimeError(\"No matches found for data_key='%s' at '%s'\" % (self.data_key, prefix))\n        if len(objs) > 1:\n            raise RuntimeError(\"Too many matches for data_key='%s' at '%s' (%s)\" % (self.data_key, prefix, objs))\n        return objs[0]\n\n    @property",
            "def prefix(self):\n        return \"%s/%s/%s/%s\" % (self.product, self.horizontal, self.vertical, self.date)\n\n    @cached_property\n    def source(self):\n        return \"s3:",
            "def source(self):\n        return \"s3:",
            "def exists(self):\n        return self.s3.exists(self.source)",
            "def get_coordinates(self):\n        # use pre-fetched coordinate bounds (instead of loading from the dataset)\n        spatial_coords = get_tile_coordinates(self.horizontal, self.vertical)\n        time_coords = podpac.Coordinates([_parse_modis_date(self.date)], [\"time\"], crs=spatial_coords.crs)\n        return podpac.coordinates.merge_dims([spatial_coords, time_coords])\n\n\nclass MODISComposite(S3Mixin, TileCompositorRaw):\n    \"\"\"MODIS whole-world compositor.\n    For documentation about the data, start here: https:",
            "class MODISComposite(S3Mixin, TileCompositorRaw):\n    \"\"\"MODIS whole-world compositor.\n    For documentation about the data, start here: https:",
            "def tile_coordinates(self):\n        return [get_tile_coordinates(*hv) for hv in self.available_tiles]\n\n    @cached_property(use_cache_ctrl=True)",
            "def available_tiles(self):\n        _logger.info(\"Looking up available tiles...\")\n        return [(h, v) for h in _available(self.s3, self.product) for v in _available(self.s3, self.product, h)]",
            "def select_sources(self, coordinates, _selector=None):",
            "class MODIS(InterpolationMixin, MODISComposite):\n    pass\n\n\nif __name__ == \"__main__\":\n    from matplotlib import pyplot\n\n    # -------------------------------------------------------------------------\n    # basic modis source\n    # -------------------------------------------------------------------------\n\n    source = MODISSource(\n        product=PRODUCTS[0],\n        data_key=\"B01\",\n        horizontal=\"01\",\n        vertical=\"11\",\n        date=\"2020009\",\n        cache_ctrl=[\"disk\"],\n        cache_dataset=True,\n        cache_output=False,\n    )\n\n    print(\"source: %s\" % repr(source))\n    print(\"path: %s\" % source.source)\n    print(\"coordinates: %s\", source.coordinates)\n\n    # native coordinates\n    o1 = source.eval(source.coordinates)\n\n    # cropped and resampled using EPSG:4326 coordinates\n    c = podpac.Coordinates([podpac.clinspace(-22, -20, 200), podpac.clinspace(-176, -174, 200)], dims=[\"lat\", \"lon\"])\n    o2 = source.eval(c)\n\n    # -------------------------------------------------------------------------\n    # modis tile with time\n    # -------------------------------------------------------------------------\n\n    tile = MODISTile(\n        product=PRODUCTS[0], data_key=\"B01\", horizontal=\"01\", vertical=\"11\", cache_ctrl=[\"disk\"], cache_output=False\n    )\n\n    print(\"tile: %s\" % repr(tile))\n    print(\n        \"available dates: %s-%s (n=%d)\" % (tile.available_dates[0], tile.available_dates[-1], len(tile.available_dates))\n    )\n    print(\"coordinates: %s\" % tile.coordinates)\n\n    # existing date\n    assert \"2020009\" in tile.available_dates\n    ct1 = podpac.Coordinates([\"2020-01-09\", c[\"lat\"], c[\"lon\"]], dims=[\"time\", \"lat\", \"lon\"])\n    o2 = tile.eval(ct1)\n\n    # nearest date\n    assert \"2020087\" not in tile.available_dates\n    ct2 = podpac.Coordinates([\"2020-03-27\", c[\"lat\"], c[\"lon\"]], dims=[\"time\", \"lat\", \"lon\"])\n    o3 = tile.eval(ct2)\n\n    # time-series\n    ct3 = podpac.Coordinates([[\"2019-01-01\", \"2019-02-01\", \"2019-03-01\"], -21.45, -174.92], dims=[\"time\", \"lat\", \"lon\"])\n    o4 = tile.eval(ct3)\n\n    # -------------------------------------------------------------------------\n    # modis compositor\n    # -------------------------------------------------------------------------\n\n    node = MODIS(product=PRODUCTS[0], data_key=\"B01\", cache_ctrl=[\"disk\"], cache_output=False)\n\n    print(\"node: %s\" % repr(node))\n    print(\"sources: n=%d\" % len(node.sources))\n    print(\"   .e.g: %s\" % repr(node.sources[0]))\n\n    # single tile\n    assert len(node.select_sources(ct2)) == 1\n    o5 = node.eval(ct2)\n\n    # time-series in a single tile\n    assert len(node.select_sources(ct3)) == 1\n    o6 = node.eval(ct3)\n\n    # multiple tiles\n    ct3 = podpac.Coordinates(\n        [\"2020-01-09\", podpac.clinspace(45, 55, 200), podpac.clinspace(-80, -40, 200)], dims=[\"time\", \"lat\", \"lon\"]\n    )\n    assert len(node.select_sources(ct3)) == 7\n    o7 = node.eval(ct3)\n\n    # o7.plot()\n    # pyplot.show()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/intake_catalog.py",
        "comments": [
            "//intake.readthedocs.io/en/latest/index.html)",
            "//intake.readthedocs.io/en/latest/catalog.html#local-catalogs",
            "//intake.readthedocs.io/en/latest/api_base.html#intake.catalog.Catalog",
            "//intake.readthedocs.io/en/latest/api_base.html#intake.catalog.entry.CatalogEntry"
        ],
        "docstrings": [
            "\"\"\"\n    Support for Intake Catalogs (https://intake.readthedocs.io/en/latest/index.html)\n    This primarily supports CSV data sources while we expand for Intake Catalogs.\n\n    Parameters\n    ----------\n    uri : str, required\n        Intake Catalog uri (local path to catalog yml file, or remote uri)\n        See https://intake.readthedocs.io/en/latest/catalog.html#local-catalogs\n    source : str, required\n        Intake Catalog source\n    field : str, optional,\n        If source is a dataframe with multiple fields, this specifies the field to use for analysis.for\n        Can be defined in the metadata in the intake catalog source.\n    dims : dict, optional\n        Dictionary defining the coordinates dimensions in the intake catalog source.\n        Keys are the podpac dimensions (lat, lon, time, alt) in stacked or unstacked form.\n        Values are the identifiers which locate the coordinates in the datasource.\n        Can be defined in the metadata in the intake catalog source.\n        Examples:\n            {'lat': 'lat column', 'time': 'time column'}\n            {'lat_lon': ['lat column', 'lon column']}\n            {'time': 'time'}\n    crs : str, optional\n        Coordinate reference system of the coordinates.\n        Can be defined in the metadata in the intake catalog source.\n    query : str, optional\n        A pandas dataframe query which will sub-select the rows in the data. For example, self.source_data = self.datasource.read().query(self.query)\n\n\n    Attributes\n    ----------\n    catalog : :class:`intake.catalog.Catalog`\n        Loaded intake catalog class\n        See https://intake.readthedocs.io/en/latest/api_base.html#intake.catalog.Catalog\n    dataset : :class:`intake.catalog.local.CatalogEntry`\n        Loaded intake catalog data source\n        See https://intake.readthedocs.io/en/latest/api_base.html#intake.catalog.entry.CatalogEntry\n    \"\"\"",
            "\"\"\"Get coordinates from catalog definition or input dims\"\"\"",
            "\"\"\"Get Data from intake catalog source definition\"\"\""
        ],
        "code_snippets": [
            "class IntakeCatalog(podpac.data.DataSource):\n    \"\"\"\n    Support for Intake Catalogs (https:",
            "def catalog(self):\n        return intake.open_catalog(self.uri)\n\n    @cached_property",
            "def dataset(self):\n        return getattr(self.catalog, self.source)\n\n    @cached_property",
            "def source_data(self):\n        data = self.dataset.read()\n        if self.dataset.container == \"dataframe\" and self.query:\n            data = data.query(self.query)\n        return data\n\n    # TODO: validators may not be necessary\n\n    # @tl.validate('uri')\n    #",
            "def _validate_uri(self, proposed):\n    #     p = proposed['value']\n    #     self.catalog = intake.open_catalog(p)\n    #     self.dataset = getattr(self.catalog, self.source)\n\n    # @tl.validate('source')\n    #",
            "def _validate_source(self, proposed):\n    #     s = proposed['value']\n    #     self.dataset = getattr(self.catalog, s)\n\n    @tl.validate(\"field\")",
            "def _validate_field(self, proposed):\n        f = proposed[\"value\"]\n\n        if self.dataset.container == \"dataframe\" and f is None:\n            raise ValueError(\"Field is required when source container is a dataframe\")\n\n        return f\n\n        # # more strict checking\n        # if 'fields' not in self.dataset.metadata:\n        #     raise ValueError('No fields defined in catalog metadata')\n        # if f not in self.dataset.metadata['fields'].keys():\n        #     raise ValueError('Field {} not defined in catalog'.format(f))\n\n    @tl.validate(\"dims\")",
            "def _validate_dims(self, proposed):\n        dims = proposed[\"value\"]\n\n        # TODO: this needs to be improved to expand validation\n        for dim in dims:\n            udims = dim.split(\"_\")\n            if isinstance(dims[dim], list) and len(dims[dim]) != len(udims):\n                raise ValueError(\n                    'Native Coordinate dimension \"{}\" does not have an identifier defined'.format(dims[dim])\n                )\n\n        return dims",
            "def get_coordinates(self):",
            "def get_data(self, coordinates, coordinates_index):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/smap_egi.py",
        "comments": [
            "//n5eil01u.ecs.nsidc.org/egi/request\"",
            "//nsidc.org/data/smap",
            "//developer.earthdata.nasa.gov/sdps/programmatic-access-docs",
            "//n5eil01u.ecs.nsidc.org/egi/request",
            "//urs.earthdata.nasa.gov/)",
            "//urs.earthdata.nasa.gov/)",
            "//epsg.io/6933)"
        ],
        "docstrings": [
            "\"\"\"\nPODPAC Nodes to access SMAP data via EGI Interface\n\"\"\"",
            "\"\"\"\n    SMAP Node. For more information about SMAP, see https://nsidc.org/data/smap\n\n    SMAP interface using the EGI Data Portal\n    https://developer.earthdata.nasa.gov/sdps/programmatic-access-docs\n    with the base URL: https://n5eil01u.ecs.nsidc.org/egi/request\n\n    To access data from this node, an Earthdata login is required. This can either be specified when\n    creating the node:\n    ```python\n    smap = SMAP(username=\"your_user_name\", password=\"your_password\")\n    ```\n    OR you can set the following PODPAC settings:\n    ```python\n    podpac.settings[\"username@urs.earthdata.nasa.gov\"] = \"your_user_name\"\n    podpac.settings[\"password@urs.earthdata.nasa.gov\"] = \"your_password\"\n    podpac.settings.save()  # To have this information persist\n    smap = SMAP()\n    ```\n\n    Parameters\n    ----------\n    product : str\n        One of the :list:`SMAP_PRODUCTS` strings\n    check_quality_flags : bool, optional\n        Default is True. If True, data will be filtered based on the SMAP data quality flag, and only\n        high quality data is returned.\n    data_key : str, optional\n        Default will return soil moisture and is set automatically based on the product selected. Other\n        possible data keys can be found\n\n    Attributes\n    ----------\n    nan_vals : list\n        Nan values in SMAP data\n    username : str, optional\n        Earthdata username (https://urs.earthdata.nasa.gov/)\n        If undefined, node will look for a username under setting key \"username@urs.earthdata.nasa.gov\"\n    password : str, optional\n        Earthdata password (https://urs.earthdata.nasa.gov/)\n        If undefined, node will look for a password under setting key \"password@urs.earthdata.nasa.gov\"\n    \"\"\"",
            "\"\"\"Interpret individual SMAP file from  EGI zip archive.\n\n        Parameters\n        ----------\n        filelike : filelike\n            Reference to file inside EGI zip archive\n\n        Returns\n        -------\n        podpac.UnitsDataArray\n\n        Raises\n        ------\n        ValueError\n        \"\"\"",
            "\"\"\"Append data\n\n        Parameters\n        ----------\n        all_data : podpac.UnitsDataArray\n            aggregated data\n        data : podpac.UnitsDataArray\n            new data to append\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\""
        ],
        "code_snippets": [
            "def isnat(a):\n        return a.astype(str) == \"None\"\n\n    np.isnat = isnat\n\n# Internal dependencies\nfrom podpac import Coordinates, UnitsDataArray, cached_property\nfrom podpac.datalib import EGI\n\nBASE_URL = \"https:",
            "class SMAP(EGI):\n    \"\"\"\n    SMAP Node. For more information about SMAP, see https:",
            "def short_name(self):\n        if \"SPL3SMP\" in self.product:\n            return self.product.replace(\"_AM\", \"\").replace(\"_PM\", \"\")\n        else:\n            return self.product\n\n    # pull _data_key, lat_key, lon_key, and version from product dict\n    @cached_property",
            "def _product_data(self):\n        return SMAP_PRODUCT_DICT[self.product]\n\n    @property",
            "def udims(self):\n        return [\"lat\", \"lon\", \"time\"]\n\n    @property",
            "def lat_key(self):\n        return self._product_data[0]\n\n    @property",
            "def lon_key(self):\n        return self._product_data[1]\n\n    @property",
            "def _data_key(self):\n        if self.data_key is None:\n            return self._product_data[2]\n        else:\n            return self.data_key\n\n    @property",
            "def quality_flag_key(self):\n        return self._product_data[3]\n\n    @property",
            "def version(self):\n        try:\n            return nasaCMR.get_collection_entries(short_name=self.product)[-1][\"version_id\"]\n        except:\n            _log.warning(\"Could not automatically retrieve newest product version id from NASA CMR.\")\n            return self._product_data[4]\n\n    @property",
            "def coverage(self):\n        if self.quality_flag_key:\n            return (self._data_key, self.quality_flag_key, self.lat_key, self.lon_key)\n        else:\n            return (self._data_key, self.lat_key, self.lon_key)",
            "def read_file(self, filelike):\n        \"\"\"Interpret individual SMAP file from  EGI zip archive.\n\n        Parameters\n        ----------\n        filelike : filelike\n            Reference to file inside EGI zip archive\n\n        Returns\n        -------\n        podpac.UnitsDataArray\n\n        Raises\n        ------\n        ValueError\n        \"\"\"\n        ds = h5py.File(filelike, \"r\")\n\n        # handle data\n        data = ds[self._data_key][()]\n\n        if self.check_quality_flags and self.quality_flag_key:\n            flag = ds[self.quality_flag_key][()]\n            flag = flag > 0\n            [flag] == np.nan\n\n        data = np.array([data])  # add extra dimension for time slice\n\n        # handle time\n        if \"SPL3\" in self.product:\n            # TODO: make this py2.7 compatible\n            # take the midpoint between the range identified in the file\n            t_start = np.datetime64(ds[\"Metadata/Extent\"].attrs[\"rangeBeginningDateTime\"].replace(\"Z\", \"\"))\n            t_end = np.datetime64(ds[\"Metadata/Extent\"].attrs[\"rangeEndingDateTime\"].replace(\"Z\", \"\"))\n            time = np.array([t_start + (t_end - t_start) / 2])\n            time = time.astype(\"datetime64[D]\")\n\n        elif \"SPL4\" in self.product:\n            time_unit = ds[\"time\"].attrs[\"units\"].decode()\n            time = xr.coding.times.decode_cf_datetime(ds[\"time\"][()][0], units=time_unit)\n            time = time.astype(\"datetime64[h]\")\n\n        # handle spatial coordinates\n        if \"SPL3\" in self.product:\n\n            # take nan mean along each axis\n            lons = ds[self.lon_key][()]\n            lats = ds[self.lat_key][()]\n            lons[lons == self.nan_vals[0]] = np.nan\n            lats[lats == self.nan_vals[0]] = np.nan\n\n            # short-circuit if all lat/lon are non\n            if np.all(np.isnan(lats)) and np.all(np.isnan(lons)):\n                return None\n\n            # make podpac coordinates\n            lon = np.nanmean(lons, axis=0)\n            lat = np.nanmean(lats, axis=1)\n            c = Coordinates([time, lat, lon], dims=[\"time\", \"lat\", \"lon\"])\n\n        elif \"SPL4\" in self.product:\n            # lat/lon coordinates in EPSG:6933 (https:",
            "def append_file(self, all_data, data):\n        \"\"\"Append data\n\n        Parameters\n        ----------\n        all_data : podpac.UnitsDataArray\n            aggregated data\n        data : podpac.UnitsDataArray\n            new data to append\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\"\n        if all_data.shape[1:] == data.shape[1:]:\n            data.lat.data[:] = all_data.lat.data\n            data.lon.data[:] = all_data.lon.data\n        else:\n            # select only data with finite coordinates\n            data = data.isel(lon=np.isfinite(data.lon), lat=np.isfinite(data.lat))\n\n            # select lat based on the old data\n            lat = all_data.lat.sel(lat=data.lat, method=\"nearest\")\n\n            # When the difference between old and new coordintaes are large, it means there are new coordinates\n            Ilat = (np.abs(lat.data - data.lat) > 1e-3).data\n            # Use the new data's coordinates for the new coordinates\n            lat.data[Ilat] = data.lat.data[Ilat]\n\n            # Repeat for lon\n            lon = all_data.lon.sel(lon=data.lon, method=\"nearest\")\n            Ilon = (np.abs(lon.data - data.lon) > 1e-3).data\n            lon.data[Ilon] = data.lon.data[Ilon]\n\n            # Assign to data\n            data.lon.data[:] = lon.data\n            data.lat.data[:] = lat.data\n\n        return all_data.combine_first(data)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/weathercitizen_sensorburst_pb2.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/cosmos_stations.py",
        "comments": [
            "//cosmos.hwr.arizona.edu/Probes/StationDat/\")",
            "//cosmos.hwr.arizona.edu/Probes/\")"
        ],
        "docstrings": [
            "\"\"\"Returns a list of values for all the station for a particular key\n\n        Parameters\n        -----------\n        key: str\n           Key describing the station data. See self.available_data_keys for available keys.\n\n        Returns\n        --------\n        list\n            A list of the values for the keys for each station\n        \"\"\"",
            "\"\"\"helper function for stations_value\"\"\"",
            "\"\"\"Returns the COSMOS station's label given it's lat/lon coordinates\n\n        Parameters\n        -----------\n        lat_lon : podpac.Coordinates\n            The lat/lon locations whose station name will be returned. Note, the lat/lon coordinates have to match\n            exactly the coordinates given in station_data[N]['location'], where N is the station.\n            This should be Coordinates object with 'lat_lon' stacked coordinates as one of the dimensions.\n\n        Returns\n        --------\n        list\n            List of COSMOS station names corresponding to the given coordinates. If a coordinate has no match, then\n            \"None\" is returned.\n        \"\"\"",
            "\"\"\"Returns the lat/lon coordinates of COSMOS stations that match the given labels\n\n        Parameters\n        ------------\n        label: str, list\n            Strings that partially describe a COSMOS station label.\n\n        Returns\n        --------\n        podpac.Coordinates\n            The coordinates of the COSMOS stations matching the input data\n        \"\"\"",
            "\"\"\"Helper function to get source indices for partially matched labels\"\"\"",
            "\"\"\"Returns the calibration information for a station. Users must supply a label or lat_lon coordinates.\n\n        Parameters\n        ------------\n        label: str, List (optional)\n            Labels describing the station.\n\n        lat_lon: podpac.Coordinates (optional)\n            Coordinates of the COSMOS station. Note, this object has to have a 'lat_lon' dimension which matches exactly\n            with the COSMOS stations.\n\n        Returns\n        --------\n        list\n            A list of dictionaries containing the calibration data for the requested stations.\n        \"\"\"",
            "\"\"\"Returns the site properties for a station. Users must supply a label or lat_lon coordinates.\n\n        Parameters\n        ------------\n        label: str, List (optional)\n            Labels describing the station.\n\n        lat_lon: podpac.Coordinates (optional)\n            Coordinates of the COSMOS station. Note, this object has to have a 'lat_lon' dimension which matches exactly\n            with the COSMOS stations.\n\n        Returns\n        --------\n        list\n            A list of dictionaries containing the properties for the requested stations.\n        \"\"\"",
            "\"\"\"Returns the station data. Users must supply a label or lat_lon coordinates.\n\n        Parameters\n        ------------\n        label: str, List (optional)\n            Labels describing the station.\n\n        lat_lon: podpac.Coordinates (optional)\n            Coordinates of the COSMOS station. Note, this object has to have a 'lat_lon' dimension which matches exactly\n            with the COSMOS stations.\n\n        Returns\n        --------\n        list\n            A list of dictionaries containing the data for the requested stations.\n        \"\"\""
        ],
        "code_snippets": [
            "def _convert_str_to_vals(properties):\n    IGNORE_KEYS = [\"sitenumber\"]\n    for k, v in properties.items():\n        if not isinstance(v, string_types) or k in IGNORE_KEYS:\n            continue\n        try:\n            if \",\" in v:\n                properties[k] = tuple([float(vv) for vv in v.split(\",\")])\n            else:\n                properties[k] = float(v)\n        except ValueError:\n            try:\n                properties[k] = np.datetime64(v)\n            except ValueError:\n                pass\n    return properties",
            "class COSMOSStation(DataSource):\n    _repr_keys = [\"label\", \"network\", \"location\"]\n\n    url = tl.Unicode(\"http:",
            "def raw_data(self):\n        _logger.info(\"Downloading station data from {}\".format(self.station_data_url))\n\n        r = _get_from_url(self.station_data_url)\n        if r is None:\n            raise ConnectionError(\n                \"COSMOS data cannot be retrieved. Is the site {} down?\".format(self.station_calibration_url)\n            )\n        return r.text\n\n    @cached_property",
            "def data_columns(self):\n        return self.raw_data.split(\"\\n\", 1)[0].split(\" \")\n\n    @property",
            "def site_number(self):\n        return str(self.station_data[\"sitenumber\"])\n\n    @property",
            "def station_data_url(self):\n        return self.url + self.site_number + \"/smcounts.txt\"\n\n    @property",
            "def station_calibration_url(self):\n        return self.url + self.site_number + \"/calibrationInfo.php\"\n\n    @property",
            "def station_properties_url(self):\n        return self.url + self.site_number + \"/index.php\"",
            "def get_data(self, coordinates, coordinates_index):\n        data = np.loadtxt(StringIO(self.raw_data), skiprows=1, usecols=self.data_columns.index(\"SOILM\"))[\n            coordinates_index[0]\n        ]\n        data[data > 100] = np.nan\n        data[data < 0] = np.nan\n        data /= 100.0  # Make it fractional\n        return self.create_output_array(coordinates, data=data.reshape(coordinates.shape))",
            "def get_coordinates(self):\n        lat_lon = self.station_data[\"location\"]\n        time = np.atleast_2d(\n            np.loadtxt(\n                StringIO(self.raw_data),\n                skiprows=1,\n                usecols=[self.data_columns.index(\"YYYY-MM-DD\"), self.data_columns.index(\"HH:MM\")],\n                dtype=str,\n            )\n        )\n        if time.size == 0:\n            time = np.datetime64(\"NaT\")\n        else:\n            time = np.array([t[0] + \"T\" + t[1] for t in time], np.datetime64)\n        c = podpac.Coordinates([time, [lat_lon[0], lat_lon[1]]], [\"time\", [\"lat\", \"lon\"]])\n        return c\n\n    @property",
            "def label(self):\n        return self.station_data[\"label\"]\n\n    @property",
            "def network(self):\n        return self.station_data[\"network\"]\n\n    @property",
            "def location(self):\n        return self.station_data[\"location\"]\n\n    @cached_property(use_cache_ctrl=True)",
            "def calibration_data(self):\n        cd = _get_from_url(self.station_calibration_url)\n        if cd is None:\n            raise ConnectionError(\n                \"COSMOS data cannot be retrieved. Is the site {} down?\".format(self.station_calibration_url)\n            )\n        cd = cd.json()\n        cd[\"items\"] = [_convert_str_to_vals(i) for i in cd[\"items\"]]\n        return cd\n\n    @cached_property(use_cache_ctrl=True)",
            "def site_properties(self):\n        r = _get_from_url(self.station_properties_url)\n        if r is None:\n            raise ConnectionError(\n                \"COSMOS data cannot be retrieved. Is the site {} down?\".format(self.station_properties_url)\n            )\n        soup = bs4.BeautifulSoup(r.text, \"lxml\")\n        regex = re.compile(\"Soil Organic Carbon\")\n        loc = soup.body.findAll(text=regex)[0].parent.parent\n        label, value = loc.findAll(\"div\")\n        labels = [l.strip() for l in label.children if \"br\" not in str(l)]\n        values = [l.strip() for l in value.children if \"br\" not in str(l) and l.strip() != \"\"]\n\n        properties = {k: v for k, v in zip(labels, values)}\n\n        return _convert_str_to_vals(properties)\n\n\nclass COSMOSStationsRaw(TileCompositorRaw):\n    url = tl.Unicode(\"http:",
            "class COSMOSStationsRaw(TileCompositorRaw):\n    url = tl.Unicode(\"http:",
            "def _stations_data_raw(self):\n        url = self.url + self.stations_url\n        r = _get_from_url(url)\n        if r is None:\n            raise ConnectionError(\"COSMOS data cannot be retrieved. Is the site {} down?\".format(url))\n\n        t = r.text\n\n        # Fix the JSON\n        t_f = re.sub(':\\s?\",', ': \"\",', t)  # Missing closing parenthesis\n        if t_f[-5:] == \",\\n]}\\n\":  # errant comma\n            t_f = t_f[:-5] + \"\\n]}\\n\"\n\n        return t_f\n\n    @cached_property",
            "def stations_data(self):\n        stations = json.loads(self._stations_data_raw)\n        stations[\"items\"] = [_convert_str_to_vals(i) for i in stations[\"items\"]]\n        return stations\n\n    @cached_property(use_cache_ctrl=True)",
            "def source_coordinates(self):\n        lat_lon = np.array(self.stations_value(\"location\"))[self.has_data]\n        c = podpac.Coordinates([[lat_lon[:, 0], lat_lon[:, 1]]], [\"lat_lon\"])\n        return c\n\n    @cached_property",
            "def has_data(self):\n        return ~(np.array(self.stations_value(\"lastdat\")) == \"YYYY-MM-DD\")\n\n    @cached_property",
            "def sources(self):\n        return np.array([COSMOSStation(station_data=item) for item in self.stations_data[\"items\"]])[self.has_data]\n\n    @property",
            "def available_data_keys(self):\n        return list(self.stations_data[\"items\"][0].keys())\n\n    ## UTILITY FUNCTIONS",
            "def stations_value(self, key, stations_data=None):\n        \"\"\"Returns a list of values for all the station for a particular key\n\n        Parameters\n        -----------\n        key: str\n           Key describing the station data. See self.available_data_keys for available keys.\n\n        Returns\n        --------\n        list\n            A list of the values for the keys for each station\n        \"\"\"\n        if key not in self.available_data_keys:\n            raise ValueError(\"Input key {} is not in available keys {}\".format(key, self.available_data_keys))\n\n        return self._stations_value(key, stations_data)",
            "def _stations_value(self, key, stations_data=None):",
            "def stations_label(self):\n        return self.stations_value(\"label\")",
            "def label_from_latlon(self, lat_lon):\n        \"\"\"Returns the COSMOS station's label given it's lat/lon coordinates\n\n        Parameters\n        -----------\n        lat_lon : podpac.Coordinates\n            The lat/lon locations whose station name will be returned. Note, the lat/lon coordinates have to match\n            exactly the coordinates given in station_data[N]['location'], where N is the station.\n            This should be Coordinates object with 'lat_lon' stacked coordinates as one of the dimensions.\n\n        Returns\n        --------\n        list\n            List of COSMOS station names corresponding to the given coordinates. If a coordinate has no match, then\n            \"None\" is returned.\n        \"\"\"\n        if \"lon_lat\" in lat_lon.dims:\n            lat_lon = lat_lon.transpose(\"lon_lat\")\n        elif \"lat_lon\" not in lat_lon.dims:\n            raise ValueError(\"The coordinates object must have a stacked 'lat_lon' dimension.\")\n\n        labels_map = {s[\"location\"]: s[\"label\"] for s in self.stations_data[\"items\"]}\n        labels = [labels_map.get(ll, None) for ll in lat_lon.xcoords[\"lat_lon\"]]\n        return labels",
            "def latlon_from_label(self, label):\n        \"\"\"Returns the lat/lon coordinates of COSMOS stations that match the given labels\n\n        Parameters\n        ------------\n        label: str, list\n            Strings that partially describe a COSMOS station label.\n\n        Returns\n        --------\n        podpac.Coordinates\n            The coordinates of the COSMOS stations matching the input data\n        \"\"\"\n        if not isinstance(label, list):\n            label = [label]\n\n        ind = self._get_label_inds(label)\n        if ind.size == 0:\n            return podpac.Coordinates([])  # Empty\n\n        lat_lon = np.array(self.stations_value(\"location\"))[ind].squeeze()\n        c = podpac.Coordinates([[lat_lon[0], lat_lon[1]]], [\"lat_lon\"])\n\n        return c",
            "def _get_label_inds(self, label):",
            "def get_calibration_data(self, label=None, lat_lon=None):\n        \"\"\"Returns the calibration information for a station. Users must supply a label or lat_lon coordinates.\n\n        Parameters\n        ------------\n        label: str, List (optional)\n            Labels describing the station.\n\n        lat_lon: podpac.Coordinates (optional)\n            Coordinates of the COSMOS station. Note, this object has to have a 'lat_lon' dimension which matches exactly\n            with the COSMOS stations.\n\n        Returns\n        --------\n        list\n            A list of dictionaries containing the calibration data for the requested stations.\n        \"\"\"\n\n        if label is None and lat_lon is None:\n            raise ValueError(\"Must supply either 'label' or 'lat_lon'\")\n\n        if lat_lon is not None:\n            label = self.label_from_latlon(lat_lon)\n\n        if isinstance(label, string_types):\n            label = [label]\n\n        inds = self._get_label_inds(label)\n\n        return [self.sources[i].calibration_data for i in inds]",
            "def get_site_properties(self, label=None, lat_lon=None):\n        \"\"\"Returns the site properties for a station. Users must supply a label or lat_lon coordinates.\n\n        Parameters\n        ------------\n        label: str, List (optional)\n            Labels describing the station.\n\n        lat_lon: podpac.Coordinates (optional)\n            Coordinates of the COSMOS station. Note, this object has to have a 'lat_lon' dimension which matches exactly\n            with the COSMOS stations.\n\n        Returns\n        --------\n        list\n            A list of dictionaries containing the properties for the requested stations.\n        \"\"\"\n\n        if label is None and lat_lon is None:\n            raise ValueError(\"Must supply either 'label' or 'lat_lon'\")\n\n        if lat_lon is not None:\n            label = self.label_from_latlon(lat_lon)\n\n        if isinstance(label, string_types):\n            label = [label]\n\n        inds = self._get_label_inds(label)\n\n        return [self.sources[i].site_properties for i in inds]",
            "def get_station_data(self, label=None, lat_lon=None):\n        \"\"\"Returns the station data. Users must supply a label or lat_lon coordinates.\n\n        Parameters\n        ------------\n        label: str, List (optional)\n            Labels describing the station.\n\n        lat_lon: podpac.Coordinates (optional)\n            Coordinates of the COSMOS station. Note, this object has to have a 'lat_lon' dimension which matches exactly\n            with the COSMOS stations.\n\n        Returns\n        --------\n        list\n            A list of dictionaries containing the data for the requested stations.\n        \"\"\"\n\n        if label is None and lat_lon is None:\n            raise ValueError(\"Must supply either 'label' or 'lat_lon'\")\n\n        if lat_lon is not None:\n            label = self.label_from_latlon(lat_lon)\n\n        if isinstance(label, string_types):\n            label = [label]\n\n        inds = self._get_label_inds(label)\n\n        return [self.stations_data[\"items\"][i] for i in inds]",
            "class COSMOSStations(InterpolationMixin, COSMOSStationsRaw):\n    @tl.default(\"interpolation\")",
            "def _interpolation_default(self):\n        return {\"method\": \"nearest\", \"params\": {\"use_selector\": False, \"remove_nan\": False, \"time_scale\": \"1,M\"}}\n\n\nif __name__ == \"__main__\":\n    bounds = {\"lat\": [40, 46], \"lon\": [-78, -68]}\n    cs = COSMOSStations(\n        cache_ctrl=[\"ram\", \"disk\"],\n        interpolation={\"method\": \"nearest\", \"params\": {\"use_selector\": False, \"remove_nan\": True, \"time_scale\": \"1,M\"}},\n    )\n    csr = COSMOSStationsRaw(\n        cache_ctrl=[\"ram\", \"disk\"],\n        interpolation={\"method\": \"nearest\", \"params\": {\"use_selector\": False, \"remove_nan\": True, \"time_scale\": \"1,M\"}},\n    )\n\n    sd = cs.stations_data\n    ci = cs.source_coordinates.select(bounds)\n    ce = podpac.coordinates.merge_dims(\n        [podpac.Coordinates([podpac.crange(\"2018-05-01\", \"2018-06-01\", \"1,D\", \"time\")]), ci]\n    )\n    cg = podpac.Coordinates(\n        [\n            podpac.clinspace(ci[\"lat\"].bounds[1], ci[\"lat\"].bounds[0], 12, \"lat\"),\n            podpac.clinspace(ci[\"lon\"].bounds[1], ci[\"lon\"].bounds[0], 16, \"lon\"),\n            ce[\"time\"],\n        ]\n    )\n    o = cs.eval(ce)\n    o_r = csr.eval(ce)\n    og = cs.eval(cg)\n\n    # Test helper functions\n    labels = cs.stations_label\n    lat_lon = cs.latlon_from_label(\"Manitou\")\n    labels = cs.label_from_latlon(lat_lon)\n    lat_lon2 = cs.latlon_from_label(\"No Match Here\")\n    cal = cs.get_calibration_data(\"Manitou\")\n    props = cs.get_site_properties(\"Manitou\")\n\n    from matplotlib import rcParams\n\n    rcParams[\"axes.labelsize\"] = 12\n    rcParams[\"xtick.labelsize\"] = 10\n    rcParams[\"ytick.labelsize\"] = 10\n    rcParams[\"legend.fontsize\"] = 8\n    rcParams[\"lines.linewidth\"] = 2\n    rcParams[\"font.size\"] = 12\n\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    from pandas.plotting import register_matplotlib_converters\n\n    register_matplotlib_converters()\n\n    fig = plt.figure(figsize=(6.5, 3), dpi=300)\n    plt.plot(o.time, o.data, \"o-\")\n    ax = plt.gca()\n    plt.ylim(0, 1)\n    plt.legend(cs.label_from_latlon(ce))\n    # plt.plot(o_r.time, o_r.data, \".-\")\n    plt.ylabel(\"Soil Moisture ($m^3/m^3$)\")\n    plt.xlabel(\"Date\")\n    # plt.xticks(rotation=90)\n    fig.autofmt_xdate()\n    ax.fmt_xdata = mdates.DateFormatter(\"%m-%d\")\n    plt.title(\"COSMOS Data for 2018 over lat (40, 46) by lon (-78,-68)\")\n    plt.tight_layout()\n    plt.show()\n\n    print(\"Done\")"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/gfs.py",
        "comments": [
            "//%s/%s/%s/%s/%s/%s\" % (BUCKET, self.parameter, self.level, self.date, self.hour, self.forecast)"
        ],
        "docstrings": [
            "\"\"\"Raw GFS data from S3\n\n    Attributes\n    ----------\n    parameter : str\n        parameter, e.g. 'SOIM'.\n    level : str\n        depth, e.g. \"0-10 m DPTH\"\n    date : str\n        source date in '%Y%m%d' format, e.g. '20200130'\n    hour : str\n        source hour, e.g. '1200'\n    forecast : str\n        forecast time in hours from the source date and hour, e.g. '003'\n    \"\"\"",
            "\"\"\"Composited and interpolated GFS data from S3\n\n    Attributes\n    ----------\n    parameter : str\n        parameter, e.g. 'SOIM'.\n    level : str\n        source depth, e.g. \"0-10 m DPTH\"\n    date : str\n        source date in '%Y%m%d' format, e.g. '20200130'\n    hour : str\n        source hour, e.g. '1200'\n    \"\"\"",
            "\"\"\"\n    The latest composited and interpolated GFS data from S3\n\n    Arguments\n    ---------\n    parameter : str\n        parameter, e.g. 'SOIM'.\n    level : str\n        source depth, e.g. \"0-10 m DPTH\"\n\n    Returns\n    -------\n    node : GFS\n        GFS node with the latest forecast data available for the given parameter and level.\n    \"\"\""
        ],
        "code_snippets": [
            "class GFSSourceRaw(DiskCacheMixin, RasterioRaw):\n    \"\"\"Raw GFS data from S3\n\n    Attributes\n    ----------\n    parameter : str\n        parameter, e.g. 'SOIM'.\n    level : str\n        depth, e.g. \"0-10 m DPTH\"\n    date : str\n        source date in '%Y%m%d' format, e.g. '20200130'\n    hour : str\n        source hour, e.g. '1200'\n    forecast : str\n        forecast time in hours from the source date and hour, e.g. '003'\n    \"\"\"\n\n    parameter = tl.Unicode().tag(attr=True)\n    level = tl.Unicode().tag(attr=True)\n    date = tl.Unicode().tag(attr=True)\n    hour = tl.Unicode().tag(attr=True)\n    forecast = tl.Unicode().tag(attr=True)\n\n    @property\n    def source(self):\n        return \"s3:",
            "def source(self):\n        return \"s3:",
            "class GFS(S3Mixin, DiskCacheMixin, TileCompositor):\n    \"\"\"Composited and interpolated GFS data from S3\n\n    Attributes\n    ----------\n    parameter : str\n        parameter, e.g. 'SOIM'.\n    level : str\n        source depth, e.g. \"0-10 m DPTH\"\n    date : str\n        source date in '%Y%m%d' format, e.g. '20200130'\n    hour : str\n        source hour, e.g. '1200'\n    \"\"\"\n\n    parameter = tl.Unicode().tag(attr=True, required=True)\n    level = tl.Unicode().tag(attr=True, required=True)\n    date = tl.Unicode().tag(attr=True, required=True)\n    hour = tl.Unicode().tag(attr=True, required=True)\n\n    @property",
            "def _repr_keys(self):\n        return [\"parameter\", \"level\", \"date\", \"hour\"] + super()._repr_keys\n\n    @property",
            "def prefix(self):\n        return \"%s/%s/%s/%s/%s/\" % (BUCKET, self.parameter, self.level, self.date, self.hour)\n\n    @cached_property(use_cache_ctrl=True)",
            "def forecasts(self):\n        return [path.replace(self.prefix, \"\") for path in self.s3.find(self.prefix)]\n\n    @cached_property",
            "def sources(self):\n        params = {\n            \"parameter\": self.parameter,\n            \"level\": self.level,\n            \"date\": self.date,\n            \"hour\": self.hour,\n            \"cache_ctrl\": self.cache_ctrl,\n        }\n        return np.array([GFSSourceRaw(forecast=forecast, **params) for forecast in self.forecasts])\n\n    @cached_property",
            "def source_coordinates(self):\n        base_time = datetime.datetime.strptime(\"%s %s\" % (self.date, self.hour), \"%Y%m%d %H%M\")\n        forecast_times = [base_time + datetime.timedelta(hours=int(h)) for h in self.forecasts]\n        return Coordinates(\n            [[dt.strftime(\"%Y-%m-%d %H:%M\") for dt in forecast_times]], dims=[\"time\"], validate_crs=False\n        )",
            "def GFSLatest(parameter=None, level=None, **kwargs):\n    \"\"\"\n    The latest composited and interpolated GFS data from S3\n\n    Arguments\n    ---------\n    parameter : str\n        parameter, e.g. 'SOIM'.\n    level : str\n        source depth, e.g. \"0-10 m DPTH\"\n\n    Returns\n    -------\n    node : GFS\n        GFS node with the latest forecast data available for the given parameter and level.\n    \"\"\"\n\n    s3 = s3fs.S3FileSystem(anon=True)\n\n    # get latest date\n    prefix = \"%s/%s/%s/\" % (BUCKET, parameter, level)\n    dates = [path.replace(prefix, \"\") for path in s3.ls(prefix)]\n    if not dates:\n        raise RuntimeError(\"No data found at '%s'\" % prefix)\n    date = max(dates)\n\n    # get latest hour\n    prefix = \"%s/%s/%s/%s/\" % (BUCKET, parameter, level, date)\n    hours = [path.replace(prefix, \"\") for path in s3.ls(prefix)]\n    if not hours:\n        raise RuntimeError(\"No data found at '%s'\" % prefix)\n    hour = max(hours)\n\n    # node\n    return GFS(parameter=parameter, level=level, date=date, hour=hour, **kwargs)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_weathercitizen.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestWeatherCitizen(object):\n    data_key = \"pressure\"\n    uuid = \"re5wm615\"",
            "def test_eval_source_coordinates(self):\n        node = podpac.datalib.weathercitizen.WeatherCitizen(data_key=self.data_key, uuid=self.uuid)\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"parsing timezone aware datetimes is deprecated\")\n            o = node.eval(node.coordinates[:3])",
            "def test_eval_interpolated(self):\n        node = podpac.datalib.weathercitizen.WeatherCitizen(data_key=self.data_key, uuid=self.uuid)\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"parsing timezone aware datetimes is deprecated\")\n            o = node.eval(podpac.Coordinates([0, 0], dims=[\"lat\", \"lon\"]))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_satutils.py",
        "comments": [
            "//earth-search.aws.element84.com/v0\""
        ],
        "docstrings": [],
        "code_snippets": [
            "class TestLandsat8(object):",
            "def test_landsat8(self):\n        lat = [39.5, 40.5]\n        lon = [-110, -105]\n        time = [\"2020-12-09\", \"2020-12-10\"]\n        c = podpac.Coordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n\n        node = podpac.datalib.satutils.Landsat8(\n            stac_api_url=STAC_API_URL,\n            asset=\"B01\",\n        )\n        output = node.eval(c)\n        assert np.isfinite(output).sum() > 0\n\n\n@pytest.mark.skip(reason=\"requester pays\")\n@pytest.mark.integration",
            "class TestSentinel2(object):",
            "def test_sentinel2(self):\n        lat = [39.5, 40.5]\n        lon = [-110, -105]\n        time = [\"2020-12-09\", \"2020-12-10\"]\n        c = podpac.Coordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n\n        with podpac.settings:\n            podpac.settings[\"AWS_REQUESTER_PAYS\"] = True\n            node = podpac.datalib.satutils.Sentinel2(\n                stac_api_url=STAC_API_URL, asset=\"B01\", aws_region_name=\"eu-central-1\"\n            )\n            output = node.eval(c)\n            assert np.isfinite(output).sum() > 0"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_soilscape.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestSoilscape(object):",
            "def test_common_coordinates(self):\n        point_interpolation = {\n            \"method\": \"nearest\",\n            \"params\": {\"use_selector\": False, \"remove_nan\": True, \"time_scale\": \"1,M\", \"respect_bounds\": False},\n        }\n        soilscape = podpac.datalib.soilscape.SoilSCAPE20min(\n            site=\"Canton_OK\", data_key=\"soil_moisture\", interpolation=point_interpolation\n        )\n        for ck, c in COORDINATES.items():\n            if \"cosmos\" in ck:\n                continue\n            print(\"Evaluating: \", ck)\n            o = soilscape.eval(c)\n            assert np.any(np.isfinite(o.data))",
            "def test_site_raw(self):\n        sm = podpac.datalib.soilscape.SoilSCAPE20minRaw(site=\"Canton_OK\", data_key=\"soil_moisture\")\n        coords_source = sm.make_coordinates(time=sm.sources[0].coordinates[\"time\"][:5])\n        coords_interp_time = sm.make_coordinates(time=\"2016-01-01\")\n        coords_interp_alt = sm.make_coordinates(time=sm.sources[0].coordinates[\"time\"][:5], depth=5)\n        o1 = sm.eval(coords_source)\n        o2 = sm.eval(coords_interp_time)\n        o3 = sm.eval(coords_interp_alt)",
            "def test_site_interpolated(self):\n        sm = podpac.datalib.soilscape.SoilSCAPE20min(site=\"Canton_OK\", data_key=\"soil_moisture\")\n        coords_source = sm.make_coordinates(time=sm.sources[0].coordinates[\"time\"][:5])\n        coords_interp_time = sm.make_coordinates(time=\"2016-01-01\")\n        coords_interp_alt = sm.make_coordinates(time=sm.sources[0].coordinates[\"time\"][:5], depth=5)\n        o1 = sm.eval(coords_source)\n        o2 = sm.eval(coords_interp_time)\n        o3 = sm.eval(coords_interp_alt)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_cosmos.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestCOSMOS(object):",
            "def test_common_coordinates(self):\n        point_interpolation = {\n            \"method\": \"nearest\",\n            \"params\": {\"use_selector\": False, \"remove_nan\": True, \"time_scale\": \"1,M\", \"respect_bounds\": False},\n        }\n        cosmos = podpac.datalib.cosmos_stations.COSMOSStations()\n        cosmos_raw = podpac.datalib.cosmos_stations.COSMOSStationsRaw()\n        cosmos_filled = podpac.datalib.cosmos_stations.COSMOSStations(interpolation=point_interpolation)\n        for ck, c in COORDINATES.items():\n            if ck != \"cosmos_region\":\n                continue\n            print(\"Evaluating: \", ck)\n            o_f = cosmos_filled.eval(c)\n            assert np.any(np.isfinite(o_f.data))\n            o = cosmos.eval(c)\n            o_r = cosmos.eval(c)\n            if \"soilscape\" in ck:\n                assert np.any(np.isnan(o.data))\n                assert np.any(np.isnan(o_r.data))\n                continue\n            assert np.any(np.isfinite(o.data))\n            assert np.any(np.isfinite(o_r.data))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/coordinates_for_tests.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_terrain_tiles.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestTerrainTiles(object):",
            "def test_common_coordinates(self):\n        node = TerrainTiles()\n        for ck, c in COORDINATES.items():\n            print(\"Evaluating: \", ck)\n            o = node.eval(c)\n            assert np.any(np.isfinite(o.data))",
            "def test_terrain_tiles(self):\n        c = Coordinates([clinspace(40, 43, 1000), clinspace(-76, -72, 1000)], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates(\n            [clinspace(40, 43, 1000), clinspace(-76, -72, 1000), [\"2018-01-01\", \"2018-01-02\"]],\n            dims=[\"lat\", \"lon\", \"time\"],\n        )\n\n        node = TerrainTiles(tile_format=\"geotiff\", zoom=8)\n        output = node.eval(c)\n        assert np.any(np.isfinite(output))\n\n        output = node.eval(c2)\n        assert np.any(np.isfinite(output))\n\n        node = TerrainTiles(tile_format=\"geotiff\", zoom=8, cache_ctrl=[\"ram\", \"disk\"])\n        output = node.eval(c)\n        assert np.any(np.isfinite(output))\n\n        # tile urls\n        print(np.array(get_tile_urls(\"geotiff\", 1)))\n        print(np.array(get_tile_urls(\"geotiff\", 9, coordinates=c)))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_soilgrids.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestSoilGrids(object):",
            "def test_common_coordinates(self):\n        soil_organic_carbon = podpac.datalib.soilgrids.SoilGridsSOC(layer=\"soc_0-5cm_Q0.95\")\n        for ck, c in COORDINATES.items():\n            print(\"Evaluating: \", ck)\n            o = soil_organic_carbon.eval(c)\n            assert np.any(np.isfinite(o.data))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_modis.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestMODIS(object):",
            "def test_common_coordinates(self):\n        modis = podpac.datalib.modis_pds.MODIS(product=\"MCD43A4.006\", data_key=\"B01\")  #  Band 01, 620 - 670nm\n        for ck, c in COORDINATES.items():\n            print(\"Evaluating: \", ck)\n            o = modis.eval(c)\n            assert np.any(np.isfinite(o.data))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_smap_egi.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestSMAP_EGI(object):",
            "def test_eval_level_3(self):\n        # level 3 access\n        c = podpac.Coordinates(\n            [\n                podpac.clinspace(-82, -81, 10),\n                podpac.clinspace(38, 39, 10),\n                podpac.clinspace(\"2015-07-06\", \"2015-07-08\", 10),\n            ],\n            dims=[\"lon\", \"lat\", \"time\"],\n        )\n\n        node = podpac.datalib.smap_egi.SMAP(product=\"SPL3SMP_AM\")\n        output = node.eval(c)\n        print(output)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/datalib/test/test_gfs.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestGFS(object):\n    parameter = \"SOIM\"\n    level = \"0-10 m DPTH\"\n\n    @classmethod",
            "def setup_class(cls):\n        # find an existing date\n        s3 = s3fs.S3FileSystem(anon=True)\n        prefix = \"%s/%s/%s/\" % (gfs.BUCKET, cls.parameter, cls.level)\n        dates = [path.replace(prefix, \"\") for path in s3.ls(prefix)]\n        cls.date = dates[0]",
            "def test_source(self):\n        # specify source datetime and forecast\n        gfs_soim = gfs.GFSSourceRaw(\n            parameter=self.parameter,\n            level=self.level,\n            date=self.date,\n            hour=\"1200\",\n            forecast=\"003\",\n            anon=True,\n        )\n\n        o = gfs_soim.eval(gfs_soim.coordinates)",
            "def test_composited(self):\n        # specify source datetime, select forecast at evaluation from time coordinates\n        gfs_soim = gfs.GFS(parameter=self.parameter, level=self.level, date=self.date, hour=\"1200\", anon=True)\n\n        # whole world forecast at 15:30\n        forecast_time = datetime.datetime.strptime(self.date + \" 15:30\", \"%Y%m%d %H:%M\")\n        coords = gfs_soim.sources[0].coordinates\n        c = podpac.Coordinates([coords[\"lat\"], coords[\"lon\"], forecast_time], dims=[\"lat\", \"lon\", \"time\"])\n        o = gfs_soim.eval(c)\n\n        # time series: get the forecast at lat=42, lon=275 every hour for 6 hours\n        start = forecast_time\n        stop = forecast_time + datetime.timedelta(hours=6)\n        c = podpac.Coordinates([42, 282, podpac.crange(start, stop, \"1,h\")], dims=[\"lat\", \"lon\", \"time\"])\n        o = gfs_soim.eval(c)",
            "def test_latest(self):\n        # get latest source, select forecast at evaluation\n        gfs_soim = gfs.GFSLatest(parameter=self.parameter, level=self.level, anon=True)\n\n        # latest whole world forecast\n        forecast_time = datetime.datetime.strptime(gfs_soim.date + \" \" + gfs_soim.hour, \"%Y%m%d %H%M\")\n        coords = gfs_soim.sources[0].coordinates\n        c = podpac.Coordinates([coords[\"lat\"], coords[\"lon\"], forecast_time], dims=[\"lat\", \"lon\", \"time\"])\n        o = gfs_soim.eval(c)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/alglib/__init__.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nDatalib Public API\n\nThis module gets imported in the root __init__.py\nand exposed its contents to podpac.datalib\n\"\"\""
        ],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/alglib/climatology.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nPODPAC node to compute beta fit of seasonal variables\n\"\"\"",
            "\"\"\"\n    This fits a beta distribution to day of the year in the requested coordinates over a window. It returns the beta\n    distribution parameters 'a', and 'b' as part of the output. It may also return a number of percentiles.\n\n    Attributes\n    -----------\n    percentiles: list, optional\n        Default is []. After computing the beta distribution, optionally compute the value of the function for the given\n        percentiles in the list. The results will be available as an output named ['d0', 'd1',...] for each entry in\n        the list.\n    \"\"\""
        ],
        "code_snippets": [
            "class BetaFitDayOfYear(DayOfYearWindow):\n    \"\"\"\n    This fits a beta distribution to day of the year in the requested coordinates over a window. It returns the beta\n    distribution parameters 'a', and 'b' as part of the output. It may also return a number of percentiles.\n\n    Attributes\n    -----------\n    percentiles: list, optional\n        Default is []. After computing the beta distribution, optionally compute the value of the function for the given\n        percentiles in the list. The results will be available as an output named ['d0', 'd1',...] for each entry in\n        the list.\n    \"\"\"\n\n    percentiles = tl.List().tag(attr=True)\n    rescale = tl.Bool(True).tag(attr=True)\n\n    @property",
            "def outputs(self):\n        return [\"a\", \"b\"] + [\"d{}\".format(i) for i in range(len(self.percentiles))]",
            "def function(self, data, output):\n        # define the fit function\n        try:\n            data[data == 1] -= 1e-6\n            data[data == 0] += 1e-6\n            a, b, loc, scale = beta.fit(data, floc=0, fscale=1)\n        except FitSolverError as e:\n            print(e)\n            return output\n\n        # populate outputs for this point\n        output.loc[{\"output\": \"a\"}] = a\n        output.loc[{\"output\": \"b\"}] = b\n        for ii, d in enumerate(self.percentiles):\n            output.loc[{\"output\": \"d\" + str(ii)}] = beta.ppf(d, a, b)\n\n        return output",
            "def rescale_outputs(self, output, scale_max, scale_min):\n        output[..., 2:] = (output[..., 2:] * (scale_max - scale_min)) + scale_min\n        return output"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/style.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Summary\n\n    Attributes\n    ----------\n    name : str\n        data name\n    units : TYPE\n        data units\n    clim : list\n        [low, high], color map limits\n    colormap : str\n        matplotlib colormap name\n    cmap : matplotlib.cm.ColorMap\n        matplotlib colormap property\n    enumeration_colors : dict\n        data colors (replaces colormap/cmap)\n    enumeration_legend : dict\n        data legend, should correspond with enumeration_colors\n    \"\"\"",
            "\"\"\"Convert enumeration_colors into a tuple suitable for matplotlib ListedColormap.\"\"\"",
            "\"\"\"Convert enumeration_legend into a tuple suitable for matplotlib.\"\"\"",
            "\"\"\"JSON-serialized style definition\n\n        The `json` can be used to create new styles.\n\n        See Also\n        ----------\n        from_json\n        \"\"\"",
            "\"\"\"\n        Attempting to expose style units to get_ui_spec(). This will grab defaults in general.\n        BUT this will not set defaults for each particular node.\n        \"\"\"",
            "\"\"\"Create podpac Style from a style JSON definition.\n\n        Parameters\n        -----------\n        s : str\n            JSON definition\n\n        Returns\n        --------\n        Style\n            podpac Style object\n        \"\"\""
        ],
        "code_snippets": [
            "class Style(tl.HasTraits):\n    \"\"\"Summary\n\n    Attributes\n    ----------\n    name : str\n        data name\n    units : TYPE\n        data units\n    clim : list\n        [low, high], color map limits\n    colormap : str\n        matplotlib colormap name\n    cmap : matplotlib.cm.ColorMap\n        matplotlib colormap property\n    enumeration_colors : dict\n        data colors (replaces colormap/cmap)\n    enumeration_legend : dict\n        data legend, should correspond with enumeration_colors\n    \"\"\"",
            "def __init__(self, node=None, *args, **kwargs):\n        if node:\n            self.name = node.__class__.__name__\n            self.units = node.units\n        super(Style, self).__init__(*args, **kwargs)\n\n    name = tl.Unicode()\n    units = tl.Unicode(allow_none=True, default_value=\"\")\n    clim = tl.List(default_value=[None, None])\n    colormap = tl.Unicode(allow_none=True, default_value=None)\n    enumeration_legend = tl.Dict(key_trait=tl.Int(), value_trait=tl.Unicode(), default_value=None, allow_none=True)\n    enumeration_colors = tl.Dict(key_trait=tl.Int(), default_value=None, allow_none=True)\n    default_enumeration_legend = tl.Unicode(default_value=DEFAULT_ENUMERATION_LEGEND)\n    default_enumeration_color = tl.Any(default_value=DEFAULT_ENUMERATION_COLOR)\n\n    @tl.validate(\"colormap\")",
            "def _validate_colormap(self, d):\n        if isinstance(d[\"value\"], six.string_types):\n            matplotlib.cm.get_cmap(d[\"value\"])\n        if d[\"value\"] and self.enumeration_colors:\n            raise TypeError(\"Style can have a colormap or enumeration_colors, but not both\")\n        return d[\"value\"]\n\n    @tl.validate(\"enumeration_colors\")",
            "def _validate_enumeration_colors(self, d):\n        enum_colors = d[\"value\"]\n        if enum_colors and self.colormap:\n            raise TypeError(\"Style can have a colormap or enumeration_colors, but not both\")\n        return enum_colors\n\n    @tl.validate(\"enumeration_legend\")",
            "def _validate_enumeration_legend(self, d):\n        # validate against enumeration_colors\n        enum_legend = d[\"value\"]\n        if not self.enumeration_colors:\n            raise TypeError(\"Style enumeration_legend requires enumeration_colors\")\n        if set(enum_legend) != set(self.enumeration_colors):\n            raise ValueError(\"Style enumeration_legend keys must match enumeration_colors keys\")\n        return enum_legend\n\n    @property",
            "def full_enumeration_colors(self):",
            "def full_enumeration_legend(self):",
            "def cmap(self):\n        if self.colormap:\n            return matplotlib.cm.get_cmap(self.colormap)\n        elif self.enumeration_colors:\n            return ListedColormap(self.full_enumeration_colors)\n        else:\n            return matplotlib.cm.get_cmap(\"viridis\")\n\n    @property",
            "def json(self):\n        \"\"\"JSON-serialized style definition\n\n        The `json` can be used to create new styles.\n\n        See Also\n        ----------\n        from_json\n        \"\"\"\n\n        return json.dumps(self.definition, separators=(\",\", \":\"), cls=JSONEncoder)\n\n    @classmethod",
            "def get_style_ui(self):\n        \"\"\"\n        Attempting to expose style units to get_ui_spec(). This will grab defaults in general.\n        BUT this will not set defaults for each particular node.\n        \"\"\"\n        d = OrderedDict()\n        if self.name:\n            d[\"name\"] = self.name\n        if self.units:\n            d[\"units\"] = self.units\n        if self.colormap:\n            d[\"colormap\"] = self.colormap\n        if self.enumeration_legend:\n            d[\"enumeration_legend\"] = self.enumeration_legend\n        if self.enumeration_colors:\n            d[\"enumeration_colors\"] = self.enumeration_colors\n        if self.default_enumeration_legend != DEFAULT_ENUMERATION_LEGEND:\n            d[\"default_enumeration_legend\"] = self.default_enumeration_legend\n        if self.default_enumeration_color != DEFAULT_ENUMERATION_COLOR:\n            d[\"default_enumeration_color\"] = self.default_enumeration_color\n        if self.clim != [None, None]:\n            d[\"clim\"] = self.clim\n        return d\n\n    @property",
            "def definition(self):\n        d = OrderedDict()\n        if self.name:\n            d[\"name\"] = self.name\n        if self.units:\n            d[\"units\"] = self.units\n        if self.colormap:\n            d[\"colormap\"] = self.colormap\n        if self.enumeration_legend:\n            d[\"enumeration_legend\"] = self.enumeration_legend\n        if self.enumeration_colors:\n            d[\"enumeration_colors\"] = self.enumeration_colors\n        if self.default_enumeration_legend != DEFAULT_ENUMERATION_LEGEND:\n            d[\"default_enumeration_legend\"] = self.default_enumeration_legend\n        if self.default_enumeration_color != DEFAULT_ENUMERATION_COLOR:\n            d[\"default_enumeration_color\"] = self.default_enumeration_color\n        if self.clim != [None, None]:\n            d[\"clim\"] = self.clim\n        return d\n\n    @classmethod",
            "def from_definition(cls, d):\n        # parse enumeration keys to int\n        if \"enumeration_colors\" in d:\n            d[\"enumeration_colors\"] = {int(key): value for key, value in d[\"enumeration_colors\"].items()}\n        if \"enumeration_legend\" in d:\n            d[\"enumeration_legend\"] = {int(key): value for key, value in d[\"enumeration_legend\"].items()}\n        return cls(**d)\n\n    @classmethod",
            "def from_json(cls, s):\n        \"\"\"Create podpac Style from a style JSON definition.\n\n        Parameters\n        -----------\n        s : str\n            JSON definition\n\n        Returns\n        --------\n        Style\n            podpac Style object\n        \"\"\"\n\n        d = json.loads(s)\n        return cls.from_definition(d)",
            "def __eq__(self, other):\n        if not isinstance(other, Style):\n            return False\n\n        return self.json == other.json"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/units.py",
        "comments": [
            "//xarray.pydata.org/en/stable/generated/xarray.open_dataarray.html#xarray.open_dataarray."
        ],
        "docstrings": [
            "\"\"\"Units module summary\n\nAttributes\n----------\nureg : TYPE\n    Description\n\"\"\"",
            "\"\"\"Like xarray.DataArray, but transfers units\"\"\"",
            "\"\"\"Converts the UnitsDataArray units to the requested unit\n\n        Parameters\n        ----------\n        unit : pint.UnitsRegistry unit\n            The desired unit from podpac.unit\n\n        Returns\n        -------\n        UnitsDataArray\n            The array converted to the desired unit\n\n        Raises\n        --------\n        DimensionalityError\n            If the requested unit is not dimensionally consistent with the original unit.\n        \"\"\"",
            "\"\"\"Converts the UnitsDataArray units to the base SI units.\n\n        Returns\n        -------\n        UnitsDataArray\n            The units data array converted to the base SI units\n        \"\"\"",
            "\"\"\"\n        Helper function for converting Node outputs to alternative formats.\n\n        Parameters\n        -----------\n        format: str\n            Format to which output should be converted. This is uses the to_* functions provided by xarray\n        *args: *list\n            Extra arguments for a particular output function\n        **kwargs: **dict\n            Extra keyword arguments for a particular output function\n\n        Returns\n        --------\n        io.BytesIO()\n            In-memory version of the file or Python object. Note, depending on the input arguments, the file may instead\n            be saved to disk.\n\n        Notes\n        ------\n        This is a helper function for accessing existing to_* methods provided by the base xarray.DataArray object, with\n        a few additional formats supported:\n            * json\n            * png, jpg, jpeg\n            * tiff (GEOtiff)\n        \"\"\"",
            "\"\"\"Return a base64-encoded image of the data.\n\n        Parameters\n        ----------\n        format : str, optional\n            Default is 'png'. Type of image.\n        vmin : number, optional\n            Minimum value of colormap\n        vmax : vmax, optional\n            Maximum value of colormap\n        return_base64: bool, optional\n            Default is False. Normally this returns an io.BytesIO, but if True, will return a base64 encoded string.\n\n\n        Returns\n        -------\n        BytesIO/str\n            Binary or Base64 encoded image.\n        \"\"\"",
            "\"\"\"\n        For documentation, see `core.units.to_geotiff`\n        \"\"\"",
            "\"\"\"Partially transpose the UnitsDataArray based on the input dimensions. The remaining\n        dimensions will have their original order, and will be included at the end of the\n        transpose.\n\n        Parameters\n        ----------\n        new_dims : list\n            List of dimensions in the order they should be transposed\n\n        Returns\n        -------\n        UnitsDataArray\n            The UnitsDataArray transposed according to the user inputs\n        \"\"\"",
            "\"\"\"This function sets the values of the dataarray equal to 'value' where ever mask is True.\n        This operation happens in-place.\n\n        Set the UnitsDataArray data to have a particular value, possibly using a mask\n        in general, want to handle cases where value is a single value, an array,\n        or a UnitsDataArray, and likewise for mask to be None, ndarray, or UnitsDataArray\n        For now, focus on case where value is a single value and mask is a UnitsDataArray\n\n\n\n        Parameters\n        ----------\n        value : Number\n            A constant number that will replace the masked values.\n        mask : UnitsDataArray\n            A UnitsDataArray representing a boolean index.\n\n        Notes\n        ------\n        This function modifies the UnitsDataArray inplace\n        \"\"\"",
            "\"\"\"\n        Open an :class:`podpac.UnitsDataArray` from a file or file-like object containing a single data variable.\n\n        This is a wrapper around :func:`xarray.open_datarray`.\n        The inputs to this function are passed directly to :func:`xarray.open_datarray`.\n        See http://xarray.pydata.org/en/stable/generated/xarray.open_dataarray.html#xarray.open_dataarray.\n\n        The DataArray passed back from :func:`xarray.open_datarray` is used to create a units data array using :func:`creare_dataarray`.\n\n        Returns\n        -------\n        :class:`podpac.UnitsDataArray`\n        \"\"\"",
            "\"\"\"Shortcut to create :class:`podpac.UnitsDataArray`\n\n        Parameters\n        ----------\n        c : :class:`podpac.Coordinates`\n            PODPAC Coordinates\n        data : np.ndarray, optional\n            Data to fill in. Defaults to np.nan.\n        dtype : type, optional\n            Data type. Defaults to float.\n        **kwargs\n            keyword arguments to pass to :class:`podpac.UnitsDataArray` constructor\n\n        Returns\n        -------\n        :class:`podpac.UnitsDataArray`\n        \"\"\"",
            "\"\"\"Return a base64-encoded image of data\n\n    Parameters\n    ----------\n    data : array-like\n        data to output, usually a UnitsDataArray\n    format : str, optional\n        Default is 'png'. Type of image.\n    vmin : number, optional\n        Minimum value of colormap\n    vmax : vmax, optional\n        Maximum value of colormap\n    return_base64: bool, optional\n        Default is False. Normally this returns an io.BytesIO, but if True, will return a base64 encoded string.\n\n\n    Returns\n    -------\n    BytesIO/str\n        Binary or Base64 encoded image.\n    \"\"\"",
            "\"\"\"Export a UnitsDataArray to a Geotiff\n\n    Params\n    -------\n    fp:  str, file object or pathlib.Path object\n        A filename or URL, a file object opened in binary ('rb') mode, or a Path object. If not supplied, the results will\n        be written to a memfile object\n    data: UnitsDataArray, xr.DataArray, np.ndarray\n        The data to be saved. If there is more than 1 band, this should be the last dimension of the array.\n        If given a np.ndarray, ensure that the 'lat' dimension is aligned with the rows of the data, with an appropriate\n        geotransform.\n    geotransform: tuple, optional\n        The geotransform that describes the input data. If not given, will look for data.attrs['geotransform']\n    crs: str, optional\n        The coordinate reference system for the data\n    kwargs: **dict\n        Additional key-word arguments that overwrite defaults used in the `rasterio.open` function. This function\n        populates the following defaults:\n                drive=\"GTiff\"\n                height=data.shape[0]\n                width=data.shape[1]\n                count=data.shape[2]\n                dtype=data.dtype\n                mode=\"w\"\n\n    Returns\n    --------\n    MemoryFile, list\n        If fp is given, results a list of the results for writing to each band r.append(dst.write(data[..., i], i + 1))\n        If fp is None, returns the MemoryFile object\n    \"\"\""
        ],
        "code_snippets": [
            "class UnitsDataArray(xr.DataArray):",
            "def __init__(self, *args, **kwargs):\n        super(UnitsDataArray, self).__init__(*args, **kwargs)\n        self = self._pp_deserialize()",
            "def __array_wrap__(self, obj, context=None):\n        new_var = super(UnitsDataArray, self).__array_wrap__(obj, context)\n        if self.attrs.get(\"units\"):\n            if context and settings[\"ENABLE_UNITS\"]:\n                new_var.attrs[\"units\"] = context[0](ureg.Quantity(1, self.attrs.get(\"units\"))).u\n            elif settings[\"ENABLE_UNITS\"]:\n                new_var = self._copy_units(new_var)\n        return new_var",
            "def _apply_binary_op_to_units(self, func, other, x):\n        if (self.attrs.get(\"units\", None) or getattr(other, \"units\", None)) and settings[\"ENABLE_UNITS\"]:\n            x.attrs[\"units\"] = func(\n                ureg.Quantity(1, getattr(self, \"units\", \"1\")), ureg.Quantity(1, getattr(other, \"units\", \"1\"))\n            ).u\n        return x",
            "def _get_unit_multiplier(self, other):\n        multiplier = 1\n        if (self.attrs.get(\"units\", None) or getattr(other, \"units\", None)) and settings[\"ENABLE_UNITS\"]:\n            otheru = ureg.Quantity(1, getattr(other, \"units\", \"1\"))\n            myu = ureg.Quantity(1, getattr(self, \"units\", \"1\"))\n            multiplier = otheru.to(myu.u).magnitude\n        return multiplier\n\n    # pow is different because resulting unit depends on argument, not on\n    # unit of argument (which must be unitless)",
            "def __pow__(self, other):\n        x = super(UnitsDataArray, self).__pow__(other)\n        if self.attrs.get(\"units\") and settings[\"ENABLE_UNITS\"]:\n            x.attrs[\"units\"] = pow(\n                ureg.Quantity(1, getattr(self, \"units\", \"1\")), ureg.Quantity(other, getattr(other, \"units\", \"1\"))\n            ).u\n        return x",
            "def _copy_units(self, x):\n        if self.attrs.get(\"units\", None):\n            x.attrs[\"units\"] = self.attrs.get(\"units\")\n        return x",
            "def to(self, unit):\n        \"\"\"Converts the UnitsDataArray units to the requested unit\n\n        Parameters\n        ----------\n        unit : pint.UnitsRegistry unit\n            The desired unit from podpac.unit\n\n        Returns\n        -------\n        UnitsDataArray\n            The array converted to the desired unit\n\n        Raises\n        --------\n        DimensionalityError\n            If the requested unit is not dimensionally consistent with the original unit.\n        \"\"\"\n        x = self.copy()\n        if self.attrs.get(\"units\", None):\n            myu = ureg.Quantity(1, getattr(self, \"units\", \"1\"))\n            multiplier = myu.to(unit).magnitude\n            x = x * multiplier\n            x.attrs[\"units\"] = unit\n        return x",
            "def to_base_units(self):\n        \"\"\"Converts the UnitsDataArray units to the base SI units.\n\n        Returns\n        -------\n        UnitsDataArray\n            The units data array converted to the base SI units\n        \"\"\"\n        if self.attrs.get(\"units\", None):\n            myu = ureg.Quantity(1, getattr(self, \"units\", \"1\")).to_base_units()\n            return self.to(myu.u)\n        else:\n            return self.copy()",
            "def to_netcdf(self, *args, **kwargs):\n        o = self\n        for d in self.dims:\n            if \"_\" in d and \"dim\" not in d:  # This it is stacked\n                try:\n                    o = o.reset_index(d)\n                except KeyError:\n                    pass  # This is fine, actually didn't need to reset because not a real dim\n        o._pp_serialize()\n        r = super(UnitsDataArray, o).to_netcdf(*args, **kwargs)\n        self._pp_deserialize()\n        return r",
            "def to_format(self, format, *args, **kwargs):\n        \"\"\"\n        Helper function for converting Node outputs to alternative formats.\n\n        Parameters\n        -----------\n        format: str\n            Format to which output should be converted. This is uses the to_* functions provided by xarray\n        *args: *list\n            Extra arguments for a particular output function\n        **kwargs: **dict\n            Extra keyword arguments for a particular output function\n\n        Returns\n        --------\n        io.BytesIO()\n            In-memory version of the file or Python object. Note, depending on the input arguments, the file may instead\n            be saved to disk.\n\n        Notes\n        ------\n        This is a helper function for accessing existing to_* methods provided by the base xarray.DataArray object, with\n        a few additional formats supported:\n            * json\n            * png, jpg, jpeg\n            * tiff (GEOtiff)\n        \"\"\"\n        self._pp_serialize()\n        if format in [\"netcdf\", \"nc\", \"hdf5\", \"hdf\"]:\n            r = self.to_netcdf(*args, **kwargs)\n        elif format in [\"json\", \"dict\"]:\n            r = self.to_dict()\n            if format == \"json\":\n                r = json.dumps(r, cls=JSONEncoder)\n        elif format in [\"png\", \"jpg\", \"jpeg\"]:\n            r = self.to_image(format, *args, **kwargs)\n        elif format.upper() in [\"TIFF\", \"TIF\", \"GEOTIFF\"]:\n            r = self.to_geotiff(*args, **kwargs)\n\n        elif format in [\"pickle\", \"pkl\"]:\n            r = cPickle.dumps(self)\n        elif format == \"zarr_part\":\n            from podpac.core.data.zarr_source import Zarr\n            import zarr\n\n            if \"part\" in kwargs:\n                part = kwargs.pop(\"part\")\n                part = tuple([slice(*sss) for sss in part])\n            else:\n                part = slice(None)\n\n            zn = Zarr(source=kwargs.pop(\"source\"))\n            store = zn._get_store()\n\n            zf = zarr.open(store, *args, **kwargs)\n\n            if \"output\" in self.dims:\n                for key in self.coords[\"output\"].data:\n                    zf[key][part] = self.sel(output=key).data\n            else:\n                data_key = kwargs.get(\"data_key\", \"data\")\n                zf[data_key][part] = self.data\n            r = zn.source\n        else:\n            try:\n                getattr(self, \"to_\" + format)(*args, **kwargs)\n            except:\n                raise NotImplementedError(\"Format {} is not implemented.\".format(format))\n        self._pp_deserialize()\n        return r",
            "def to_image(self, format=\"png\", vmin=None, vmax=None, return_base64=False):\n        \"\"\"Return a base64-encoded image of the data.\n\n        Parameters\n        ----------\n        format : str, optional\n            Default is 'png'. Type of image.\n        vmin : number, optional\n            Minimum value of colormap\n        vmax : vmax, optional\n            Maximum value of colormap\n        return_base64: bool, optional\n            Default is False. Normally this returns an io.BytesIO, but if True, will return a base64 encoded string.\n\n\n        Returns\n        -------\n        BytesIO/str\n            Binary or Base64 encoded image.\n        \"\"\"\n        return to_image(self, format, vmin, vmax, return_base64)",
            "def to_geotiff(self, fp=None, geotransform=None, crs=None, **kwargs):\n        \"\"\"\n        For documentation, see `core.units.to_geotiff`\n        \"\"\"\n        return to_geotiff(fp, self, geotransform=geotransform, crs=crs, **kwargs)",
            "def _pp_serialize(self):\n        if self.attrs.get(\"units\"):\n            self.attrs[\"units\"] = str(self.attrs[\"units\"])\n        if self.attrs.get(\"layer_style\") and not isinstance(self.attrs[\"layer_style\"], string_types):\n            self.attrs[\"layer_style\"] = self.attrs[\"layer_style\"].json\n        if self.attrs.get(\"bounds\"):\n            if isinstance(self.attrs[\"bounds\"], dict) and \"time\" in self.attrs[\"bounds\"]:\n                time_bounds = self.attrs[\"bounds\"][\"time\"]\n                new_bounds = []\n                for tb in time_bounds:\n                    if isinstance(tb, np.datetime64):\n                        new_bounds.append(str(tb))\n                    else:\n                        new_bounds.append(tb)\n                self.attrs[\"bounds\"][\"time\"] = tuple(new_bounds)\n            self.attrs[\"bounds\"] = json.dumps(self.attrs[\"bounds\"])\n        if self.attrs.get(\"boundary_data\") is not None and not isinstance(self.attrs[\"boundary_data\"], string_types):\n            self.attrs[\"boundary_data\"] = json.dumps(self.attrs[\"boundary_data\"])",
            "def _pp_deserialize(self):\n        # Deserialize units\n        if self.attrs.get(\"units\") and isinstance(self.attrs[\"units\"], string_types):\n            self.attrs[\"units\"] = ureg(self.attrs[\"units\"]).u\n\n        # Deserialize layer_stylers\n        if self.attrs.get(\"layer_style\") and isinstance(self.attrs[\"layer_style\"], string_types):\n            self.attrs[\"layer_style\"] = podpac.core.style.Style.from_json(self.attrs[\"layer_style\"])\n\n        if self.attrs.get(\"bounds\") and isinstance(self.attrs[\"bounds\"], string_types):\n            self.attrs[\"bounds\"] = json.loads(self.attrs[\"bounds\"])\n            if \"time\" in self.attrs[\"bounds\"]:\n                time_bounds = self.attrs[\"bounds\"][\"time\"]\n                new_bounds = []\n                for tb in time_bounds:\n                    if isinstance(tb, string_types):\n                        new_bounds.append(np.datetime64(tb))\n                    else:\n                        new_bounds.append(tb)\n                self.attrs[\"bounds\"][\"time\"] = tuple(new_bounds)\n        if self.attrs.get(\"boundary_data\") and isinstance(self.attrs[\"boundary_data\"], string_types):\n            self.attrs[\"boundary_data\"] = json.loads(self.attrs[\"boundary_data\"])\n\n        # Deserialize the multi-index\n        for dim in self.dims:\n            if dim in self.coords or \"-\" in dim:  # The \"-\" is for multi-dimensional stacked coordinates\n                continue\n            try:\n                self = self.set_index(**{dim: dim.split(\"-\")[0].split(\"_\")})\n            except ValueError as e:\n                _logger.warning(\"Tried to rebuild stacked coordinates but failed with error: {}\".format(e))\n        return self",
            "def __getitem__(self, key):\n        # special cases when key is also a DataArray\n        # and has only one dimension\n        if isinstance(key, xr.DataArray) and len(key.dims) == 1:\n            # transpose with shared dims first\n            shared_dims = [dim for dim in self.dims if dim in key.dims]\n            missing_dims = [dim for dim in self.dims if dim not in key.dims]\n            xT = self.transpose(*shared_dims + missing_dims)\n\n            # index\n            outT = xT[key.data]\n\n            # transpose back to original dimensions\n            out = outT.transpose(*self.dims)\n            return out\n\n        return super(UnitsDataArray, self).__getitem__(key)\n\n    #",
            "def reduce(self, func, *args, **kwargs):\n    #         new_var = super(UnitsDataArray, self).reduce(func, *args, **kwargs)\n    #         if self.attrs.get(\"units\", None):\n    #            new_var.attrs['units'] = self.units\n    #         return new_var",
            "def part_transpose(self, new_dims):\n        \"\"\"Partially transpose the UnitsDataArray based on the input dimensions. The remaining\n        dimensions will have their original order, and will be included at the end of the\n        transpose.\n\n        Parameters\n        ----------\n        new_dims : list\n            List of dimensions in the order they should be transposed\n\n        Returns\n        -------\n        UnitsDataArray\n            The UnitsDataArray transposed according to the user inputs\n        \"\"\"\n        shared_dims = [dim for dim in new_dims if dim in self.dims]\n        self_only_dims = [dim for dim in self.dims if dim not in new_dims]\n\n        return self.transpose(*shared_dims + self_only_dims)",
            "def set(self, value, mask):\n        \"\"\"This function sets the values of the dataarray equal to 'value' where ever mask is True.\n        This operation happens in-place.\n\n        Set the UnitsDataArray data to have a particular value, possibly using a mask\n        in general, want to handle cases where value is a single value, an array,\n        or a UnitsDataArray, and likewise for mask to be None, ndarray, or UnitsDataArray\n        For now, focus on case where value is a single value and mask is a UnitsDataArray\n\n\n\n        Parameters\n        ----------\n        value : Number\n            A constant number that will replace the masked values.\n        mask : UnitsDataArray\n            A UnitsDataArray representing a boolean index.\n\n        Notes\n        ------\n        This function modifies the UnitsDataArray inplace\n        \"\"\"\n\n        if isinstance(mask, xr.DataArray) and isinstance(value, Number):\n            orig_dims = deepcopy(self.dims)\n\n            # find out status of all dims\n            shared_dims = [dim for dim in mask.dims if dim in self.dims]\n            self_only_dims = [dim for dim in self.dims if dim not in mask.dims]\n            mask_only_dims = [dim for dim in mask.dims if dim not in self.dims]\n\n            # don't handle case where there are mask_only_dims\n            if len(mask_only_dims) > 0:\n                return\n\n            # transpose self to have same order of dims as mask so those shared dims\n            # come first and in the same order in both cases\n            self = self.transpose(*shared_dims + self_only_dims)\n\n            # set the values approved by ok_mask to be value\n            self.values[mask.values, ...] = value\n\n            # set self to have the same dims (and same order) as when first started\n            self = self.transpose(*orig_dims)\n\n    @classmethod",
            "def open(cls, *args, **kwargs):\n        \"\"\"\n        Open an :class:`podpac.UnitsDataArray` from a file or file-like object containing a single data variable.\n\n        This is a wrapper around :func:`xarray.open_datarray`.\n        The inputs to this function are passed directly to :func:`xarray.open_datarray`.\n        See http:",
            "def create(cls, c, data=np.nan, outputs=None, dtype=float, **kwargs):\n        \"\"\"Shortcut to create :class:`podpac.UnitsDataArray`\n\n        Parameters\n        ----------\n        c : :class:`podpac.Coordinates`\n            PODPAC Coordinates\n        data : np.ndarray, optional\n            Data to fill in. Defaults to np.nan.\n        dtype : type, optional\n            Data type. Defaults to float.\n        **kwargs\n            keyword arguments to pass to :class:`podpac.UnitsDataArray` constructor\n\n        Returns\n        -------\n        :class:`podpac.UnitsDataArray`\n        \"\"\"\n        if not isinstance(c, podpac.Coordinates):\n            raise TypeError(\"`UnitsDataArray.create` expected Coordinates object, not '%s'\" % type(c))\n\n        # data array\n        if np.shape(data) == ():\n            shape = c.shape\n            if outputs is not None:\n                shape = shape + (len(outputs),)\n\n            if data is None:\n                data = np.empty(shape, dtype=dtype)\n            elif data == 0:\n                data = np.zeros(shape, dtype=dtype)\n            elif data == 1:\n                data = np.ones(shape, dtype=dtype)\n            else:\n                data = np.full(shape, data, dtype=dtype)\n        else:\n            if outputs is not None and len(outputs) != data.shape[-1]:\n                raise ValueError(\n                    \"data with shape %s does not match provided outputs %s (%d != %d)\"\n                    % (data.shape, outputs, data.shape[-1], len(outputs))\n                )\n            data = data.astype(dtype)\n\n        # coords and dims\n        coords = c.xcoords\n        dims = c.xdims\n\n        if outputs is not None:\n            dims = dims + (\"output\",)\n            coords[\"output\"] = outputs\n\n        # crs attr\n        if \"attrs\" in kwargs:\n            if \"crs\" not in kwargs[\"attrs\"]:\n                kwargs[\"attrs\"][\"crs\"] = c.crs\n        else:\n            kwargs[\"attrs\"] = {\"crs\": c.crs}\n\n        return cls(data, coords=coords, dims=dims, **kwargs)\n\n\nfor tp in (\"mul\", \"matmul\", \"truediv\", \"div\"):\n    meth = \"__{:s}__\".format(tp)",
            "def make_func(meth, tp):",
            "def func(self, other):\n            x = getattr(super(UnitsDataArray, self), meth)(other)\n            x2 = self._apply_binary_op_to_units(getattr(operator, tp), other, x)\n            units = x2.attrs.get(\"units\")\n            x2.attrs = self.attrs\n            if units is not None:\n                x2.attrs[\"units\"] = units\n            return x2\n\n        return func\n\n    func = make_func(meth, tp)\n    func.__name__ = meth\n    setattr(UnitsDataArray, meth, func)\n\n\nfor tp in (\"add\", \"sub\", \"mod\", \"floordiv\"):  # , \"divmod\", ):\n    meth = \"__{:s}__\".format(tp)",
            "def make_func(meth, tp):",
            "def func(self, other):\n            multiplier = self._get_unit_multiplier(other)\n            x = getattr(super(UnitsDataArray, self), meth)(other * multiplier)\n            x2 = self._apply_binary_op_to_units(getattr(operator, tp), other, x)\n            x2.attrs = self.attrs\n            return x2\n\n        return func\n\n    func = make_func(meth, tp)\n    func.__name__ = meth\n    setattr(UnitsDataArray, meth, func)\n\n\nfor tp in (\"lt\", \"le\", \"eq\", \"ne\", \"gt\", \"ge\"):\n    meth = \"__{:s}__\".format(tp)",
            "def make_func(meth):",
            "def func(self, other):\n            multiplier = self._get_unit_multiplier(other)\n            return getattr(super(UnitsDataArray, self), meth)(other * multiplier)\n\n        return func\n\n    func = make_func(meth)\n    func.__name__ = meth\n    setattr(UnitsDataArray, meth, func)\n\n\nfor tp in (\"mean\", \"min\", \"max\", \"sum\", \"cumsum\"):",
            "def make_func(tp):",
            "def func(self, *args, **kwargs):\n            x = getattr(super(UnitsDataArray, self), tp)(*args, **kwargs)\n            return self._copy_units(x)\n\n        return func\n\n    func = make_func(tp)\n    func.__name__ = tp\n    setattr(UnitsDataArray, tp, func)\n\ndel func",
            "def to_image(data, format=\"png\", vmin=None, vmax=None, return_base64=False):\n    \"\"\"Return a base64-encoded image of data\n\n    Parameters\n    ----------\n    data : array-like\n        data to output, usually a UnitsDataArray\n    format : str, optional\n        Default is 'png'. Type of image.\n    vmin : number, optional\n        Minimum value of colormap\n    vmax : vmax, optional\n        Maximum value of colormap\n    return_base64: bool, optional\n        Default is False. Normally this returns an io.BytesIO, but if True, will return a base64 encoded string.\n\n\n    Returns\n    -------\n    BytesIO/str\n        Binary or Base64 encoded image.\n    \"\"\"\n\n    import matplotlib\n    import matplotlib.cm\n    from matplotlib.image import imsave\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        matplotlib.use(\"agg\")\n\n    if format != \"png\":\n        raise ValueError(\"Invalid image format '%s', must be 'png'\" % format)\n\n    style = None\n    if isinstance(data, xr.DataArray):\n        style = data.attrs.get(\"layer_style\", None)\n        if isinstance(style, string_types):\n            style = Style.from_json(style)\n        dims = data.squeeze().dims\n        y = data.coords[dims[0]]\n        x = data.coords[dims[1]]\n        data = data.data\n        if y[1] > y[0]:\n            data = data[::-1, :]\n        if x[1] < x[0]:\n            data = data[:, ::1]\n\n    data = data.squeeze()\n\n    if not np.any(np.isfinite(data)):\n        vmin = 0\n        vmax = 1\n    else:\n        if vmin is None or np.isnan(vmin):\n            if style is not None and style.clim[0] != None:\n                vmin = style.clim[0]\n            else:\n                vmin = np.nanmin(data)\n        if vmax is None or np.isnan(vmax):\n            if style is not None and style.clim[1] != None:\n                vmax = style.clim[1]\n            else:\n                vmax = np.nanmax(data)\n    if vmax == vmin:\n        vmax += 1e-15\n\n    # get the colormap\n    if style is None:\n        cmap = matplotlib.cm.viridis\n    else:\n        cmap = style.cmap\n\n    c = (data - vmin) / (vmax - vmin)\n    i = cmap(c, bytes=True)\n    i[np.isnan(c), 3] = 0\n    im_data = BytesIO()\n    imsave(im_data, i, format=format)\n    im_data.seek(0)\n    if return_base64:\n        return base64.b64encode(im_data.getvalue())\n    else:\n        return im_data",
            "def to_geotiff(fp, data, geotransform=None, crs=None, **kwargs):\n    \"\"\"Export a UnitsDataArray to a Geotiff\n\n    Params\n    -------\n    fp:  str, file object or pathlib.Path object\n        A filename or URL, a file object opened in binary ('rb') mode, or a Path object. If not supplied, the results will\n        be written to a memfile object\n    data: UnitsDataArray, xr.DataArray, np.ndarray\n        The data to be saved. If there is more than 1 band, this should be the last dimension of the array.\n        If given a np.ndarray, ensure that the 'lat' dimension is aligned with the rows of the data, with an appropriate\n        geotransform.\n    geotransform: tuple, optional\n        The geotransform that describes the input data. If not given, will look for data.attrs['geotransform']\n    crs: str, optional\n        The coordinate reference system for the data\n    kwargs: **dict\n        Additional key-word arguments that overwrite defaults used in the `rasterio.open` function. This function\n        populates the following defaults:\n                drive=\"GTiff\"\n                height=data.shape[0]\n                width=data.shape[1]\n                count=data.shape[2]\n                dtype=data.dtype\n                mode=\"w\"\n\n    Returns\n    --------\n    MemoryFile, list\n        If fp is given, results a list of the results for writing to each band r.append(dst.write(data[..., i], i + 1))\n        If fp is None, returns the MemoryFile object\n    \"\"\"\n\n    # This only works for data that essentially has lat/lon only\n    dims = list(data.coords.keys())\n    if \"lat\" not in dims or \"lon\" not in dims:\n        raise NotImplementedError(\"Cannot export GeoTIFF for dataset with lat/lon coordinates.\")\n    if \"time\" in dims and len(data.coords[\"time\"]) > 1:\n        raise NotImplemented(\"Cannot export GeoTIFF for dataset with multiple times,\")\n    if \"alt\" in dims and len(data.coords[\"alt\"]) > 1:\n        raise NotImplemented(\"Cannot export GeoTIFF for dataset with multiple altitudes.\")\n\n    # TODO: add proper checks, etc. to make sure we handle edge cases and throw errors when we cannot support\n    #       i.e. do work to remove this warning.\n    _logger.warning(\"GeoTIFF export assumes data is in a uniform, non-rotated coordinate system.\")\n\n    # Get the crs and geotransform that describes the coordinates\n    if crs is None:\n        crs = data.attrs.get(\"crs\")\n    if crs is None:\n        raise ValueError(\n            \"The `crs` of the data needs to be provided to save as GeoTIFF. If supplying a UnitsDataArray, created \"\n            \" through a PODPAC Node, the crs should be automatically populated. If not, please file an issue.\"\n        )\n    if geotransform is None:\n        geotransform = data.attrs.get(\"geotransform\")\n        # Geotransform should ALWAYS be defined as (lon_origin, lon_dj, lon_di, lat_origin, lat_dj, lat_di)\n        # if isinstance(data, xr.DataArray) and data.dims.index('lat') > data.dims.index('lon'):\n        # geotransform = geotransform[3:] + geotransform[:3]\n\n    if geotransform is None:\n        try:\n            geotransform = Coordinates.from_xarray(data).geotransform\n        except (TypeError, AttributeError):\n            raise ValueError(\n                \"The `geotransform` of the data needs to be provided to save as GeoTIFF. If the geotransform attribute \"\n                \"wasn't automatically populated as part of the dataset, it means that the data is in a non-uniform \"\n                \"coordinate system. This can sometimes happen when the data is transformed to a different CRS than the \"\n                \"native CRS, which can cause the coordinates to seems non-uniform due to floating point precision. \"\n            )\n\n    # Make all types into a numpy array\n    if isinstance(data, xr.DataArray):\n        data = data.data\n\n    # Get the data\n    dtype = kwargs.get(\"dtype\", np.float32)\n    data = data.astype(dtype).squeeze()\n\n    if len(data.shape) == 2:\n        data = data[:, :, None]\n\n    geotransform = affine.Affine.from_gdal(*geotransform)\n\n    # Update the kwargs that rasterio will use. Anything added by the user will take priority.\n    kwargs2 = dict(\n        driver=\"GTiff\",\n        height=data.shape[0],\n        width=data.shape[1],\n        count=data.shape[2],\n        dtype=data.dtype,\n        crs=crs,\n        transform=geotransform,\n    )\n    kwargs2.update(kwargs)\n\n    # Write the file\n    if fp is None:\n        # Write to memory file\n        r = rasterio.io.MemoryFile()\n        with r.open(**kwargs2) as dst:\n            for i in range(data.shape[2]):\n                dst.write(data[..., i], i + 1)\n    else:\n        r = []\n        kwargs2[\"mode\"] = \"w\"\n        with rasterio.open(fp, **kwargs2) as dst:\n            for i in range(data.shape[2]):\n                r.append(dst.write(data[..., i], i + 1))\n\n    return r"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "def lazy_module(modname, *args, **kwargs):\n        # Python 2 complains about unicode strings, so we turn the modname into a str\n        return lazy_import._old_lazy_module(str(modname), *args, **kwargs)\n\n    # Patch\n    lazy_import.lazy_module = lazy_module\ndel sys\n\nrequests = lazy_import.lazy_module(\"requests\")\npint = lazy_import.lazy_module(\"pint\")\nmatplotlib = lazy_import.lazy_module(\"matplotlib\")\nplt = lazy_import.lazy_module(\"matplotlib.pyplot\")\nnp = lazy_import.lazy_module(\"numpy\")\ntl = lazy_import.lazy_module(\"traitlets\")\n# xr = lazy_import.lazy_module(\"xarray\")"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/authentication.py",
        "comments": [
            "//2.python-requests.org/en/master/api/#sessionapi"
        ],
        "docstrings": [
            "\"\"\"\nPODPAC Authentication\n\"\"\"",
            "\"\"\"Set authentication credentials for a remote URL in the :class:`podpac.settings`.\n\n    Parameters\n    ----------\n    hostname : str\n        Hostname for `uname` and `password`.\n    uname : str, optional\n        Username to store in settings for `hostname`.\n        If no username is provided and the username does not already exist in the settings,\n        the user will be prompted to enter one.\n    password : str, optional\n        Password to store in settings for `hostname`\n        If no password is provided and the password does not already exist in the settings,\n        the user will be prompted to enter one.\n    \"\"\"",
            "\"\"\"Returns username stored in settings for accessing `self.hostname`.\n        The username is stored under key `username@<hostname>`\n\n        Returns\n        -------\n        str\n            username stored in settings for accessing `self.hostname`\n\n        Raises\n        ------\n        ValueError\n            Raises a ValueError if not username is stored in settings for `self.hostname`\n        \"\"\"",
            "\"\"\"Returns password stored in settings for accessing `self.hostname`.\n        The password is stored under key `password@<hostname>`\n\n        Returns\n        -------\n        str\n            password stored in settings for accessing `self.hostname`\n\n        Raises\n        ------\n        ValueError\n            Raises a ValueError if not password is stored in settings for `self.hostname`\n        \"\"\"",
            "\"\"\"Requests Session object for making calls to remote `self.hostname`\n        See https://2.python-requests.org/en/master/api/#sessionapi\n\n        Returns\n        -------\n        :class:requests.Session\n            Requests Session class with `auth` attribute defined\n        \"\"\"",
            "\"\"\"Shortcut to :func:`podpac.authentication.set_crendentials` using class member :attr:`self.hostname` for the hostname\n\n        Parameters\n        ----------\n        username : str, optional\n            Username to store in settings for `self.hostname`.\n            If no username is provided and the username does not already exist in the settings,\n            the user will be prompted to enter one.\n        password : str, optional\n            Password to store in settings for `self.hostname`\n            If no password is provided and the password does not already exist in the settings,\n            the user will be prompted to enter one.\n        \"\"\"",
            "\"\"\"Creates a :class:`requests.Session` with username and password defined\n\n        Returns\n        -------\n        :class:`requests.Session`\n        \"\"\"",
            "\"\"\"Creates an authenticated :class:`requests.Session` with username and password defined\n\n        Returns\n        -------\n        :class:`requests.Session`\n\n        Notes\n        -----\n        The session is authenticated against the user-provided self.check_url\n        \"\"\"",
            "\"\"\"Mixin to add S3 credentials and access to a Node.\"\"\""
        ],
        "code_snippets": [
            "def set_credentials(hostname, uname=None, password=None):\n    \"\"\"Set authentication credentials for a remote URL in the :class:`podpac.settings`.\n\n    Parameters\n    ----------\n    hostname : str\n        Hostname for `uname` and `password`.\n    uname : str, optional\n        Username to store in settings for `hostname`.\n        If no username is provided and the username does not already exist in the settings,\n        the user will be prompted to enter one.\n    password : str, optional\n        Password to store in settings for `hostname`\n        If no password is provided and the password does not already exist in the settings,\n        the user will be prompted to enter one.\n    \"\"\"\n\n    if hostname is None or hostname == \"\":\n        raise ValueError(\"`hostname` must be defined\")\n\n    # see whats stored in settings already\n    u_settings = settings.get(\"username@{}\".format(hostname))\n    p_settings = settings.get(\"password@{}\".format(hostname))\n\n    # get username from 1. function input 2. settings 3. python input()\n    u = uname or u_settings or getpass.getpass(\"Username: \")\n    p = password or p_settings or getpass.getpass()\n\n    # set values in settings\n    settings[\"username@{}\".format(hostname)] = u\n    settings[\"password@{}\".format(hostname)] = p\n\n    _log.debug(\"Set credentials for hostname {}\".format(hostname))",
            "class RequestsSessionMixin(tl.HasTraits):\n    hostname = tl.Unicode(allow_none=False)\n    auth_required = tl.Bool(default_value=False)\n\n    @property",
            "def username(self):\n        \"\"\"Returns username stored in settings for accessing `self.hostname`.\n        The username is stored under key `username@<hostname>`\n\n        Returns\n        -------\n        str\n            username stored in settings for accessing `self.hostname`\n\n        Raises\n        ------\n        ValueError\n            Raises a ValueError if not username is stored in settings for `self.hostname`\n        \"\"\"\n        key = \"username@{}\".format(self.hostname)\n        username = settings.get(key)\n        if not username:\n            raise ValueError(\n                \"No username found for hostname '{0}'. Use `{1}.set_credentials(username='<username>', password='<password>') to store credentials for this host\".format(\n                    self.hostname, self.__class__.__name__\n                )\n            )\n\n        return username\n\n    @property",
            "def password(self):\n        \"\"\"Returns password stored in settings for accessing `self.hostname`.\n        The password is stored under key `password@<hostname>`\n\n        Returns\n        -------\n        str\n            password stored in settings for accessing `self.hostname`\n\n        Raises\n        ------\n        ValueError\n            Raises a ValueError if not password is stored in settings for `self.hostname`\n        \"\"\"\n        key = \"password@{}\".format(self.hostname)\n        password = settings.get(key)\n        if not password:\n            raise ValueError(\n                \"No password found for hostname {0}. Use `{1}.set_credentials(username='<username>', password='<password>') to store credentials for this host\".format(\n                    self.hostname, self.__class__.__name__\n                )\n            )\n\n        return password\n\n    @cached_property\n    def session(self):\n        \"\"\"Requests Session object for making calls to remote `self.hostname`\n        See https:",
            "def session(self):\n        \"\"\"Requests Session object for making calls to remote `self.hostname`\n        See https:",
            "def set_credentials(self, username=None, password=None):\n        \"\"\"Shortcut to :func:`podpac.authentication.set_crendentials` using",
            "class member :attr:`self.hostname` for the hostname\n\n        Parameters\n        ----------\n        username : str, optional\n            Username to store in settings for `self.hostname`.\n            If no username is provided and the username does not already exist in the settings,\n            the user will be prompted to enter one.\n        password : str, optional\n            Password to store in settings for `self.hostname`\n            If no password is provided and the password does not already exist in the settings,\n            the user will be prompted to enter one.\n        \"\"\"\n        return set_credentials(self.hostname, uname=username, password=password)",
            "def _create_session(self):\n        \"\"\"Creates a :class:`requests.Session` with username and password defined\n\n        Returns\n        -------\n        :class:`requests.Session`\n        \"\"\"\n        s = requests.Session()\n\n        try:\n            s.auth = (self.username, self.password)\n        except ValueError as e:\n            if self.auth_required:\n                raise e\n            else:\n                _log.warning(\"No auth provided for session\")\n\n        return s",
            "class NASAURSSessionMixin(RequestsSessionMixin):\n    check_url = tl.Unicode()\n    hostname = tl.Unicode(default_value=\"urs.earthdata.nasa.gov\")\n    auth_required = tl.Bool(True)",
            "def _create_session(self):\n        \"\"\"Creates an authenticated :class:`requests.Session` with username and password defined\n\n        Returns\n        -------\n        :class:`requests.Session`\n\n        Notes\n        -----\n        The session is authenticated against the user-provided self.check_url\n        \"\"\"\n\n        try:\n            s = pydap_setup_session(self.username, self.password, check_url=self.check_url)\n        except ValueError as e:\n            if self.auth_required:\n                raise e\n            else:\n                _log.warning(\"No auth provided for session\")\n\n        return s",
            "class S3Mixin(tl.HasTraits):",
            "def _get_access_key_id(self):\n        return settings[\"AWS_ACCESS_KEY_ID\"]\n\n    @tl.default(\"aws_secret_access_key\")",
            "def _get_secret_access_key(self):\n        return settings[\"AWS_SECRET_ACCESS_KEY\"]\n\n    @tl.default(\"aws_region_name\")",
            "def _get_region_name(self):\n        return settings[\"AWS_REGION_NAME\"]\n\n    @tl.default(\"aws_requester_pays\")",
            "def _get_requester_pays(self):\n        return settings[\"AWS_REQUESTER_PAYS\"]\n\n    @cached_property",
            "def s3(self):\n        # this has to be done here for multithreading to work\n        s3fs = lazy_module(\"s3fs\")\n\n        if self.anon:\n            return s3fs.S3FileSystem(anon=True, client_kwargs=self.aws_client_kwargs)\n        else:\n            return s3fs.S3FileSystem(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                client_kwargs=self.aws_client_kwargs,\n                config_kwargs=self.config_kwargs,\n                requester_pays=self.aws_requester_pays,\n            )"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/utils.py",
        "comments": [
            "//docs.python.org/3/library/logging.html#levels",
            "//docs.python.org/3/library/logging.html#logrecord-attributes",
            "//docs.python.org/3/library/logging.html#logrecord-attributes",
            "//2.python-requests.org/en/master/api/#requests.Response.text"
        ],
        "docstrings": [
            "\"\"\"\nUtils Summary\n\"\"\"",
            "\"\"\"Decorator: replaces commond fields in a function docstring\n\n    Parameters\n    -----------\n    doc_dict : dict\n        Dictionary of parameters that will be used to format a doctring. e.g. func.__doc__.format(**doc_dict)\n    \"\"\"",
            "\"\"\"Utility method to determine if trait is defined on object without\n    call to default (@tl.default)\n\n    Parameters\n    ----------\n    object : object\n        Class with traits\n    trait_name : str\n        Class property to investigate\n\n    Returns\n    -------\n    bool\n        True if the trait exists on the object and is defined\n        False if the trait does not exist on the object or the trait is not defined\n    \"\"\"",
            "\"\"\"Convience method to create a log file that only logs\n    podpac related messages\n\n    Parameters\n    ----------\n    filename : str, optional\n        Filename of the log file. Defaults to ``podpac.log``\n    level : int, optional\n        Log level to use (0 - 50). Defaults to ``logging.INFO`` (20)\n        See https://docs.python.org/3/library/logging.html#levels\n    format : str, optional\n        String format for log messages.\n        See https://docs.python.org/3/library/logging.html#logrecord-attributes\n        for creating format. Default is:\n        format='[%(asctime)s] %(name)s.%(funcName)s[%(lineno)d] - %(levelname)s - %(message)s'\n\n    Returns\n    -------\n    logging.Logger, logging.Handler, logging.Formatter\n        Returns the constructed logger, handler, and formatter for the log file\n    \"\"\"",
            "\"\"\"OrderedDict trait\"\"\"",
            "\"\"\"A coercing numpy array trait.\"\"\"",
            "\"\"\"An instance of a Python tuple that accepts the 'trait' argument (like Set, List, and Dict).\"\"\"",
            "\"\"\"Helper function to get data from an url with error checking.\n\n    Parameters\n    ----------\n    url : str\n        URL to website\n    session : :class:`requests.Session`, optional\n        Requests session to use when making the GET request to `url`\n\n    Returns\n    -------\n    str\n        Text response from request.\n        See https://2.python-requests.org/en/master/api/#requests.Response.text\n    \"\"\"",
            "\"\"\"\n    Decorator that creates a property that is cached.\n\n    Keyword Arguments\n    -----------------\n    use_cache_ctrl : bool\n        If True, the property is cached using the Node cache_ctrl. If False, the property is only cached as a private\n        attribute. Default False.\n    expires : float, datetime, timedelta\n        Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        Ignored if use_cache_ctrl=False.\n\n    Notes\n    -----\n    Podpac caching using the cache_ctrl will be unreliable if the property depends on any non-tagged traits.\n    The property should only use node attrs (traits tagged with ``attr=True``).\n\n    Examples\n    --------\n\n    >>> class MyNode(Node):\n        # property that is recomputed every time\n        @property\n        def my_property(self):\n            return 0\n\n        # property is computed once for each object\n        @cached_property\n        def my_cached_property(self):\n            return 1\n\n        # property that is computed once and can be reused by other Nodes or sessions, depending on the cache_ctrl\n        @cached_property(use_cache_ctrl=True)\n        def my_persistent_cached_property(self):\n            return 2\n    \"\"\"",
            "\"\"\"Convert boolean and integer index arrays to slices.\n\n    Integer and boolean arrays are converted to slices that span the selected elements, but may include additional\n    elements. If possible, the slices are stepped.\n\n    Arguments\n    ---------\n    Is : tuple\n        tuple of indices (slice, integer array, boolean array, or single integer)\n\n    Returns\n    -------\n    Js : tuple\n        tuple of slices\n    \"\"\"",
            "\"\"\"\n    Utility that puts the OGC WMS/WCS BBox in the order specified by the CRS.\n    \"\"\"",
            "\"\"\"Evaluates every part of a node / pipeline at a point and records\n    which nodes are actively being used.\n\n    Parameters\n    ------------\n    node : podpac.Node\n        A PODPAC Node instance\n    lat : float, optional\n        Default is None. The latitude location\n    lon : float, optional\n        Default is None. The longitude location\n    time : float, np.datetime64, optional\n        Default is None. The time\n    alt : float, optional\n        Default is None. The altitude location\n    crs : str, optional\n        Default is None. The CRS of the request.\n    nested : bool, optional\n        Default is False. If True, will return a nested version of the\n        output dictionary isntead\n\n    Returns\n    dict\n        A dictionary that contains the following for each node:\n        ```\n        {\n            \"active\": bool,   # If the node is being used or not\n            \"value\": float,   # The value of the node evaluated at that point\n            \"inputs\": list,   # List of names of input nodes (based on definition)\n            \"name\": str,      # node.style.name\n            \"node_hash\": str, # The node's hash or node.base_ref\n        }\n        ```\n    \"\"\"",
            "\"\"\"Needed to build partial node definitions\"\"\"",
            "\"\"\"Needed to flatten the inputs list for all the dependencies\"\"\"",
            "\"\"\"Needed for the nested version of the pipeline\"\"\"",
            "\"\"\"\n    Returns a dictionary describing the specifications for each Node in a module.\n\n    Parameters\n    -----------\n    module: module\n        The Python module for which the ui specs should be summarized. Only the top-level\n        classes will be included in the spec. (i.e. no recursive search through submodules)\n    category: str, optional\n        Default is \"default\". Top-level category name for the group of Nodes.\n    help_as_html: bool, optional\n        Default is False. If True, the docstrings will be converted to html before storing in the spec.\n\n    Returns\n    --------\n    dict\n        Dictionary of {category: {Node1: spec_1, Node2: spec2, ...}} describing the specs for each Node.\n    \"\"\""
        ],
        "code_snippets": [
            "def common_doc(doc_dict):\n    \"\"\"Decorator: replaces commond fields in a function docstring\n\n    Parameters\n    -----------\n    doc_dict : dict\n        Dictionary of parameters that will be used to format a doctring. e.g. func.__doc__.format(**doc_dict)\n    \"\"\"",
            "def _decorator(func):\n        if func.__doc__ is None:\n            return func\n\n        func.__doc__ = func.__doc__.format(**doc_dict)\n        return func\n\n    return _decorator",
            "def trait_is_defined(obj, trait_name):\n    \"\"\"Utility method to determine if trait is defined on object without\n    call to default (@tl.default)\n\n    Parameters\n    ----------\n    object : object\n        Class with traits\n    trait_name : str\n        Class property to investigate\n\n    Returns\n    -------\n    bool\n        True if the trait exists on the object and is defined\n        False if the trait does not exist on the object or the trait is not defined\n    \"\"\"\n    return obj.has_trait(trait_name) and trait_name in obj._trait_values\n\n\ndef create_logfile(\n    filename=settings.settings[\"LOG_FILE_PATH\"],\n    level=logging.INFO,\n    format=\"[%(asctime)s] %(name)s.%(funcName)s[%(lineno)d] - %(levelname)s - %(message)s\",\n):\n    \"\"\"Convience method to create a log file that only logs\n    podpac related messages\n\n    Parameters\n    ----------\n    filename : str, optional\n        Filename of the log file. Defaults to ``podpac.log``\n    level : int, optional\n        Log level to use (0 - 50). Defaults to ``logging.INFO`` (20)\n        See https:",
            "class OrderedDictTrait(tl.Dict):",
            "def validate(self, obj, value):\n            if value == {}:\n                value = OrderedDict()\n            elif not isinstance(value, OrderedDict):\n                raise tl.TraitError(\n                    \"The '%s' trait of an %s instance must be an OrderedDict, but a value of %s %s was specified\"\n                    % (self.name, obj.__class__.__name__, value, type(value))\n                )\n            super(OrderedDictTrait, self).validate(obj, value)\n            return value\n\nelse:\n    OrderedDictTrait = tl.Dict",
            "class ArrayTrait(tl.TraitType):",
            "def __init__(self, ndim=None, shape=None, dtype=None, dtypes=None, default_value=None, *args, **kwargs):\n        if ndim is not None and shape is not None and len(shape) != ndim:\n            raise ValueError(\"Incompatible ndim and shape (ndim=%d, shape=%s)\" % (ndim, shape))\n        if dtype is not None and not isinstance(dtype, type):\n            if dtype not in np.typeDict:\n                raise ValueError(\"Unknown dtype '%s'\" % dtype)\n            dtype = np.typeDict[dtype]\n        self.ndim = ndim\n        self.shape = shape\n        self.dtype = dtype\n        super(ArrayTrait, self).__init__(default_value=default_value, *args, **kwargs)",
            "def validate(self, obj, value):\n        # coerce type\n        if not isinstance(value, np.ndarray):\n            value = np.array(value)\n\n        # ndim\n        if self.ndim is not None and self.ndim != value.ndim:\n            raise tl.TraitError(\n                \"The '%s' trait of an %s instance must have ndim %d, but a value with ndim %d was specified\"\n                % (self.name, obj.__class__.__name__, self.ndim, value.ndim)\n            )\n\n        # shape\n        if self.shape is not None and self.shape != value.shape:\n            raise tl.TraitError(\n                \"The '%s' trait of an %s instance must have shape %s, but a value %s with shape %s was specified\"\n                % (self.name, obj.__class__.__name__, self.shape, value, value.shape)\n            )\n\n        # dtype\n        if self.dtype is not None:\n            try:\n                value = value.astype(self.dtype)\n            except:\n                raise tl.TraitError(\n                    \"The '%s' trait of an %s instance must have dtype %s, but a value with dtype %s was specified\"\n                    % (self.name, obj.__class__.__name__, self.dtype, value.dtype)\n                )\n\n        return value",
            "class TupleTrait(tl.List):",
            "def validate(self, obj, value):\n        value = super(TupleTrait, self).validate(obj, value)\n        return tuple(value)",
            "class NodeTrait(tl.Instance):\n    _schema = {\"test\": \"info\"}",
            "def __init__(self, *args, **kwargs):\n        from podpac import Node as _Node\n\n        super(NodeTrait, self).__init__(_Node, *args, **kwargs)",
            "def validate(self, obj, value):\n        super(NodeTrait, self).validate(obj, value)\n        if podpac.core.settings.settings[\"DEBUG\"]:\n            value = deepcopy(value)\n        return value",
            "class DimsTrait(tl.List):\n    _schema = {\"test\": \"info\"}",
            "def __init__(self, *args, **kwargs):\n        super().__init__(tl.Enum([\"lat\", \"lon\", \"time\", \"alt\"]), *args, minlen=1, maxlen=4, **kwargs)\n\n    #",
            "def validate(self, obj, value):\n    #     super().validate(obj, value)\n    #     if podpac.core.settings.settings[\"DEBUG\"]:\n    #         value = deepcopy(value)\n    #     return value",
            "class JSONEncoder(json.JSONEncoder):",
            "def default(self, obj):\n        # podpac objects with definitions\n        if isinstance(\n            obj, (podpac.Coordinates, podpac.Node, podpac.interpolators.Interpolate, podpac.core.style.Style)\n        ):\n            return obj.definition\n\n        # podpac Interpolator type\n        if isinstance(obj, type) and obj in podpac.core.interpolation.INTERPOLATORS:\n            return obj().definition\n\n        # pint Units\n        if isinstance(obj, podpac.core.units.ureg.Unit):\n            return str(obj)\n\n        # datetime64\n        if isinstance(obj, np.datetime64):\n            return obj.astype(str)\n\n        # timedelta64\n        if isinstance(obj, np.timedelta64):\n            return podpac.core.coordinates.utils.make_timedelta_string(obj)\n\n        # datetime\n        if isinstance(obj, (datetime.datetime, datetime.date)):\n            return obj.isoformat()\n\n        # dataframe\n        if isinstance(obj, pd.DataFrame):\n            return obj.to_json()\n\n        # numpy array\n        if isinstance(obj, np.ndarray):\n            if np.issubdtype(obj.dtype, np.datetime64):\n                return obj.astype(str).tolist()\n            if np.issubdtype(obj.dtype, np.timedelta64):\n                return [podpac.core.coordinates.utils.make_timedelta_string(e) for e in obj]\n            if np.issubdtype(obj.dtype, np.number):\n                return obj.tolist()\n            else:\n                try:\n                    # completely serialize the individual elements using the custom encoder\n                    return json.loads(json.dumps([e for e in obj], cls=JSONEncoder))\n                except TypeError as e:\n                    raise TypeError(\"Cannot serialize numpy array\\n%s\" % e)\n\n        # raise the TypeError\n        return json.JSONEncoder.default(self, obj)",
            "def is_json_serializable(obj, cls=json.JSONEncoder):\n    try:\n        json.dumps(obj, cls=cls)\n    except:\n        return False\n    else:\n        return True",
            "def _get_param(params, key):\n    if key not in params:\n        if key.upper() not in params:\n            return None\n        key = key.upper()\n    if isinstance(params[key], list):\n        return params[key][0]\n    return params.get(key, None)",
            "def _get_query_params_from_url(url):\n    if isinstance(url, string_types):\n        url = urllib.parse_qs(urllib.urlparse(url).query)\n\n    # Capitalize the keywords for consistency\n    params = {}\n    for k in url:\n        params[k.upper()] = url[k]\n\n    return params",
            "def _get_from_url(url, session=None):\n    \"\"\"Helper function to get data from an url with error checking.\n\n    Parameters\n    ----------\n    url : str\n        URL to website\n    session : :class:`requests.Session`, optional\n        Requests session to use when making the GET request to `url`\n\n    Returns\n    -------\n    str\n        Text response from request.\n        See https:",
            "def cached_property(*args, **kwargs):\n    \"\"\"\n    Decorator that creates a property that is cached.\n\n    Keyword Arguments\n    -----------------\n    use_cache_ctrl : bool\n        If True, the property is cached using the Node cache_ctrl. If False, the property is only cached as a private\n        attribute. Default False.\n    expires : float, datetime, timedelta\n        Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        Ignored if use_cache_ctrl=False.\n\n    Notes\n    -----\n    Podpac caching using the cache_ctrl will be unreliable if the property depends on any non-tagged traits.\n    The property should only use node attrs (traits tagged with ``attr=True``).\n\n    Examples\n    --------\n\n    >>>",
            "class MyNode(Node):\n        # property that is recomputed every time\n        @property",
            "def my_property(self):\n            return 0\n\n        # property is computed once for each object\n        @cached_property",
            "def my_cached_property(self):\n            return 1\n\n        # property that is computed once and can be reused by other Nodes or sessions, depending on the cache_ctrl\n        @cached_property(use_cache_ctrl=True)",
            "def my_persistent_cached_property(self):\n            return 2\n    \"\"\"\n\n    use_cache_ctrl = kwargs.pop(\"use_cache_ctrl\", False)\n    expires = kwargs.pop(\"expires\", None)\n\n    if args and (len(args) != 1 or not callable(args[0])):\n        raise TypeError(\"cached_property decorator does not accept any positional arguments\")\n\n    if kwargs:\n        raise TypeError(\"cached_property decorator does not accept keyword argument '%s'\" % list(kwargs.keys())[0])",
            "def d(fn):\n        key = \"_podpac_cached_property_%s\" % fn.__name__\n\n        @property",
            "def wrapper(self):\n            if hasattr(self, key):\n                value = getattr(self, key)\n            elif use_cache_ctrl and self.has_cache(key):\n                value = self.get_cache(key)\n                setattr(self, key, value)\n            else:\n                value = fn(self)\n                setattr(self, key, value)\n                if use_cache_ctrl:\n                    self.put_cache(value, key, expires=expires)\n            return value\n\n        return wrapper\n\n    if args:\n        return d(args[0])\n    else:\n        return d",
            "def ind2slice(Is):\n    \"\"\"Convert boolean and integer index arrays to slices.\n\n    Integer and boolean arrays are converted to slices that span the selected elements, but may include additional\n    elements. If possible, the slices are stepped.\n\n    Arguments\n    ---------\n    Is : tuple\n        tuple of indices (slice, integer array, boolean array, or single integer)\n\n    Returns\n    -------\n    Js : tuple\n        tuple of slices\n    \"\"\"\n\n    if isinstance(Is, tuple):\n        return tuple(_ind2slice(I) for I in Is)\n    else:\n        return _ind2slice(Is)",
            "def _ind2slice(I):\n    # already a slice\n    if isinstance(I, slice):\n        return I\n\n    # convert to numpy array\n    I = np.atleast_1d(I)\n\n    # convert boolean array to index array\n    if I.dtype == bool:\n        (I,) = np.where(I)\n\n    # empty slice\n    if I.size == 0:\n        return slice(0, 0)\n\n    # singleton\n    if I.size == 1:\n        return I[0]\n\n    # stepped slice\n    diff = np.diff(I)\n    if diff.size and np.all(diff == diff[0]) and diff[0] != 0:\n        return slice(I.min(), I.max() + diff[0], diff[0])\n\n    # non-stepped slice\n    return slice(I.min(), I.max() + 1)",
            "def resolve_bbox_order(bbox, crs, size):\n    \"\"\"\n    Utility that puts the OGC WMS/WCS BBox in the order specified by the CRS.\n    \"\"\"\n    crs = pyproj.CRS(crs)\n    r = 1\n    if crs.axis_info[0].direction != \"north\":\n        r = -1\n    lat_start, lon_start = bbox[:2][::r]\n    lat_stop, lon_stop = bbox[2::][::r]\n    size = size[::r]\n\n    return {\"lat\": [lat_start, lat_stop, size[0]], \"lon\": [lon_start, lon_stop, size[1]]}",
            "def probe_node(node, lat=None, lon=None, time=None, alt=None, crs=None, nested=False):\n    \"\"\"Evaluates every part of a node / pipeline at a point and records\n    which nodes are actively being used.\n\n    Parameters\n    ------------\n    node : podpac.Node\n        A PODPAC Node instance\n    lat : float, optional\n        Default is None. The latitude location\n    lon : float, optional\n        Default is None. The longitude location\n    time : float, np.datetime64, optional\n        Default is None. The time\n    alt : float, optional\n        Default is None. The altitude location\n    crs : str, optional\n        Default is None. The CRS of the request.\n    nested : bool, optional\n        Default is False. If True, will return a nested version of the\n        output dictionary isntead\n\n    Returns\n    dict\n        A dictionary that contains the following for each node:\n        ```\n        {\n            \"active\": bool,   # If the node is being used or not\n            \"value\": float,   # The value of the node evaluated at that point\n            \"inputs\": list,   # List of names of input nodes (based on definition)\n            \"name\": str,      # node.style.name\n            \"node_hash\": str, # The node's hash or node.base_ref\n        }\n        ```\n    \"\"\"",
            "def partial_definition(key, definition):",
            "def flatten_list(l):",
            "def get_entry(key, out, definition):",
            "def get_ui_node_spec(module=None, category=\"default\", help_as_html=False):\n    \"\"\"\n    Returns a dictionary describing the specifications for each Node in a module.\n\n    Parameters\n    -----------\n    module: module\n        The Python module for which the ui specs should be summarized. Only the top-level\n        classes will be included in the spec. (i.e. no recursive search through submodules)\n    category: str, optional\n        Default is \"default\". Top-level category name for the group of Nodes.\n    help_as_html: bool, optional\n        Default is False. If True, the docstrings will be converted to html before storing in the spec.\n\n    Returns\n    --------\n    dict\n        Dictionary of {category: {Node1: spec_1, Node2: spec2, ...}} describing the specs for each Node.\n    \"\"\"\n    import podpac\n    import podpac.datalib  # May not be imported by default\n\n    spec = {}\n\n    if module is None:\n        modcat = zip(\n            [podpac.data, podpac.algorithm, podpac.compositor, podpac.datalib],\n            [\"data\", \"algorithm\", \"compositor\", \"datalib\"],\n        )\n        for mod, cat in modcat:\n            spec.update(get_ui_node_spec(mod, cat, help_as_html=help_as_html))\n        return spec\n\n    spec[category] = {}\n    disabled_categories = [\"Algorithm\", \"DataSource\", \"DroughtMonitorCategory\", \"DroughtCategory\", \"IntakeCatalog\"]\n    for obj in dir(module):\n        # print(obj)\n        if obj in disabled_categories:\n            ob = getattr(module, obj)\n            # print(ob)\n            # print(ob.get_ui_spec())\n            # would be fairly annoying to have to check all of the attrs for abstract\n            # still need a better solution\n            continue\n        ob = getattr(module, obj)\n        if not inspect.isclass(ob):\n            continue\n        if not issubclass(ob, podpac.Node):\n            continue\n        spec[category][obj] = ob.get_ui_spec(help_as_html=help_as_html)\n\n    return spec"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/common_test_utils.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nUtils Summary\n\"\"\"",
            "\"\"\"Generates every combination of stacked and unstacked coordinates podpac expects to handle\n\n    Parameters\n    -----------\n    lat: podpac.core.coordinates.Coordinates1d, optional\n        1D coordinate object used to create the Coordinate objects that contain the latitude dimension. By default uses:\n        UniformCoord(start=0, stop=2, size=3)\n    lon: podpac.core.coordinates.Coordinates1d, optional\n        Same as above but for longitude. By default uses:\n        UniformCoord(start=2, stop=6, size=3)\n    alt: podpac.core.coordinates.Coordinates1d, optional\n        Same as above but for longitude. By default uses:\n        UniformCoord(start=6, stop=12, size=3)\n    time: podpac.core.coordinates.Coordinates1d, optional\n        Same as above but for longitude. By default uses:\n        UniformCoord(start='2018-01-01T00:00:00', stop='2018-03-01T00:00:00', size=3)\n\n    Returns\n    -------\n    OrderedDict:\n        Dictionary of all the podpac.Core.Coordinate objects podpac expects to handle. The dictionary keys is a tuple of\n        coordinate dimensions, and the values are the actual Coordinate objects.\n\n    Notes\n    ------\n    When custom lat, lon, alt, and time 1D coordinates are given, only those with the same number of coordinates are\n    stacked together. For example, if lat, lon, alt, and time have sizes 3, 4, 5, and 6, respectively, no stacked\n    coordinates are created. Also, no exception or warning is thrown for this case.\n    \"\"\""
        ],
        "code_snippets": [
            "def get_dims_list():\n    return [\n        (\"lat\",),\n        (\"lon\",),\n        (\"alt\",),\n        (\"tim\",),\n        (\"lat\", \"lon\"),\n        (\"lat\", \"alt\"),\n        (\"lat\", \"tim\"),\n        (\"lon\", \"lat\"),\n        (\"lon\", \"alt\"),\n        (\"lon\", \"tim\"),\n        (\"alt\", \"lat\"),\n        (\"alt\", \"lon\"),\n        (\"alt\", \"tim\"),\n        (\"tim\", \"lat\"),\n        (\"tim\", \"lon\"),\n        (\"tim\", \"alt\"),\n        (\"lat\", \"lon\", \"alt\"),\n        (\"lat\", \"lon\", \"tim\"),\n        (\"lat\", \"alt\", \"tim\"),\n        (\"lat\", \"tim\", \"alt\"),\n        (\"lon\", \"lat\", \"alt\"),\n        (\"lon\", \"lat\", \"tim\"),\n        (\"lon\", \"alt\", \"tim\"),\n        (\"lon\", \"tim\", \"alt\"),\n        (\"alt\", \"lat\", \"lon\"),\n        (\"alt\", \"lat\", \"tim\"),\n        (\"alt\", \"lon\", \"lat\"),\n        (\"alt\", \"lon\", \"tim\"),\n        (\"alt\", \"tim\", \"lat\"),\n        (\"alt\", \"tim\", \"lon\"),\n        (\"tim\", \"lat\", \"lon\"),\n        (\"tim\", \"lat\", \"alt\"),\n        (\"tim\", \"lon\", \"lat\"),\n        (\"tim\", \"lon\", \"alt\"),\n        (\"tim\", \"alt\", \"lat\"),\n        (\"tim\", \"alt\", \"lon\"),\n        (\"lat\", \"lon\", \"alt\", \"tim\"),\n        (\"lat\", \"lon\", \"tim\", \"alt\"),\n        (\"lon\", \"lat\", \"alt\", \"tim\"),\n        (\"lon\", \"lat\", \"tim\", \"alt\"),\n        (\"alt\", \"lat\", \"lon\", \"tim\"),\n        (\"alt\", \"lon\", \"lat\", \"tim\"),\n        (\"alt\", \"tim\", \"lat\", \"lon\"),\n        (\"alt\", \"tim\", \"lon\", \"lat\"),\n        (\"tim\", \"lat\", \"lon\", \"alt\"),\n        (\"tim\", \"lon\", \"lat\", \"alt\"),\n        (\"tim\", \"alt\", \"lat\", \"lon\"),\n        (\"tim\", \"alt\", \"lon\", \"lat\"),\n        (\"lat_lon\",),\n        (\"lat_alt\",),\n        (\"lat_tim\",),\n        (\"lon_lat\",),\n        (\"lon_alt\",),\n        (\"lon_tim\",),\n        (\"alt_lat\",),\n        (\"alt_lon\",),\n        (\"alt_tim\",),\n        (\"tim_lat\",),\n        (\"tim_lon\",),\n        (\"tim_alt\",),\n        (\"lat_lon\", \"alt\"),\n        (\"lat_lon\", \"tim\"),\n        (\"lat_alt\", \"tim\"),\n        (\"lat_tim\", \"alt\"),\n        (\"lon_lat\", \"tim\"),\n        (\"lon_lat\", \"alt\"),\n        (\"lon_alt\", \"tim\"),\n        (\"lon_tim\", \"alt\"),\n        (\"alt_lat\", \"tim\"),\n        (\"alt_lon\", \"tim\"),\n        (\"alt_tim\", \"lat\"),\n        (\"alt_tim\", \"lon\"),\n        (\"tim_lat\", \"alt\"),\n        (\"tim_lon\", \"alt\"),\n        (\"tim_alt\", \"lat\"),\n        (\"tim_alt\", \"lon\"),\n        (\"lat\", \"alt_tim\"),\n        (\"lat\", \"tim_alt\"),\n        (\"lon\", \"alt_tim\"),\n        (\"lon\", \"tim_alt\"),\n        (\"alt\", \"lat_lon\"),\n        (\"alt\", \"lat_tim\"),\n        (\"alt\", \"lon_lat\"),\n        (\"alt\", \"lon_tim\"),\n        (\"alt\", \"tim_lat\"),\n        (\"alt\", \"tim_lon\"),\n        (\"tim\", \"lat_lon\"),\n        (\"tim\", \"lat_alt\"),\n        (\"tim\", \"lon_lat\"),\n        (\"tim\", \"lon_alt\"),\n        (\"tim\", \"alt_lat\"),\n        (\"tim\", \"alt_lon\"),\n        (\"lat_lon\", \"alt_tim\"),\n        (\"lat_lon\", \"tim_alt\"),\n        (\"lon_lat\", \"tim_alt\"),\n        (\"lon_lat\", \"alt_tim\"),\n        (\"alt_tim\", \"lat_lon\"),\n        (\"alt_tim\", \"lon_lat\"),\n        (\"tim_alt\", \"lat_lon\"),\n        (\"tim_alt\", \"lon_lat\"),\n        (\"lat_lon_alt\", \"tim\"),\n        (\"lat_lon_tim\", \"alt\"),\n        (\"lon_lat_tim\", \"alt\"),\n        (\"lon_lat_alt\", \"tim\"),\n        (\"alt_lat_lon\", \"tim\"),\n        (\"alt_lon_lat\", \"tim\"),\n        (\"tim_lat_lon\", \"alt\"),\n        (\"tim_lon_lat\", \"alt\"),\n        (\"alt\", \"lat_lon_tim\"),\n        (\"alt\", \"lon_lat_tim\"),\n        (\"alt\", \"tim_lat_lon\"),\n        (\"alt\", \"tim_lon_lat\"),\n        (\"tim\", \"lat_lon_alt\"),\n        (\"tim\", \"lon_lat_alt\"),\n        (\"tim\", \"alt_lat_lon\"),\n        (\"tim\", \"alt_lon_lat\"),\n        (\"lat\", \"lon\", \"alt_tim\"),\n        (\"lat\", \"lon\", \"tim_alt\"),\n        (\"lon\", \"lat\", \"alt_tim\"),\n        (\"lon\", \"lat\", \"tim_alt\"),\n        (\"alt\", \"tim\", \"lat_lon\"),\n        (\"alt\", \"tim\", \"lon_lat\"),\n        (\"tim\", \"alt\", \"lat_lon\"),\n        (\"tim\", \"alt\", \"lon_lat\"),\n        (\"alt\", \"lat_lon\", \"tim\"),\n        (\"alt\", \"lon_lat\", \"tim\"),\n        (\"tim\", \"lat_lon\", \"alt\"),\n        (\"tim\", \"lon_lat\", \"alt\"),\n        (\"lat_lon\", \"alt\", \"tim\"),\n        (\"lat_lon\", \"tim\", \"alt\"),\n        (\"lon_lat\", \"alt\", \"tim\"),\n        (\"lon_lat\", \"tim\", \"alt\"),\n        (\"alt_tim\", \"lat\", \"lon\"),\n        (\"alt_tim\", \"lon\", \"lat\"),\n        (\"tim_alt\", \"lat\", \"lon\"),\n        (\"tim_alt\", \"lon\", \"lat\"),\n        (\"lat_lon_alt\",),\n        (\"lat_lon_tim\",),\n        (\"lat_alt_tim\",),\n        (\"lat_tim_alt\",),\n        (\"lon_lat_alt\",),\n        (\"lon_lat_tim\",),\n        (\"lon_alt_tim\",),\n        (\"lon_tim_alt\",),\n        (\"alt_lat_lon\",),\n        (\"alt_lat_tim\",),\n        (\"alt_lon_lat\",),\n        (\"alt_lon_tim\",),\n        (\"alt_tim_lat\",),\n        (\"alt_tim_lon\",),\n        (\"tim_lat_lon\",),\n        (\"tim_lat_alt\",),\n        (\"tim_lon_lat\",),\n        (\"tim_lon_alt\",),\n        (\"tim_alt_lat\",),\n        (\"tim_alt_lon\",),\n        (\"lat_lon_alt_tim\",),\n        (\"lat_lon_tim_alt\",),\n        (\"lon_lat_alt_tim\",),\n        (\"lon_lat_tim_alt\",),\n        (\"alt_lat_lon_tim\",),\n        (\"alt_lon_lat_tim\",),\n        (\"alt_tim_lat_lon\",),\n        (\"alt_tim_lon_lat\",),\n        (\"tim_lat_lon_alt\",),\n        (\"tim_lon_lat_alt\",),\n        (\"tim_alt_lat_lon\",),\n        (\"tim_alt_lon_lat\",),\n    ]",
            "def make_coordinate_combinations(lat=None, lon=None, alt=None, time=None):\n    \"\"\"Generates every combination of stacked and unstacked coordinates podpac expects to handle\n\n    Parameters\n    -----------\n    lat: podpac.core.coordinates.Coordinates1d, optional\n        1D coordinate object used to create the Coordinate objects that contain the latitude dimension. By default uses:\n        UniformCoord(start=0, stop=2, size=3)\n    lon: podpac.core.coordinates.Coordinates1d, optional\n        Same as above but for longitude. By default uses:\n        UniformCoord(start=2, stop=6, size=3)\n    alt: podpac.core.coordinates.Coordinates1d, optional\n        Same as above but for longitude. By default uses:\n        UniformCoord(start=6, stop=12, size=3)\n    time: podpac.core.coordinates.Coordinates1d, optional\n        Same as above but for longitude. By default uses:\n        UniformCoord(start='2018-01-01T00:00:00', stop='2018-03-01T00:00:00', size=3)\n\n    Returns\n    -------\n    OrderedDict:\n        Dictionary of all the podpac.Core.Coordinate objects podpac expects to handle. The dictionary keys is a tuple of\n        coordinate dimensions, and the values are the actual Coordinate objects.\n\n    Notes\n    ------\n    When custom lat, lon, alt, and time 1D coordinates are given, only those with the same number of coordinates are\n    stacked together. For example, if lat, lon, alt, and time have sizes 3, 4, 5, and 6, respectively, no stacked\n    coordinates are created. Also, no exception or warning is thrown for this case.\n    \"\"\"\n\n    # make the 1D coordinates\n    if lat is None:\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n    if lon is None:\n        lon = ArrayCoordinates1d([2, 4, 6], name=\"lon\")\n    if alt is None:\n        alt = ArrayCoordinates1d([6, 9, 12], name=\"alt\")\n    if time is None:\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-02-01\", \"2018-03-01\"], name=\"time\")\n\n    d = dict([(\"lat\", lat), (\"lon\", lon), (\"alt\", alt), (\"tim\", time)])\n\n    dims_list = get_dims_list()\n\n    # make the stacked coordinates\n    for dim in [dim for dims in dims_list for dim in dims if \"_\" in dim]:\n        cs = [d[k] for k in dim.split(\"_\")]\n        if any(c.size != cs[0].size for c in cs):\n            continue  # can't stack these\n        d[dim] = StackedCoordinates(cs)\n\n    # make the ND coordinates\n    coord_collection = OrderedDict()\n    for dims in dims_list:\n        if any(dim not in d for dim in dims):\n            continue\n        coord_collection[dims] = Coordinates([d[dim] for dim in dims])\n    return coord_collection"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/node.py",
        "comments": [
            "//my-site.org/pipeline_definition.json",
            "//my-bucket-name/pipeline_definition.json",
            "//\"):",
            "//\"):"
        ],
        "docstrings": [
            "\"\"\"\nNode Summary\n\"\"\"",
            "\"\"\"The set of coordinates requested by a user. The Node will be evaluated using these coordinates.\"\"\"",
            "\"\"\"Default is None. Optional input array used to store the output data. When supplied, the node will not\n            allocate its own memory for the output array. This array needs to have the correct dimensions,\n            coordinates, and coordinate reference system.\"\"\"",
            "\"\"\"The selector function is an optimization that enables nodes to only select data needed by an interpolator.\n            It returns a new Coordinates object, and an index object that indexes into the `coordinates` parameter\n            If not provided, the Coordinates.intersect() method will be used instead.\"\"\"",
            "\"\"\"\n        :class:`podpac.UnitsDataArray`\n            Unit-aware xarray DataArray containing the results of the node evaluation.\n        \"\"\"",
            "\"\"\"\n        OrderedDict\n            Dictionary containing the location of the Node, the name of the plugin (if required), as well as any\n            parameters and attributes that were tagged by children.\n        \"\"\"",
            "\"\"\"How to initialize the array. Options are:\n                nan: uses np.full(..., np.nan) (Default option)\n                empty: uses np.empty\n                zeros: uses np.zeros()\n                ones: uses np.ones\n                full: uses np.full(..., fillval)\n                data: uses the fillval as the input array\n        \"\"\"",
            "\"\"\"\n        :class:`podpac.UnitsDataArray`\n            Unit-aware xarray DataArray of the desired size initialized using the method specified.\n        \"\"\"",
            "\"\"\"Base class for exceptions when using podpac nodes\"\"\"",
            "\"\"\"Raised node definition errors, such as when the definition is circular or is not yet unavailable.\"\"\"",
            "\"\"\"The base class for all Nodes, which defines the common interface for everything.\n\n    Attributes\n    ----------\n    cache_output: bool\n        Should the node's output be cached? If not provided or None, uses default based on settings\n        (CACHE_NODE_OUTPUT_DEFAULT for general Nodes, and CACHE_DATASOURCE_OUTPUT_DEFAULT  for DataSource nodes).\n        If True, outputs will be cached and retrieved from cache. If False, outputs will not be cached OR retrieved from cache (even if\n        they exist in cache).\n    force_eval: bool\n        Default is False. Should the node's cached output be updated from the source data? If True it ignores the cache\n        when computing outputs but puts results into the cache (thereby updating the cache)\n    cache_ctrl: :class:`podpac.core.cache.cache.CacheCtrl`\n        Class that controls caching. If not provided, uses default based on settings.\n    dtype : type\n        The numpy datatype of the output. Currently only ``float`` is supported.\n    style : :class:`podpac.Style`\n        Object discribing how the output of a node should be displayed. This attribute is planned for deprecation in the\n        future.\n    units : str\n        The units of the output data. Must be pint compatible.\n    outputs : list\n        For multiple-output nodes, the names of the outputs. Default is ``None`` for standard nodes.\n    output : str\n        For multiple-output nodes only, specifies a particular output to evaluate, if desired. Must be one of ``outputs``.\n\n    Notes\n    -----\n    Additional attributes are available for debugging after evaluation, including:\n     * ``_requested_coordinates``: the requested coordinates of the most recent call to eval\n     * ``_output``: the output of the most recent call to eval\n     * ``_from_cache``: whether the most recent call to eval used the cache\n     * ``_multi_threaded``: whether the most recent call to eval was executed using multiple threads\n    \"\"\"",
            "\"\"\"Do not overwrite me\"\"\"",
            "\"\"\"Only overwrite me if absolutely necessary\n\n        Parameters\n        ----------\n        **kwargs\n            Keyword arguments provided by user when class was instantiated.\n\n        Returns\n        -------\n        dict\n            Keyword arguments that will be passed to the standard intialization function.\n        \"\"\"",
            "\"\"\"Overwrite this method if a node needs to do any additional initialization after the standard initialization.\"\"\"",
            "\"\"\"List of node attributes\"\"\"",
            "\"\"\"\n        Evaluate the node at the given coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        **kwargs: **dict\n            Additional key-word arguments passed down the node pipelines, used internally\n\n        Returns\n        -------\n        output : {eval_return}\n        \"\"\"",
            "\"\"\"\n        Evaluate the node for each of the coordinates in the group.\n\n        Parameters\n        ----------\n        group : podpac.CoordinatesGroup\n            Group of coordinates to evaluate.\n\n        Returns\n        -------\n        outputs : list\n            evaluation output, list of UnitsDataArray objects\n        \"\"\"",
            "\"\"\"\n        Get all available coordinates for the Node. Implemented in child classes.\n\n        Returns\n        -------\n        coord_list : list\n            list of available coordinates (Coordinates objects)\n        \"\"\"",
            "\"\"\"Get the full available coordinate bounds for the Node.\n\n        Arguments\n        ---------\n        crs : str\n            Desired CRS for the bounds.\n            If not specified, the default CRS in the podpac settings is used. Optional.\n\n        Returns\n        -------\n        bounds : dict\n            Bounds for each dimension. Keys are dimension names and values are tuples (min, max).\n        crs : str\n            The CRS for the bounds.\n        \"\"\"",
            "\"\"\"\n        Initialize an output data array\n\n        Parameters\n        ----------\n        coords : podpac.Coordinates\n            {arr_coords}\n        data : None, number, or array-like (optional)\n            {arr_init_type}\n        attrs : dict\n            Attributes to add to output -- UnitsDataArray.create uses the 'crs' portion contained in here\n        outputs : list[string], optional\n            Default is self.outputs. List of strings listing the outputs\n        **kwargs\n            {arr_kwargs}\n\n        Returns\n        -------\n        {arr_return}\n        \"\"\"",
            "\"\"\"Evaluates every part of a node / pipeline at a point and records\n        which nodes are actively being used.\n\n        Parameters\n        ------------\n        lat : float, optional\n            Default is None. The latitude location\n        lon : float, optional\n            Default is None. The longitude location\n        time : float, np.datetime64, optional\n            Default is None. The time\n        alt : float, optional\n            Default is None. The altitude location\n        crs : str, optional\n            Default is None. The CRS of the request.\n\n        Returns\n        dict\n            A dictionary that contains the following for each node:\n            ```\n            {\n                \"active\": bool,   # If the node is being used or not\n                \"value\": float,   # The value of the node evaluated at that point\n                \"inputs\": list,   # List of names of input nodes (based on definition)\n                \"name\": str,      # node.style.name or self.base_ref if the style name is empty\n                \"node_hash\": str, # The node's hash\n            }\n            ```\n        \"\"\"",
            "\"\"\"\n        Default reference/name in node definitions\n\n        Returns\n        -------\n        str\n            Name of the node in node definitions\n        \"\"\"",
            "\"\"\"\n        Full node definition.\n\n        Returns\n        -------\n        OrderedDict\n            Dictionary-formatted node definition.\n        \"\"\"",
            "\"\"\"Definition for this node in JSON format.\"\"\"",
            "\"\"\"Definition for this node in JSON format, with indentation suitable for display.\"\"\"",
            "\"\"\"hash for this node, used in caching and to determine equality.\"\"\"",
            "\"\"\"\n        Write node to file.\n\n        Arguments\n        ---------\n        path : str\n            path to write to\n\n        See Also\n        --------\n        load : load podpac Node from file.\n        \"\"\"",
            "\"\"\"\n        Get cached data for this node.\n\n        Parameters\n        ----------\n        key : str\n            Key for the cached data, e.g. 'output'\n        coordinates : podpac.Coordinates, optional\n            Coordinates for which the cached data should be retrieved. Omit for coordinate-independent data.\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        ------\n        NodeException\n            Cached data not found.\n        \"\"\"",
            "\"\"\"\n        Cache data for this node.\n\n        Parameters\n        ----------\n        data : any\n            The data to cache.\n        key : str\n            Unique key for the data, e.g. 'output'\n        coordinates : podpac.Coordinates, optional\n            Coordinates that the cached data depends on. Omit for coordinate-independent data.\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        overwrite : bool, optional\n            Overwrite existing data, default True.\n\n        Raises\n        ------\n        NodeException\n            Cached data already exists (and overwrite is False)\n        \"\"\"",
            "\"\"\"\n        Check for cached data for this node.\n\n        Parameters\n        ----------\n        key : str\n            Key for the cached data, e.g. 'output'\n        coordinates : podpac.Coordinates, optional\n            Coordinates for which the cached data should be retrieved. Omit for coordinate-independent data.\n\n        Returns\n        -------\n        bool\n            True if there is cached data for this node, key, and coordinates.\n        \"\"\"",
            "\"\"\"\n        Clear cached data for this node.\n\n        Parameters\n        ----------\n        key : str\n            Delete cached objects with this key. If `'*'`, cached data is deleted for all keys.\n        coordinates : podpac.Coordinates, str, optional\n            Default is None. Delete cached objects for these coordinates. If `'*'`, cached data is deleted for all\n            coordinates, including coordinate-independent data. If None, will only affect coordinate-independent data.\n        mode: str, optional\n            Specify which cache stores are affected. Default 'all'.\n\n\n        See Also\n        ---------\n        `podpac.core.cache.cache.CacheCtrl.clear` to remove ALL cache for ALL nodes.\n        \"\"\"",
            "\"\"\"\n        Create podpac Node from a dictionary definition.\n\n        Arguments\n        ---------\n        d : dict\n            node definition\n\n        Returns\n        -------\n        :class:`Node`\n            podpac Node\n\n        See Also\n        --------\n        definition : node definition as a dictionary\n        from_json : create podpac node from a JSON definition\n        load : create a node from file\n        \"\"\"",
            "\"\"\"\n        Create podpac Node from a JSON definition.\n\n        Arguments\n        ---------\n        s : str\n            JSON-formatted node definition\n\n        Returns\n        -------\n        :class:`Node`\n            podpac Node\n\n        See Also\n        --------\n        json : node definition as a JSON string\n        load : create a node from file\n        \"\"\"",
            "\"\"\"\n        Create podpac Node from file.\n\n        Arguments\n        ---------\n        path : str\n            path to text file containing a JSON-formatted node definition\n\n        Returns\n        -------\n        :class:`Node`\n            podpac Node\n\n        See Also\n        --------\n        save : save a node to file\n        \"\"\"",
            "\"\"\"\n        Create podpac Node from a WMS/WCS request.\n\n        Arguments\n        ---------\n        url : str, dict\n            The raw WMS/WCS request url, or a dictionary of query parameters\n\n        Returns\n        -------\n        :class:`Node`\n            A full Node with sub-nodes based on the definition of the node from the URL\n\n        Notes\n        -------\n        The request can specify the PODPAC node by four different mechanism:\n\n        * Direct node name: PODPAC will look for an appropriate node in podpac.datalib\n        * JSON definition passed using the 'PARAMS' query string: Need to specify the special LAYER/COVERAGE value of\n          \"%PARAMS%\"\n        * By pointing at the JSON definition retrievable with a http GET request:\n          e.g. by setting LAYER/COVERAGE value to https://my-site.org/pipeline_definition.json\n        * By pointing at the JSON definition retrievable from an S3 bucket that the user has access to:\n          e.g by setting LAYER/COVERAGE value to s3://my-bucket-name/pipeline_definition.json\n        \"\"\"",
            "\"\"\"\n        Create podpac Node from a WMS/WCS request.\n\n        Arguments\n        ---------\n        name : str\n            The name of the PODPAC Node / Layer\n        params : dict, optional\n            Default is None. Dictionary of parameters to modify node attributes, style, or completely/partially define the node.\n            This dictionary can either be a `Node.definition` or `Node.definition['attrs']`. Node, the specified `name` always\n            take precidence over anything defined in `params` (e.g. params['node'] won't be used).\n\n        Returns\n        -------\n        :class:`Node`\n            A full Node with sub-nodes based on the definition of the node from the node name and parameters\n\n        \"\"\"",
            "\"\"\"Get spec of node attributes for building a ui\n\n        Parameters\n        ----------\n        help_as_html : bool, optional\n            Default is False. If True, the docstrings will be converted to html before storing in the spec.\n\n        Returns\n        -------\n        dict\n            Spec for this node that is readily json-serializable\n        \"\"\"",
            "\"\"\"\n        I will manually define generic defaults here. Eventually we may want to\n        dig into this and create node specific styling. This will have to be done under each\n        node. But may be difficult to add style to each node?\n\n        Example: podpac.core.algorithm.utility.SinCoords.Style ----> returns a tl.Instance\n        BUT if I do:\n        podpac.core.algorithm.utility.SinCoords().style.json ---> outputs style\n\n        ERROR if no parenthesis are given. So how can this be done without instantiating the class?\n\n        Will need to ask @MPU how to define a node specific style.\n        \"\"\"",
            "\"\"\"check if inputs of a node are stored in nodes, if not add them\n\n    Parameters\n    -----------\n    nodes: OrderedDict\n        Keys: Node names (strings)\n        Values: Node objects\n\n    name: string\n        the Node whose inputs are being checked\n\n    value: string, list, dictionary:\n        the Node (or collection of Nodes) which is being looked\n\n    definition: pipeline definition\n\n    Returns\n    --------\n    node: the node searched for\n\n    Note: this function calles _process_kwargs, which alters nodes by loading a Node if it is not yet in nodes.\n\n    \"\"\"",
            "\"\"\"create a node and add it to nodes\n\n    Parameters\n    -----------\n    nodes: OrderedDict\n        Keys: Node names (strings)\n        Values: Node objects\n\n    name: string\n        the Node which will be created\n\n    d: the definition of the node to be created\n\n    definition: pipeline definition\n\n    Returns\n    --------\n    Nothing, but loads the node with name \"name\" and definition \"d\" into nodes\n\n\n    \"\"\"",
            "\"\"\"Mixin to use no cache by default.\"\"\"",
            "\"\"\"Mixin to add disk caching to the Node by default.\"\"\""
        ],
        "code_snippets": [
            "class NodeException(Exception):",
            "class NodeDefinitionError(NodeException):",
            "class Node(tl.HasTraits):\n    \"\"\"The base class for all Nodes, which defines the common interface for everything.\n\n    Attributes\n    ----------\n    cache_output: bool\n        Should the node's output be cached? If not provided or None, uses default based on settings\n        (CACHE_NODE_OUTPUT_DEFAULT for general Nodes, and CACHE_DATASOURCE_OUTPUT_DEFAULT  for DataSource nodes).\n        If True, outputs will be cached and retrieved from cache. If False, outputs will not be cached OR retrieved from cache (even if\n        they exist in cache).\n    force_eval: bool\n        Default is False. Should the node's cached output be updated from the source data? If True it ignores the cache\n        when computing outputs but puts results into the cache (thereby updating the cache)\n    cache_ctrl: :class:`podpac.core.cache.cache.CacheCtrl`\n        Class that controls caching. If not provided, uses default based on settings.\n    dtype : type\n        The numpy datatype of the output. Currently only ``float`` is supported.\n    style : :class:`podpac.Style`\n        Object discribing how the output of a node should be displayed. This attribute is planned for deprecation in the\n        future.\n    units : str\n        The units of the output data. Must be pint compatible.\n    outputs : list\n        For multiple-output nodes, the names of the outputs. Default is ``None`` for standard nodes.\n    output : str\n        For multiple-output nodes only, specifies a particular output to evaluate, if desired. Must be one of ``outputs``.\n\n    Notes\n    -----\n    Additional attributes are available for debugging after evaluation, including:\n     * ``_requested_coordinates``: the requested coordinates of the most recent call to eval\n     * ``_output``: the output of the most recent call to eval\n     * ``_from_cache``: whether the most recent call to eval used the cache\n     * ``_multi_threaded``: whether the most recent call to eval was executed using multiple threads\n    \"\"\"\n\n    outputs = tl.List(trait=tl.Unicode(), allow_none=True).tag(attr=True)\n    output = tl.Unicode(default_value=None, allow_none=True).tag(attr=True)\n    units = tl.Unicode(default_value=None, allow_none=True).tag(attr=True, hidden=True)\n    style = tl.Instance(Style)\n\n    dtype = tl.Enum([float], default_value=float)\n    cache_output = tl.Bool()\n    force_eval = tl.Bool(False)\n    cache_ctrl = tl.Instance(CacheCtrl, allow_none=True)\n\n    # list of attribute names, used by __repr__ and __str__ to display minimal info about the node\n    # e.g. data sources use ['source']\n    _repr_keys = []\n\n    @tl.default(\"outputs\")",
            "def _default_outputs(self):\n        return None\n\n    @tl.validate(\"output\")",
            "def _validate_output(self, d):\n        if d[\"value\"] is not None:\n            if self.outputs is None:\n                raise TypeError(\"Invalid output '%s' (output must be None for single-output nodes).\" % self.output)\n            if d[\"value\"] not in self.outputs:\n                raise ValueError(\"Invalid output '%s' (available outputs are %s)\" % (self.output, self.outputs))\n        return d[\"value\"]\n\n    @tl.default(\"style\")",
            "def _default_style(self):\n        return Style()\n\n    @tl.validate(\"units\")",
            "def _validate_units(self, d):\n        ureg.Unit(d[\"value\"])  # will throw an exception if this is not a valid pint Unit\n        return d[\"value\"]\n\n    @tl.default(\"cache_output\")",
            "def _cache_output_default(self):\n        return settings[\"CACHE_NODE_OUTPUT_DEFAULT\"]\n\n    @tl.default(\"cache_ctrl\")",
            "def _cache_ctrl_default(self):\n        return get_default_cache_ctrl()\n\n    # debugging\n    _requested_coordinates = tl.Instance(Coordinates, allow_none=True)\n    _output = tl.Instance(UnitsDataArray, allow_none=True)\n    _from_cache = tl.Bool(allow_none=True, default_value=None)\n    # Flag that is True if the Node was run multi-threaded, or None if the question doesn't apply\n    _multi_threaded = tl.Bool(allow_none=True, default_value=None)\n\n    # util\n    _definition_guard = False\n    _traits_initialized_guard = False",
            "def __init__(self, **kwargs):",
            "def _first_init(self, **kwargs):\n        \"\"\"Only overwrite me if absolutely necessary\n\n        Parameters\n        ----------\n        **kwargs\n            Keyword arguments provided by user when class was instantiated.\n\n        Returns\n        -------\n        dict\n            Keyword arguments that will be passed to the standard intialization function.\n        \"\"\"\n        return kwargs",
            "def init(self):",
            "def attrs(self):",
            "def _repr_info(self):\n        keys = self._repr_keys[:]\n        if self.trait_is_defined(\"output\") and self.output is not None:\n            if \"output\" not in keys:\n                keys.append(\"output\")\n        elif self.trait_is_defined(\"outputs\") and self.outputs is not None:\n            if \"outputs\" not in keys:\n                keys.append(\"outputs\")\n        return \", \".join(\"%s=%s\" % (key, repr(getattr(self, key))) for key in keys)",
            "def __repr__(self):\n        return \"<%s(%s)>\" % (self.__class__.__name__, self._repr_info)",
            "def __str__(self):\n        return \"<%s(%s) attrs: %s>\" % (self.__class__.__name__, self._repr_info, \", \".join(self.attrs))\n\n    @common_doc(COMMON_DOC)",
            "def eval(self, coordinates, **kwargs):\n        \"\"\"\n        Evaluate the node at the given coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        **kwargs: **dict\n            Additional key-word arguments passed down the node pipelines, used internally\n\n        Returns\n        -------\n        output : {eval_return}\n        \"\"\"\n\n        output = kwargs.get(\"output\", None)\n        # check crs compatibility\n        if output is not None and \"crs\" in output.attrs and output.attrs[\"crs\"] != coordinates.crs:\n            raise ValueError(\n                \"Output coordinate reference system ({}) does not match\".format(output.crs)\n                + \"request Coordinates coordinate reference system ({})\".format(coordinates.crs)\n            )\n\n        if settings[\"DEBUG\"]:\n            self._requested_coordinates = coordinates\n        item = \"output\"\n\n        # get standardized coordinates for caching\n        cache_coordinates = coordinates.transpose(*sorted(coordinates.dims)).simplify()\n\n        if not self.force_eval and self.cache_output and self.has_cache(item, cache_coordinates):\n            data = self.get_cache(item, cache_coordinates)\n            if output is not None:\n                order = [dim for dim in output.dims if dim not in data.dims] + list(data.dims)\n                output.transpose(*order)[:] = data\n            self._from_cache = True\n        else:\n            data = self._eval(coordinates, **kwargs)\n            if self.cache_output:\n                self.put_cache(data, item, cache_coordinates)\n            self._from_cache = False\n\n        # extract single output, if necessary\n        # subclasses should extract single outputs themselves if possible, but this provides a backup\n        if \"output\" in data.dims and self.output is not None:\n            data = data.sel(output=self.output)\n\n        # transpose data to match the dims order of the requested coordinates\n        order = [dim for dim in coordinates.xdims if dim in data.dims]\n        if \"output\" in data.dims:\n            order.append(\"output\")\n        data = data.part_transpose(order)\n\n        if settings[\"DEBUG\"]:\n            self._output = data\n\n        # Add style information\n        data.attrs[\"layer_style\"] = self.style\n\n        if self.units is not None:\n            data.attrs[\"units\"] = self.units\n\n        # Add crs if it is missing\n        if \"crs\" not in data.attrs:\n            data.attrs[\"crs\"] = coordinates.crs\n\n        return data",
            "def _eval(self, coordinates, output=None, _selector=None):\n        raise NotImplementedError",
            "def eval_group(self, group):\n        \"\"\"\n        Evaluate the node for each of the coordinates in the group.\n\n        Parameters\n        ----------\n        group : podpac.CoordinatesGroup\n            Group of coordinates to evaluate.\n\n        Returns\n        -------\n        outputs : list\n            evaluation output, list of UnitsDataArray objects\n        \"\"\"\n\n        return [self.eval(coords) for coords in group]",
            "def find_coordinates(self):\n        \"\"\"\n        Get all available coordinates for the Node. Implemented in child classes.\n\n        Returns\n        -------\n        coord_list : list\n            list of available coordinates (Coordinates objects)\n        \"\"\"\n\n        raise NotImplementedError",
            "def get_bounds(self, crs=\"default\"):\n        \"\"\"Get the full available coordinate bounds for the Node.\n\n        Arguments\n        ---------\n        crs : str\n            Desired CRS for the bounds.\n            If not specified, the default CRS in the podpac settings is used. Optional.\n\n        Returns\n        -------\n        bounds : dict\n            Bounds for each dimension. Keys are dimension names and values are tuples (min, max).\n        crs : str\n            The CRS for the bounds.\n        \"\"\"\n\n        if crs == \"default\":\n            crs = podpac.settings[\"DEFAULT_CRS\"]\n\n        bounds = {}\n        for coords in self.find_coordinates():\n            ct = coords.transform(crs)\n            for dim, (lo, hi) in ct.bounds.items():\n                if dim not in bounds:\n                    bounds[dim] = (lo, hi)\n                else:\n                    bounds[dim] = (min(lo, bounds[dim][0]), max(hi, bounds[dim][1]))\n\n        return bounds, crs\n\n    @common_doc(COMMON_DOC)",
            "def create_output_array(self, coords, data=np.nan, attrs=None, outputs=None, **kwargs):\n        \"\"\"\n        Initialize an output data array\n\n        Parameters\n        ----------\n        coords : podpac.Coordinates\n            {arr_coords}\n        data : None, number, or array-like (optional)\n            {arr_init_type}\n        attrs : dict\n            Attributes to add to output -- UnitsDataArray.create uses the 'crs' portion contained in here\n        outputs : list[string], optional\n            Default is self.outputs. List of strings listing the outputs\n        **kwargs\n            {arr_kwargs}\n\n        Returns\n        -------\n        {arr_return}\n        \"\"\"\n\n        if attrs is None:\n            attrs = {}\n\n        if \"layer_style\" not in attrs:\n            attrs[\"layer_style\"] = self.style\n        if \"crs\" not in attrs:\n            attrs[\"crs\"] = coords.crs\n        if \"units\" not in attrs and self.units is not None:\n            attrs[\"units\"] = ureg.Unit(self.units)\n        if \"geotransform\" not in attrs:\n            try:\n                attrs[\"geotransform\"] = coords.geotransform\n            except (TypeError, AttributeError):\n                pass\n        if outputs is None:\n            outputs = self.outputs\n        if outputs == []:\n            outputs = None\n\n        return UnitsDataArray.create(coords, data=data, outputs=outputs, dtype=self.dtype, attrs=attrs, **kwargs)",
            "def trait_is_defined(self, name):\n        return trait_is_defined(self, name)",
            "def probe(self, lat=None, lon=None, time=None, alt=None, crs=None):\n        \"\"\"Evaluates every part of a node / pipeline at a point and records\n        which nodes are actively being used.\n\n        Parameters\n        ------------\n        lat : float, optional\n            Default is None. The latitude location\n        lon : float, optional\n            Default is None. The longitude location\n        time : float, np.datetime64, optional\n            Default is None. The time\n        alt : float, optional\n            Default is None. The altitude location\n        crs : str, optional\n            Default is None. The CRS of the request.\n\n        Returns\n        dict\n            A dictionary that contains the following for each node:\n            ```\n            {\n                \"active\": bool,   # If the node is being used or not\n                \"value\": float,   # The value of the node evaluated at that point\n                \"inputs\": list,   # List of names of input nodes (based on definition)\n                \"name\": str,      # node.style.name or self.base_ref if the style name is empty\n                \"node_hash\": str, # The node's hash\n            }\n            ```\n        \"\"\"\n        return probe_node(self, lat, lon, time, alt, crs)\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # Serialization\n    # -----------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def base_ref(self):\n        \"\"\"\n        Default reference/name in node definitions\n\n        Returns\n        -------\n        str\n            Name of the node in node definitions\n        \"\"\"\n        return self.__class__.__name__\n\n    @property",
            "def _base_definition(self):\n        d = OrderedDict()\n\n        # node and plugin\n        if self.__module__ == \"podpac\":\n            d[\"node\"] = self.__class__.__name__\n        elif self.__module__.startswith(\"podpac.\"):\n            _, module = self.__module__.split(\".\", 1)\n            d[\"node\"] = \"%s.%s\" % (module, self.__class__.__name__)\n        else:\n            d[\"plugin\"] = self.__module__\n            d[\"node\"] = self.__class__.__name__\n\n        # attrs/inputs\n        attrs = {}\n        inputs = {}\n        for name in self.attrs:\n            value = getattr(self, name)\n\n            if (\n                isinstance(value, Node)\n                or (\n                    isinstance(value, (list, tuple, np.ndarray))\n                    and (len(value) > 0)\n                    and all(isinstance(elem, Node) for elem in value)\n                )\n                or (\n                    isinstance(value, dict)\n                    and (len(value) > 0)\n                    and all(isinstance(elem, Node) for elem in value.values())\n                )\n            ):\n                inputs[name] = value\n            else:\n                attrs[name] = value\n\n        if \"units\" in attrs and attrs[\"units\"] is None:\n            del attrs[\"units\"]\n\n        if \"outputs\" in attrs and attrs[\"outputs\"] is None:\n            del attrs[\"outputs\"]\n\n        if \"output\" in attrs and attrs[\"output\"] is None:\n            del attrs[\"output\"]\n\n        if attrs:\n            d[\"attrs\"] = attrs\n\n        if inputs:\n            d[\"inputs\"] = inputs\n\n        # style\n        if self.style.definition:\n            d[\"style\"] = self.style.definition\n\n        return d\n\n    @cached_property",
            "def definition(self):\n        \"\"\"\n        Full node definition.\n\n        Returns\n        -------\n        OrderedDict\n            Dictionary-formatted node definition.\n        \"\"\"\n\n        if getattr(self, \"_definition_guard\", False):\n            raise NodeDefinitionError(\"node definition has a circular dependency\")\n\n        if not getattr(self, \"_traits_initialized_guard\", False):\n            raise NodeDefinitionError(\"node is not yet fully initialized\")\n\n        try:\n            self._definition_guard = True\n\n            nodes = []\n            refs = []\n            definitions = []",
            "def add_node(node):\n                for ref, n in zip(refs, nodes):\n                    if node == n:\n                        return ref\n\n                # get base definition\n                d = node._base_definition\n\n                if \"inputs\" in d:\n                    # sort and shallow copy\n                    d[\"inputs\"] = OrderedDict([(key, d[\"inputs\"][key]) for key in sorted(d[\"inputs\"].keys())])\n\n                    # replace nodes with references, adding nodes depth first\n                    for key, value in d[\"inputs\"].items():\n                        if isinstance(value, Node):\n                            d[\"inputs\"][key] = add_node(value)\n                        elif isinstance(value, (list, tuple, np.ndarray)):\n                            d[\"inputs\"][key] = [add_node(item) for item in value]\n                        elif isinstance(value, dict):\n                            d[\"inputs\"][key] = {k: add_node(v) for k, v in value.items()}\n                        else:\n                            raise TypeError(\"Invalid input '%s' of type '%s': %s\" % (key, type(value)))\n\n                if \"attrs\" in d:\n                    # sort and shallow copy\n                    d[\"attrs\"] = OrderedDict([(key, d[\"attrs\"][key]) for key in sorted(d[\"attrs\"].keys())])\n\n                # get base ref and then ensure it is unique\n                ref = node.base_ref\n                while ref in refs:\n                    if re.search(\"_[1-9][0-9]*$\", ref):\n                        ref, i = ref.rsplit(\"_\", 1)\n                        i = int(i)\n                    else:\n                        i = 0\n                    ref = \"%s_%d\" % (ref, i + 1)\n\n                nodes.append(node)\n                refs.append(ref)\n                definitions.append(d)\n\n                return ref\n\n            # add top level node\n            add_node(self)\n\n            # finalize, verify serializable, and return\n            definition = OrderedDict(zip(refs, definitions))\n            definition[\"podpac_version\"] = podpac.__version__\n            json.dumps(definition, cls=JSONEncoder)\n            return definition\n\n        finally:\n            self._definition_guard = False\n\n    @property",
            "def json(self):",
            "def json_pretty(self):",
            "def hash(self):",
            "def save(self, path):\n        \"\"\"\n        Write node to file.\n\n        Arguments\n        ---------\n        path : str\n            path to write to\n\n        See Also\n        --------\n        load : load podpac Node from file.\n        \"\"\"\n\n        with open(path, \"w\") as f:\n            json.dump(self.definition, f, separators=(\",\", \":\"), cls=JSONEncoder)",
            "def __eq__(self, other):\n        if not isinstance(other, Node):\n            return False\n        return self.hash == other.hash",
            "def __ne__(self, other):\n        if not isinstance(other, Node):\n            return True\n        return self.hash != other.hash\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # Caching Interface\n    # -----------------------------------------------------------------------------------------------------------------",
            "def get_cache(self, key, coordinates=None):\n        \"\"\"\n        Get cached data for this node.\n\n        Parameters\n        ----------\n        key : str\n            Key for the cached data, e.g. 'output'\n        coordinates : podpac.Coordinates, optional\n            Coordinates for which the cached data should be retrieved. Omit for coordinate-independent data.\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        ------\n        NodeException\n            Cached data not found.\n        \"\"\"\n\n        try:\n            self.definition\n        except NodeDefinitionError as e:\n            raise NodeException(\"Cache unavailable, %s (key='%s')\" % (e.args[0], key))\n\n        if self.cache_ctrl is None or not self.has_cache(key, coordinates=coordinates):\n            raise NodeException(\"cached data not found for key '%s' and coordinates %s\" % (key, coordinates))\n\n        return self.cache_ctrl.get(self, key, coordinates=coordinates)",
            "def put_cache(self, data, key, coordinates=None, expires=None, overwrite=True):\n        \"\"\"\n        Cache data for this node.\n\n        Parameters\n        ----------\n        data : any\n            The data to cache.\n        key : str\n            Unique key for the data, e.g. 'output'\n        coordinates : podpac.Coordinates, optional\n            Coordinates that the cached data depends on. Omit for coordinate-independent data.\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        overwrite : bool, optional\n            Overwrite existing data, default True.\n\n        Raises\n        ------\n        NodeException\n            Cached data already exists (and overwrite is False)\n        \"\"\"\n\n        try:\n            self.definition\n        except NodeDefinitionError as e:\n            raise NodeException(\"Cache unavailable, %s (key='%s')\" % (e.args[0], key))\n\n        if self.cache_ctrl is None:\n            return\n\n        if not overwrite and self.has_cache(key, coordinates=coordinates):\n            raise NodeException(\"Cached data already exists for key '%s' and coordinates %s\" % (key, coordinates))\n\n        with thread_manager.cache_lock:\n            self.cache_ctrl.put(self, data, key, coordinates=coordinates, expires=expires, update=overwrite)",
            "def has_cache(self, key, coordinates=None):\n        \"\"\"\n        Check for cached data for this node.\n\n        Parameters\n        ----------\n        key : str\n            Key for the cached data, e.g. 'output'\n        coordinates : podpac.Coordinates, optional\n            Coordinates for which the cached data should be retrieved. Omit for coordinate-independent data.\n\n        Returns\n        -------\n        bool\n            True if there is cached data for this node, key, and coordinates.\n        \"\"\"\n\n        try:\n            self.definition\n        except NodeDefinitionError as e:\n            raise NodeException(\"Cache unavailable, %s (key='%s')\" % (e.args[0], key))\n\n        if self.cache_ctrl is None:\n            return False\n\n        with thread_manager.cache_lock:\n            return self.cache_ctrl.has(self, key, coordinates=coordinates)",
            "def rem_cache(self, key, coordinates=None, mode=\"all\"):\n        \"\"\"\n        Clear cached data for this node.\n\n        Parameters\n        ----------\n        key : str\n            Delete cached objects with this key. If `'*'`, cached data is deleted for all keys.\n        coordinates : podpac.Coordinates, str, optional\n            Default is None. Delete cached objects for these coordinates. If `'*'`, cached data is deleted for all\n            coordinates, including coordinate-independent data. If None, will only affect coordinate-independent data.\n        mode: str, optional\n            Specify which cache stores are affected. Default 'all'.\n\n\n        See Also\n        ---------\n        `podpac.core.cache.cache.CacheCtrl.clear` to remove ALL cache for ALL nodes.\n        \"\"\"\n\n        try:\n            self.definition\n        except NodeDefinitionError as e:\n            raise NodeException(\"Cache unavailable, %s (key='%s')\" % (e.args[0], key))\n\n        if self.cache_ctrl is None:\n            return\n\n        self.cache_ctrl.rem(self, item=key, coordinates=coordinates, mode=mode)\n\n    # --------------------------------------------------------#\n    #  Class Methods (Deserialization)\n    # --------------------------------------------------------#\n\n    @classmethod",
            "def from_definition(cls, definition):\n        \"\"\"\n        Create podpac Node from a dictionary definition.\n\n        Arguments\n        ---------\n        d : dict\n            node definition\n\n        Returns\n        -------\n        :class:`Node`\n            podpac Node\n\n        See Also\n        --------\n        definition : node definition as a dictionary\n        from_json : create podpac node from a JSON definition\n        load : create a node from file\n        \"\"\"\n\n        if \"podpac_version\" in definition and definition[\"podpac_version\"] != podpac.__version__:\n            warnings.warn(\n                \"node definition version mismatch \"\n                \"(this node was created with podpac version '%s', \"\n                \"but your current podpac version is '%s')\" % (definition[\"podpac_version\"], podpac.__version__)\n            )\n\n        if len(definition) == 0:\n            raise ValueError(\"Invalid definition: definition cannot be empty.\")\n\n        # parse node definitions in order\n        nodes = OrderedDict()\n        output_node = None\n        for name, d in definition.items():\n            if name == \"podpac_output_node\":\n                output_node = d\n                continue\n            if name == \"podpac_version\":\n                continue\n\n            if \"node\" not in d:\n                raise ValueError(\"Invalid definition for node '%s': 'node' property required\" % name)\n\n            _process_kwargs(name, d, definition, nodes)\n\n        # look for podpac_output_node attribute\n        if output_node is None:\n            return list(nodes.values())[-1]\n\n        if output_node not in nodes:\n            raise ValueError(\n                \"Invalid definition for value 'podpac_output_node': reference to nonexistent node '%s' in lookup_attrs\"\n                % (output_node)\n            )\n        return nodes[output_node]\n\n    @classmethod",
            "def from_json(cls, s):\n        \"\"\"\n        Create podpac Node from a JSON definition.\n\n        Arguments\n        ---------\n        s : str\n            JSON-formatted node definition\n\n        Returns\n        -------\n        :class:`Node`\n            podpac Node\n\n        See Also\n        --------\n        json : node definition as a JSON string\n        load : create a node from file\n        \"\"\"\n\n        d = json.loads(s, object_pairs_hook=OrderedDict)\n        return cls.from_definition(d)\n\n    @classmethod",
            "def load(cls, path):\n        \"\"\"\n        Create podpac Node from file.\n\n        Arguments\n        ---------\n        path : str\n            path to text file containing a JSON-formatted node definition\n\n        Returns\n        -------\n        :class:`Node`\n            podpac Node\n\n        See Also\n        --------\n        save : save a node to file\n        \"\"\"\n\n        with open(path) as f:\n            d = json.load(f, object_pairs_hook=OrderedDict)\n        return cls.from_definition(d)\n\n    @classmethod",
            "def from_url(cls, url):\n        \"\"\"\n        Create podpac Node from a WMS/WCS request.\n\n        Arguments\n        ---------\n        url : str, dict\n            The raw WMS/WCS request url, or a dictionary of query parameters\n\n        Returns\n        -------\n        :class:`Node`\n            A full Node with sub-nodes based on the definition of the node from the URL\n\n        Notes\n        -------\n        The request can specify the PODPAC node by four different mechanism:\n\n        * Direct node name: PODPAC will look for an appropriate node in podpac.datalib\n        * JSON definition passed using the 'PARAMS' query string: Need to specify the special LAYER/COVERAGE value of\n          \"%PARAMS%\"\n        * By pointing at the JSON definition retrievable with a http GET request:\n          e.g. by setting LAYER/COVERAGE value to https:",
            "def from_name_params(cls, name, params=None):\n        \"\"\"\n        Create podpac Node from a WMS/WCS request.\n\n        Arguments\n        ---------\n        name : str\n            The name of the PODPAC Node / Layer\n        params : dict, optional\n            Default is None. Dictionary of parameters to modify node attributes, style, or completely/partially define the node.\n            This dictionary can either be a `Node.definition` or `Node.definition['attrs']`. Node, the specified `name` always\n            take precidence over anything defined in `params` (e.g. params['node'] won't be used).\n\n        Returns\n        -------\n        :class:`Node`\n            A full Node with sub-nodes based on the definition of the node from the node name and parameters\n\n        \"\"\"\n        layer = name\n        p = params\n\n        d = None\n        if p is None:\n            p = {}\n        definition = {}\n        # If one of the special names are in the params list, then add params to the root layer\n        if \"node\" in p or \"plugin\" in p or \"style\" in p or \"attrs\" in p:\n            definition.update(p)\n        else:\n            definition[\"attrs\"] = p\n        definition.update({\"node\": layer})  # The user-specified node name ALWAYS takes precidence.\n        d = OrderedDict({layer.replace(\".\", \"-\"): definition})\n\n        return cls.from_definition(d)\n\n    @classmethod",
            "def get_ui_spec(cls, help_as_html=False):\n        \"\"\"Get spec of node attributes for building a ui\n\n        Parameters\n        ----------\n        help_as_html : bool, optional\n            Default is False. If True, the docstrings will be converted to html before storing in the spec.\n\n        Returns\n        -------\n        dict\n            Spec for this node that is readily json-serializable\n        \"\"\"\n        filter = []\n        spec = {\"help\": cls.__doc__, \"module\": cls.__module__ + \".\" + cls.__name__, \"attrs\": {}, \"style\": {}}\n        # Strip out starting spaces in the help text so that markdown parsing works correctly\n        if spec[\"help\"] is None:\n            spec[\"help\"] = \"No help text to display.\"\n        spec[\"help\"] = spec[\"help\"].replace(\"\\n    \", \"\\n\")\n\n        if help_as_html:\n            from numpydoc.docscrape_sphinx import SphinxDocString\n            from docutils.core import publish_string\n\n            tmp = SphinxDocString(spec[\"help\"])\n            tmp2 = publish_string(str(tmp), writer_name=\"html\")\n            slc = slice(tmp2.index(b'<div class=\"document\">'), tmp2.index(b\"</body>\"))\n            spec[\"help\"] = tmp2[slc].decode()\n\n        # find any default values that are defined by function with decorators\n        # e.g. using @tl.default(\"trait_name\")\n        #",
            "def _default_trait_name(self): ...\n        function_defaults = {}\n        for attr in dir(cls):\n            atr = getattr(cls, attr)\n            if not isinstance(atr, tl.traitlets.DefaultHandler):\n                continue\n            try:\n                try:\n                    def_val = atr(cls())\n                except:\n                    def_val = atr(cls)\n                if isinstance(def_val, NodeTrait):\n                    def_val = def_val.name\n                    print(\"Changing Nodetrait to string\")\n                # if \"NodeTrait\" not in str(atr(cls)):\n                function_defaults[atr.trait_name] = def_val\n            except Exception:\n                _logger.warning(\n                    \"For node {}: Failed to generate default from function for trait {}\".format(\n                        cls.__name__, atr.trait_name\n                    )\n                )\n\n        for attr in dir(cls):\n            if attr in filter:\n                continue\n            attrt = getattr(cls, attr)\n            if not isinstance(attrt, tl.TraitType):\n                continue\n            if not attrt.metadata.get(\"attr\", False):\n                continue\n            type_ = attrt.__class__.__name__\n\n            try:\n                schema = getattr(attrt, \"_schema\")\n            except:\n                schema = None\n\n            type_extra = str(attrt)\n            if type_ == \"Union\":\n                type_ = [t.__class__.__name__ for t in attrt.trait_types]\n                type_extra = \"Union\"\n            elif type_ == \"Instance\":\n                type_ = attrt.klass.__name__\n                if type_ == \"Node\":\n                    type_ = \"NodeTrait\"\n                type_extra = attrt.klass\n            elif type_ == \"Dict\" and schema is None:\n                try:\n                    schema = {\n                        \"key\": getattr(attrt, \"_key_trait\").__class__.__name__,\n                        \"value\": getattr(attrt, \"_value_trait\").__class__.__name__,\n                    }\n                except Exception as e:\n                    print(\"Could not find schema for\", attrt, \" of type\", type_)\n                    schema = None\n\n            required = attrt.metadata.get(\"required\", False)\n            hidden = attrt.metadata.get(\"hidden\", False)\n            if attr in function_defaults:\n                default_val = function_defaults[attr]\n            else:\n                default_val = attrt.default()\n            if not isinstance(type_extra, str):\n                type_extra = str(type_extra)\n            try:\n                if np.isnan(default_val):\n                    default_val = \"nan\"\n            except:\n                pass\n\n            if default_val == tl.Undefined:\n                default_val = None\n\n            spec[\"attrs\"][attr] = {\n                \"type\": type_,\n                \"type_str\": type_extra,  # May remove this if not needed\n                \"values\": getattr(attrt, \"values\", None),\n                \"default\": default_val,\n                \"help\": attrt.help,\n                \"required\": required,\n                \"hidden\": hidden,\n                \"schema\": schema,\n            }\n\n        try:\n            # This returns the\n            style_json = json.loads(cls().style.json)  # load the style from the cls\n        except:\n            style_json = {}\n\n        spec[\"style\"] = style_json  # this does not work, because node not created yet?\n\n        \"\"\"\n        I will manually define generic defaults here. Eventually we may want to\n        dig into this and create node specific styling. This will have to be done under each\n        node. But may be difficult to add style to each node?\n\n        Example: podpac.core.algorithm.utility.SinCoords.Style ----> returns a tl.Instance\n        BUT if I do:\n        podpac.core.algorithm.utility.SinCoords().style.json ---> outputs style\n\n        ERROR if no parenthesis are given. So how can this be done without instantiating the class?\n\n        Will need to ask @MPU how to define a node specific style.\n        \"\"\"\n        # spec[\"style\"] = {\n        #     \"name\": \"?\",\n        #     \"units\": \"m\",\n        #     \"clim\": [-1.0, 1.0],\n        #     \"colormap\": \"jet\",\n        #     \"enumeration_legend\": \"?\",\n        #     \"enumeration_colors\": \"?\",\n        #     \"default_enumeration_legend\": \"unknown\",\n        #     \"default_enumeration_color\": (0.2, 0.2, 0.2),\n        # }\n\n        spec.update(getattr(cls, \"_ui_spec\", {}))\n        return spec",
            "def _lookup_input(nodes, name, value, definition):\n    \"\"\"check if inputs of a node are stored in nodes, if not add them\n\n    Parameters\n    -----------\n    nodes: OrderedDict\n        Keys: Node names (strings)\n        Values: Node objects\n\n    name: string\n        the Node whose inputs are being checked\n\n    value: string, list, dictionary:\n        the Node (or collection of Nodes) which is being looked\n\n    definition: pipeline definition\n\n    Returns\n    --------\n    node: the node searched for\n\n    Note: this function calles _process_kwargs, which alters nodes by loading a Node if it is not yet in nodes.\n\n    \"\"\"\n    # containers\n    if isinstance(value, list):\n        return [_lookup_input(nodes, name, elem, definition) for elem in value]\n\n    if isinstance(value, dict):\n        return {k: _lookup_input(nodes, name, v, definition) for k, v in value.items()}\n\n    # node reference\n    if not isinstance(value, six.string_types):\n        raise ValueError(\n            \"Invalid definition for node '%s': invalid reference '%s' of type '%s' in inputs\"\n            % (name, value, type(value))\n        )\n    # node not yet discovered yet\n    if not value in nodes:\n        # Look for it in the definition items:\n        for found_name, d in definition.items():\n            if value != found_name:\n                continue\n            # Load the node into nodes\n            _process_kwargs(found_name, d, definition, nodes)\n\n            break\n\n    if not value in nodes:\n        raise ValueError(\n            \"Invalid definition for node '%s': reference to nonexistent node '%s' in inputs\" % (name, value)\n        )\n    node = nodes[value]\n\n    # copy in debug mode\n    if settings[\"DEBUG\"]:\n        node = deepcopy(node)\n\n    return node",
            "def _lookup_attr(nodes, name, value):\n    # containers\n    if isinstance(value, list):\n        return [_lookup_attr(nodes, name, elem) for elem in value]\n\n    if isinstance(value, dict):\n        return {k: _lookup_attr(nodes, name, v) for k, v in value.items()}\n\n    if not isinstance(value, six.string_types):\n        raise ValueError(\n            \"Invalid definition for node '%s': invalid reference '%s' of type '%s' in lookup_attrs\"\n            % (name, value, type(value))\n        )\n\n    # node\n    elems = value.split(\".\")\n    if elems[0] not in nodes:\n        raise ValueError(\n            \"Invalid definition for node '%s': reference to nonexistent node '%s' in lookup_attrs\" % (name, elems[0])\n        )\n\n    # subattrs\n    attr = nodes[elems[0]]\n    for n in elems[1:]:\n        if not hasattr(attr, n):\n            raise ValueError(\n                \"Invalid definition for node '%s': reference to nonexistent attribute '%s' in lookup_attrs value '%s\"\n                % (name, n, value)\n            )\n        attr = getattr(attr, n)\n\n    # copy in debug mode\n    if settings[\"DEBUG\"]:\n        attr = deepcopy(attr)\n\n    return attr",
            "def _process_kwargs(name, d, definition, nodes):\n    \"\"\"create a node and add it to nodes\n\n    Parameters\n    -----------\n    nodes: OrderedDict\n        Keys: Node names (strings)\n        Values: Node objects\n\n    name: string\n        the Node which will be created\n\n    d: the definition of the node to be created\n\n    definition: pipeline definition\n\n    Returns\n    --------\n    Nothing, but loads the node with name \"name\" and definition \"d\" into nodes\n\n\n    \"\"\"\n    # get node class\n    module_root = d.get(\"plugin\", \"podpac\")\n    node_string = \"%s.%s\" % (module_root, d[\"node\"])\n    module_name, node_name = node_string.rsplit(\".\", 1)\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        raise ValueError(\"Invalid definition for node '%s': no module found '%s'\" % (name, module_name))\n    try:\n        node_class = getattr(module, node_name)\n    except AttributeError:\n        raise ValueError(\n            \"Invalid definition for node '%s': class '%s' not found in module '%s'\" % (name, node_name, module_name)\n        )\n\n    kwargs = {}\n    for k, v in d.get(\"attrs\", {}).items():\n        kwargs[k] = v\n\n    for k, v in d.get(\"inputs\", {}).items():\n        kwargs[k] = _lookup_input(nodes, name, v, definition)\n\n    for k, v in d.get(\"lookup_attrs\", {}).items():\n        kwargs[k] = _lookup_attr(nodes, name, v)\n\n    if \"style\" in d:\n        style_class = getattr(node_class, \"style\", Style)\n        if isinstance(style_class, tl.TraitType):\n            # Now we actually have to look through the class to see\n            # if there is a custom initializer for style\n            for attr in dir(node_class):\n                atr = getattr(node_class, attr)\n                if not isinstance(atr, tl.traitlets.DefaultHandler) or atr.trait_name != \"style\":\n                    continue\n                try:\n                    style_class = atr(node_class)\n                except Exception as e:\n                    # print (\"couldn't make style from class\", e)\n                    try:\n                        style_class = atr(node_class())\n                    except:\n                        # print (\"couldn't make style from class instance\", e)\n                        style_class = style_class.klass\n        try:\n            kwargs[\"style\"] = style_class.from_definition(d[\"style\"])\n        except Exception as e:\n            kwargs[\"style\"] = Style.from_definition(d[\"style\"])\n            # print (\"couldn't make style from inferred style class\", e)\n\n    for k in d:\n        if k not in [\"node\", \"inputs\", \"attrs\", \"lookup_attrs\", \"plugin\", \"style\"]:\n            raise ValueError(\"Invalid definition for node '%s': unexpected property '%s'\" % (name, k))\n\n    nodes[name] = node_class(**kwargs)\n\n\n# --------------------------------------------------------#\n#  Mixins\n# --------------------------------------------------------#",
            "class NoCacheMixin(tl.HasTraits):",
            "def _cache_ctrl_default(self):\n        return CacheCtrl([])",
            "class DiskCacheMixin(tl.HasTraits):",
            "def _cache_ctrl_default(self):\n        # get the default cache_ctrl and addd a disk cache store if necessary\n        default_ctrl = get_default_cache_ctrl()\n        stores = default_ctrl._cache_stores\n        if not any(isinstance(store, DiskCacheStore) for store in default_ctrl._cache_stores):\n            stores.append(DiskCacheStore())\n        return CacheCtrl(stores)\n\n\n# --------------------------------------------------------#\n#  Decorators\n# --------------------------------------------------------#"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/settings.py",
        "comments": [
            "//boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#environment-variable-configuration>`_",
            "//boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#environment-variable-configuration>`_"
        ],
        "docstrings": [
            "\"\"\"\nPodpac Settings\n\"\"\"",
            "\"\"\"\n    Persistently stored podpac settings\n\n    Podpac settings are persistently stored in a ``settings.json`` file created at runtime.\n    By default, podpac will create a settings json file in the users\n    home directory (``~/.config/podpac/settings.json``), or $XDG_CONFIG_HOME/podpac/settings.json when first run.\n\n    Default settings can be overridden or extended by:\n      * editing the ``settings.json`` file in the settings directory (i.e. ``~/.podpac/settings.json`` or\n      ``$XDG_CONFIG_HOME/podpac/settings.json``)\n      * creating a ``settings.json`` in the current working directory (i.e. ``./settings.json``)\n\n    If ``settings.json`` files exist in multiple places, podpac will load settings in the following order,\n    overwriting previously loaded settings in the process (i.e. highest numbered settings file prefered):\n      1. podpac settings defaults\n      2. settings directory  (``~/.podpac/settings.json``  or ``$XDG_CONFIG_HOME/podpac/settings.json``)\n      3. current working directory settings (``./settings.json``)\n\n    :attr:`settings.settings_path` shows the path of the last loaded settings file (e.g. the active settings file).\n    To persistently update the active settings file as changes are made at runtime,\n    set the ``settings['AUTOSAVE_SETTINGS']`` field to ``True``. The active setting file can be persistently\n    saved at any time using :meth:`settings.save`.\n\n    The default settings are shown below:\n\n    Attributes\n    ----------\n    DEFAULT_CRS : str\n        Default coordinate reference system for spatial coordinates. Defaults to 'EPSG:4326'.\n    AWS_ACCESS_KEY_ID : str\n        The access key for your AWS account.\n        See the `boto3 documentation\n        <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#environment-variable-configuration>`_\n        for more details.\n    AWS_SECRET_ACCESS_KEY : str\n        The secret key for your AWS account.\n        See the `boto3 documentation\n        <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#environment-variable-configuration>`_\n        for more details.\n    AWS_REGION_NAME : str\n        Name of the AWS region, e.g. us-west-1, us-west-2, etc.\n    AWS_BUDGET_AMOUNT : float\n        Budget amount for AWS resources\n    AWS_BUDGET_EMAIL : str\n        Notification email for when AWS usage reaches 80% of the `AWS_BUDGET_AMOUNT`\n    AWS_REQUESTER_PAYS : bool\n        Allow access to \"requester pays\" S3 buckets\n    DEFAULT_CACHE : list\n        Defines a default list of cache stores in priority order. Defaults to `['ram']`.\n    CACHE_NODE_OUTPUT_DEFAULT : bool\n        Default value for node ``cache_output`` trait. If True, the outputs of nodes (eval) will be automatically cached.\n    CACHE_DATASOURCE_OUTPUT_DEFAULT : bool\n        Default value for DataSource nodes ``cache_output`` trait. If True, the outputs of nodes (eval) will be automatically cached.\n    RAM_CACHE_MAX_BYTES : int\n        Maximum RAM cache size in bytes.\n        Note, for RAM cache only, the limit is applied to the total amount of RAM used by the python process;\n        not just the contents of the RAM cache. The python process will not be restrited by this limit,\n        but once the limit is reached, additions to the cache will be subject to it.\n        Defaults to ``1e9`` (~1G).\n        Set to `None` explicitly for no limit.\n    DISK_CACHE_MAX_BYTES : int\n        Maximum disk space for use by the disk cache in bytes.\n        Defaults to ``10e9`` (~10G).\n        Set to `None` explicitly for no limit.\n    S3_CACHE_MAX_BYTES : int\n        Maximum storage space for use by the s3 cache in bytes.\n        Defaults to ``10e9`` (~10G).\n        Set to `None` explicitly for no limit.\n    DISK_CACHE_DIR : str\n        Subdirectory to use for the disk cache. Defaults to ``'cache'`` in the podpac root directory.\n        Use settings.cache_path to access this settings (this property looks for the environmental variable\n        `XDG_CACHE_HOME` to adjust the location of the cache directory)\n    S3_CACHE_DIR : str\n        Subdirectory to use for S3 cache (within the specified S3 bucket). Defaults to ``'cache'``.\n    RAM_CACHE_ENABLED: bool\n        Enable caching to RAM. Note that if disabled, some nodes may fail. Defaults to ``True``.\n    DISK_CACHE_ENABLED: bool\n        Enable caching to disk. Note that if disabled, some nodes may fail. Defaults to ``True``.\n    S3_CACHE_ENABLED: bool\n        Enable caching to RAM. Note that if disabled, some nodes may fail. Defaults to ``True``.\n    ROOT_PATH : str\n        Path to primary podpac working directory. Defaults to the ``.podpac`` directory in the users home directory.\n    S3_BUCKET_NAME : str\n        The AWS S3 Bucket to use for cloud based processing.\n    FUNCTION_S3_INPUT : str\n        Folder within :attr:`S3_BUCKET_NAME` to use for triggering node evaluation.\n    FUNCTION_S3_OUTPUT : str\n        Folder within :attr:`S3_BUCKET_NAME` to use for outputs.\n    FUNCTION_FORCE_COMPUTE : bool\n        Force the lambda function to compute pipeline, even if result is already cached.\n    AUTOSAVE_SETTINGS: bool\n        Save settings automatically as they are changed during runtime. Defaults to ``False``.\n    MULTITHREADING: bool\n        Uses multithreaded evaluation, when applicable. Defaults to ``False``.\n    N_THREADS: int\n        Number of threads to use (only if MULTITHREADING is True). Defaults to ``10``.\n    CHUNK_SIZE: int, 'auto', None\n        Chunk size for iterative evaluation, when applicable (e.g. Reduce Nodes). Use None for no iterative evaluation,\n        and 'auto' to automatically calculate a chunk size based on the system. Defaults to ``None``.\n    \"\"\"",
            "\"\"\"Load default settings\"\"\"",
            "\"\"\"Load user settings from settings.json file\n\n        Parameters\n        ----------\n        path : str\n            Full path to containing directory of settings file\n        filename : str\n            Filename of custom settings file\n        \"\"\"",
            "\"\"\"Path to the last loaded ``settings.json`` file\n\n        Returns\n        -------\n        str\n            Path to the last loaded ``settings.json`` file\n        \"\"\"",
            "\"\"\"Path to the cache\n\n        Returns\n        -------\n        str\n            Path to where the cache is stored\n        \"\"\"",
            "\"\"\"\n        Show the podpac default settings\n        \"\"\"",
            "\"\"\"\n        Save current settings to active settings file\n\n        :attr:`settings.settings_path` shows the path to the currently active settings file\n\n        Parameters\n        ----------\n        filepath : str, optional\n            Path to settings file to save. Defaults to :attr:`self.settings_filepath`\n        \"\"\"",
            "\"\"\"\n        Reset settings to defaults.\n\n        This method will ignore the value in :attr:`AUTOSAVE_SETTINGS`.\n        To persistenly reset settings to defaults, call :meth:`settings.save()` after this method.\n        \"\"\"",
            "\"\"\"\n        Load a new settings file to be active\n\n        :attr:`settings.settings_path` shows the path to the currently active settings file\n\n        Parameters\n        ----------\n        path : str, optional\n            Path to directory which contains the settings file. Defaults to :attr:`DEFAULT_SETTINGS['ROOT_PATH']`\n        filename : str, optional\n            Filename of the settings file. Defaults to 'settings.json'\n        \"\"\"",
            "\"\"\"Allow unsafe evaluation for this podpac environment\n\n        Parameters\n        ----------\n        allow : bool, optional\n            Enable unsafe evaluation. Defaults to False.\n        \"\"\""
        ],
        "code_snippets": [
            "class PodpacSettings(dict):\n    \"\"\"\n    Persistently stored podpac settings\n\n    Podpac settings are persistently stored in a ``settings.json`` file created at runtime.\n    By default, podpac will create a settings json file in the users\n    home directory (``~/.config/podpac/settings.json``), or $XDG_CONFIG_HOME/podpac/settings.json when first run.\n\n    Default settings can be overridden or extended by:\n      * editing the ``settings.json`` file in the settings directory (i.e. ``~/.podpac/settings.json`` or\n      ``$XDG_CONFIG_HOME/podpac/settings.json``)\n      * creating a ``settings.json`` in the current working directory (i.e. ``./settings.json``)\n\n    If ``settings.json`` files exist in multiple places, podpac will load settings in the following order,\n    overwriting previously loaded settings in the process (i.e. highest numbered settings file prefered):\n      1. podpac settings defaults\n      2. settings directory  (``~/.podpac/settings.json``  or ``$XDG_CONFIG_HOME/podpac/settings.json``)\n      3. current working directory settings (``./settings.json``)\n\n    :attr:`settings.settings_path` shows the path of the last loaded settings file (e.g. the active settings file).\n    To persistently update the active settings file as changes are made at runtime,\n    set the ``settings['AUTOSAVE_SETTINGS']`` field to ``True``. The active setting file can be persistently\n    saved at any time using :meth:`settings.save`.\n\n    The default settings are shown below:\n\n    Attributes\n    ----------\n    DEFAULT_CRS : str\n        Default coordinate reference system for spatial coordinates. Defaults to 'EPSG:4326'.\n    AWS_ACCESS_KEY_ID : str\n        The access key for your AWS account.\n        See the `boto3 documentation\n        <https:",
            "def __init__(self):\n        self._loaded = False\n\n        # call dict init\n        super(PodpacSettings, self).__init__()\n\n        # load settings from default locations\n        self.load()\n\n        # set loaded flag\n        self._loaded = True",
            "def __setitem__(self, key, value):\n\n        # get old value if it exists\n        try:\n            old_val = deepcopy(self[key])\n        except KeyError:\n            old_val = None\n\n        super(PodpacSettings, self).__setitem__(key, value)\n\n        # save settings file if value has changed\n        if self._loaded and self[\"AUTOSAVE_SETTINGS\"] and old_val != value:\n            self.save()",
            "def __getitem__(self, key):\n\n        # return none if the parameter does not exist\n        try:\n            return super(PodpacSettings, self).__getitem__(key)\n        except KeyError:\n            return None",
            "def _load_defaults(self):",
            "def _load_user_settings(self, path=None, filename=\"settings.json\"):\n        \"\"\"Load user settings from settings.json file\n\n        Parameters\n        ----------\n        path : str\n            Full path to containing directory of settings file\n        filename : str\n            Filename of custom settings file\n        \"\"\"\n\n        # custom file path - only used if path is not None\n        filepath = os.path.join(path, filename) if path is not None else None\n\n        # home path location is in the ROOT_PATH\n        root_filepath = os.path.join(self[\"ROOT_PATH\"], filename)\n\n        # cwd path\n        cwd_filepath = os.path.join(os.getcwd(), filename)\n\n        # set settings path to default to start\n        self._settings_filepath = root_filepath\n\n        # if input path is specifed, create the input path if it doesn't exist\n        if path is not None:\n\n            # make empty settings path\n            if not os.path.exists(path):\n                raise ValueError(\"Input podpac settings path does not exist: {}\".format(path))\n\n        # order of paths to import settings - the later settings will overwrite earlier ones\n        filepath_choices = [root_filepath, cwd_filepath, filepath]\n\n        # try path choices in order, overwriting earlier ones with later ones\n        for p in filepath_choices:\n            # reset json settings\n            json_settings = None\n\n            # see if the path exists\n            if p is not None and os.path.exists(p):\n\n                try:\n                    with open(p, \"r\") as f:\n                        json_settings = json.load(f)\n                except JSONDecodeError:\n\n                    # if the root_filepath settings file is broken, raise\n                    if p == root_filepath:\n                        raise\n\n                # if path exists and settings loaded then load those settings into the dict\n                if json_settings is not None:\n                    for key in json_settings:\n                        self[key] = json_settings[key]\n\n                    # save this path as the active\n                    self._settings_filepath = p\n\n    @property",
            "def settings_path(self):\n        \"\"\"Path to the last loaded ``settings.json`` file\n\n        Returns\n        -------\n        str\n            Path to the last loaded ``settings.json`` file\n        \"\"\"\n        return self._settings_filepath\n\n    @property",
            "def cache_path(self):\n        \"\"\"Path to the cache\n\n        Returns\n        -------\n        str\n            Path to where the cache is stored\n        \"\"\"\n        if os.path.isabs(settings[\"DISK_CACHE_DIR\"]):\n            path = settings[\"DISK_CACHE_DIR\"]\n        else:\n            path = os.path.join(os.environ.get(\"XDG_CACHE_HOME\", settings[\"ROOT_PATH\"]), settings[\"DISK_CACHE_DIR\"])\n\n        return path\n\n    @property",
            "def defaults(self):\n        \"\"\"\n        Show the podpac default settings\n        \"\"\"\n        return DEFAULT_SETTINGS",
            "def save(self, filepath=None):\n        \"\"\"\n        Save current settings to active settings file\n\n        :attr:`settings.settings_path` shows the path to the currently active settings file\n\n        Parameters\n        ----------\n        filepath : str, optional\n            Path to settings file to save. Defaults to :attr:`self.settings_filepath`\n        \"\"\"\n\n        # custom filepath\n        if filepath is not None:\n            self._settings_filepath = filepath\n\n        # if no settings path is found, create\n        if not os.path.exists(self._settings_filepath):\n            os.makedirs(os.path.dirname(self._settings_filepath), exist_ok=True)\n\n        with open(self._settings_filepath, \"w\") as f:\n            json.dump(self, f, indent=4)",
            "def reset(self):\n        \"\"\"\n        Reset settings to defaults.\n\n        This method will ignore the value in :attr:`AUTOSAVE_SETTINGS`.\n        To persistenly reset settings to defaults, call :meth:`settings.save()` after this method.\n        \"\"\"\n\n        # ignore autosave\n        self[\"AUTOSAVE_SETTINGS\"] = False\n\n        # clear all values\n        self.clear()\n\n        # load default settings\n        self._load_defaults()",
            "def load(self, path=None, filename=\"settings.json\"):\n        \"\"\"\n        Load a new settings file to be active\n\n        :attr:`settings.settings_path` shows the path to the currently active settings file\n\n        Parameters\n        ----------\n        path : str, optional\n            Path to directory which contains the settings file. Defaults to :attr:`DEFAULT_SETTINGS['ROOT_PATH']`\n        filename : str, optional\n            Filename of the settings file. Defaults to 'settings.json'\n        \"\"\"\n        # load default settings\n        self._load_defaults()\n\n        # load user settings\n        self._load_user_settings(path, filename)\n\n        # it breaks things to set these paths to None, set back to default if set to None\n        if self[\"ROOT_PATH\"] is None:\n            self[\"ROOT_PATH\"] = DEFAULT_SETTINGS[\"ROOT_PATH\"]\n\n        if self[\"DISK_CACHE_DIR\"] is None:\n            self[\"DISK_CACHE_DIR\"] = DEFAULT_SETTINGS[\"DISK_CACHE_DIR\"]\n\n        if self[\"S3_CACHE_DIR\"] is None:\n            self[\"S3_CACHE_DIR\"] = DEFAULT_SETTINGS[\"S3_CACHE_DIR\"]\n\n    @property",
            "def allow_unsafe_eval(self):\n        return \"PODPAC_UNSAFE_EVAL\" in os.environ and os.environ[\"PODPAC_UNSAFE_EVAL\"] == self[\"UNSAFE_EVAL_HASH\"]",
            "def set_unsafe_eval(self, allow=False):\n        _logger.warning(\n            \"DEPRECATION WARNING: The `set_unsafe_eval` method has been deprecated and will be removed in future versions of PODPAC. Use `allow_unrestricted_code_execution` instead. \"\n        )\n        self.allow_unrestricted_code_execution(allow)",
            "def allow_unrestricted_code_execution(self, allow=False):\n        \"\"\"Allow unsafe evaluation for this podpac environment\n\n        Parameters\n        ----------\n        allow : bool, optional\n            Enable unsafe evaluation. Defaults to False.\n        \"\"\"\n        if allow:\n            os.environ[\"PODPAC_UNSAFE_EVAL\"] = self[\"UNSAFE_EVAL_HASH\"]\n            _logger.warning(\n                \"Setting unrestricted code execution can results in vulnerabilities on publically accessible servers. Use with caution.\"\n            )\n        else:\n            if \"PODPAC_UNSAFE_EVAL\" in os.environ:\n                os.environ.pop(\"PODPAC_UNSAFE_EVAL\")",
            "def __enter__(self):\n        # save original settings\n        self._original = {k: v for k, v in self.items()}",
            "def __exit__(self, type, value, traceback):\n        # restore original settings\n        for k, v in self._original.items():\n            self[k] = v\n\n\n# load settings dict when module is loaded\nsettings = PodpacSettings()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/disk_cache_store.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Cache that uses a folder on a local disk file system.\"\"\"",
            "\"\"\"Initialize a cache that uses a folder on a local disk file system.\"\"\"",
            "\"\"\"\n        Remove expired entries and orphaned metadata.\n        \"\"\""
        ],
        "code_snippets": [
            "class DiskCacheStore(FileCacheStore):",
            "def __init__(self):",
            "def size(self):\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(self._root_dir_path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n        return total_size\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # helper methods\n    # -----------------------------------------------------------------------------------------------------------------\n\n    def search(self, node, item=CacheWildCard(), coordinates=CacheWildCard()):\n        pattern = self._path_join(self._get_node_dir(node), self._get_filename_pattern(node, item, coordinates))\n        return [path for path in glob.glob(pattern) if not path.endswith(\".meta\")]\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # file storage abstraction\n    # -----------------------------------------------------------------------------------------------------------------",
            "def _save(self, path, s, metadata=None):\n        with open(path, \"wb\") as f:\n            f.write(s)\n\n        if metadata:\n            metadata_path = \"%s.meta\" % path\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata, f)",
            "def _load(self, path):\n        with open(path, \"rb\") as f:\n            return f.read()",
            "def _path_join(self, path, *paths):\n        return os.path.join(path, *paths)",
            "def _basename(self, path):\n        return os.path.basename(path)",
            "def _remove(self, path):\n        os.remove(path)\n        if os.path.exists(\"%s.meta\" % path):\n            os.remove(\"%s.meta\" % path)",
            "def _exists(self, path):\n        return os.path.exists(path)",
            "def _is_empty(self, directory):\n        return os.path.exists(directory) and os.path.isdir(directory) and not os.listdir(directory)",
            "def _rmdir(self, directory):\n        os.rmdir(directory)",
            "def _rmtree(self, path, ignore_errors=False):\n        shutil.rmtree(path, ignore_errors=True)",
            "def _make_dir(self, path):\n        if not os.path.exists(path):\n            os.makedirs(path)",
            "def _dirname(self, path):\n        return os.path.dirname(path)",
            "def _get_metadata(self, path, key):\n        metadata_path = \"%s.meta\" % path\n        try:\n            with open(metadata_path, \"r\") as f:\n                metadata = json.load(f)\n        except IOError:\n            # missing, permissions\n            logger.exception(\"Error reading metadata file: '%s'\" % metadata_path)\n            return None\n        except ValueError:\n            # invalid json\n            logger.exception(\"Error reading metadata file: '%s'\" % metadata_path)\n            return None\n\n        return metadata.get(key)",
            "def _set_metadata(self, path, key, value):\n        metadata_path = \"%s.meta\" % path\n\n        # read existing\n        try:\n            with open(metadata_path, \"r\") as f:\n                metadata = json.load(f)\n        except IOError:\n            # missing, permissions\n            logger.exception(\"Error reading metadata file: '%s'\" % metadata_path)\n            metadata = {}\n        except ValueError:\n            # invalid json\n            logger.exception(\"Error reading metadata file: '%s'\" % metadata_path)\n            metadata = {}\n\n        # write\n        metadata[key] = value\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f)",
            "def cleanup(self):\n        \"\"\"\n        Remove expired entries and orphaned metadata.\n        \"\"\"\n\n        for root, dirnames, filenames in os.walk(self._root_dir_path):\n            for filename in fnmatch.filter(filenames, \"*.meta\"):\n                metadata_path = os.path.join(root, filename)\n                path = os.path.join(root, filename[:-5])  # strip .meta\n                if not os.path.exists(path):  # orphaned\n                    os.remove(metadata_path)\n                elif self._expired(path):\n                    # _expired removes the entry automatically\n                    pass\n\n        # remove empty directories\n        for root, dirnames, filenames in os.walk(self._root_dir_path):\n            for dirname in dirnames:\n                path = os.path.join(root, dirname)\n                if not os.path.exists(path):\n                    continue\n                if not [f for r, d, fs in os.walk(path) for f in fs]:\n                    shutil.rmtree(path)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/cache_ctrl.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    Get the default CacheCtrl according to the settings.\n\n    Returns\n    -------\n    ctrl : CacheCtrl or None\n        Default CachCtrl\n    \"\"\"",
            "\"\"\"\n    Make a cache_ctrl from a list of cache store types.\n\n    Arguments\n    ---------\n    names : str or list\n        cache name or names, e.g. 'ram' or ['ram', 'disk'].\n\n    Returns\n    -------\n    ctrl : CacheCtrl\n        CachCtrl using the specified cache names\n    \"\"\"",
            "\"\"\"\n    Clear the entire default cache_ctrl.\n\n    Arguments\n    ---------\n    mode : str\n        determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n    \"\"\"",
            "\"\"\"Objects of this class are used to manage multiple CacheStore objects of different types\n    (e.g. RAM, local disk, s3) and serve as the interface to the caching module.\n    \"\"\"",
            "\"\"\"Initialize a CacheCtrl object with a list of CacheStore objects.\n        Care should be taken to provide the cache_stores list in the order that\n        they should be interogated. CacheStore objects with faster access times\n        (e.g. RAM) should appear before others (e.g. local disk, or s3).\n\n        Parameters\n        ----------\n        cache_stores : list, optional\n            list of CacheStore objects to manage, in the order that they should be interrogated.\n        \"\"\"",
            "\"\"\"Cache data for specified node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"",
            "\"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheError\n            If the data is not in the cache.\n        \"\"\"",
            "\"\"\"Check for cached data for this node\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n\n        Returns\n        -------\n        has_cache : bool\n             True if there as a cached object for this node for the given key and coordinates.\n        \"\"\"",
            "\"\"\"Delete cached data for this node.\n\n        Parameters\n        ----------\n        node : Node, str\n            node requesting storage.\n        item : str\n            Delete only cached objects with this item/key. Use `'*'` to match all keys.\n        coordinates : :class:`podpac.Coordinates`, str\n            Delete only cached objects for these coordinates. Use `'*'` to match all coordinates.\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n        \"\"\"",
            "\"\"\"\n        Clear all cached data.\n\n        Parameters\n        ------------\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n        \"\"\"",
            "\"\"\"\n        Cleanup all cache stores.\n\n        Removes expired cache entries, orphaned metadata, empty directories, etc.\n        \"\"\""
        ],
        "code_snippets": [
            "def get_default_cache_ctrl():\n    \"\"\"\n    Get the default CacheCtrl according to the settings.\n\n    Returns\n    -------\n    ctrl : CacheCtrl or None\n        Default CachCtrl\n    \"\"\"\n\n    if settings.get(\"DEFAULT_CACHE\") is None:  # missing or None\n        return CacheCtrl([])\n\n    return make_cache_ctrl(settings[\"DEFAULT_CACHE\"])",
            "def make_cache_ctrl(names):\n    \"\"\"\n    Make a cache_ctrl from a list of cache store types.\n\n    Arguments\n    ---------\n    names : str or list\n        cache name or names, e.g. 'ram' or ['ram', 'disk'].\n\n    Returns\n    -------\n    ctrl : CacheCtrl\n        CachCtrl using the specified cache names\n    \"\"\"\n\n    if isinstance(names, six.string_types):\n        names = [names]\n\n    for name in names:\n        if name not in _CACHE_STORES:\n            raise ValueError(\"Unknown cache store type '%s', options are %s\" % (name, list(_CACHE_STORES)))\n\n    return CacheCtrl([_CACHE_STORES[name]() for name in names])",
            "def clear_cache(mode=\"all\"):\n    \"\"\"\n    Clear the entire default cache_ctrl.\n\n    Arguments\n    ---------\n    mode : str\n        determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n    \"\"\"\n\n    cache_ctrl = get_default_cache_ctrl()\n    cache_ctrl.clear(mode=mode)",
            "def cache_cleanup():\n    cache_ctrl = get_default_cache_ctrl()\n    cache_ctrl.cleanup()",
            "class CacheCtrl(object):\n\n    \"\"\"Objects of this class are used to manage multiple CacheStore objects of different types\n    (e.g. RAM, local disk, s3) and serve as the interface to the caching module.\n    \"\"\"",
            "def __init__(self, cache_stores=[]):\n        \"\"\"Initialize a CacheCtrl object with a list of CacheStore objects.\n        Care should be taken to provide the cache_stores list in the order that\n        they should be interogated. CacheStore objects with faster access times\n        (e.g. RAM) should appear before others (e.g. local disk, or s3).\n\n        Parameters\n        ----------\n        cache_stores : list, optional\n            list of CacheStore objects to manage, in the order that they should be interrogated.\n        \"\"\"\n\n        self._cache_stores = cache_stores",
            "def __repr__(self):\n        return \"CacheCtrl(cache_stores=%s)\" % self.cache_stores\n\n    @property",
            "def cache_stores(self):\n        return [_CACHE_NAMES[store.__class__] for store in self._cache_stores]",
            "def _get_cache_stores_by_mode(self, mode=\"all\"):\n        return [c for c in self._cache_stores if mode in c.cache_modes]",
            "def put(self, node, data, item, coordinates=None, expires=None, mode=\"all\", update=True):\n        \"\"\"Cache data for specified node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"\n\n        if not isinstance(node, podpac.Node):\n            raise TypeError(\"Invalid node (must be of type Node, not '%s')\" % type(node))\n\n        if not isinstance(item, six.string_types):\n            raise TypeError(\"Invalid item (must be a string, not '%s')\" % (type(item)))\n\n        if not isinstance(coordinates, podpac.Coordinates) and coordinates is not None:\n            raise TypeError(\"Invalid coordinates (must be of type 'Coordinates', not '%s')\" % type(coordinates))\n\n        if mode not in _CACHE_MODES:\n            raise ValueError(\"Invalid mode (must be one of %s, not '%s')\" % (_CACHE_MODES, mode))\n\n        if item == \"*\":\n            raise ValueError(\"Invalid item ('*' is reserved)\")\n\n        for c in self._get_cache_stores_by_mode(mode):\n            c.put(node=node, data=data, item=item, coordinates=coordinates, expires=expires, update=update)",
            "def get(self, node, item, coordinates=None, mode=\"all\"):\n        \"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheError\n            If the data is not in the cache.\n        \"\"\"\n\n        if not isinstance(node, podpac.Node):\n            raise TypeError(\"Invalid node (must be of type Node, not '%s')\" % type(node))\n\n        if not isinstance(item, six.string_types):\n            raise TypeError(\"Invalid item (must be a string, not '%s')\" % (type(item)))\n\n        if not isinstance(coordinates, podpac.Coordinates) and coordinates is not None:\n            raise TypeError(\"Invalid coordinates (must be of type 'Coordinates', not '%s')\" % type(coordinates))\n\n        if mode not in _CACHE_MODES:\n            raise ValueError(\"Invalid mode (must be one of %s, not '%s')\" % (_CACHE_MODES, mode))\n\n        if item == \"*\":\n            raise ValueError(\"Invalid item ('*' is reserved)\")\n\n        for c in self._get_cache_stores_by_mode(mode):\n            if c.has(node=node, item=item, coordinates=coordinates):\n                return c.get(node=node, item=item, coordinates=coordinates)\n        raise CacheException(\"Requested data is not in any cache stores.\")",
            "def has(self, node, item, coordinates=None, mode=\"all\"):\n        \"\"\"Check for cached data for this node\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n\n        Returns\n        -------\n        has_cache : bool\n             True if there as a cached object for this node for the given key and coordinates.\n        \"\"\"\n\n        if not isinstance(node, podpac.Node):\n            raise TypeError(\"Invalid node (must be of type Node, not '%s')\" % type(node))\n\n        if not isinstance(item, six.string_types):\n            raise TypeError(\"Invalid item (must be a string, not '%s')\" % (type(item)))\n\n        if not isinstance(coordinates, podpac.Coordinates) and coordinates is not None:\n            raise TypeError(\"Invalid coordinates (must be of type 'Coordinates', not '%s')\" % type(coordinates))\n\n        if mode not in _CACHE_MODES:\n            raise ValueError(\"Invalid mode (must be one of %s, not '%s')\" % (_CACHE_MODES, mode))\n\n        if item == \"*\":\n            raise ValueError(\"Invalid item ('*' is reserved)\")\n\n        for c in self._get_cache_stores_by_mode(mode):\n            if c.has(node=node, item=item, coordinates=coordinates):\n                return True\n\n        return False",
            "def rem(self, node, item, coordinates=None, mode=\"all\"):\n        \"\"\"Delete cached data for this node.\n\n        Parameters\n        ----------\n        node : Node, str\n            node requesting storage.\n        item : str\n            Delete only cached objects with this item/key. Use `'*'` to match all keys.\n        coordinates : :class:`podpac.Coordinates`, str\n            Delete only cached objects for these coordinates. Use `'*'` to match all coordinates.\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n        \"\"\"\n\n        if not isinstance(node, podpac.Node):\n            raise TypeError(\"Invalid node (must be of type Node, not '%s')\" % type(node))\n\n        if not isinstance(item, six.string_types):\n            raise TypeError(\"Invalid item (must be a string, not '%s')\" % (type(item)))\n\n        if not isinstance(coordinates, podpac.Coordinates) and coordinates is not None and coordinates != \"*\":\n            raise TypeError(\"Invalid coordinates (must be '*' or of type 'Coordinates', not '%s')\" % type(coordinates))\n\n        if mode not in _CACHE_MODES:\n            raise ValueError(\"Invalid mode (must be one of %s, not '%s')\" % (_CACHE_MODES, mode))\n\n        if item == \"*\":\n            item = CacheWildCard()\n\n        if coordinates == \"*\":\n            coordinates = CacheWildCard()\n\n        for c in self._get_cache_stores_by_mode(mode):\n            c.rem(node=node, item=item, coordinates=coordinates)",
            "def clear(self, mode=\"all\"):\n        \"\"\"\n        Clear all cached data.\n\n        Parameters\n        ------------\n        mode : str\n            determines what types of the `CacheStore` are affected. Options: 'ram', 'disk', 'network', 'all'. Default 'all'.\n        \"\"\"\n\n        if mode not in _CACHE_MODES:\n            raise ValueError(\"Invalid mode (must be one of %s, not '%s')\" % (_CACHE_MODES, mode))\n\n        for c in self._get_cache_stores_by_mode(mode):\n            c.clear()",
            "def cleanup(self):\n        \"\"\"\n        Cleanup all cache stores.\n\n        Removes expired cache entries, orphaned metadata, empty directories, etc.\n        \"\"\"\n\n        for c in self._cache_stores:\n            c.cleanup()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/cache_store.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nDefines the common interface for a cache store.\n\"\"\"",
            "\"\"\"Abstract parent class for classes representing actual data stores (e.g. RAM, local disk, network storage).\n    Includes implementation of common hashing operations and call signature for required abstract methods:\n    put(), get(), rem(), has()\n    \"\"\"",
            "\"\"\"Return size of cache store in bytes\"\"\"",
            "\"\"\"Cache data for specified node.\n\n        Parameters\n        -----------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"",
            "\"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheError\n            If the data is not in the cache or is expired.\n        \"\"\"",
            "\"\"\"Delete cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str, optional\n            Delete only cached objects with this item name or key.\n        coordinates : :class:`podpac.Coordinates`\n            Delete only cached objects for these coordinates.\n        \"\"\"",
            "\"\"\"Check for cached data for this node\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n\n        Returns\n        -------\n        has_cache : bool\n             True if there as a valid cached object for this node for the given key and coordinates.\n        \"\"\"",
            "\"\"\"\n        Clear all cached data.\n        \"\"\"",
            "\"\"\"\n        Cache housekeeping, e.g. remove expired entries.\n        \"\"\""
        ],
        "code_snippets": [
            "class CacheStore(object):\n    \"\"\"Abstract parent class for classes representing actual data stores (e.g. RAM, local disk, network storage).\n    Includes implementation of common hashing operations and call signature for required abstract methods:\n    put(), get(), rem(), has()\n    \"\"\"\n\n    cache_modes = []\n    _limit_setting = None",
            "def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\n    @property",
            "def max_size(self):\n        return settings.get(self._limit_setting)\n\n    @property",
            "def size(self):",
            "def put(self, node, data, item, coordinates=None, expires=None, update=True):\n        \"\"\"Cache data for specified node.\n\n        Parameters\n        -----------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"\n        raise NotImplementedError",
            "def get(self, node, item, coordinates=None):\n        \"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheError\n            If the data is not in the cache or is expired.\n        \"\"\"\n        raise NotImplementedError",
            "def rem(self, node=None, item=None, coordinates=None):\n        \"\"\"Delete cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str, optional\n            Delete only cached objects with this item name or key.\n        coordinates : :class:`podpac.Coordinates`\n            Delete only cached objects for these coordinates.\n        \"\"\"\n        raise NotImplementedError",
            "def has(self, node, item, coordinates=None):\n        \"\"\"Check for cached data for this node\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item or key, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n\n        Returns\n        -------\n        has_cache : bool\n             True if there as a valid cached object for this node for the given key and coordinates.\n        \"\"\"\n        raise NotImplementedError",
            "def clear(self, node):\n        \"\"\"\n        Clear all cached data.\n        \"\"\"\n        raise NotImplementedError",
            "def cleanup(self):\n        \"\"\"\n        Cache housekeeping, e.g. remove expired entries.\n        \"\"\"\n        raise NotImplementedError"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/utils.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Represent wildcard matches for inputs to remove operations (`rem`)\n    that can match multiple items in the cache.\n    \"\"\"",
            "\"\"\"\n    Parse and calculate an expiration timestamp.\n\n    Arguments\n    ---------\n    value : float, datetime, timedelta, str\n            User-friendly expiration value.\n             * string values are parsed as datetime or timedelta.\n             * timedeltas are added to the current time.\n             * floats are interpreted as timestamps\n\n    Returns\n    -------\n    expiration : float\n            expiration timestamp\n    \"\"\""
        ],
        "code_snippets": [
            "class CacheException(Exception):\n    pass",
            "class CacheWildCard(object):\n    \"\"\"Represent wildcard matches for inputs to remove operations (`rem`)\n    that can match multiple items in the cache.\n    \"\"\"",
            "def __eq__(self, other):\n        return True",
            "def expiration_timestamp(value):\n    \"\"\"\n    Parse and calculate an expiration timestamp.\n\n    Arguments\n    ---------\n    value : float, datetime, timedelta, str\n            User-friendly expiration value.\n             * string values are parsed as datetime or timedelta.\n             * timedeltas are added to the current time.\n             * floats are interpreted as timestamps\n\n    Returns\n    -------\n    expiration : float\n            expiration timestamp\n    \"\"\"\n\n    if value is None:\n        return None\n\n    if isinstance(value, float):\n        return value\n\n    expires = value\n\n    # parse string datetime or timedelta\n    if isinstance(expires, string_types):\n        try:\n            expires = np.datetime64(expires).item()\n        except:\n            pass\n\n    if isinstance(expires, string_types) and \",\" in expires:\n        try:\n            expires = np.timedelta64(*expires.split(\",\")).item()\n        except:\n            pass\n\n    # extract datetime or timedelta from numpy types\n    if isinstance(expires, (np.datetime64, np.timedelta64)):\n        expires = expires.item()\n\n    # calculate and return expiration date\n    if isinstance(expires, datetime.datetime):\n        return expires.timestamp()\n    elif isinstance(expires, datetime.date):\n        return datetime.datetime.combine(expires, datetime.datetime.min.time()).timestamp()\n    elif isinstance(expires, datetime.timedelta):\n        return (datetime.datetime.now() + expires).timestamp()\n    elif isinstance(value, string_types):\n        raise ValueError(\"Invalid expiration date or delta '%s'\" % value)\n    else:\n        raise TypeError(\"Invalid expiration date or delta '%s' of type %s\" % (value, type(value)))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/s3_cache_store.py",
        "comments": [
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_object",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.delete_object"
        ],
        "docstrings": [
            "\"\"\"Initialize a cache that uses a folder on a local disk file system.\n\n        Parameters\n        ----------\n        max_size : None, optional\n            Maximum allowed size of the cache store in bytes. Defaults to podpac 'S3_CACHE_MAX_BYTES' setting, or no limit if this setting does not exist.\n        use_settings_limit : bool, optional\n            Use podpac settings to determine cache limits if True, this will also cause subsequent runtime changes to podpac settings module to effect the limit on this cache. Default is True.\n        s3_bucket : str, optional\n            bucket name, overides settings\n        aws_region_name : str, optional\n            e.g. 'us-west-1', 'us-west-2','us-east-1'\n        aws_access_key_id : str, optional\n            overides podpac settings if both `aws_access_key_id` and `aws_secret_access_key` are specified\n        aws_secret_access_key : str, optional\n            overides podpac settings if both `aws_access_key_id` and `aws_secret_access_key` are specified\n        \"\"\"",
            "\"\"\"\n        Remove expired entries.\n        \"\"\"",
            "\"\"\"Fileglob to match files that could be storing cached data for specified node,key,coordinates\n\n        Parameters\n        ----------\n        node : podpac.core.node.Node\n        item : str, CacheWildCard\n            CacheWildCard indicates to match any key\n        coordinates : podpac.core.coordinates.coordinates.Coordinates, CacheWildCard, None\n            CacheWildCard indicates to macth any coordinates\n\n        Returns\n        -------\n        TYPE : str\n            Fileglob of existing paths that match the request\n        \"\"\""
        ],
        "code_snippets": [
            "class S3CacheStore(FileCacheStore):  # pragma: no cover\n\n    cache_mode = \"s3\"\n    cache_modes = set([\"s3\", \"all\"])\n    _limit_setting = \"S3_CACHE_MAX_BYTES\"\n    _delim = \"/\"",
            "def __init__(self, s3_bucket=None, aws_region_name=None, aws_access_key_id=None, aws_secret_access_key=None):\n        \"\"\"Initialize a cache that uses a folder on a local disk file system.\n\n        Parameters\n        ----------\n        max_size : None, optional\n            Maximum allowed size of the cache store in bytes. Defaults to podpac 'S3_CACHE_MAX_BYTES' setting, or no limit if this setting does not exist.\n        use_settings_limit : bool, optional\n            Use podpac settings to determine cache limits if True, this will also cause subsequent runtime changes to podpac settings module to effect the limit on this cache. Default is True.\n        s3_bucket : str, optional\n            bucket name, overides settings\n        aws_region_name : str, optional\n            e.g. 'us-west-1', 'us-west-2','us-east-1'\n        aws_access_key_id : str, optional\n            overides podpac settings if both `aws_access_key_id` and `aws_secret_access_key` are specified\n        aws_secret_access_key : str, optional\n            overides podpac settings if both `aws_access_key_id` and `aws_secret_access_key` are specified\n        \"\"\"\n\n        if not settings[\"S3_CACHE_ENABLED\"]:\n            raise CacheException(\"S3 cache is disabled in the podpac settings.\")\n\n        self._root_dir_path = settings[\"S3_CACHE_DIR\"]\n\n        if s3_bucket is None:\n            s3_bucket = settings[\"S3_BUCKET_NAME\"]\n        if aws_access_key_id is None or aws_secret_access_key is None:\n            aws_access_key_id = settings[\"AWS_ACCESS_KEY_ID\"]\n            aws_secret_access_key = settings[\"AWS_SECRET_ACCESS_KEY\"]\n        if aws_region_name is None:\n            aws_region_name = settings[\"AWS_REGION_NAME\"]\n        aws_session = boto3.session.Session(region_name=aws_region_name)\n        self._s3_client = aws_session.client(\n            \"s3\",\n            # config= boto3.session.Config(signature_version='s3v4'),\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n        )\n        self._s3_bucket = s3_bucket\n\n        try:\n            self._s3_client.head_bucket(Bucket=self._s3_bucket)\n        except Exception as e:\n            raise e\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # main cache API\n    # -----------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def size(self):\n        paginator = self._s3_client.get_paginator(\"list_objects\")\n        operation_parameters = {\"Bucket\": self._s3_bucket, \"Prefix\": self._root_dir_path}\n        page_iterator = paginator.paginate(**operation_parameters)\n        total_size = 0\n        for page in page_iterator:\n            if \"Contents\" in page:\n                for obj in page[\"Contents\"]:\n                    total_size += obj[\"Size\"]\n        return total_size",
            "def cleanup(self):\n        \"\"\"\n        Remove expired entries.\n        \"\"\"\n\n        pass  # TODO metadata\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # helper methods\n    # -----------------------------------------------------------------------------------------------------------------\n\n    def search(self, node, item=CacheWildCard(), coordinates=CacheWildCard()):\n        \"\"\"Fileglob to match files that could be storing cached data for specified node,key,coordinates\n\n        Parameters\n        ----------\n        node : podpac.core.node.Node\n        item : str, CacheWildCard\n            CacheWildCard indicates to match any key\n        coordinates : podpac.core.coordinates.coordinates.Coordinates, CacheWildCard, None\n            CacheWildCard indicates to macth any coordinates\n\n        Returns\n        -------\n        TYPE : str\n            Fileglob of existing paths that match the request\n        \"\"\"\n\n        delim = self._delim\n        prefix = self._get_node_dir(node)\n        prefix = prefix if prefix.endswith(delim) else prefix + delim\n        response = self._s3_client.list_objects_v2(Bucket=self._s3_bucket, Prefix=prefix, Delimiter=delim)\n\n        if response[\"KeyCount\"] > 0:\n            obj_names = [o[\"Key\"].replace(prefix, \"\") for o in response[\"Contents\"]]\n        else:\n            obj_names = []\n\n        node_dir = self._get_node_dir(node)\n        obj_names = fnmatch.filter(obj_names, self._get_filename_pattern(node, item, coordinates))\n        paths = [self._path_join(node_dir, filename) for filename in obj_names]\n        return paths\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # file storage abstraction\n    # -----------------------------------------------------------------------------------------------------------------\n\n    def _save(self, path, s, metadata=None):\n        # note s needs to be b'bytes' or file below\n        # https:",
            "def _save(self, path, s, metadata=None):\n        # note s needs to be b'bytes' or file below\n        # https:",
            "def _load(self, path):\n        response = self._s3_client.get_object(Bucket=self._s3_bucket, Key=path)\n        return response[\"Body\"].read()",
            "def _path_join(self, *paths):\n        return self._delim.join(paths)",
            "def _basename(self, path):\n        if self._delim in path:\n            dirname, basename = path.rsplit(self._delim, 1)\n        else:\n            basename = path\n        return basename",
            "def _remove(self, path):\n        self._s3_client.delete_object(Bucket=self._s3_bucket, Key=path)",
            "def _exists(self, path):\n        response = self._s3_client.list_objects_v2(Bucket=self._s3_bucket, Prefix=path)\n        obj_count = response[\"KeyCount\"]\n        return obj_count == 1 and response[\"Contents\"][0][\"Key\"] == path",
            "def _make_dir(self, path):\n        # Does not need to do anything for S3 as the prefix is just part of the object name.\n        # note: I believe AWS uses prefixes to decide how to partition objects in a bucket which could affect performance.\n        pass",
            "def _rmtree(self, path):\n        paginator = self._s3_client.get_paginator(\"list_objects_v2\")\n        pages = paginator.paginate(Bucket=self._s3_bucket, Prefix=path)\n\n        to_delete = dict(Objects=[])\n        for item in pages.search(\"Contents\"):\n            if item:\n                to_delete[\"Objects\"].append(dict(Key=item[\"Key\"]))\n            if len(to_delete[\"Objects\"]) >= 1000:\n                self._s3_client.delete_objects(Bucket=self._s3_bucket, Delete=to_delete)\n                to_delete = dict(Objects=[])\n\n        if len(to_delete[\"Objects\"]):\n            self._s3_client.delete_objects(Bucket=self._s3_bucket, Delete=to_delete)",
            "def _is_empty(self, directory):\n        if not directory.endswith(self._delim):\n            directory += self._delim\n        response = self._s3_client.list_objects_v2(Bucket=self._s3_bucket, Prefix=directory, MaxKeys=2)\n        # TODO throw an error if key count is zero as this indicates `directory` is not an existing directory.\n        return response[\"KeyCount\"] == 1\n\n    def _rmdir(self, directory):\n        # s3 can have \"empty\" directories\n        # should check if directory is empty and the delete\n        # delete_objects could be used if recursive=True is specified to this function.\n        # NOTE: This can remove the object representing the prefix without deleting other objects with the prefix.\n        #       This is because s3 is really a key/value store. This function should maybe be changed to throw an\n        #       error if the prefix is not \"empty\" or should delete all objects with the prefix. The former would\n        #       be more consistent with the os function used in the DiskCacheStore.\n        # ToDo: 1) examine boto3 response, 2) handle object versioning (as it stands this will apply to the \"null version\")\n        #      https:",
            "def _rmdir(self, directory):\n        # s3 can have \"empty\" directories\n        # should check if directory is empty and the delete\n        # delete_objects could be used if recursive=True is specified to this function.\n        # NOTE: This can remove the object representing the prefix without deleting other objects with the prefix.\n        #       This is because s3 is really a key/value store. This function should maybe be changed to throw an\n        #       error if the prefix is not \"empty\" or should delete all objects with the prefix. The former would\n        #       be more consistent with the os function used in the DiskCacheStore.\n        # ToDo: 1) examine boto3 response, 2) handle object versioning (as it stands this will apply to the \"null version\")\n        #      https:",
            "def _dirname(self, path):\n        dirname, basename = path.rsplit(self._delim, 1)\n        return dirname",
            "def _get_metadata(self, path, key):\n        return None  # TODO metadata",
            "def _set_metadata(self, path, key, value):\n        pass  # TODO metadata"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/ram_cache_store.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    RAM CacheStore.\n\n    Notes\n    -----\n     * the cache is thread-safe, but not yet accessible across separate processes\n     * there is not yet a max RAM usage setting or a removal policy.\n    \"\"\"",
            "\"\"\"Summary\n\n        Raises\n        ------\n        CacheException\n            Description\n\n        Parameters\n        ----------\n        max_size : None, optional\n            Maximum allowed size of the cache store in bytes. Defaults to podpac 'S3_CACHE_MAX_BYTES' setting, or no limit if this setting does not exist.\n        use_settings_limit : bool, optional\n            Use podpac settings to determine cache limits if True, this will also cause subsequent runtime changes to podpac settings module to effect the limit on this cache. Default is True.\n        \"\"\"",
            "\"\"\"Cache data for specified node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"",
            "\"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheException\n            If the data is not in the cache, or is expired.\n        \"\"\"",
            "\"\"\"Check for cached data for this node\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n\n        Returns\n        -------\n        has_cache : bool\n             True if there as a cached object for this node for the given key and coordinates.\n        \"\"\"",
            "\"\"\"Delete cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        key : str, optional\n            Delete only cached objects with this key.\n        coordinates : :class:`podpac.Coordinates`\n            Delete only cached objects for these coordinates.\n        \"\"\"",
            "\"\"\"Remove all entries from the cache.\"\"\"",
            "\"\"\"Remove all expired entries.\"\"\"",
            "\"\"\"Check if the given entry is expired. Expired entries are removed.\"\"\""
        ],
        "code_snippets": [
            "class RamCacheStore(CacheStore):\n    \"\"\"\n    RAM CacheStore.\n\n    Notes\n    -----\n     * the cache is thread-safe, but not yet accessible across separate processes\n     * there is not yet a max RAM usage setting or a removal policy.\n    \"\"\"\n\n    cache_mode = \"ram\"\n    cache_modes = set([\"ram\", \"all\"])\n    _limit_setting = \"RAM_CACHE_MAX_BYTES\"",
            "def __init__(self, max_size=None, use_settings_limit=True):\n        \"\"\"Summary\n\n        Raises\n        ------\n        CacheException\n            Description\n\n        Parameters\n        ----------\n        max_size : None, optional\n            Maximum allowed size of the cache store in bytes. Defaults to podpac 'S3_CACHE_MAX_BYTES' setting, or no limit if this setting does not exist.\n        use_settings_limit : bool, optional\n            Use podpac settings to determine cache limits if True, this will also cause subsequent runtime changes to podpac settings module to effect the limit on this cache. Default is True.\n        \"\"\"\n        if not settings[\"RAM_CACHE_ENABLED\"]:\n            raise CacheException(\"RAM cache is disabled in the podpac settings.\")\n\n        super(CacheStore, self).__init__()",
            "def _get_full_key(self, node, key, coordinates):\n        return (node.json, key, coordinates.json if coordinates is not None else None)\n\n    @property",
            "def size(self):\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss  # this is actually the total size of the process",
            "def put(self, node, data, item, coordinates=None, expires=None, update=True):\n        \"\"\"Cache data for specified node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"\n\n        if not hasattr(_thread_local, \"cache\"):\n            _thread_local.cache = {}\n\n        full_key = self._get_full_key(node, item, coordinates)\n\n        if not update and self.has(node, item, coordinates):\n            raise CacheException(\"Cache entry already exists. Use update=True to overwrite.\")\n\n        self.rem(node, item, coordinates)\n\n        # check size\n        if self.max_size is not None:\n            if self.size > self.max_size:\n                # cleanup and check again\n                self.cleanup()\n\n            if self.size > self.max_size:\n                # TODO removal policy (using create time, last access, etc)\n                warnings.warn(\n                    \"Warning: Process is using more RAM than the specified limit in settings.RAM_CACHE_MAX_BYTES. \"\n                    \"No longer caching. Consider increasing this limit or try clearing the cache \"\n                    \"(e.g. podpac.utils.clear_cache(mode='RAM') to clear ALL cached results in RAM)\",\n                    UserWarning,\n                )\n                return False\n\n        # store\n        entry = {\"data\": data, \"created\": time.time(), \"accessed\": None, \"expires\": expiration_timestamp(expires)}\n\n        _thread_local.cache[full_key] = entry",
            "def get(self, node, item, coordinates=None):\n        \"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheException\n            If the data is not in the cache, or is expired.\n        \"\"\"\n\n        if not hasattr(_thread_local, \"cache\"):\n            _thread_local.cache = {}\n\n        full_key = self._get_full_key(node, item, coordinates)\n\n        if full_key not in _thread_local.cache:\n            raise CacheException(\"Cache miss. Requested data not found.\")\n\n        if self._expired(full_key):\n            raise CacheException(\"Cache miss. Requested data expired.\")\n\n        self._set_metadata(full_key, \"accessed\", time.time())\n        return copy.deepcopy(_thread_local.cache[full_key][\"data\"])",
            "def has(self, node, item, coordinates=None):\n        \"\"\"Check for cached data for this node\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n\n        Returns\n        -------\n        has_cache : bool\n             True if there as a cached object for this node for the given key and coordinates.\n        \"\"\"\n\n        if not hasattr(_thread_local, \"cache\"):\n            _thread_local.cache = {}\n\n        full_key = self._get_full_key(node, item, coordinates)\n        return full_key in _thread_local.cache and not self._expired(full_key)\n\n    def rem(self, node, item=CacheWildCard(), coordinates=CacheWildCard()):\n        \"\"\"Delete cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        key : str, optional\n            Delete only cached objects with this key.\n        coordinates : :class:`podpac.Coordinates`\n            Delete only cached objects for these coordinates.\n        \"\"\"\n\n        if not hasattr(_thread_local, \"cache\"):\n            _thread_local.cache = {}\n\n        node_key = node.json\n\n        if not isinstance(coordinates, CacheWildCard):\n            coordinates_key = coordinates.json if coordinates is not None else None\n\n        # loop through keys looking for matches\n        rem_keys = []\n        for nk, k, ck in _thread_local.cache.keys():\n            if nk != node_key:\n                continue\n            if not isinstance(item, CacheWildCard) and k != item:\n                continue\n            if not isinstance(coordinates, CacheWildCard) and ck != coordinates_key:\n                continue\n\n            rem_keys.append((nk, k, ck))\n\n        for k in rem_keys:\n            del _thread_local.cache[k]",
            "def clear(self):",
            "def cleanup(self):",
            "def _get_metadata(self, full_key, key):\n        return _thread_local.cache[full_key].get(key)",
            "def _set_metadata(self, full_key, key, value):\n        _thread_local.cache[full_key][key] = value",
            "def _expired(self, full_key):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/file_cache_store.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Abstract class with functionality common to persistent CacheStore objects (e.g. local disk, s3) that store things using multiple paths (filepaths or object paths)\"\"\"",
            "\"\"\"Check for valid cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object key, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n\n        Returns\n        -------\n        has_cache : bool\n             True if there is a valid cached object for this node for the given key and coordinates.\n        \"\"\"",
            "\"\"\"Cache data for specified node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"",
            "\"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheError\n            If the data is not in the cache.\n        \"\"\"",
            "\"\"\"Delete cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage\n        item : str, CacheWildCard, optional\n            Delete cached objects item, or any item if `item` is a CacheWildCard.\n        coordinates : :class:`podpac.Coordinates`, CacheWildCard, None, optional\n            Delete only cached objects for these coordinates, or any coordinates if `coordinates` is a CacheWildCard. `None` specifically indicates entries that do not have coordinates.\n        \"\"\"",
            "\"\"\"\n        Clear all cached data.\n        \"\"\"",
            "\"\"\"\n        Search for matching cached objects.\n        \"\"\"",
            "\"\"\"\n        Find the path for a specific cached object.\n        \"\"\""
        ],
        "code_snippets": [
            "def _hash_string(s):\n    return hash_alg(s.encode()).hexdigest()",
            "class FileCacheStore(CacheStore):",
            "def has(self, node, item, coordinates=None):\n        \"\"\"Check for valid cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object key, e.g. 'output'.\n        coordinates: Coordinate, optional\n            Coordinates for which cached object should be checked\n\n        Returns\n        -------\n        has_cache : bool\n             True if there is a valid cached object for this node for the given key and coordinates.\n        \"\"\"\n\n        path = self.find(node, item, coordinates)\n        return path is not None and not self._expired(path)",
            "def put(self, node, data, item, coordinates=None, expires=None, update=True):\n        \"\"\"Cache data for specified node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        data : any\n            Data to cache\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n        expires : float, datetime, timedelta\n            Expiration date. If a timedelta is supplied, the expiration date will be calculated from the current time.\n        update : bool\n            If True existing data in cache will be updated with `data`, If False, error will be thrown if attempting put something into the cache with the same node, key, coordinates of an existing entry.\n        \"\"\"\n\n        # check for valid existing entry (expired entries are automatically ignored and overwritten)\n        if self.has(node, item, coordinates):\n            if not update:\n                raise CacheException(\"Cache entry already exists. Use `update=True` to overwrite.\")\n            else:\n                self._remove(self.find(node, item, coordinates))\n\n        # serialize\n        root = self._get_filename(node, item, coordinates)\n\n        if isinstance(data, podpac.core.units.UnitsDataArray):\n            ext = \"uda.nc\"\n            s = data.to_netcdf()\n        elif isinstance(data, xr.DataArray):\n            ext = \"xrda.nc\"\n            s = data.to_netcdf()\n        elif isinstance(data, xr.Dataset):\n            ext = \"xrds.nc\"\n            s = data.to_netcdf()\n        elif isinstance(data, np.ndarray):\n            ext = \"npy\"\n            with io.BytesIO() as f:\n                np.save(f, data)\n                s = f.getvalue()\n        elif isinstance(data, podpac.Coordinates):\n            ext = \"coords.json\"\n            s = data.json.encode()\n        elif isinstance(data, podpac.Node):\n            ext = \"node.json\"\n            s = data.json.encode()\n        elif is_json_serializable(data):\n            ext = \"json\"\n            s = json.dumps(data).encode()\n        else:\n            warnings.warn(\n                \"Object of type '%s' is not json serializable; caching object to file using pickle, which \"\n                \"may not be compatible with other Python versions or podpac versions.\" % type(data)\n            )\n            ext = \"pkl\"\n            s = pickle.dumps(data)\n\n        # check size\n        if self.max_size is not None:\n            new_size = self.size + len(s)\n\n            if new_size > self.max_size:\n                # cleanup and check again\n                self.cleanup()\n\n            if new_size > self.max_size:\n                # TODO removal policy (using create time, last access, etc)\n                warnings.warn(\n                    \"Warning: {cache_mode} cache is full. No longer caching. Consider increasing the limit in \"\n                    \"settings.{cache_limit_setting} or try clearing the cache (e.g. podpac.utils.clear_cache(, \"\n                    \"mode='{cache_mode}') to clear ALL cached results in {cache_mode} cache)\".format(\n                        cache_mode=self.cache_mode, cache_limit_setting=self._limit_setting\n                    ),\n                    UserWarning,\n                )\n                return False\n\n        # save\n        node_dir = self._get_node_dir(node)\n        path = self._path_join(node_dir, \"%s.%s\" % (root, ext))\n\n        metadata = {\"created\": time.time(), \"accessed\": None, \"expires\": expiration_timestamp(expires)}\n\n        self._make_dir(node_dir)\n        self._save(path, s, metadata=metadata)\n\n        return True",
            "def get(self, node, item, coordinates=None):\n        \"\"\"Get cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage.\n        item : str\n            Cached object item, e.g. 'output'.\n        coordinates : :class:`podpac.Coordinates`, optional\n            Coordinates for which cached object should be retrieved, for coordinate-dependent data such as evaluation output\n\n        Returns\n        -------\n        data : any\n            The cached data.\n\n        Raises\n        -------\n        CacheError\n            If the data is not in the cache.\n        \"\"\"\n\n        path = self.find(node, item, coordinates)\n        if path is None:\n            raise CacheException(\"Cache miss. Requested data not found.\")\n\n        # get metadata\n        if self._expired(path):\n            raise CacheException(\"Cache miss. Requested data expired.\")\n\n        # read\n        s = self._load(path)\n        self._set_metadata(path, \"accessed\", time.time())\n\n        # deserialize\n        if path.endswith(\".uda.nc\"):\n            x = xr.open_dataarray(s)\n            data = podpac.core.units.UnitsDataArray(x)._pp_deserialize()\n        elif path.endswith(\".xrda.nc\"):\n            data = xr.open_dataarray(s)\n        elif path.endswith(\".xrds.nc\"):\n            data = xr.open_dataset(s)\n        elif path.endswith(\".npy\"):\n            with io.BytesIO(s) as b:\n                data = np.load(b)\n        elif path.endswith(\".coords.json\"):\n            data = podpac.Coordinates.from_json(s.decode())\n        elif path.endswith(\".node.json\"):\n            data = podpac.Node.from_json(s.decode())\n        elif path.endswith(\".json\"):\n            data = json.loads(s.decode())\n        elif path.endswith(\".pkl\"):\n            data = pickle.loads(s)\n        else:\n            raise RuntimeError(\"Unexpected cached file type '%s'\" % self._basename(path))\n\n        return data\n\n    def rem(self, node, item=CacheWildCard(), coordinates=CacheWildCard()):\n        \"\"\"Delete cached data for this node.\n\n        Parameters\n        ------------\n        node : Node\n            node requesting storage\n        item : str, CacheWildCard, optional\n            Delete cached objects item, or any item if `item` is a CacheWildCard.\n        coordinates : :class:`podpac.Coordinates`, CacheWildCard, None, optional\n            Delete only cached objects for these coordinates, or any coordinates if `coordinates` is a CacheWildCard. `None` specifically indicates entries that do not have coordinates.\n        \"\"\"\n\n        # delete matching cached objects\n        for path in self.search(node, item=item, coordinates=coordinates):\n            self._remove(path)\n\n        # remove empty node directories\n        if not self.search(node):\n            path = self._get_node_dir(node=node)\n            while self._is_empty(path):\n                self._rmtree(path)\n                path = self._dirname(path)",
            "def clear(self):\n        \"\"\"\n        Clear all cached data.\n        \"\"\"\n\n        self._rmtree(self._root_dir_path)\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # helper methods\n    # -----------------------------------------------------------------------------------------------------------------\n\n    def search(self, node, item=CacheWildCard(), coordinates=CacheWildCard()):\n        \"\"\"\n        Search for matching cached objects.\n        \"\"\"\n        raise NotImplementedError",
            "def find(self, node, item, coordinates=None):\n        \"\"\"\n        Find the path for a specific cached object.\n        \"\"\"\n\n        paths = self.search(node, item=item, coordinates=coordinates)\n\n        if len(paths) == 0:\n            return None\n        elif len(paths) == 1:\n            return paths[0]\n        elif len(paths) > 1:\n            return RuntimeError(\"Too many cached files matching '%s'\" % self._root_dir_path)",
            "def _get_node_dir(self, node):\n        fullclass = str(node.__class__)[8:-2]\n        subdirs = fullclass.split(\".\")\n        return self._path_join(self._root_dir_path, *subdirs)",
            "def _get_filename(self, node, key, coordinates):\n        prefix = self._sanitize(\"%s-%s\" % (node.base_ref, key))\n        filename = \"%s_%s_%s_%s\" % (prefix, node.hash, _hash_string(key), coordinates.hash if coordinates else \"None\")\n        return filename",
            "def _get_filename_pattern(self, node, key, coordinates):\n        match_prefix = \"*\"\n        match_node = node.hash\n\n        if isinstance(key, CacheWildCard):\n            match_key = \"*\"\n        else:\n            match_key = _hash_string(key)\n\n        if isinstance(coordinates, CacheWildCard):\n            match_coordinates = \"*\"\n        elif coordinates is None:\n            match_coordinates = \"None\"\n        else:\n            match_coordinates = coordinates.hash\n\n        match_filename = \"%s_%s_%s_%s.*\" % (match_prefix, match_node, match_key, match_coordinates)\n        return match_filename",
            "def _sanitize(self, s):\n        return re.sub(\"[_:<>/\\\\\\\\*]+\", \"-\", s)  # replaces _:<>/\\*",
            "def _expired(self, path):\n        expires = self._get_metadata(path, \"expires\")\n        if expires is not None and time.time() >= expires:\n            self._remove(path)\n            return True\n        return False\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # file storage abstraction\n    # -----------------------------------------------------------------------------------------------------------------",
            "def _save(self, path, s):\n        raise NotImplementedError",
            "def _load(self, path):\n        raise NotImplementedError",
            "def _path_join(self, path, *paths):\n        raise NotImplementedError",
            "def _basename(self, path):\n        raise NotImplementedError",
            "def _remove(self, path):\n        raise NotImplementedError",
            "def _exists(self, path):\n        raise NotImplementedError",
            "def _is_empty(self, directory):\n        raise NotImplementedError",
            "def _rmdir(self, directory):\n        raise NotImplementedError",
            "def _make_dir(self, path):\n        raise NotImplementedError",
            "def _dirname(self, path):\n        raise NotImplementedError",
            "def _get_metadata(self, path, key):\n        raise NotImplementedError",
            "def _set_metadata(self, path, key, value):\n        raise NotImplementedError"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/test/test_cache_ctrl.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class CacheCtrlTestNode(podpac.Node):\n    pass\n\n\nNODE = CacheCtrlTestNode()",
            "class TestCacheCtrl(object):",
            "def setup_method(self):\n        # store the current settings\n        self.settings_orig = copy.deepcopy(podpac.settings)\n\n        # delete the ram cache\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        if hasattr(_thread_local, \"cache\"):\n            delattr(_thread_local, \"cache\")\n\n        # use an fresh temporary disk cache\n        self.test_cache_dir = tempfile.mkdtemp(prefix=\"podpac-test-\")\n        podpac.settings[\"DISK_CACHE_DIR\"] = self.test_cache_dir",
            "def teardown_method(self):\n        # delete the ram cache\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        if hasattr(_thread_local, \"cache\"):\n            delattr(_thread_local, \"cache\")\n\n        # delete the disk cache\n        shutil.rmtree(self.test_cache_dir, ignore_errors=True)\n\n        # reset the settings\n        for key in podpac.settings:\n            podpac.settings[key] = self.settings_orig[key]",
            "def test_init_default(self):\n        ctrl = CacheCtrl()\n        assert len(ctrl._cache_stores) == 0\n        assert ctrl.cache_stores == []\n        repr(ctrl)",
            "def test_init_list(self):\n        ctrl = CacheCtrl(cache_stores=[])\n        assert len(ctrl._cache_stores) == 0\n        assert ctrl.cache_stores == []\n        repr(ctrl)\n\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore()])\n        assert len(ctrl._cache_stores) == 1\n        assert isinstance(ctrl._cache_stores[0], RamCacheStore)\n        assert ctrl.cache_stores == [\"ram\"]\n        repr(ctrl)\n\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n        assert len(ctrl._cache_stores) == 2\n        assert isinstance(ctrl._cache_stores[0], RamCacheStore)\n        assert isinstance(ctrl._cache_stores[1], DiskCacheStore)\n        assert ctrl.cache_stores == [\"ram\", \"disk\"]\n        repr(ctrl)",
            "def test_put_has_get(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # has False\n        assert not ctrl._cache_stores[0].has(NODE, \"key\")\n        assert not ctrl._cache_stores[1].has(NODE, \"key\")\n        assert not ctrl.has(NODE, \"key\")\n\n        # put\n        ctrl.put(NODE, 10, \"key\")\n\n        # has True\n        assert ctrl._cache_stores[0].has(NODE, \"key\")\n        assert ctrl._cache_stores[1].has(NODE, \"key\")\n        assert ctrl.has(NODE, \"key\")\n\n        # get value\n        assert ctrl._cache_stores[0].get(NODE, \"key\") == 10\n        assert ctrl._cache_stores[1].get(NODE, \"key\") == 10\n        assert ctrl.get(NODE, \"key\") == 10",
            "def test_partial_has_get(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # has False\n        assert not ctrl._cache_stores[0].has(NODE, \"key\")\n        assert not ctrl._cache_stores[1].has(NODE, \"key\")\n        assert not ctrl.has(NODE, \"key\")\n\n        # put only in disk\n        ctrl._cache_stores[1].put(NODE, 10, \"key\")\n\n        # has\n        assert not ctrl._cache_stores[0].has(NODE, \"key\")\n        assert ctrl._cache_stores[1].has(NODE, \"key\")\n        assert ctrl.has(NODE, \"key\")\n\n        # get\n        with pytest.raises(CacheException, match=\"Cache miss\"):\n            ctrl._cache_stores[0].get(NODE, \"key\")\n        assert ctrl._cache_stores[1].get(NODE, \"key\") == 10\n        assert ctrl.get(NODE, \"key\") == 10",
            "def test_get_cache_miss(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        with pytest.raises(CacheException, match=\"Requested data is not in any cache stores\"):\n            ctrl.get(NODE, \"key\")",
            "def test_put_rem(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # put and check has\n        ctrl.put(NODE, 10, \"key\")\n        assert ctrl.has(NODE, \"key\")\n\n        # rem other and check has\n        ctrl.rem(NODE, \"other\")\n        assert ctrl.has(NODE, \"key\")\n\n        # rem and check has\n        ctrl.rem(NODE, \"key\")\n        assert not ctrl.has(NODE, \"key\")",
            "def test_rem_wildcard_key(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # put and check has\n        ctrl.put(NODE, 10, \"key\")\n        assert ctrl.has(NODE, \"key\")\n\n        # rem other and check has\n        ctrl.rem(NODE, item=\"*\")\n        assert not ctrl.has(NODE, \"key\")",
            "def test_rem_wildcard_coordinates(self):\n        pass",
            "def test_put_clear(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # put and check has\n        ctrl.put(NODE, 10, \"key\")\n        assert ctrl.has(NODE, \"key\")\n\n        # clear and check has\n        ctrl.clear()\n\n        # check has\n        assert not ctrl.has(NODE, \"key\")",
            "def test_put_has_mode(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # put disk and check has\n        assert not ctrl.has(NODE, \"key\")\n\n        ctrl.put(NODE, 10, \"key\", mode=\"disk\")\n        assert not ctrl._cache_stores[0].has(NODE, \"key\")\n        assert not ctrl.has(NODE, \"key\", mode=\"ram\")\n        assert ctrl._cache_stores[1].has(NODE, \"key\")\n        assert ctrl.has(NODE, \"key\", mode=\"disk\")\n        assert ctrl.has(NODE, \"key\")\n\n        # put ram and check has\n        ctrl.clear()\n        assert not ctrl.has(NODE, \"key\")\n\n        ctrl.put(NODE, 10, \"key\", mode=\"ram\")\n        assert ctrl._cache_stores[0].has(NODE, \"key\")\n        assert ctrl.has(NODE, \"key\", mode=\"ram\")\n        assert not ctrl._cache_stores[1].has(NODE, \"key\")\n        assert not ctrl.has(NODE, \"key\", mode=\"disk\")\n        assert ctrl.has(NODE, \"key\")",
            "def test_put_has_expires(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        ctrl.put(NODE, 10, \"key1\", expires=\"1,D\")\n        ctrl.put(NODE, 10, \"key2\", expires=\"-1,D\")\n        assert ctrl.has(NODE, \"key1\")\n        assert not ctrl.has(NODE, \"key2\")",
            "def test_put_get_expires(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        ctrl.put(NODE, 10, \"key1\", expires=\"1,D\")\n        ctrl.put(NODE, 10, \"key2\", expires=\"-1,D\")\n        assert ctrl.get(NODE, \"key1\") == 10\n        with pytest.raises(CacheException, match=\"Requested data is not in any cache stores\"):\n            ctrl.get(NODE, \"key2\")",
            "def test_cleanup(self):\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        ctrl.put(NODE, 10, \"key1\", expires=\"1,D\")\n        ctrl.put(NODE, 10, \"key2\", expires=\"-1,D\")\n\n        # 2 entries (even though one is expired)\n        assert len(_thread_local.cache) == 2\n        assert len(ctrl._cache_stores[1].search(NODE)) == 2\n\n        ctrl.cleanup()\n\n        # only 1 entry (the expired entry has been removed)\n        assert len(_thread_local.cache) == 1\n        assert len(ctrl._cache_stores[1].search(NODE)) == 1",
            "def test_invalid_node(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # type\n        with pytest.raises(TypeError, match=\"Invalid node\"):\n            ctrl.put(\"node\", 10, \"key\")\n\n        with pytest.raises(TypeError, match=\"Invalid node\"):\n            ctrl.get(\"node\", \"key\")\n\n        with pytest.raises(TypeError, match=\"Invalid node\"):\n            ctrl.has(\"node\", \"key\")\n\n        with pytest.raises(TypeError, match=\"Invalid node\"):\n            ctrl.rem(\"node\", \"key\")",
            "def test_invalid_key(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # type\n        with pytest.raises(TypeError, match=\"Invalid item\"):\n            ctrl.put(NODE, 10, 10)\n\n        with pytest.raises(TypeError, match=\"Invalid item\"):\n            ctrl.get(NODE, 10)\n\n        with pytest.raises(TypeError, match=\"Invalid item\"):\n            ctrl.has(NODE, 10)\n\n        with pytest.raises(TypeError, match=\"Invalid item\"):\n            ctrl.rem(NODE, 10)\n\n        # wildcard\n        with pytest.raises(ValueError, match=\"Invalid item\"):\n            ctrl.put(NODE, 10, \"*\")\n\n        with pytest.raises(ValueError, match=\"Invalid item\"):\n            ctrl.get(NODE, \"*\")\n\n        with pytest.raises(ValueError, match=\"Invalid item\"):\n            ctrl.has(NODE, \"*\")\n\n        # allowed\n        ctrl.rem(NODE, \"*\")",
            "def test_invalid_coordinates(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        # type\n        with pytest.raises(TypeError, match=\"Invalid coordinates\"):\n            ctrl.put(NODE, 10, \"key\", coordinates=\"coords\")\n\n        with pytest.raises(TypeError, match=\"Invalid coordinates\"):\n            ctrl.get(NODE, \"key\", coordinates=\"coords\")\n\n        with pytest.raises(TypeError, match=\"Invalid coordinates\"):\n            ctrl.has(NODE, \"key\", coordinates=\"coords\")\n\n        with pytest.raises(TypeError, match=\"Invalid coordinates\"):\n            ctrl.rem(NODE, \"key\", coordinates=\"coords\")",
            "def test_invalid_mode(self):\n        ctrl = CacheCtrl(cache_stores=[RamCacheStore(), DiskCacheStore()])\n\n        with pytest.raises(ValueError, match=\"Invalid mode\"):\n            ctrl.put(NODE, 10, \"key\", mode=\"other\")\n\n        with pytest.raises(ValueError, match=\"Invalid mode\"):\n            ctrl.get(NODE, \"key\", mode=\"other\")\n\n        with pytest.raises(ValueError, match=\"Invalid mode\"):\n            ctrl.has(NODE, \"key\", mode=\"other\")\n\n        with pytest.raises(ValueError, match=\"Invalid mode\"):\n            ctrl.rem(NODE, \"key\", mode=\"other\")\n\n        with pytest.raises(ValueError, match=\"Invalid mode\"):\n            ctrl.clear(mode=\"other\")",
            "def test_get_default_cache_ctrl():\n    with podpac.settings:\n        podpac.settings[\"DEFAULT_CACHE\"] = []\n        ctrl = get_default_cache_ctrl()\n        assert isinstance(ctrl, CacheCtrl)\n        assert ctrl._cache_stores == []\n\n        podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n        ctrl = get_default_cache_ctrl()\n        assert isinstance(ctrl, CacheCtrl)\n        assert len(ctrl._cache_stores) == 1\n        assert isinstance(ctrl._cache_stores[0], RamCacheStore)",
            "class TestMakeCacheCtrl(object):",
            "def test_str(self):\n        ctrl = make_cache_ctrl(\"ram\")\n        assert isinstance(ctrl, CacheCtrl)\n        assert len(ctrl._cache_stores) == 1\n        assert isinstance(ctrl._cache_stores[0], RamCacheStore)\n\n        ctrl = make_cache_ctrl(\"disk\")\n        assert len(ctrl._cache_stores) == 1\n        assert isinstance(ctrl._cache_stores[0], DiskCacheStore)",
            "def test_list(self):\n        ctrl = make_cache_ctrl([\"ram\", \"disk\"])\n        assert len(ctrl._cache_stores) == 2\n        assert isinstance(ctrl._cache_stores[0], RamCacheStore)\n        assert isinstance(ctrl._cache_stores[1], DiskCacheStore)\n\n        ctrl = make_cache_ctrl([\"ram\", \"disk\"])\n        assert len(ctrl._cache_stores) == 2\n        assert isinstance(ctrl._cache_stores[0], RamCacheStore)\n        assert isinstance(ctrl._cache_stores[1], DiskCacheStore)",
            "def test_invalid(self):\n        with pytest.raises(ValueError, match=\"Unknown cache store type\"):\n            ctrl = make_cache_ctrl(\"other\")\n\n        with pytest.raises(ValueError, match=\"Unknown cache store type\"):\n            ctrl = make_cache_ctrl([\"other\"])",
            "def test_clear_cache():\n    with podpac.settings:\n        # make a default cache\n        podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n\n        # fill the default cache\n        node = podpac.algorithm.Arange()\n        node.put_cache(0, \"mykey\")\n        assert node.has_cache(\"mykey\")\n\n        clear_cache()\n\n        assert not node.has_cache(\"mykey\")",
            "def test_cache_cleanup():\n    with podpac.settings:\n        # make a default cache\n        podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n\n        cache_cleanup()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/cache/test/test_cache_stores.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class BaseCacheStoreTests(object):\n    Store = None\n    enabled_setting = None\n    limit_setting = None",
            "def setup_method(self):\n        self.settings_orig = copy.deepcopy(podpac.settings)",
            "def teardown_method(self):\n        for key in podpac.settings:\n            podpac.settings[key] = self.settings_orig[key]",
            "def test_init(self):\n        store = self.Store()",
            "def test_disabled(self):\n        podpac.settings[self.enabled_setting] = False\n        with pytest.raises(CacheException, match=\"cache is disabled\"):\n            store = self.Store()",
            "def test_put_has_get(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 20, \"mykey2\")\n        store.put(NODE1, 30, \"mykeyA\", COORDS1)\n        store.put(NODE1, 40, \"mykeyB\", COORDS1)\n        store.put(NODE1, 50, \"mykeyA\", COORDS2)\n        store.put(NODE2, 110, \"mykey1\")\n        store.put(NODE2, 120, \"mykeyA\", COORDS1)\n\n        assert store.has(NODE1, \"mykey1\") is True\n        assert store.has(NODE1, \"mykey2\") is True\n        assert store.has(NODE1, \"mykeyA\", COORDS1) is True\n        assert store.has(NODE1, \"mykeyB\", COORDS1) is True\n        assert store.has(NODE1, \"mykeyA\", COORDS2) is True\n        assert store.has(NODE2, \"mykey1\") is True\n        assert store.has(NODE2, \"mykeyA\", COORDS1) is True\n\n        assert store.get(NODE1, \"mykey1\") == 10\n        assert store.get(NODE1, \"mykey2\") == 20\n        assert store.get(NODE1, \"mykeyA\", COORDS1) == 30\n        assert store.get(NODE1, \"mykeyB\", COORDS1) == 40\n        assert store.get(NODE1, \"mykeyA\", COORDS2) == 50\n        assert store.get(NODE2, \"mykey1\") == 110\n        assert store.get(NODE2, \"mykeyA\", COORDS1) == 120",
            "def test_has_empty(self):\n        store = self.Store()\n        assert store.has(NODE1, \"mykey\") is False",
            "def test_get_empty(self):\n        store = self.Store()\n        with pytest.raises(CacheException, match=\"Cache miss\"):\n            store.get(NODE1, \"mykey\")",
            "def test_rem_empty(self):\n        store = self.Store()\n        store.rem(NODE1, \"mykey\")",
            "def test_update(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        assert store.get(NODE1, \"mykey1\") == 10\n\n        # raise exception and do not change\n        with pytest.raises(CacheException, match=\"Cache entry already exists.\"):\n            store.put(NODE1, 10, \"mykey1\", update=False)\n        assert store.get(NODE1, \"mykey1\") == 10\n\n        # update\n        store.put(NODE1, 20, \"mykey1\")\n        assert store.get(NODE1, \"mykey1\") == 20",
            "def test_get_put_none(self):\n        store = self.Store()\n        store.put(NODE1, None, \"mykey\")\n        assert store.get(NODE1, \"mykey\") is None",
            "def test_rem_object(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 20, \"mykey2\")\n        store.put(NODE1, 30, \"mykeyA\", COORDS1)\n        store.put(NODE1, 40, \"mykeyB\", COORDS1)\n        store.put(NODE1, 50, \"mykeyA\", COORDS2)\n        store.put(NODE2, 110, \"mykey1\")\n        store.put(NODE2, 120, \"mykeyA\", COORDS1)\n\n        store.rem(NODE1, item=\"mykey1\")\n        store.rem(NODE1, item=\"mykeyA\", coordinates=COORDS1)\n\n        assert store.has(NODE1, \"mykey1\") is False\n        assert store.has(NODE1, \"mykey2\") is True\n        assert store.has(NODE1, \"mykeyA\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyB\", COORDS1) is True\n        assert store.has(NODE1, \"mykeyA\", COORDS2) is True\n        assert store.has(NODE2, \"mykey1\") is True\n        assert store.has(NODE2, \"mykeyA\", COORDS1) is True",
            "def test_rem_key(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 20, \"mykey2\")\n        store.put(NODE1, 30, \"mykeyA\", COORDS1)\n        store.put(NODE1, 40, \"mykeyB\", COORDS1)\n        store.put(NODE1, 50, \"mykeyA\", COORDS2)\n        store.put(NODE2, 110, \"mykey1\")\n        store.put(NODE2, 120, \"mykeyA\", COORDS1)\n\n        store.rem(NODE1, item=\"mykey1\")\n        store.rem(NODE1, item=\"mykeyA\")\n\n        assert store.has(NODE1, \"mykey1\") is False\n        assert store.has(NODE1, \"mykey2\") is True\n        assert store.has(NODE1, \"mykeyA\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyB\", COORDS1) is True\n        assert store.has(NODE1, \"mykeyA\", COORDS2) is False\n        assert store.has(NODE2, \"mykey1\") is True\n        assert store.has(NODE2, \"mykeyA\", COORDS1) is True",
            "def test_rem_coordinates(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 20, \"mykey2\")\n        store.put(NODE1, 30, \"mykeyA\", COORDS1)\n        store.put(NODE1, 40, \"mykeyB\", COORDS1)\n        store.put(NODE1, 50, \"mykeyA\", COORDS2)\n        store.put(NODE2, 110, \"mykey1\")\n        store.put(NODE2, 120, \"mykeyA\", COORDS1)\n\n        store.rem(NODE1, coordinates=COORDS1)\n\n        assert store.has(NODE1, \"mykey1\") is True\n        assert store.has(NODE1, \"mykey2\") is True\n        assert store.has(NODE1, \"mykeyA\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyB\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyA\", COORDS2) is True\n        assert store.has(NODE2, \"mykey1\") is True\n        assert store.has(NODE2, \"mykeyA\", COORDS1) is True",
            "def test_rem_node(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 20, \"mykey2\")\n        store.put(NODE1, 30, \"mykeyA\", COORDS1)\n        store.put(NODE1, 40, \"mykeyB\", COORDS1)\n        store.put(NODE1, 50, \"mykeyA\", COORDS2)\n        store.put(NODE2, 110, \"mykey1\")\n        store.put(NODE2, 120, \"mykeyA\", COORDS1)\n\n        store.rem(NODE1)\n\n        assert store.has(NODE1, \"mykey1\") is False\n        assert store.has(NODE1, \"mykey2\") is False\n        assert store.has(NODE1, \"mykeyA\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyB\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyA\", COORDS2) is False\n        assert store.has(NODE2, \"mykey1\") is True\n        assert store.has(NODE2, \"mykeyA\", COORDS1) is True",
            "def test_clear(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 20, \"mykey2\")\n        store.put(NODE1, 30, \"mykeyA\", COORDS1)\n        store.put(NODE1, 40, \"mykeyB\", COORDS1)\n        store.put(NODE1, 50, \"mykeyA\", COORDS2)\n        store.put(NODE2, 110, \"mykey1\")\n        store.put(NODE2, 120, \"mykeyA\", COORDS1)\n\n        store.clear()\n\n        assert store.has(NODE1, \"mykey1\") is False\n        assert store.has(NODE1, \"mykey2\") is False\n        assert store.has(NODE1, \"mykeyA\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyB\", COORDS1) is False\n        assert store.has(NODE1, \"mykeyA\", COORDS2) is False\n        assert store.has(NODE2, \"mykey1\") is False\n        assert store.has(NODE2, \"mykeyA\", COORDS1) is False",
            "def test_max_size(self):\n        store = self.Store()\n        assert store.max_size == podpac.settings[self.limit_setting]\n\n        podpac.settings[self.limit_setting] = 1000\n        assert store.max_size == 1000",
            "def test_limit(self):\n        podpac.settings[self.limit_setting] = 10\n        store = self.Store()\n\n        store.put(NODE1, \"11111111\", \"mykey1\")\n\n        with pytest.warns(UserWarning, match=\"Warning: .* cache is full\"):\n            store.put(NODE1, \"11111111\", \"mykey2\")",
            "def test_expiration(self):\n        store = self.Store()\n\n        # timestamp\n        expires = time.time() + 100  # in 100 seconds\n        store.put(NODE1, 10, \"mykey1\", expires=expires)\n        assert store.has(NODE1, \"mykey1\") is True\n\n        expires = time.time() - 100  # 100 seconds ago\n        store.put(NODE1, 10, \"mykey2\", expires=expires)\n        assert store.has(NODE1, \"mykey2\") is False\n\n        # datetime\n        expires = datetime.datetime.now() + datetime.timedelta(1)  # in 1 day\n        store.put(NODE1, 10, \"mykey3\", expires=expires)\n        assert store.has(NODE1, \"mykey3\") is True\n\n        expires = datetime.datetime.now() - datetime.timedelta(1)  # 1 day ago\n        store.put(NODE1, 10, \"mykey4\", expires=expires)\n        assert store.has(NODE1, \"mykey4\") is False\n\n        # timedelta\n        expires = datetime.timedelta(1)  # in 1 day\n        store.put(NODE1, 10, \"mykey5\", expires=expires)\n        assert store.has(NODE1, \"mykey5\") is True\n\n        expires = -datetime.timedelta(1)  # 1 day ago\n        store.put(NODE1, 10, \"mykey6\", expires=expires)\n        assert store.has(NODE1, \"mykey6\") is False\n\n        # string datetime\n        expires = \"3000-01-01\"  # in the year 3000\n        store.put(NODE1, 10, \"mykey7\", expires=expires)\n        assert store.has(NODE1, \"mykey7\") is True\n\n        expires = \"1000-01-01\"  # in the year 1000\n        store.put(NODE1, 10, \"mykey8\", expires=expires)\n        assert store.has(NODE1, \"mykey8\") is False\n\n        # string timedelta\n        expires = \"1,D\"  # in 1 day\n        store.put(NODE1, 10, \"mykey9\", expires=expires)\n        assert store.has(NODE1, \"mykey9\") is True\n\n        expires = \"-1,D\"  # 1 day ago\n        store.put(NODE1, 10, \"mykey10\", expires=expires)\n        assert store.has(NODE1, \"mykey10\") is False\n\n        # None\n        expires = None  # never (default)\n        store.put(NODE1, 10, \"mykey11\", expires=None)\n        assert store.has(NODE1, \"mykey11\") is True",
            "def test_expiration_put(self):\n        store = self.Store()\n\n        # exception putting data that is not expired when update=False\n        expires = time.time() + 100  # in 100 seconds\n        store.put(NODE1, 10, \"mykey1\", expires=expires)\n        with pytest.raises(CacheException, match=\"Cache entry already exists\"):\n            store.put(NODE1, 10, \"mykey1\", update=False)\n        store.put(NODE1, 10, \"mykey1\")\n\n        # no exception putting data that is expired even when update=False\n        expires = time.time() - 100  # 100 seconds ago\n        store.put(NODE1, 10, \"mykey2\", expires=expires)\n        store.put(NODE1, 10, \"mykey2\", update=False)",
            "def test_expiration_get(self):\n        store = self.Store()\n\n        # getting unexpired data\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        assert store.get(NODE1, \"mykey1\") == 10\n\n        # exception getting expired data\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        with pytest.raises(CacheException, match=\"Cache miss. Requested data expired\"):\n            store.get(NODE1, \"mykey2\")",
            "def test_clean_basic(self):\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time())\n        store.cleanup()",
            "class FileCacheStoreTests(BaseCacheStoreTests):",
            "def test_cache_units_data_array(self):\n        store = self.Store()\n\n        data = podpac.core.units.UnitsDataArray([1, 2, 3], attrs={\"units\": \"m\"})\n        store.put(NODE1, data, \"mykey\")\n        cached = store.get(NODE1, \"mykey\")\n        assert isinstance(cached, podpac.core.units.UnitsDataArray)\n        xr.testing.assert_identical(cached, data)  # assert_identical checks attributes as wel",
            "def test_cache_xarray(self):\n        store = self.Store()\n\n        # data array\n        data = xr.DataArray([1, 2, 3])\n        store.put(NODE1, data, \"mykey\")\n        cached = store.get(NODE1, \"mykey\")\n        xr.testing.assert_identical(cached, data)  # assert_identical checks attributes as wel\n\n        # dataset\n        data = xr.Dataset({\"a\": [1, 2, 3]})\n        store.put(NODE1, data, \"mykey2\")\n        cached = store.get(NODE1, \"mykey2\")\n        xr.testing.assert_identical(cached, data)  # assert_identical checks attributes as wel",
            "def test_cache_podpac(self):\n        store = self.Store()\n\n        # coords\n        store.put(NODE1, COORDS1, \"mykey\")\n        cached = store.get(NODE1, \"mykey\")\n        assert cached == COORDS1\n\n        # node\n        store.put(NODE1, NODE2, \"mykey2\")\n        cached = store.get(NODE1, \"mykey2\")\n        assert cached.json == NODE2.json",
            "def test_cache_numpy(self):\n        store = self.Store()\n\n        data = np.array([1, 2, 3])\n        store.put(NODE1, data, \"mykey\")\n        cached = store.get(NODE1, \"mykey\")\n        np.testing.assert_equal(cached, data)",
            "def test_pkl_fallback(self):\n        store = self.Store()\n\n        data = [xr.DataArray([1, 2, 3]), np.array([1, 2, 3])]\n        with pytest.warns(UserWarning, match=\"caching object to file using pickle\"):\n            store.put(NODE1, data, \"mykey\")\n        cached = store.get(NODE1, \"mykey\")\n        xr.testing.assert_equal(cached[0], data[0])\n        np.testing.assert_equal(cached[1], data[1])",
            "class TestRamCacheStore(BaseCacheStoreTests):\n    Store = RamCacheStore\n    enabled_setting = \"RAM_CACHE_ENABLED\"\n    limit_setting = \"RAM_CACHE_MAX_BYTES\"",
            "def setup_method(self):\n        super(TestRamCacheStore, self).setup_method()\n\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        if hasattr(_thread_local, \"cache\"):\n            delattr(_thread_local, \"cache\")",
            "def teardown_method(self):\n        super(TestRamCacheStore, self).teardown_method()\n\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        if hasattr(_thread_local, \"cache\"):\n            delattr(_thread_local, \"cache\")\n\n    @pytest.mark.skip(reason=\"not testable\")",
            "def test_size(self):\n        pass\n\n    @pytest.mark.skip(reason=\"not testable\")",
            "def test_limit(self):\n        super(TestRamCacheStore, self).test_size()",
            "def test_cleanup(self):\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(_thread_local.cache) == 2\n\n        store.cleanup()\n        assert len(_thread_local.cache) == 1",
            "def test_has_auto_cleanup(self):\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(_thread_local.cache) == 2\n\n        assert store.has(NODE1, \"mykey1\") is True\n        assert store.has(NODE1, \"mykey2\") is False\n\n        assert len(_thread_local.cache) == 1",
            "def test_get_auto_cleanup(self):\n        from podpac.core.cache.ram_cache_store import _thread_local\n\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(_thread_local.cache) == 2\n\n        assert store.get(NODE1, \"mykey1\") == 10\n        with pytest.raises(CacheException, match=\"Cache miss. Requested data expired\"):\n            store.get(NODE1, \"mykey2\")\n\n        assert len(_thread_local.cache) == 1",
            "class TestDiskCacheStore(FileCacheStoreTests):\n    Store = DiskCacheStore\n    enabled_setting = \"DISK_CACHE_ENABLED\"\n    limit_setting = \"DISK_CACHE_MAX_BYTES\"",
            "def setup_method(self):\n        super(TestDiskCacheStore, self).setup_method()\n\n        self.test_cache_dir = tempfile.mkdtemp(prefix=\"podpac-test-\")\n        podpac.settings[\"DISK_CACHE_DIR\"] = self.test_cache_dir",
            "def teardown_method(self):\n        super(TestDiskCacheStore, self).teardown_method()\n\n        shutil.rmtree(self.test_cache_dir, ignore_errors=True)",
            "def test_cache_dir(self):\n        with podpac.settings:\n\n            # absolute path\n            podpac.settings[\"DISK_CACHE_DIR\"] = self.test_cache_dir\n            expected = self.test_cache_dir\n            store = DiskCacheStore()\n            store.put(NODE1, 10, \"mykey1\")\n            assert store.find(NODE1, \"mykey1\").startswith(expected)\n            store.clear()\n\n            # relative path\n            podpac.settings[\"DISK_CACHE_DIR\"] = \"_testcache_\"\n            expected = os.path.join(\n                os.environ.get(\"XDG_CACHE_HOME\", os.path.join(os.path.expanduser(\"~\"), \".config\", \"podpac\")),\n                \"_testcache_\",\n            )\n            store = DiskCacheStore()\n            store.clear()\n            store.put(NODE1, 10, \"mykey1\")\n            assert store.find(NODE1, \"mykey1\").startswith(expected)\n            store.clear()",
            "def test_rem_node_dir(self):\n        store = self.Store()\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, 10, \"mykey2\")\n        store.put(NODE2, 10, \"mykey1\")\n\n        assert store._exists(store._get_node_dir(NODE1))\n        assert store._exists(store._get_node_dir(NODE2))\n\n        store.rem(NODE1, \"mykey1\")\n        assert store._exists(store._get_node_dir(NODE1))\n        assert store._exists(store._get_node_dir(NODE2))\n\n        store.rem(NODE1, \"mykey2\")\n        assert not store._exists(store._get_node_dir(NODE1))\n        assert store._exists(store._get_node_dir(NODE2))\n\n        store.rem(NODE2)\n        assert not store._exists(store._get_node_dir(NODE1))\n        assert not store._exists(store._get_node_dir(NODE2))",
            "def test_size(self):\n        store = self.Store()\n        assert store.size == 0\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, np.array([0, 1, 2]), \"mykey2\")\n\n        p1 = store.find(NODE1, \"mykey1\", None)\n        p2 = store.find(NODE1, \"mykey2\", None)\n        expected_size = (\n            os.path.getsize(p1)\n            + os.path.getsize(\"%s.meta\" % p1)\n            + os.path.getsize(p2)\n            + os.path.getsize(\"%s.meta\" % p2)\n        )\n        assert store.size == expected_size",
            "def test_cleanup(self):\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(store.search(NODE1)) == 2\n\n        store.cleanup()\n        assert len(store.search(NODE1)) == 1\n\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() - 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(store.search(NODE1)) == 2\n\n        store.cleanup()\n        assert len(store.search(NODE1)) == 0\n        assert not store._exists(store._get_node_dir(NODE1))  # empty node directories are removed",
            "def test_has_auto_cleanup(self):\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(store.search(NODE1)) == 2\n\n        assert store.has(NODE1, \"mykey1\") is True\n        assert store.has(NODE1, \"mykey2\") is False\n\n        assert len(store.search(NODE1)) == 1",
            "def test_get_auto_cleanup(self):\n        store = self.Store()\n        store.put(NODE1, 10, \"mykey1\", expires=time.time() + 100)\n        store.put(NODE1, 10, \"mykey2\", expires=time.time() - 100)\n        assert len(store.search(NODE1)) == 2\n\n        assert store.get(NODE1, \"mykey1\") == 10\n        with pytest.raises(CacheException, match=\"Cache miss. Requested data expired\"):\n            store.get(NODE1, \"mykey2\")\n\n        assert len(store.search(NODE1)) == 1\n\n\n@pytest.mark.aws",
            "class TestS3CacheStore(FileCacheStoreTests):\n    Store = S3CacheStore\n    enabled_setting = \"S3_CACHE_ENABLED\"\n    limit_setting = \"S3_CACHE_MAX_BYTES\"\n    test_cache_dir = \"tmp_cache\"",
            "def setup_method(self):\n        super(TestS3CacheStore, self).setup_method()\n\n        podpac.settings[\"S3_CACHE_DIR\"] = self.test_cache_dir",
            "def teardown_method(self):\n        try:\n            store = S3CacheStore()\n            store._rmtree(self.test_cache_dir)\n        except:\n            pass\n\n        super(TestS3CacheStore, self).teardown_method()",
            "def test_size(self):\n        store = self.Store()\n        assert store.size == 0\n\n        store.put(NODE1, 10, \"mykey1\")\n        store.put(NODE1, np.array([0, 1, 2]), \"mykey2\")\n        assert store.size == 142"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_style.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestStyle(object):",
            "def test_init(self):\n        s = Style()",
            "def test_init_from_node(self):\n        from podpac.core.node import Node\n\n        node = Node()\n        s = Style(node)",
            "def test_cmap(self):\n        style = Style()\n        assert style.cmap.name == \"viridis\"\n\n        style = Style(colormap=\"cividis\")\n        assert style.cmap.name == \"cividis\"\n\n        style = Style(enumeration_colors=({0: \"c\", 1: \"k\"}))\n        assert style.cmap.name == \"from_list\"\n        assert style.cmap.colors == (\"c\", \"k\")\n\n        with pytest.raises(TypeError, match=\"Style can have a colormap or enumeration_colors\"):\n            style = Style(colormap=\"cividis\", enumeration_colors=({0: \"c\", 1: \"k\"}))",
            "def test_enumeration(self):\n        # matplotlib enumeration tuples\n        style = Style(\n            enumeration_colors={1: \"r\", 3: \"o\"},\n            enumeration_legend={1: \"apples\", 3: \"oranges\"},\n            default_enumeration_color=\"k\",\n        )\n        assert style.full_enumeration_colors == (\"r\", \"k\", \"o\")\n        assert style.full_enumeration_legend == (\"apples\", \"unknown\", \"oranges\")\n\n        # negative key\n        style = Style(\n            enumeration_colors={-1: \"r\", 1: \"o\"},\n            enumeration_legend={-1: \"apples\", 1: \"oranges\"},\n            default_enumeration_color=\"k\",\n        )\n        assert style.full_enumeration_colors == (\"r\", \"k\", \"o\")\n        assert style.full_enumeration_legend == (\"apples\", \"unknown\", \"oranges\")\n\n        # invalid\n        with pytest.raises(ValueError, match=\"Style enumeration_legend keys must match enumeration_colors keys\"):\n            style = Style(enumeration_colors={1: \"r\", 3: \"o\"}, enumeration_legend={1: \"apples\"})\n\n        with pytest.raises(TypeError, match=\"Style enumeration_legend requires enumeration_colors\"):\n            style = Style(enumeration_legend={-1: \"apples\", 3: \"oranges\"})",
            "def test_serialization(self):\n        # default\n        style = Style()\n        d = style.definition\n        assert isinstance(d, OrderedDict)\n        assert len(d.keys()) == 0\n\n        s = Style.from_json(style.json)\n        assert isinstance(s, Style)\n\n        # with traits\n        style = Style(name=\"test\", units=\"meters\", colormap=\"cividis\", clim=(-1, 1))\n        d = style.definition\n        assert isinstance(d, OrderedDict)\n        assert set(d.keys()) == {\"name\", \"units\", \"colormap\", \"clim\"}\n        assert d[\"name\"] == \"test\"\n        assert d[\"units\"] == \"meters\"\n        assert d[\"colormap\"] == \"cividis\"\n        assert d[\"clim\"] == [-1, 1]\n\n        s = Style.from_json(style.json)\n        assert s.name == style.name\n        assert s.units == style.units\n        assert s.colormap == style.colormap\n        assert s.clim == style.clim\n\n        # enumeration traits\n        style = Style(enumeration_legend=({0: \"apples\", 1: \"oranges\"}), enumeration_colors=({0: \"r\", 1: \"o\"}))\n        d = style.definition\n        assert isinstance(d, OrderedDict)\n        assert set(d.keys()) == {\"enumeration_legend\", \"enumeration_colors\"}\n        assert d[\"enumeration_legend\"] == {0: \"apples\", 1: \"oranges\"}\n        assert d[\"enumeration_colors\"] == {0: \"r\", 1: \"o\"}\n\n        s = Style.from_json(style.json)\n        assert s.enumeration_legend == style.enumeration_legend\n        assert s.enumeration_colors == style.enumeration_colors\n        assert s.cmap.colors == style.cmap.colors",
            "def test_eq(self):\n        style1 = Style(name=\"test\")\n        style2 = Style(name=\"test\")\n        style3 = Style(name=\"other\")\n\n        assert style1 is not style2\n        assert style1 is not style3\n        assert style2 is not style3\n\n        assert style1 == style1\n        assert style2 == style2\n        assert style3 == style3\n\n        assert style1 == style2\n        assert style1 != style3"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_node.py",
        "comments": [
            "//testwms/?map=map&&service={service}&request=GetMap&{layername}={layer}&styles=&format=image%2Fpng\"",
            "//raw.githubusercontent.com/creare-com/podpac/develop/podpac/core/pipeline/test/test.json'})[2:],",
            "//podpac-s3/test/test.json'})[2:]  # Tested locally, works fine. Hard to test with CI",
            "//mobility-devel.crearecomputing.com/geowatch?&SERVICE=WMS&REQUEST=GetMap&VERSION=1.3.0&\"",
            "//mobility-devel.crearecomputing.com/geowatch?&SERVICE=WMS&REQUEST=GetMap&VERSION=1.3.0&\""
        ],
        "docstrings": [
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Min\",\n                \"inputs\": { \"source\": 10 }\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Min\",\n                \"inputs\": { \"source\": \"nonexistent\" }\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": \"a.coord_name\" }\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": 10 }\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": \"nonexistent.coord_name\" }\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": \"a.nonexistent\" }\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Arange\",\n                \"invalid_property\": \"value\"\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"mynode\": {\n                \"plugin\": \"test_node\",\n                \"node\": \"MyPluginNode\"\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"mynode\": {\n                \"plugin\": \"missing\",\n                \"node\": \"MyPluginNode\"\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Arange\"\n            },\n            \"mean\": {\n                \"node\": \"algorithm.Convolution\",\n                \"lookup_attrs\": {\"source\": \"a\"},\n                \"attrs\": {\"kernel_type\": \"mean,3\", \"kernel_dims\": [\"lat\", \"lon\"]}\n            },\n            \"c\": {\n                \"node\": \"algorithm.Arithmetic\",\n                \"lookup_attrs\": {\"A\": \"a\", \"B\": \"mean\"},\n                \"attrs\": {\"eqn\": \"a-b\"}\n            }\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Arange\"\n            },\n            \"podpac_version\": \"other\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n            \"podpac_version\": \"3.2.0\",\n            \"podpac_output_node\": \"Arithmetic\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"",
            "\"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\",\n            \"podpac_output_node\": \"Sum\"\n        }\n        \"\"\""
        ],
        "code_snippets": [
            "class TestNode(object):",
            "def test_style(self):\n        node = Node()\n        assert isinstance(node.style, Style)",
            "def test_units(self):\n        node = Node(units=\"meters\")\n\n        with pytest.raises(UndefinedUnitError):\n            Node(units=\"abc\")",
            "def test_outputs(self):\n        node = Node()\n        assert node.outputs is None\n\n        node = Node(outputs=[\"a\", \"b\"])\n        assert node.outputs == [\"a\", \"b\"]",
            "def test_output(self):\n        node = Node()\n        assert node.output is None\n\n        node = Node(outputs=[\"a\", \"b\"])\n        assert node.output is None\n\n        node = Node(outputs=[\"a\", \"b\"], output=\"b\")\n        assert node.output == \"b\"\n\n        # must be one of the outputs\n        with pytest.raises(ValueError, match=\"Invalid output\"):\n            node = Node(outputs=[\"a\", \"b\"], output=\"other\")\n\n        # only valid for multiple-output nodes\n        with pytest.raises(TypeError, match=\"Invalid output\"):\n            node = Node(output=\"other\")",
            "def test_cache_output(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            node = Node()\n            assert not node.cache_output\n\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = True\n            node = Node()\n            assert node.cache_output",
            "def test_cache_ctrl(self):\n        # settings\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n            node = Node()\n            assert node.cache_ctrl is not None\n            assert len(node.cache_ctrl._cache_stores) == 1\n            assert isinstance(node.cache_ctrl._cache_stores[0], RamCacheStore)\n\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\", \"disk\"]\n            node = Node()\n            assert node.cache_ctrl is not None\n            assert len(node.cache_ctrl._cache_stores) == 2\n            assert isinstance(node.cache_ctrl._cache_stores[0], RamCacheStore)\n            assert isinstance(node.cache_ctrl._cache_stores[1], DiskCacheStore)\n\n        # specify\n        node = Node(cache_ctrl=[\"ram\"])\n        assert node.cache_ctrl is not None\n        assert len(node.cache_ctrl._cache_stores) == 1\n        assert isinstance(node.cache_ctrl._cache_stores[0], RamCacheStore)\n\n        node = Node(cache_ctrl=[\"ram\", \"disk\"])\n        assert node.cache_ctrl is not None\n        assert len(node.cache_ctrl._cache_stores) == 2\n        assert isinstance(node.cache_ctrl._cache_stores[0], RamCacheStore)\n        assert isinstance(node.cache_ctrl._cache_stores[1], DiskCacheStore)",
            "def test_tagged_attr_readonly(self):",
            "class MyNode(Node):\n            my_attr = tl.Any().tag(attr=True)\n\n        with podpac.settings:\n            podpac.settings[\"DEBUG\"] = False\n            node = MyNode()\n            assert node.traits()[\"my_attr\"].read_only\n\n            podpac.settings[\"DEBUG\"] = True\n            node = MyNode()\n            assert not node.traits()[\"my_attr\"].read_only",
            "def test_trait_is_defined(self):\n        node = Node()\n        if tl.version_info[0] >= 5:\n            assert not node.trait_is_defined(\"units\")\n        else:\n            assert node.trait_is_defined(\"units\")",
            "def test_init(self):",
            "class MyNode(Node):\n            init_run = False",
            "def init(self):\n                super(MyNode, self).init()\n                self.init_run = True\n\n        node = MyNode()\n        assert node.init_run",
            "def test_attrs(self):",
            "class MyNode(Node):\n            my_attr = tl.Any().tag(attr=True)\n            my_trait = tl.Any()\n\n        n = MyNode()\n        assert \"my_attr\" in n.attrs\n        assert \"my_trait\" not in n.attrs",
            "def test_repr(self):\n        n = Node()\n        repr(n)\n\n        n = Node(outputs=[\"a\", \"b\"])\n        repr(n)\n        assert \"outputs=\" in repr(n)\n        assert \"output=\" not in repr(n)\n\n        n = Node(outputs=[\"a\", \"b\"], output=\"a\")\n        repr(n)\n        assert \"outputs=\" not in repr(n)\n        assert \"output=\" in repr(n)",
            "def test_str(self):\n        n = Node()\n        str(n)\n\n        n = Node(outputs=[\"a\", \"b\"])\n        str(n)\n        assert \"outputs=\" in str(n)\n        assert \"output=\" not in str(n)\n\n        n = Node(outputs=[\"a\", \"b\"], output=\"a\")\n        str(n)\n        assert \"outputs=\" not in str(n)\n        assert \"output=\" in str(n)",
            "def test_eval_group(self):",
            "class MyNode(Node):",
            "def eval(self, coordinates, output=None, selector=None):\n                return self.create_output_array(coordinates)\n\n        c1 = podpac.Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = podpac.Coordinates([[10, 11], [10, 11, 12]], dims=[\"lat\", \"lon\"])\n        g = podpac.coordinates.GroupCoordinates([c1, c2])\n\n        node = MyNode()\n        outputs = node.eval_group(g)\n        assert isinstance(outputs, list)\n        assert len(outputs) == 2\n        assert isinstance(outputs[0], UnitsDataArray)\n        assert isinstance(outputs[1], UnitsDataArray)\n        assert outputs[0].shape == (2, 2)\n        assert outputs[1].shape == (2, 3)\n\n        # invalid\n        with pytest.raises(Exception):\n            node.eval_group(c1)\n\n        with pytest.raises(Exception):\n            node.eval(g)",
            "def test_eval_not_implemented(self):\n        node = Node()\n        with pytest.raises(NotImplementedError):\n            node.eval(podpac.Coordinates([]))\n\n        with pytest.raises(NotImplementedError):\n            node.eval(podpac.Coordinates([]), output=None)",
            "def test_find_coordinates_not_implemented(self):\n        node = Node()\n        with pytest.raises(NotImplementedError):\n            node.find_coordinates()",
            "def test_get_bounds(self):",
            "class MyNode(Node):",
            "def find_coordinates(self):\n                return [\n                    podpac.Coordinates([[0, 1, 2], [0, 10, 20]], dims=[\"lat\", \"lon\"], crs=\"EPSG:2193\"),\n                    podpac.Coordinates([[3, 4], [30, 40]], dims=[\"lat\", \"lon\"], crs=\"EPSG:2193\"),\n                ]\n\n        node = MyNode()\n\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CRS\"] = \"EPSG:4326\"\n\n            # specify crs\n            bounds, crs = node.get_bounds(crs=\"EPSG:2193\")\n            assert bounds == {\"lat\": (0, 4), \"lon\": (0, 40)}\n            assert crs == \"EPSG:2193\"\n\n            # default crs\n            bounds, crs = node.get_bounds()\n            assert bounds == {\n                \"lat\": (-75.81397534013118, -75.81362774074242),\n                \"lon\": (82.92787904584206, 82.9280189659297),\n            }\n            assert crs == \"EPSG:4326\"",
            "class TestCreateOutputArray(object):",
            "def test_create_output_array_default(self):\n        c = podpac.Coordinates([podpac.clinspace((0, 0), (1, 1), 10), [0, 1, 2]], dims=[\"lat_lon\", \"time\"])\n        node = Node()\n\n        output = node.create_output_array(c)\n        assert isinstance(output, UnitsDataArray)\n        assert output.shape == c.shape\n        assert output.dtype == node.dtype\n        assert output.crs == c.crs\n        assert np.all(np.isnan(output))",
            "def test_create_output_array_data(self):\n        c = podpac.Coordinates([podpac.clinspace((0, 0), (1, 1), 10), [0, 1, 2]], dims=[\"lat_lon\", \"time\"])\n        node = Node()\n\n        output = node.create_output_array(c, data=0)\n        assert isinstance(output, UnitsDataArray)\n        assert output.shape == c.shape\n        assert output.dtype == node.dtype\n        assert output.crs == c.crs\n        assert np.all(output == 0.0)\n\n    @pytest.mark.xfail(reason=\"not yet supported.\")",
            "def test_create_output_array_dtype(self):\n        c = podpac.Coordinates([podpac.clinspace((0, 0), (1, 1), 10), [0, 1, 2]], dims=[\"lat_lon\", \"time\"])\n        node = Node(dtype=bool)\n\n        output = node.create_output_array(c, data=0)\n        assert isinstance(output, UnitsDataArray)\n        assert output.shape == c.shape\n        assert output.dtype == node.dtype\n        assert output.crs == c.crs\n        assert np.all(~output)",
            "def test_create_output_array_units(self):\n        c = podpac.Coordinates([podpac.clinspace((0, 0), (1, 1), 10), [0, 1, 2]], dims=[\"lat_lon\", \"time\"])\n        node = Node(units=\"meters\")\n\n        output = node.create_output_array(c)\n        assert isinstance(output, UnitsDataArray)\n\n        from podpac.core.units import ureg as _ureg\n\n        assert output.units == _ureg.meters",
            "def test_create_output_array_crs(self):\n        crs = \"+proj=merc +lat_ts=56.5 +ellps=GRS80\"\n        c = podpac.Coordinates([podpac.clinspace((0, 0), (1, 1), 10), [0, 1, 2]], dims=[\"lat_lon\", \"time\"], crs=crs)\n        node = Node()\n\n        output = node.create_output_array(c)\n        assert output.crs == crs",
            "class TestNodeEval(object):",
            "def test_extract_output(self):\n        coords = podpac.Coordinates([[0, 1, 2, 3], [0, 1]], dims=[\"lat\", \"lon\"])",
            "class MyNode1(Node):\n            outputs = [\"a\", \"b\", \"c\"]",
            "def _eval(self, coordinates, output=None, selector=None):\n                return self.create_output_array(coordinates)\n\n        # don't extract when no output field is requested\n        node = MyNode1()\n        out = node.eval(coords)\n        assert out.shape == (4, 2, 3)\n\n        # do extract when an output field is requested\n        node = MyNode1(output=\"b\")\n        out = node.eval(coords)\n        assert out.shape == (4, 2)\n\n        # should still work if the node has already extracted it",
            "class MyNode2(Node):\n            outputs = [\"a\", \"b\", \"c\"]",
            "def _eval(self, coordinates, output=None, selector=None):\n                out = self.create_output_array(coordinates)\n                return out.sel(output=self.output)\n\n        node = MyNode2(output=\"b\")\n        out = node.eval(coords)\n        assert out.shape == (4, 2)",
            "def test_evaluate_transpose(self):",
            "class MyNode(Node):",
            "def _eval(self, coordinates, output=None, selector=None):\n                coords = coordinates.transpose(\"lat\", \"lon\")\n                data = np.arange(coords.size).reshape(coords.shape)\n                a = self.create_output_array(coords, data=data)\n                if output is None:\n                    output = a\n                else:\n                    output[:] = a.transpose(*output.dims)\n                return output\n\n        coords = podpac.Coordinates([[0, 1, 2, 3], [0, 1]], dims=[\"lat\", \"lon\"])\n\n        node = MyNode()\n        o1 = node.eval(coords)\n        o2 = node.eval(coords.transpose(\"lon\", \"lat\"))\n\n        # returned output should match the requested coordinates and data should be transposed\n        assert o1.dims == (\"lat\", \"lon\")\n        assert o2.dims == (\"lon\", \"lat\")\n        np.testing.assert_array_equal(o2.transpose(\"lat\", \"lon\").data, o1.data)\n\n        # with transposed output\n        o3 = node.create_output_array(coords.transpose(\"lon\", \"lat\"))\n        o4 = node.eval(coords, output=o3)\n\n        assert o3.dims == (\"lon\", \"lat\")  # stay the same\n        assert o4.dims == (\"lat\", \"lon\")  # match requested coordinates\n        np.testing.assert_equal(o3.transpose(\"lat\", \"lon\").data, o4.data)",
            "def test_eval_get_cache(self):\n        podpac.settings[\"RAM_CACHE_ENABLED\"] = True",
            "class MyNode(Node):",
            "def _eval(self, coordinates, output=None, selector=None):\n                coords = coordinates.transpose(\"lat\", \"lon\")\n                data = np.arange(coords.size).reshape(coords.shape)\n                a = self.create_output_array(coords, data=data)\n                if output is None:\n                    output = a\n                else:\n                    output[:] = a.transpose(*output.dims)\n                return output\n\n        coords = podpac.Coordinates([[0, 1, 2, 3], [0, 1]], dims=[\"lat\", \"lon\"])\n\n        node = MyNode(cache_output=True, cache_ctrl=CacheCtrl([RamCacheStore()]))\n\n        # first eval\n        o1 = node.eval(coords)\n        assert node._from_cache == False\n\n        # get from cache\n        o2 = node.eval(coords)\n        assert node._from_cache == True\n        np.testing.assert_array_equal(o2, o1)\n\n        # get from cache with output\n        o3 = node.eval(coords, output=o1)\n        assert node._from_cache == True\n        np.testing.assert_array_equal(o3, o1)\n\n        # get from cache with output transposed\n        o4 = node.eval(coords, output=o1.transpose(\"lon\", \"lat\"))\n        assert node._from_cache == True\n        np.testing.assert_array_equal(o4, o1)\n\n        # get from cache with coords transposed\n        o5 = node.eval(coords.transpose(\"lon\", \"lat\"))\n        assert node._from_cache == True\n        np.testing.assert_array_equal(o5, o1.transpose(\"lon\", \"lat\"))",
            "def test_eval_output_crs(self):\n        coords = podpac.Coordinates([[0, 1, 2, 3], [0, 1]], dims=[\"lat\", \"lon\"])\n\n        node = Node()\n        with pytest.raises(ValueError, match=\"Output coordinate reference system .* does not match\"):\n            node.eval(coords, output=node.create_output_array(coords.transform(\"EPSG:2193\")))",
            "class TestCaching(object):\n    @classmethod",
            "def setup_class(cls):\n        cls._ram_cache_enabled = podpac.settings[\"RAM_CACHE_ENABLED\"]\n\n        podpac.settings[\"RAM_CACHE_ENABLED\"] = True",
            "class MyNode(Node):\n            pass\n\n        cls.node = MyNode(cache_ctrl=CacheCtrl([RamCacheStore()]))\n        cls.node.rem_cache(key=\"*\", coordinates=\"*\")\n\n        cls.coords = podpac.Coordinates([0, 0], dims=[\"lat\", \"lon\"])\n        cls.coords2 = podpac.Coordinates([1, 1], dims=[\"lat\", \"lon\"])\n\n    @classmethod",
            "def teardown_class(cls):\n        cls.node.rem_cache(key=\"*\", coordinates=\"*\")\n\n        podpac.settings[\"RAM_CACHE_ENABLED\"] = cls._ram_cache_enabled",
            "def setup_method(self, method):\n        self.node.rem_cache(key=\"*\", coordinates=\"*\")",
            "def teardown_method(self, method):\n        self.node.rem_cache(key=\"*\", coordinates=\"*\")",
            "def test_has_cache(self):\n        assert not self.node.has_cache(\"test\")\n\n        self.node.put_cache(0, \"test\")\n        assert self.node.has_cache(\"test\")\n        assert not self.node.has_cache(\"test\", coordinates=self.coords)",
            "def test_has_coordinates(self):\n        assert not self.node.has_cache(\"test\", coordinates=self.coords)\n\n        self.node.put_cache(0, \"test\", coordinates=self.coords)\n\n        assert not self.node.has_cache(\"test\")\n        assert self.node.has_cache(\"test\", coordinates=self.coords)\n        assert not self.node.has_cache(\"test\", coordinates=self.coords2)",
            "def test_get_put_cache(self):\n        with pytest.raises(NodeException):\n            self.node.get_cache(\"test\")\n\n        self.node.put_cache(0, \"test\")\n        assert self.node.get_cache(\"test\") == 0",
            "def test_get_put_coordinates(self):\n        with pytest.raises(NodeException):\n            self.node.get_cache(\"test\")\n        with pytest.raises(NodeException):\n            self.node.get_cache(\"test\", coordinates=self.coords)\n        with pytest.raises(NodeException):\n            self.node.get_cache(\"test\", coordinates=self.coords2)\n\n        self.node.put_cache(0, \"test\")\n        self.node.put_cache(1, \"test\", coordinates=self.coords)\n        self.node.put_cache(2, \"test\", coordinates=self.coords2)\n\n        assert self.node.get_cache(\"test\") == 0\n        assert self.node.get_cache(\"test\", coordinates=self.coords) == 1\n        assert self.node.get_cache(\"test\", coordinates=self.coords2) == 2",
            "def test_put_overwrite(self):\n        self.node.put_cache(0, \"test\")\n        assert self.node.get_cache(\"test\") == 0\n\n        with pytest.raises(NodeException):\n            self.node.put_cache(1, \"test\", overwrite=False)\n        assert self.node.get_cache(\"test\") == 0\n\n        self.node.put_cache(1, \"test\")\n        assert self.node.get_cache(\"test\") == 1",
            "def test_rem_all(self):\n        self.node.put_cache(0, \"a\")\n        self.node.put_cache(0, \"b\")\n        self.node.put_cache(0, \"a\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords2)\n        self.node.put_cache(0, \"d\", coordinates=self.coords)\n\n        self.node.rem_cache(key=\"*\", coordinates=\"*\")\n        assert not self.node.has_cache(\"a\")\n        assert not self.node.has_cache(\"b\")\n        assert not self.node.has_cache(\"a\", coordinates=self.coords)\n        assert not self.node.has_cache(\"c\", coordinates=self.coords)\n        assert not self.node.has_cache(\"c\", coordinates=self.coords2)\n        assert not self.node.has_cache(\"d\", coordinates=self.coords)",
            "def test_rem_key(self):\n        self.node.put_cache(0, \"a\")\n        self.node.put_cache(0, \"b\")\n        self.node.put_cache(0, \"a\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords2)\n        self.node.put_cache(0, \"d\", coordinates=self.coords)\n\n        self.node.rem_cache(key=\"a\", coordinates=\"*\")\n\n        assert not self.node.has_cache(\"a\")\n        assert not self.node.has_cache(\"a\", coordinates=self.coords)\n        assert self.node.has_cache(\"b\")\n        assert self.node.has_cache(\"c\", coordinates=self.coords)\n        assert self.node.has_cache(\"c\", coordinates=self.coords2)\n        assert self.node.has_cache(\"d\", coordinates=self.coords)",
            "def test_rem_coordinates(self):\n        self.node.put_cache(0, \"a\")\n        self.node.put_cache(0, \"b\")\n        self.node.put_cache(0, \"a\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords2)\n        self.node.put_cache(0, \"d\", coordinates=self.coords)\n\n        self.node.rem_cache(key=\"*\", coordinates=self.coords)\n\n        assert self.node.has_cache(\"a\")\n        assert not self.node.has_cache(\"a\", coordinates=self.coords)\n        assert self.node.has_cache(\"b\")\n        assert not self.node.has_cache(\"c\", coordinates=self.coords)\n        assert self.node.has_cache(\"c\", coordinates=self.coords2)\n        assert not self.node.has_cache(\"d\", coordinates=self.coords)",
            "def test_rem_key_coordinates(self):\n        self.node.put_cache(0, \"a\")\n        self.node.put_cache(0, \"b\")\n        self.node.put_cache(0, \"a\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords)\n        self.node.put_cache(0, \"c\", coordinates=self.coords2)\n        self.node.put_cache(0, \"d\", coordinates=self.coords)\n\n        self.node.rem_cache(key=\"a\", coordinates=self.coords)\n\n        assert self.node.has_cache(\"a\")\n        assert not self.node.has_cache(\"a\", coordinates=self.coords)\n        assert self.node.has_cache(\"b\")\n        assert self.node.has_cache(\"c\", coordinates=self.coords)\n        assert self.node.has_cache(\"c\", coordinates=self.coords2)\n        assert self.node.has_cache(\"d\", coordinates=self.coords)",
            "def test_put_has_expires(self):\n        self.node.put_cache(10, \"key1\", expires=\"1,D\")\n        self.node.put_cache(10, \"key2\", expires=\"-1,D\")\n        assert self.node.has_cache(\"key1\")\n        assert not self.node.has_cache(\"key2\")",
            "def test_put_get_expires(self):\n        self.node.put_cache(10, \"key1\", expires=\"1,D\")\n        self.node.put_cache(10, \"key2\", expires=\"-1,D\")\n        assert self.node.get_cache(\"key1\") == 10\n        with pytest.raises(NodeException, match=\"cached data not found\"):\n            self.node.get_cache(\"key2\")\n\n    # node definition errors\n    # this demonstrates both classes of error in the has_cache case, but only one for put/get/rem\n    # we could test both classes for put/get/rem as well, but that is not really necessary",
            "def test_has_cache_unavailable_circular(self):",
            "class MyNode(Node):\n            a = tl.Any().tag(attr=True)\n\n            @tl.default(\"a\")",
            "def _default_a(self):\n                return self.b\n\n            @property",
            "def b(self):\n                self.has_cache(\"b\")\n                return 10\n\n        node = MyNode(cache_ctrl=[\"ram\"])\n        with pytest.raises(NodeException, match=\"Cache unavailable, node definition has a circular dependency\"):\n            assert node.b == 10",
            "def test_has_cache_unavailable_uninitialized(self):",
            "class MyNode(Node):\n            a = tl.Any().tag(attr=True)\n\n            @tl.validate(\"a\")",
            "def _validate_a(self, d):\n                self.b\n                return d[\"value\"]\n\n            @property",
            "def b(self):\n                self.has_cache(\"key\")\n                return 10\n\n        with pytest.raises(NodeException, match=\"Cache unavailable, node is not yet fully initialized\"):\n            node = MyNode(a=3, cache_ctrl=[\"ram\"])",
            "def test_put_cache_unavailable_uninitialized(self):",
            "class MyNode(Node):\n            a = tl.Any().tag(attr=True)\n\n            @tl.validate(\"a\")",
            "def _validate_a(self, d):\n                self.b\n                return d[\"value\"]\n\n            @property",
            "def b(self):\n                self.put_cache(10, \"key\")\n                return 10\n\n        with pytest.raises(NodeException, match=\"Cache unavailable\"):\n            node = MyNode(a=3, cache_ctrl=[\"ram\"])",
            "def test_get_cache_unavailable_uninitialized(self):",
            "class MyNode(Node):\n            a = tl.Any().tag(attr=True)\n\n            @tl.validate(\"a\")",
            "def _validate_a(self, d):\n                self.b\n                return d[\"value\"]\n\n            @property",
            "def b(self):\n                self.get_cache(\"key\")\n                return 10\n\n        with pytest.raises(NodeException, match=\"Cache unavailable\"):\n            node = MyNode(a=3, cache_ctrl=[\"ram\"])",
            "def test_rem_cache_unavailable_uninitialized(self):",
            "class MyNode(Node):\n            a = tl.Any().tag(attr=True)\n\n            @tl.validate(\"a\")",
            "def _validate_a(self, d):\n                self.b\n                return d[\"value\"]\n\n            @property",
            "def b(self):\n                self.rem_cache(\"key\")\n                return 10\n\n        with pytest.raises(NodeException, match=\"Cache unavailable\"):\n            node = MyNode(a=3, cache_ctrl=[\"ram\"])",
            "class TestSerialization(object):\n    @classmethod",
            "def setup_class(cls):\n        a = podpac.algorithm.Arange()\n        b = podpac.data.Array(source=[10, 20, 30], coordinates=podpac.Coordinates([[0, 1, 2]], dims=[\"lat\"]))\n        c = podpac.compositor.OrderedCompositor(sources=[a, b])\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Insecure evaluation.*\")\n            cls.node = podpac.algorithm.Arithmetic(A=a, B=b, C=c, eqn=\"A + B + C\")",
            "def test_base_ref(self):\n        node = Node()\n        assert isinstance(node.base_ref, six.string_types)",
            "def test_base_definition(self):\n        node = Node()\n        d = node._base_definition\n        assert \"node\" in d\n        assert isinstance(d[\"node\"], six.string_types)",
            "def test_base_definition_attrs(self):",
            "class MyNode(Node):\n            my_attr = tl.Int().tag(attr=True)\n\n        node = MyNode(my_attr=7)\n\n        d = node._base_definition\n        assert d[\"attrs\"][\"my_attr\"] == 7",
            "def test_base_definition_inputs(self):",
            "class MyNode(Node):\n            my_attr = NodeTrait().tag(attr=True)\n\n        a = Node()\n        node = MyNode(my_attr=a)\n\n        d = node._base_definition\n        assert d[\"inputs\"][\"my_attr\"] == a",
            "def test_base_definition_inputs_array(self):",
            "class MyNode(Node):\n            my_attr = ArrayTrait().tag(attr=True)\n\n        a = Node()\n        b = Node()\n        node = MyNode(my_attr=[a, b])\n\n        d = node._base_definition\n        assert d[\"inputs\"][\"my_attr\"][0] == a\n        assert d[\"inputs\"][\"my_attr\"][1] == b",
            "def test_base_definition_inputs_dict(self):",
            "class MyNode(Node):\n            my_attr = tl.Dict().tag(attr=True)\n\n        a = Node()\n        b = Node()\n        node = MyNode(my_attr={\"a\": a, \"b\": b})\n\n        d = node._base_definition\n        assert d[\"inputs\"][\"my_attr\"][\"a\"] == a\n        assert d[\"inputs\"][\"my_attr\"][\"b\"] == b",
            "def test_base_definition_style(self):\n        node = Node(style=Style(name=\"test\"))\n        d = node._base_definition\n        assert \"style\" in node._base_definition",
            "def test_base_definition_remove_unnecessary_attrs(self):\n        node = Node(outputs=[\"a\", \"b\"], output=\"a\", units=\"m\")\n        d = node._base_definition\n        assert \"outputs\" in d[\"attrs\"]\n        assert \"output\" in d[\"attrs\"]\n        assert \"units\" in d[\"attrs\"]\n\n        node = Node()\n        d = node._base_definition\n        if \"attrs\" in d:\n            assert \"outputs\" not in d[\"attrs\"]\n            assert \"output\" not in d[\"attrs\"]\n            assert \"units\" not in d[\"attrs\"]",
            "def test_definition(self):\n        # definition\n        d = self.node.definition\n        assert isinstance(d, OrderedDict)\n        assert len(d) == 5\n\n        # from_definition\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Insecure evaluation.*\")\n            node = Node.from_definition(d)\n\n        assert node is not self.node\n        assert node == self.node\n        assert isinstance(node, podpac.algorithm.Arithmetic)\n        assert isinstance(node.inputs[\"A\"], podpac.algorithm.Arange)\n        assert isinstance(node.inputs[\"B\"], podpac.data.Array)\n        assert isinstance(node.inputs[\"C\"], podpac.compositor.OrderedCompositor)",
            "def test_definition_duplicate_base_ref(self):\n        n1 = Node(units=\"m\")\n        n2 = Node(units=\"ft\")\n        n3 = Node(units=\"in\")\n        node = podpac.compositor.OrderedCompositor(sources=[n1, n2, n3])\n        d = node.definition\n        assert n1.base_ref == n2.base_ref == n3.base_ref\n        assert len(d) == 5",
            "def test_definition_inputs_array(self):\n        global MyNodeWithArrayInput",
            "class MyNodeWithArrayInput(Node):\n            my_array = ArrayTrait().tag(attr=True)\n\n        node1 = MyNodeWithArrayInput(my_array=[podpac.algorithm.Arange()])\n        node2 = Node.from_definition(node1.definition)\n        assert node2 is not node1 and node2 == node1",
            "def test_definition_inputs_dict(self):\n        global MyNodeWithDictInput",
            "class MyNodeWithDictInput(Node):\n            my_dict = tl.Dict().tag(attr=True)\n\n        node1 = MyNodeWithDictInput(my_dict={\"a\": podpac.algorithm.Arange()})\n        node2 = Node.from_definition(node1.definition)\n        assert node2 is not node1 and node2 == node1",
            "def test_definition_version(self):\n        d = self.node.definition\n        assert \"podpac_version\" in d\n        assert d[\"podpac_version\"] == podpac.__version__",
            "def test_json(self):\n        # json\n        s = self.node.json\n        assert isinstance(s, six.string_types)\n        assert json.loads(s)\n\n        # test from_json\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Insecure evaluation.*\")\n            node = Node.from_json(s)\n        assert node is not self.node\n        assert node == self.node\n        assert isinstance(node, podpac.algorithm.Arithmetic)\n        assert isinstance(node.inputs[\"A\"], podpac.algorithm.Arange)\n        assert isinstance(node.inputs[\"B\"], podpac.data.Array)\n        assert isinstance(node.inputs[\"C\"], podpac.compositor.OrderedCompositor)",
            "def test_file(self):\n        path = tempfile.mkdtemp(prefix=\"podpac-test-\")\n        filename = os.path.join(path, \"node.json\")\n\n        # save\n        self.node.save(filename)\n        assert os.path.exists(filename)\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Insecure evaluation.*\")\n            node = Node.load(filename)\n\n        assert node is not self.node\n        assert node == self.node\n        assert isinstance(node, podpac.algorithm.Arithmetic)\n        assert isinstance(node.inputs[\"A\"], podpac.algorithm.Arange)\n        assert isinstance(node.inputs[\"B\"], podpac.data.Array)\n        assert isinstance(node.inputs[\"C\"], podpac.compositor.OrderedCompositor)",
            "def test_json_pretty(self):\n        node = Node()\n        s = node.json_pretty\n        assert isinstance(s, six.string_types)\n        json.loads(s)",
            "def test_hash(self):",
            "class N(Node):\n            my_attr = tl.Int().tag(attr=True)",
            "class M(Node):\n            my_attr = tl.Int().tag(attr=True)\n\n        n1 = N(my_attr=1)\n        n2 = N(my_attr=1)\n        n3 = N(my_attr=2)\n        m1 = M(my_attr=1)\n\n        assert n1.hash == n2.hash\n        assert n1.hash != n3.hash\n        assert n1.hash != m1.hash",
            "def test_hash_preserves_definition(self):\n        n = Node()\n        d_before = deepcopy(n.definition)\n        h = n.hash\n        d_after = deepcopy(n.definition)\n\n        assert d_before == d_after",
            "def test_hash_omit_style(self):",
            "class N(Node):\n            my_attr = tl.Int().tag(attr=True)\n\n        n1 = N(my_attr=1, style=Style(name=\"a\"))\n        n2 = N(my_attr=1, style=Style(name=\"b\"))\n\n        # json has style in it\n        assert n1.json != n2.json\n\n        # but hash does not\n        assert n1.hash == n2.hash",
            "def test_hash_omit_version(self):\n        version = podpac.__version__\n\n        try:\n            # actual version\n            n1 = Node()\n            s1 = n1.json\n            h1 = n1.hash\n\n            # spoof different version\n            podpac.__version__ = \"other\"\n            n2 = Node()\n            s2 = n2.json\n            h2 = n2.hash\n\n            # JSON should be different, but hash should be the same\n            assert s1 != s2\n            assert h1 == h2\n\n        finally:\n            # reset version\n            podpac.__version__ = version",
            "def test_eq(self):",
            "class N(Node):\n            my_attr = tl.Int().tag(attr=True)",
            "class M(Node):\n            my_attr = tl.Int().tag(attr=True)\n\n        n1 = N(my_attr=1)\n        n2 = N(my_attr=1)\n        n3 = N(my_attr=2)\n        m1 = M(my_attr=1)\n\n        # eq\n        assert n1 == n2\n        assert not n1 == n3\n        assert not n1 == m1\n        assert not n1 == \"other\"\n\n        # ne\n        assert not n1 != n2\n        assert n1 != n3\n        assert n1 != m1\n        assert n1 != \"other\"",
            "def test_eq_ignore_style(self):",
            "class N(Node):\n            my_attr = tl.Int().tag(attr=True)\n\n        n1 = N(my_attr=1, style=Style(name=\"a\"))\n        n2 = N(my_attr=1, style=Style(name=\"b\"))\n\n        # json has style in it\n        assert n1.json != n2.json\n\n        # but == and != don't care\n        assert n1 == n2\n        assert not n1 != n2\n\n    def test_from_url(self):\n        url = (\n            r\"http:",
            "def test_from_url(self):\n        url = (\n            r\"http:",
            "def test_from_url_with_plugin_style_params(self):\n        url0 = (\n            r\"https:",
            "def test_from_name_params(self):\n        # Normal\n        name = \"algorithm.Arange\"\n        node = Node.from_name_params(name)\n\n        # Normal with params\n        name = \"algorithm.CoordData\"\n        params = {\"coord_name\": \"alt\"}\n        node = Node.from_name_params(name, params)\n        assert node.coord_name == \"alt\"\n\n        # Plugin style\n        name = \"CoordData\"\n        params = {\"plugin\": \"podpac.algorithm\", \"attrs\": {\"coord_name\": \"alt\"}}\n        node = Node.from_name_params(name, params)\n        assert node.coord_name == \"alt\"",
            "def test_style(self):\n        node = podpac.data.Array(\n            source=[10, 20, 30],\n            coordinates=podpac.Coordinates([[0, 1, 2]], dims=[\"lat\"]),\n            style=Style(name=\"test\", units=\"m\"),\n        )\n\n        d = node.definition\n        assert \"style\" in d[node.base_ref]\n\n        node2 = Node.from_definition(d)\n        assert node2 is not node\n        assert isinstance(node2, podpac.data.Array)\n        assert node2.style is not node.style\n        assert node2.style == node.style\n        assert node2.style.name == \"test\"\n        assert node2.style.units == \"m\"\n\n        # default style\n        node = podpac.data.Array(source=[10, 20, 30], coordinates=podpac.Coordinates([[0, 1, 2]], dims=[\"lat\"]))\n        d = node.definition\n        assert \"style\" not in d[node.base_ref]",
            "def test_circular_definition(self):\n        # this is admittedly a contrived example in order to demonstrate the most direct case",
            "class MyNode(Node):\n            a = tl.Any().tag(attr=True)\n\n            @tl.default(\"a\")",
            "def _default_a(self):\n                self.definition\n                return 10\n\n        node = MyNode()\n        with pytest.raises(NodeDefinitionError, match=\"node definition has a circular dependency\"):\n            node.a",
            "class TestUserDefinition(object):",
            "def test_empty(self):\n        s = \"{ }\"\n        with pytest.raises(ValueError, match=\"definition cannot be empty\"):\n            Node.from_json(s)",
            "def test_no_node(self):\n        s = '{\"test\": { } }'\n        with pytest.raises(ValueError, match=\"'node' property required\"):\n            Node.from_json(s)",
            "def test_invalid_node(self):\n        # module does not exist\n        s = '{\"a\": {\"node\": \"nonexistent.Arbitrary\"} }'\n        with pytest.raises(ValueError, match=\"no module found\"):\n            Node.from_json(s)\n\n        # node does not exist in module\n        s = '{\"a\": {\"node\": \"core.Nonexistent\"} }'\n        with pytest.raises(ValueError, match=\"class 'Nonexistent' not found in module\"):\n            Node.from_json(s)",
            "def test_inputs(self):\n        # invalid type\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Min\",\n                \"inputs\": { \"source\": 10 }\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"Invalid definition for node\"):\n            Node.from_json(s)\n\n        # nonexistent node\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Min\",\n                \"inputs\": { \"source\": \"nonexistent\" }\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"Invalid definition for node\"):\n            Node.from_json(s)",
            "def test_lookup_attrs(self):\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": \"a.coord_name\" }\n            }\n        }\n        \"\"\"\n\n        node = Node.from_json(s)\n        assert isinstance(node, podpac.algorithm.CoordData)\n        assert node.coord_name == \"lat\"\n\n        # invalid type\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": 10 }\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"Invalid definition for node\"):\n            Node.from_json(s)\n\n        # nonexistent node\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": \"nonexistent.coord_name\" }\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"Invalid definition for node\"):\n            Node.from_json(s)\n\n        # nonexistent subattr\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.CoordData\",\n                \"attrs\": { \"coord_name\": \"lat\" }\n            },\n            \"b\": {\n                \"node\": \"algorithm.CoordData\",\n                \"lookup_attrs\": { \"coord_name\": \"a.nonexistent\" }\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"Invalid definition for node\"):\n            Node.from_json(s)",
            "def test_invalid_property(self):\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Arange\",\n                \"invalid_property\": \"value\"\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"unexpected property\"):\n            Node.from_json(s)",
            "def test_plugin(self):\n        global MyPluginNode",
            "class MyPluginNode(Node):\n            pass\n\n        s = \"\"\"\n        {\n            \"mynode\": {\n                \"plugin\": \"test_node\",\n                \"node\": \"MyPluginNode\"\n            }\n        }\n        \"\"\"\n\n        node = Node.from_json(s)\n        assert isinstance(node, MyPluginNode)\n\n        # missing plugin\n        s = \"\"\"\n        {\n            \"mynode\": {\n                \"plugin\": \"missing\",\n                \"node\": \"MyPluginNode\"\n            }\n        }\n        \"\"\"\n\n        with pytest.raises(ValueError, match=\"no module found\"):\n            Node.from_json(s)",
            "def test_debuggable(self):\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Arange\"\n            },\n            \"mean\": {\n                \"node\": \"algorithm.Convolution\",\n                \"lookup_attrs\": {\"source\": \"a\"},\n                \"attrs\": {\"kernel_type\": \"mean,3\", \"kernel_dims\": [\"lat\", \"lon\"]}\n            },\n            \"c\": {\n                \"node\": \"algorithm.Arithmetic\",\n                \"lookup_attrs\": {\"A\": \"a\", \"B\": \"mean\"},\n                \"attrs\": {\"eqn\": \"a-b\"}\n            }\n        }\n        \"\"\"\n\n        with warnings.catch_warnings(), podpac.settings:\n            warnings.filterwarnings(\"ignore\", \"Insecure evaluation.*\")\n\n            # normally node objects can and should be re-used\n            podpac.settings[\"DEBUG\"] = False\n            node = Node.from_json(s)\n            assert node.inputs[\"A\"] is node.inputs[\"B\"].source\n\n            # when debugging is on, node objects should be unique\n            podpac.settings[\"DEBUG\"] = True\n            node = Node.from_json(s)\n            assert node.inputs[\"A\"] is not node.inputs[\"B\"].source",
            "def test_from_definition_version_warning(self):\n        s = \"\"\"\n        {\n            \"a\": {\n                \"node\": \"algorithm.Arange\"\n            },\n            \"podpac_version\": \"other\"\n        }\n        \"\"\"\n\n        with pytest.warns(UserWarning, match=\"node definition version mismatch\"):\n            node = Node.from_json(s)",
            "def test_from_proper_json(self):\n        not_ordered_json = \"\"\"\n        {\n            \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"\n        not_ordered_json_2 = \"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"\n        ordered_json = \"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"\n        # Check that the order doesn't matter. Because .from_json returns the output node, also checks correct output_node is returned\n        not_ordered_pipe = Node.from_json(not_ordered_json)\n        not_ordered_pipe_2 = Node.from_json(not_ordered_json_2)\n        ordered_pipe = Node.from_json(ordered_json)\n        assert not_ordered_pipe.definition == ordered_pipe.definition == not_ordered_pipe_2.definition\n        assert not_ordered_pipe.hash == ordered_pipe.hash\n\n        # Check that incomplete json will throw ValueError:\n        incomplete_json = \"\"\"\n        {\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"\n        with pytest.raises(ValueError):\n            Node.from_json(incomplete_json)",
            "def test_output_node(self):\n        included_json = \"\"\"\n        {\n            \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n            \"podpac_version\": \"3.2.0\",\n            \"podpac_output_node\": \"Arithmetic\"\n        }\n        \"\"\"\n        ordered_json = \"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\"\n        }\n        \"\"\"\n        included_pipe = Node.from_json(included_json)\n        ordered_pipe = Node.from_json(ordered_json)\n        assert included_pipe.definition == ordered_pipe.definition\n        assert included_pipe.hash == ordered_pipe.hash\n\n        wrong_name_json = \"\"\"\n        {\n            \"SinCoords\": {\n                \"node\": \"core.algorithm.utility.SinCoords\",\n                \"style\": {\n                    \"colormap\": \"jet\",\n                    \"clim\": [\n                        -1.0,\n                        1.0\n                    ]\n                }\n            },\n            \"Arange\": {\n                \"node\": \"core.algorithm.utility.Arange\"\n            },\n             \"Arithmetic\": {\n                \"node\": \"core.algorithm.generic.Arithmetic\",\n                \"attrs\": {\n                    \"eqn\": \"a+b\",\n                    \"params\": {\n\n                    }\n                },\n                \"inputs\": {\n                    \"a\": \"SinCoords\",\n                    \"b\": \"Arange\"\n                }\n            },\n            \"podpac_version\": \"3.2.0\",\n            \"podpac_output_node\": \"Sum\"\n        }\n        \"\"\"\n        with pytest.raises(ValueError):\n            Node.from_json(wrong_name_json)",
            "class TestNoCacheMixin(object):",
            "class NoCacheNode(NoCacheMixin, Node):\n        pass",
            "def test_default_no_cache(self):\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n            node = self.NoCacheNode()\n            assert len(node.cache_ctrl._cache_stores) == 0",
            "def test_customizable(self):\n        podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n        node = self.NoCacheNode(cache_ctrl=[\"ram\"])\n        assert len(node.cache_ctrl._cache_stores) == 1",
            "class TestDiskCacheMixin(object):",
            "class DiskCacheNode(DiskCacheMixin, Node):\n        pass",
            "def test_default_disk_cache(self):\n        with podpac.settings:\n            # add disk cache\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n            node = self.DiskCacheNode()\n            assert len(node.cache_ctrl._cache_stores) == 2\n\n            # don't add if it is already there\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\", \"disk\"]\n            node = self.DiskCacheNode()\n            assert len(node.cache_ctrl._cache_stores) == 2",
            "def test_customizable(self):\n        node = self.DiskCacheNode(cache_ctrl=[\"ram\"])\n        assert len(node.cache_ctrl._cache_stores) == 1\n\n\n# TODO: remove this - this is currently a placeholder test until we actually have integration tests (pytest will exit with code 5 if no tests found)\n@pytest.mark.integration",
            "def tests_node_integration():\n    assert True"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_units.py",
        "comments": [
            "// a2",
            "// a2",
            "// a2",
            "// a2",
            "// (1 * ureg.inch)).to_base_units().magnitude",
            "// a2"
        ],
        "docstrings": [
            "\"\"\"\n        I believe this is not the desired behavior. We should look at this function.\n        \"\"\""
        ],
        "code_snippets": [
            "class TestUnitDataArray(object):",
            "def test_no_units_to_base_units_has_no_units(self):\n        a = UnitsDataArray(\n            np.arange(24, dtype=np.float64).reshape((3, 4, 2)),\n            coords={\"x\": np.arange(3), \"y\": np.arange(4) * 10, \"z\": np.arange(2) + 100},\n            dims=[\"x\", \"y\", \"z\"],\n        )\n        b = a.to_base_units()\n        assert b.attrs.get(\"units\", None) is None",
            "def test_reductions_maintain_units(self):\n        n_lats = 3\n        n_lons = 4\n        n_alts = 2\n        a = UnitsDataArray(\n            np.arange(n_lats * n_lons * n_alts).reshape((n_lats, n_lons, n_alts)),\n            dims=[\"lat\", \"lon\", \"alt\"],\n            attrs={\"units\": ureg.meter},\n        )\n        assert a.mean(axis=0).attrs.get(\"units\", None) is not None\n        assert a.sum(axis=0).attrs.get(\"units\", None) is not None\n        assert a.cumsum(axis=0).attrs.get(\"units\", None) is not None\n        assert a.min(axis=0).attrs.get(\"units\", None) is not None\n        assert a.max(\"lon\").attrs.get(\"units\", None) is not None\n        assert np.mean(a, axis=0).attrs.get(\"units\", None) is not None\n        assert np.sum(a, axis=0).attrs.get(\"units\", None) is not None\n        assert np.cumsum(a, axis=0).attrs.get(\"units\", None) is not None\n        assert np.min(a, axis=0).attrs.get(\"units\", None) is not None\n        assert np.max(a, axis=0).attrs.get(\"units\", None) is not None",
            "def test_reductions_over_named_axes(self):\n        n_lats = 3\n        n_lons = 4\n        n_alts = 2\n        a = UnitsDataArray(\n            np.arange(n_lats * n_lons * n_alts).reshape((n_lats, n_lons, n_alts)),\n            dims=[\"lat\", \"lon\", \"alt\"],\n            attrs={\"units\": ureg.meter},\n        )\n        assert len(a.mean([\"lat\", \"lon\"]).data) == 2",
            "def test_serialization_deserialization(self):\n        n_lats = 3\n        n_lons = 4\n        n_alts = 2\n        a = UnitsDataArray(\n            np.arange(n_lats * n_lons * n_alts).reshape((n_lats, n_lons, n_alts)),\n            dims=[\"lat\", \"lon\", \"alt\"],\n            attrs={\"units\": ureg.meter, \"layer_style\": Style()},\n        )\n        f = a.to_netcdf()\n        b = UnitsDataArray(xr.open_dataarray(f))\n        assert a.attrs[\"units\"] == b.attrs[\"units\"]\n        assert a.attrs[\"layer_style\"].json == b.attrs[\"layer_style\"].json",
            "def test_pow(self):\n        n_lats = 3\n        n_lons = 4\n        n_alts = 2\n\n        a = UnitsDataArray(\n            np.arange(n_lats * n_lons * n_alts).reshape((n_lats, n_lons, n_alts)),\n            dims=[\"lat\", \"lon\", \"alt\"],\n            attrs={\"units\": ureg.meter},\n        )\n        assert (a**2).attrs[\"units\"] == ureg.meter**2",
            "def test_set_to_value_using_UnitsDataArray_as_mask_does_nothing_if_mask_has_dim_not_in_array(self):\n        a = UnitsDataArray(\n            np.arange(24, dtype=np.float64).reshape((3, 4, 2)),\n            coords={\"x\": np.arange(3), \"y\": np.arange(4) * 10, \"z\": np.arange(2) + 100},\n            dims=[\"x\", \"y\", \"z\"],\n        )\n        b = UnitsDataArray(\n            np.arange(24, dtype=np.float64).reshape((3, 4, 2)),\n            coords={\"i\": np.arange(3), \"y\": np.arange(4) * 10, \"z\": np.arange(2) + 100},\n            dims=[\"i\", \"y\", \"z\"],\n        )\n\n        mask = b > -10\n        value = np.nan\n\n        a.set(value, mask)\n        # dims of a remain unchanged\n        assert not np.any(np.isnan(a.data))",
            "def test_set_to_value_using_UnitsDataArray_as_mask_broadcasts_to_dimensions_not_in_mask(self):\n        a = UnitsDataArray(\n            np.arange(24, dtype=np.float64).reshape((3, 4, 2)),\n            coords={\"x\": np.arange(3), \"y\": np.arange(4) * 10, \"z\": np.arange(2) + 100},\n            dims=[\"x\", \"y\", \"z\"],\n        )\n        b = a[0, :, :]\n        b = b < 3\n\n        mask = b.transpose(*(\"z\", \"y\"))\n        value = np.nan\n\n        a.set(value, mask)\n        # dims of a remain unchanged\n        assert np.all(np.array(a.dims) == np.array((\"x\", \"y\", \"z\")))\n        # shape of a remains unchanged\n        assert np.all(np.array(a.values.shape) == np.array((3, 4, 2)))\n        # a.set was broadcast across the 'x' dimension\n        for x in range(3):\n            for y in range(4):\n                for z in range(2):\n                    if y == 0 and (z == 0 or z == 1):\n                        assert np.isnan(a[x, y, z])\n                    elif y == 1 and z == 0:\n                        assert np.isnan(a[x, y, z])\n                    else:\n                        assert not np.isnan(a[x, y, z])",
            "def test_get_item_with_1d_units_data_array_as_key_boradcasts_to_correct_dimension(self):\n        a = UnitsDataArray(\n            np.arange(24).reshape((3, 4, 2)),\n            coords={\"x\": np.arange(3), \"y\": np.arange(4) * 10, \"z\": np.arange(2) + 100},\n            dims=[\"x\", \"y\", \"z\"],\n        )\n        b = a[0, :, 0]\n        b = b < 3\n        c = a[b]\n        # dims of a remain unchanged\n        assert np.all(np.array(a.dims) == np.array((\"x\", \"y\", \"z\")))\n        # shape of a remains unchanged\n        assert np.all(np.array(a.values.shape) == np.array((3, 4, 2)))\n        # dims of a remain unchanged\n        assert np.all(np.array(c.dims) == np.array((\"x\", \"y\", \"z\")))\n        # shape of a remains unchanged\n        assert np.all(np.array(c.values.shape) == np.array((3, 2, 2)))\n        # a[b] was broadcast across the 'y' dimension\n        for x in range(3):\n            for y in range(2):\n                for z in range(2):\n                    c[x, y, z] in [0, 1, 2, 3, 8, 9, 10, 11, 16, 17, 18, 19]",
            "def test_get_item_with_units_data_array_as_key_throws_index_error(self):\n        \"\"\"\n        I believe this is not the desired behavior. We should look at this function.\n        \"\"\"\n        a = UnitsDataArray(\n            np.arange(24, dtype=np.float64).reshape((3, 4, 2)),\n            coords={\"x\": np.arange(3), \"y\": np.arange(4) * 10, \"z\": np.arange(2) + 100},\n            dims=[\"x\", \"y\", \"z\"],\n        )\n        b = a < 3\n        with pytest.raises(IndexError):\n            a[b]",
            "def test_partial_transpose_specify_just_lon_swaps_lat_lon(self):\n        n_lats = 3\n        n_lons = 4\n        lat_lon = UnitsDataArray(\n            np.arange(12).reshape((n_lats, n_lons)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter}\n        )\n        lon_lat = lat_lon.part_transpose([\"lon\"])\n        for lat in range(n_lats):\n            for lon in range(n_lons):\n                lat_lon[lat, lon] == lon_lat[lon, lat]",
            "def test_partial_transpose_specify_both_swaps_lat_lon(self):\n        n_lats = 3\n        n_lons = 4\n        lat_lon = UnitsDataArray(\n            np.arange(12).reshape((n_lats, n_lons)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter}\n        )\n        lon_lat = lat_lon.part_transpose([\"lon\", \"lat\"])\n        for lat in range(n_lats):\n            for lon in range(n_lons):\n                lat_lon[lat, lon] == lon_lat[lon, lat]",
            "def test_partial_transpose_specify_none_leaves_lat_lon_untouched(self):\n        n_lats = 3\n        n_lons = 4\n        lat_lon = UnitsDataArray(\n            np.arange(12).reshape((n_lats, n_lons)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter}\n        )\n        lat_lon_2 = lat_lon.part_transpose([])\n        for lat in range(n_lats):\n            for lon in range(n_lons):\n                lat_lon[lat, lon] == lat_lon_2[lat, lon]\n\n    def test_no_units_coord(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={})\n        a3 = a1 + a2\n        a3b = a2 + a1\n        a4 = a1 > a2\n        a5 = a1 < a2\n        a6 = a1 == a2\n        a7 = a1 * a2\n        a8 = a2 / a1\n        a9 = a1",
            "def test_no_units_coord(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={})\n        a3 = a1 + a2\n        a3b = a2 + a1\n        a4 = a1 > a2\n        a5 = a1 < a2\n        a6 = a1 == a2\n        a7 = a1 * a2\n        a8 = a2 / a1\n        a9 = a1",
            "def test_first_units_coord(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={})\n        with pytest.raises(DimensionalityError):\n            a3 = a1 + a2\n        with pytest.raises(DimensionalityError):\n            a3b = a2 + a1\n        with pytest.raises(DimensionalityError):\n            a4 = a1 > a2\n        with pytest.raises(DimensionalityError):\n            a5 = a1 < a2\n        with pytest.raises(DimensionalityError):\n            a6 = a1 == a2\n        a7 = a1 * a2\n        a8 = a2 / a1\n        with pytest.raises(DimensionalityError):\n            a9 = a1",
            "def test_second_units_coord(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.inch})\n        with pytest.raises(DimensionalityError):\n            a3 = a1 + a2\n        with pytest.raises(DimensionalityError):\n            a3b = a2 + a1\n        with pytest.raises(DimensionalityError):\n            a4 = a1 > a2\n        with pytest.raises(DimensionalityError):\n            a5 = a1 < a2\n        with pytest.raises(DimensionalityError):\n            a6 = a1 == a2\n        a7 = a1 * a2\n        a8 = a2 / a1\n        with pytest.raises(DimensionalityError):\n            a9 = a1",
            "def test_units_allpass(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.inch})\n        a3 = a1 + a2\n        assert a3[0, 0].data[()] == (1 * ureg.meter + 1 * ureg.inch).to(ureg.meter).magnitude\n\n        a3b = a2 + a1\n        assert a3b[0, 0].data[()] == (1 * ureg.meter + 1 * ureg.inch).to(ureg.inch).magnitude\n\n        a4 = a1 > a2\n        assert a4[0, 0].data[()] == True\n\n        a5 = a1 < a2\n        assert a5[0, 0].data[()] == False\n\n        a6 = a1 == a2\n        assert a6[0, 0].data[()] == False\n\n        a7 = a1 * a2\n        assert a7[0, 0].to(ureg.m**2).data[()] == (1 * ureg.meter * ureg.inch).to(ureg.meter**2).magnitude\n\n        a8 = a2 / a1\n        assert a8[0, 0].to_base_units().data[()] == (1 * ureg.inch / ureg.meter).to_base_units().magnitude\n\n        a9 = a1",
            "def test_units_somefail(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.kelvin})\n        with pytest.raises(DimensionalityError):\n            a3 = a1 + a2\n        with pytest.raises(DimensionalityError):\n            a3b = a2 + a1\n        with pytest.raises(DimensionalityError):\n            a4 = a1 > a2\n        with pytest.raises(DimensionalityError):\n            a5 = a1 < a2\n        with pytest.raises(DimensionalityError):\n            a6 = a1 == a2\n\n        a7 = a1 * a2\n        assert a7[0, 0].to(ureg.meter * ureg.kelvin).data[()] == (1 * ureg.meter * ureg.kelvin).magnitude\n\n        a8 = a1 / a2\n        assert a8[0, 0].to(ureg.meter / ureg.kelvin).data[()] == (1 * ureg.meter / ureg.kelvin).magnitude\n\n        with pytest.raises(DimensionalityError):\n            a9 = a1",
            "def test_to_image(self):\n        uda = UnitsDataArray(np.ones((10, 10)))\n        assert isinstance(uda.to_image(return_base64=True), bytes)\n        assert isinstance(uda.to_image(), io.BytesIO)",
            "def test_to_image_vmin_vmax(self):\n        uda = UnitsDataArray(np.ones((10, 10)))\n        assert isinstance(uda.to_image(vmin=0, vmax=2, return_base64=True), bytes)\n        assert isinstance(uda.to_image(vmin=0, vmax=2), io.BytesIO)",
            "def test_ufuncs(self):\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.kelvin})\n\n        np.sqrt(a1)\n        np.mean(a1)\n        np.min(a1)\n        np.max(a1)\n        a1**2\n\n        # These don't have units!\n        np.dot(a2.T, a1)\n        np.std(a1)\n        np.var(a1)",
            "def test_keep_attrs(self):\n        # This tests #265\n        # Create Nodes to use the convience methods for making units data arrays\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.meter, \"test\": \"test\"})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"units\": ureg.yard})\n\n        assert \"test\" in (a1 + a2).attrs\n        assert \"test\" in (a1 * a2).attrs\n\n        # No units\n        a1 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"], attrs={\"test\": \"test\"})\n        a2 = UnitsDataArray(np.ones((4, 3)), dims=[\"lat\", \"lon\"])\n\n        assert \"test\" in (a1 + 1).attrs\n        assert \"test\" in (a1 + a2).attrs\n        assert \"test\" in (a1 * 1).attrs\n        assert \"test\" in (a1 * a2).attrs\n\n        # Order is important\n        assert \"test\" not in (1 + a1).attrs\n        assert \"test\" not in (a2 + a1).attrs\n        assert \"test\" not in (1 * a1).attrs\n        assert \"test\" not in (a2 * a1).attrs",
            "class TestCreateDataArray(object):\n    @classmethod",
            "def setup_class(cls):\n        cls.coords = Coordinates([[0, 1, 2], [0, 1, 2, 3]], dims=[\"lat\", \"lon\"])",
            "def test_default(self):\n        a = UnitsDataArray.create(self.coords)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert np.all(np.isnan(a))",
            "def test_empty(self):\n        a = UnitsDataArray.create(self.coords, data=None)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == float\n\n        a = UnitsDataArray.create(self.coords, data=None, dtype=bool)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == bool",
            "def test_zeros(self):\n        a = UnitsDataArray.create(self.coords, data=0)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == float\n        assert np.all(a == 0.0)\n\n        a = UnitsDataArray.create(self.coords, data=0, dtype=bool)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == bool\n        assert np.all(~a)",
            "def test_ones(self):\n        a = UnitsDataArray.create(self.coords, data=1)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == float\n        assert np.all(a == 1.0)\n\n        a = UnitsDataArray.create(self.coords, data=1, dtype=bool)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == bool\n        assert np.all(a)",
            "def test_full(self):\n        a = UnitsDataArray.create(self.coords, data=10)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == float\n        assert np.all(a == 10)\n\n        a = UnitsDataArray.create(self.coords, data=10, dtype=int)\n        assert isinstance(a, UnitsDataArray)\n        assert a.shape == self.coords.shape\n        assert a.dtype == int\n        assert np.all(a == 10)",
            "def test_array(self):\n        data = np.random.random(self.coords.shape)\n        a = UnitsDataArray.create(self.coords, data=data)\n        assert isinstance(a, UnitsDataArray)\n        assert a.dtype == float\n        np.testing.assert_equal(a.data, data)\n\n        data = np.round(10 * np.random.random(self.coords.shape))\n        a = UnitsDataArray.create(self.coords, data=data, dtype=int)\n        assert isinstance(a, UnitsDataArray)\n        assert a.dtype == int\n        np.testing.assert_equal(a.data, data.astype(int))",
            "def test_outputs(self):\n        a = UnitsDataArray.create(self.coords, outputs=[\"a\", \"b\", \"c\"])\n        assert a.dims == self.coords.dims + (\"output\",)\n        np.testing.assert_array_equal(a[\"output\"], [\"a\", \"b\", \"c\"])\n\n        a = UnitsDataArray.create(self.coords, data=0, outputs=[\"a\", \"b\", \"c\"])\n        assert a.dims == self.coords.dims + (\"output\",)\n        np.testing.assert_array_equal(a[\"output\"], [\"a\", \"b\", \"c\"])\n\n        a = UnitsDataArray.create(self.coords, data=1, outputs=[\"a\", \"b\", \"c\"])\n        assert a.dims == self.coords.dims + (\"output\",)\n        np.testing.assert_array_equal(a[\"output\"], [\"a\", \"b\", \"c\"])\n\n        a = UnitsDataArray.create(self.coords, data=np.nan, outputs=[\"a\", \"b\", \"c\"])\n        assert a.dims == self.coords.dims + (\"output\",)\n        np.testing.assert_array_equal(a[\"output\"], [\"a\", \"b\", \"c\"])\n\n        data = np.random.random(self.coords.shape + (3,))\n        a = UnitsDataArray.create(self.coords, data=data, outputs=[\"a\", \"b\", \"c\"])\n        assert a.dims == self.coords.dims + (\"output\",)\n        np.testing.assert_array_equal(a[\"output\"], [\"a\", \"b\", \"c\"])\n\n        data = np.random.random(self.coords.shape + (2,))\n        with pytest.raises(ValueError, match=\"data with shape .* does not match\"):\n            a = UnitsDataArray.create(self.coords, data=data, outputs=[\"a\", \"b\", \"c\"])\n\n        data = np.random.random(self.coords.shape)\n        with pytest.raises(ValueError, match=\"data with shape .* does not match\"):\n            a = UnitsDataArray.create(self.coords, data=data, outputs=[\"a\", \"b\", \"c\"])",
            "def test_invalid_coords(self):\n        with pytest.raises(TypeError):\n            UnitsDataArray.create((3, 4))",
            "class TestOpenDataArray(object):",
            "def test_open_after_create(self):\n        coords = Coordinates([[0, 1, 2], [0, 1, 2, 3]], dims=[\"lat\", \"lon\"])\n        uda_1 = UnitsDataArray.create(coords, data=np.random.rand(3, 4))\n        ncdf = uda_1.to_netcdf()\n        uda_2 = UnitsDataArray.open(ncdf)\n\n        assert isinstance(uda_2, UnitsDataArray)\n        assert np.all(uda_2.data == uda_1.data)",
            "def test_open_after_create_with_attrs(self):\n        coords = Coordinates([[0, 1, 2], [0, 1, 2, 3]], dims=[\"lat\", \"lon\"], crs=\"EPSG:4193\")\n        uda_1 = UnitsDataArray.create(coords, data=np.random.rand(3, 4), attrs={\"some_attr\": 5})\n        ncdf = uda_1.to_netcdf()\n        uda_2 = UnitsDataArray.open(ncdf)\n\n        assert isinstance(uda_2, UnitsDataArray)\n        assert np.all(uda_2.data == uda_1.data)\n\n        assert \"some_attr\" in uda_2.attrs\n        assert uda_2.attrs.get(\"some_attr\") == uda_1.attrs.get(\"some_attr\")\n\n        assert \"crs\" in uda_2.attrs\n        assert uda_2.attrs.get(\"crs\") == uda_1.attrs.get(\"crs\")",
            "def test_open_after_eval(self):\n\n        # mock node\n        data = np.random.rand(5, 5)\n        lat = np.linspace(-10, 10, 5)\n        lon = np.linspace(-10, 10, 5)\n        native_coords = Coordinates([lat, lon], [\"lat\", \"lon\"])\n        node = Array(source=data, coordinates=native_coords)\n        uda = node.eval(node.coordinates)\n\n        ncdf = uda.to_netcdf()\n        uda_2 = UnitsDataArray.open(ncdf)\n\n        assert isinstance(uda_2, UnitsDataArray)\n        assert np.all(uda_2.data == uda.data)\n\n        assert \"layer_style\" in uda_2.attrs\n        assert uda_2.attrs.get(\"layer_style\").json == uda.attrs.get(\"layer_style\").json\n\n        assert \"crs\" in uda_2.attrs\n        assert uda_2.attrs.get(\"crs\") == uda.attrs.get(\"crs\")",
            "class TestToImage(object):",
            "def test_to_image(self):\n        data = np.ones((10, 10))\n        assert isinstance(to_image(UnitsDataArray(data), return_base64=True), bytes)  # UnitsDataArray input\n        assert isinstance(to_image(xr.DataArray(data), return_base64=True), bytes)  # xr.DataArray input\n        assert isinstance(to_image(data, return_base64=True), bytes)  # np.ndarray input\n        assert isinstance(to_image(np.array([data]), return_base64=True), bytes)  # squeeze",
            "def test_to_image_vmin_vmax(self):\n        data = np.ones((10, 10))\n        assert isinstance(to_image(data, vmin=0, vmax=2, return_base64=True), bytes)",
            "class TestToGeoTiff(object):",
            "def make_square_array(self, order=1, bands=1):\n        node = Array(\n            source=np.arange(8 * bands).reshape(3 - order, 3 + order, bands),\n            coordinates=Coordinates([clinspace(4, 0, 2, \"lat\"), clinspace(1, 4, 4, \"lon\")][::order], crs=\"EPSG:4326\"),\n            outputs=[str(s) for s in list(range(bands))],\n        )\n        return node",
            "def make_rot_array(self, order=1, bands=1):\n        if order == 1:\n            geotransform = (10.0, 1.879, -1.026, 20.0, 0.684, 2.819)\n        else:\n            # I think this requires changing the geotransform? Not yet supported\n            raise NotImplementedError(\"TODO\")\n\n        rc = AffineCoordinates(geotransform=geotransform, shape=(2, 4))\n        c = Coordinates([rc], crs=\"EPSG:4326\")\n        node = Array(\n            source=np.arange(8 * bands).reshape(3 - order, 3 + order, bands),\n            coordinates=c,\n            outputs=[str(s) for s in list(range(bands))],\n        )\n        return node",
            "def test_to_geotiff_roundtrip_1band(self):\n        # lat/lon order, usual\n        node = self.make_square_array()\n        out = node.eval(node.coordinates)\n        with tempfile.NamedTemporaryFile(\"wb\") as fp:\n            out.to_geotiff(fp)\n            fp.write(b\"a\")  # for some reason needed to get good comparison\n\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, outputs=node.outputs)\n            assert rnode.coordinates == node.coordinates\n\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(rout.data, out.data)\n\n        # lon/lat order, unusual\n        node = self.make_square_array(order=-1)\n        out = node.eval(node.coordinates)\n        with tempfile.NamedTemporaryFile(\"wb\") as fp:\n            out.to_geotiff(fp)\n            fp.write(b\"a\")  # for some reason needed to get good comparison\n\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, outputs=node.outputs)\n            assert rnode.coordinates == node.coordinates\n\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(rout.data, out.data)",
            "def test_to_geotiff_roundtrip_2band(self):\n        # lat/lon order, usual\n        node = self.make_square_array(bands=2)\n        out = node.eval(node.coordinates)\n        with tempfile.NamedTemporaryFile(\"wb\") as fp:\n            out.to_geotiff(fp)\n            fp.write(b\"a\")  # for some reason needed to get good comparison\n\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, outputs=node.outputs)\n            assert rnode.coordinates == node.coordinates\n\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(rout.data, out.data)\n\n        # lon/lat order, unsual\n        node = self.make_square_array(order=-1, bands=2)\n        out = node.eval(node.coordinates)\n        with tempfile.NamedTemporaryFile(\"wb\") as fp:\n            out.to_geotiff(fp)\n            fp.write(b\"a\")  # for some reason needed to get good comparison\n\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, outputs=node.outputs)\n            assert rnode.coordinates == node.coordinates\n\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(rout.data, out.data)\n\n            # Check single output\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, outputs=node.outputs, output=node.outputs[1])\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(out.data[..., 1], rout.data)\n\n            # Check single band 1\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, band=1)\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(out.data[..., 0], rout.data)\n\n            # Check single band 2\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, band=2)\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(out.data[..., 1], rout.data)",
            "def test_to_geotiff_roundtrip_rotcoords(self):\n        # lat/lon order, usual\n        node = self.make_rot_array()\n\n        out = node.eval(node.coordinates)\n\n        with tempfile.NamedTemporaryFile(\"wb\") as fp:\n            out.to_geotiff(fp)\n            fp.write(b\"a\")  # for some reason needed to get good comparison\n\n            fp.seek(0)\n            rnode = Rasterio(source=fp.name, outputs=node.outputs, mode=\"r\")\n            assert node.coordinates == rnode.coordinates\n\n            rout = rnode.eval(rnode.coordinates)\n            np.testing.assert_almost_equal(out.data, rout.data)\n\n        # # lon/lat order, unsual\n        # node = self.make_square_array(order=-1)\n        # out = node.eval(node.coordinates)\n        # with tempfile.NamedTemporaryFile(\"wb\") as fp:\n        #     out.to_geotiff(fp)\n        #     fp.write(b\"a\")  # for some reason needed to get good comparison\n\n        #     fp.seek(0)\n        #     rnode = Rasterio(source=fp.name, outputs=node.outputs)\n        #     assert node.coordinates == rnode.coordinates\n\n        #     rout = rnode.eval(rnode.coordinates)\n        #     np.testing.assert_almost_equal(out.data, rout.data)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_settings.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestSettingsFile(object):",
            "def tmp_dir_path(self):\n        basedir = os.path.dirname(os.path.realpath(__file__))\n        path = os.path.join(basedir, \".__tmp__\")\n        return path",
            "def make_settings_tmp_dir(self):\n        path = self.tmp_dir_path()\n        os.mkdir(path)  # intentionally fails if this folder already exists as it will be deleted\n        return path",
            "def teardown_method(self):\n        path = self.tmp_dir_path()\n        try:\n            os.remove(os.path.join(path, \"settings.json\"))\n        except OSError:  # FileNotFoundError in py 3\n            pass\n\n        os.rmdir(path)  # intentionally fails if anything else is in this folder",
            "def test_settings_file_defaults_to_home_dir(self):\n        self.make_settings_tmp_dir()  # so teardown method has something ot tear down\n        settings = PodpacSettings()\n        path = os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~\"))\n        assert settings.settings_path == os.path.join(path, \".config\", \"podpac\", \"settings.json\")",
            "def test_single_saved_setting_persists(self):\n        path = self.make_settings_tmp_dir()\n\n        key = \"key\"\n        value = \"value\"\n        settings = PodpacSettings()\n        settings.load(path=path)\n        settings[\"AUTOSAVE_SETTINGS\"] = True\n        settings[key] = value\n\n        new_settings = PodpacSettings()\n        new_settings.load(path=path)\n        assert new_settings[key] == value",
            "def test_multiple_saved_settings_persist(self):\n        path = self.make_settings_tmp_dir()\n\n        key1 = \"key1\"\n        value1 = \"value1\"\n        settings = PodpacSettings()\n        settings.load(path=path)\n        settings[\"AUTOSAVE_SETTINGS\"] = True\n        settings[key1] = value1\n\n        key2 = \"key2\"\n        value2 = \"value2\"\n        settings[key2] = value2\n\n        new_settings = PodpacSettings()\n        new_settings.load(path=path)\n        assert new_settings[key1] == value1\n        assert new_settings[key2] == value2",
            "def test_misconfigured_settings_file_fall_back_on_default(self):\n        path = self.make_settings_tmp_dir()\n\n        with open(os.path.join(path, \"settings.json\"), \"w\") as f:\n            f.write(\"not proper json\")\n\n        settings = PodpacSettings()\n        settings.load(path=path)\n        assert isinstance(settings, dict)\n\n        path = os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~\"))\n        assert settings.settings_path == os.path.join(path, \".config\", \"podpac\", \"settings.json\")"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_authentication.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestAuthentication(object):",
            "def test_set_credentials(self):\n\n        with settings:\n            if \"username@test.com\" in settings:\n                del settings[\"username@test.com\"]\n\n            if \"password@test.com\" in settings:\n                del settings[\"password@test.com\"]\n\n            # require hostname\n            with pytest.raises(TypeError):\n                set_credentials()\n\n            with pytest.raises(ValueError):\n                set_credentials(None, uname=\"test\", password=\"test\")\n\n            with pytest.raises(ValueError):\n                set_credentials(\"\", uname=\"test\", password=\"test\")\n\n            # make sure these are empty at first\n            assert not settings[\"username@test.com\"]\n            assert not settings[\"password@test.com\"]\n\n            # test input/getpass\n            # TODO: how do you test this?\n\n            # set both username and pw\n            set_credentials(hostname=\"test.com\", uname=\"testuser\", password=\"testpass\")\n            assert settings[\"username@test.com\"] == \"testuser\"\n            assert settings[\"password@test.com\"] == \"testpass\"\n\n            # set username only\n            set_credentials(hostname=\"test.com\", uname=\"testuser2\")\n            assert settings[\"username@test.com\"] == \"testuser2\"\n            assert settings[\"password@test.com\"] == \"testpass\"\n\n            # set pw only\n            set_credentials(hostname=\"test.com\", password=\"testpass3\")\n            assert settings[\"username@test.com\"] == \"testuser2\"\n            assert settings[\"password@test.com\"] == \"testpass3\"\n\n            # don't do anything if neither is provided, but the settings exist\n            set_credentials(hostname=\"test.com\")\n            assert settings[\"username@test.com\"] == \"testuser2\"\n            assert settings[\"password@test.com\"] == \"testpass3\"\n\n\n# dummy class mixing in RequestsSession with hostname",
            "class SomeNodeWithHostname(RequestsSessionMixin):\n    hostname = \"myurl.org\"",
            "class SomeNode(RequestsSessionMixin):\n    pass",
            "class TestRequestsSessionMixin(object):",
            "def test_hostname(self):\n        node = SomeNode(hostname=\"someurl.org\")\n        assert node.hostname == \"someurl.org\"\n\n        # use class that implements\n        node = SomeNodeWithHostname()\n        assert node.hostname == \"myurl.org\"",
            "def test_property_value_errors(self):\n        node = SomeNode(hostname=\"propertyerrors.com\")\n\n        with pytest.raises(ValueError, match=\"set_credentials\"):\n            u = node.username\n\n        with pytest.raises(ValueError, match=\"set_credentials\"):\n            p = node.password",
            "def test_set_credentials(self):\n        with settings:\n            node = SomeNode(hostname=\"setcredentials.com\")\n            node.set_credentials(username=\"testuser\", password=\"testpass\")\n            assert settings[\"username@setcredentials.com\"] == \"testuser\"\n            assert settings[\"password@setcredentials.com\"] == \"testpass\"",
            "def test_property_values(self):\n        with settings:\n            node = SomeNode(hostname=\"propertyvalues.com\")\n            node.set_credentials(username=\"testuser2\", password=\"testpass2\")\n\n            assert_equal(node.username, \"testuser2\")\n            assert_equal(node.password, \"testpass2\")",
            "def test_session(self):\n        with settings:\n            node = SomeNode(hostname=\"session.net\")\n            node.set_credentials(username=\"testuser\", password=\"testpass\")\n\n            assert node.session\n            assert node.session.auth == (\"testuser\", \"testpass\")\n            assert isinstance(node.session, requests.Session)",
            "def test_auth_required(self):\n        with settings:\n            with pytest.raises(tl.TraitError):\n                node = SomeNode(hostname=\"auth.com\", auth_required=\"true\")\n\n            # no auth\n            node = SomeNode(hostname=\"auth.com\")\n            assert node.session\n            assert isinstance(node.session, requests.Session)\n            with pytest.raises(AttributeError):\n                node.auth\n\n            # auth required\n            if \"username@auth2.com\" in settings:\n                del settings[\"username@auth2.com\"]\n\n            if \"password@auth2.com\" in settings:\n                del settings[\"password@auth2.com\"]\n\n            node = SomeNode(hostname=\"auth2.com\", auth_required=True)\n            with pytest.raises(ValueError):\n                s = node.session\n                print(s)\n\n            node.set_credentials(username=\"testuser\", password=\"testpass\")\n            assert node.session\n            assert isinstance(node.session, requests.Session)",
            "class TestS3Mixin(object):",
            "class S3Node(S3Mixin, Node):\n        pass",
            "def test_anon(self):\n        node = self.S3Node(anon=True)\n        assert isinstance(node.s3, s3fs.S3FileSystem)\n\n    @pytest.mark.aws",
            "def test_auth(self):\n        node = self.S3Node()\n        assert isinstance(node.s3, s3fs.S3FileSystem)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_utils.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestCommonDocs(object):",
            "def test_common_docs_does_not_affect_anonymous_functions(self):\n        f = lambda x: x\n        f2 = common_doc({\"key\": \"value\"})(f)\n        assert f(42) == f2(42)\n        assert f.__doc__ is None",
            "class TestTraitletsHelpers(object):",
            "def test_trait_is_defined(self):",
            "class MyClass(tl.HasTraits):\n            a = tl.Any()\n            b = tl.Any(default_value=0)\n            c = tl.Any()\n\n            @tl.default(\"c\")",
            "def _default_b(self):\n                return \"test\"\n\n        x = MyClass(a=1, b=1, c=1)\n        assert trait_is_defined(x, \"a\")\n        assert trait_is_defined(x, \"b\")\n        assert trait_is_defined(x, \"c\")\n        assert not trait_is_defined(x, \"other\")\n\n        x = MyClass()\n        if tl.version_info[0] >= 5:\n            assert not trait_is_defined(x, \"a\")\n            assert not trait_is_defined(x, \"b\")\n            assert not trait_is_defined(x, \"c\")\n        else:\n            assert trait_is_defined(x, \"a\")\n            assert trait_is_defined(x, \"b\")\n            assert not trait_is_defined(x, \"c\")\n\n        x.c\n        assert trait_is_defined(x, \"c\")",
            "class TestLoggingHelpers(object):",
            "def test_create_logfile(self):\n        create_logfile()",
            "class TestOrderedDictTrait(object):",
            "def test(self):",
            "class MyClass(tl.HasTraits):\n            d = OrderedDictTrait()\n\n        m = MyClass(d=OrderedDict([(\"a\", 1)]))\n\n        with pytest.raises(tl.TraitError):\n            MyClass(d=[])\n\n    @pytest.mark.skipif(sys.version < \"3.6\", reason=\"python < 3.6\")",
            "def test_dict_python36(self):",
            "class MyClass(tl.HasTraits):\n            d = OrderedDictTrait()\n\n        m = MyClass(d={\"a\": 1})\n\n    @pytest.mark.skipif(sys.version >= \"3.6\", reason=\"python >= 3.6\")",
            "def test_dict_python2(self):",
            "class MyClass(tl.HasTraits):\n            d = OrderedDictTrait()\n\n        with pytest.raises(tl.TraitError):\n            m = MyClass(d={\"a\": 1})\n\n        # empty is okay, will be converted\n        m = MyClass(d={})",
            "class TestArrayTrait(object):",
            "def test(self):",
            "class MyClass(tl.HasTraits):\n            a = ArrayTrait()\n\n        # basic usage\n        o = MyClass(a=np.array([0, 4]))\n        assert isinstance(o.a, np.ndarray)\n        np.testing.assert_equal(o.a, [0, 4])\n\n        # coerce\n        o = MyClass(a=[0, 4])\n        assert isinstance(o.a, np.ndarray)\n        np.testing.assert_equal(o.a, [0, 4])",
            "def test_ndim(self):",
            "class MyClass(tl.HasTraits):\n            a = ArrayTrait(ndim=2)\n\n        MyClass(a=np.array([[0, 4]]))\n        MyClass(a=[[0, 4]])\n\n        # invalid\n        with pytest.raises(tl.TraitError):\n            MyClass(a=[4, 5])",
            "def test_shape(self):",
            "class MyClass(tl.HasTraits):\n            a = ArrayTrait(shape=(2, 2))\n\n        MyClass(a=np.array([[0, 1], [2, 3]]))\n        MyClass(a=[[0, 1], [2, 3]])\n\n        # invalid\n        with pytest.raises(tl.TraitError):\n            MyClass(a=np.array([0, 1, 2, 3]))",
            "def test_dtype(self):",
            "class MyClass(tl.HasTraits):\n            a = ArrayTrait(dtype=float)\n\n        m = MyClass(a=np.array([0.0, 1.0]))\n        assert m.a.dtype == float\n\n        m = MyClass(a=[0.0, 1.0])\n        assert m.a.dtype == float\n\n        # astype\n        m = MyClass(a=[0, 1])\n        assert m.a.dtype == float\n\n        # invalid\n        with pytest.raises(tl.TraitError):\n            MyClass(a=np.array([\"a\", \"b\"]))",
            "def test_args(self):\n        # shape and ndim must match\n        t = ArrayTrait(ndim=2, shape=(2, 2))\n\n        with pytest.raises(ValueError):\n            ArrayTrait(ndim=1, shape=(2, 2))\n\n        # dtype lookup\n        t = ArrayTrait(dtype=\"datetime64\")\n        assert t.dtype == np.datetime64\n\n        # invalid dtype\n        with pytest.raises(ValueError):\n            ArrayTrait(dtype=\"notatype\")",
            "class TestNodeTrait(object):",
            "def test(self):",
            "class MyClass(tl.HasTraits):\n            node = NodeTrait()\n\n        t = MyClass(node=podpac.Node())\n\n        with pytest.raises(tl.TraitError):\n            MyClass(node=0)",
            "def test_debug(self):",
            "class MyClass(tl.HasTraits):\n            node = NodeTrait()\n\n        node = podpac.Node()\n\n        with podpac.settings:\n            podpac.settings[\"DEBUG\"] = False\n            t = MyClass(node=node)\n            assert t.node is node\n\n            podpac.settings[\"DEBUG\"] = True\n            t = MyClass(node=node)\n            assert t.node is not node",
            "class TestTupleTrait(object):",
            "def test_trait(self):",
            "class MyClass(tl.HasTraits):\n            t = TupleTrait(trait=tl.Int())\n\n        MyClass(t=(1, 2, 3))\n\n        with pytest.raises(tl.TraitError):\n            MyClass(t=(\"a\", \"b\", \"c\"))",
            "def test_tuple(self):",
            "class MyClass(tl.HasTraits):\n            t = TupleTrait(trait=tl.Int())\n\n        a = MyClass(t=(1, 2, 3))\n        assert isinstance(a.t, tuple)\n\n        a = MyClass(t=[1, 2, 3])\n        assert isinstance(a.t, tuple)",
            "class TestJSONEncoder(object):",
            "def test_coordinates(self):\n        coordinates = podpac.coordinates.Coordinates([0], dims=[\"time\"])\n        json.dumps(coordinates, cls=JSONEncoder)",
            "def test_node(self):\n        node = podpac.Node()\n        json.dumps(node, cls=JSONEncoder)",
            "def test_style(self):\n        style = podpac.core.style.Style()\n        json.dumps(style, cls=JSONEncoder)",
            "def test_interpolation(self):\n        interpolation = podpac.core.interpolation.interpolation.Interpolate()\n        json.dumps(interpolation, cls=JSONEncoder)",
            "def test_interpolator(self):\n        kls = podpac.core.interpolation.INTERPOLATORS[0]\n        json.dumps(kls, cls=JSONEncoder)",
            "def test_units(self):\n        units = podpac.core.units.ureg.Unit(\"meters\")\n        json.dumps(units, cls=JSONEncoder)",
            "def test_datetime64(self):\n        dt = np.datetime64()\n        json.dumps(dt, cls=JSONEncoder)",
            "def test_timedelta64(self):\n        td = np.timedelta64()\n        json.dumps(td, cls=JSONEncoder)",
            "def test_datetime(self):\n        now = datetime.datetime.now()\n        json.dumps(now, cls=JSONEncoder)",
            "def test_date(self):\n        today = datetime.date.today()\n        json.dumps(today, cls=JSONEncoder)",
            "def test_dataframe(self):\n        df = pd.DataFrame()\n        json.dumps(df, cls=JSONEncoder)",
            "def test_array_datetime64(self):\n        a = np.array([\"2018-01-01\", \"2018-01-02\"]).astype(np.datetime64)\n        json.dumps(a, cls=JSONEncoder)",
            "def test_array_timedelta64(self):\n        a = np.array([np.timedelta64(1, \"D\"), np.timedelta64(1, \"D\")])\n        json.dumps(a, cls=JSONEncoder)",
            "def test_array_numerical(self):\n        a = np.array([0.0, 1.0, 2.0])\n        json.dumps(a, cls=JSONEncoder)",
            "def test_array_node(self):\n        a = np.array([podpac.Node(), podpac.Node()])\n        json.dumps(a, cls=JSONEncoder)",
            "def test_array_unserializable(self):",
            "class MyClass(object):\n            pass\n\n        a = np.array([MyClass()])\n        with pytest.raises(TypeError, match=\"Cannot serialize numpy array\"):\n            json.dumps(a, cls=JSONEncoder)",
            "def test_unserializable(self):\n        value = xr.DataArray([])\n        with pytest.raises(TypeError, match=\"not JSON serializable\"):\n            json.dumps(value, cls=JSONEncoder)",
            "def test_is_json_serializable(self):\n        assert is_json_serializable(\"test\")\n        assert not is_json_serializable(xr.DataArray([]))",
            "class TestCachedPropertyDecorator(object):",
            "def test_cached_property(self):",
            "class MyNode(podpac.Node):\n            my_property_called = 0\n            my_cached_property_called = 0\n            my_cache_ctrl_property_called = 0\n\n            @property",
            "def my_property(self):\n                self.my_property_called += 1\n                return 10\n\n            @cached_property",
            "def my_cached_property(self):\n                self.my_cached_property_called += 1\n                return 20\n\n            @cached_property(use_cache_ctrl=True)",
            "def my_cache_ctrl_property(self):\n                self.my_cache_ctrl_property_called += 1\n                return 30\n\n        a = MyNode(cache_ctrl=[\"ram\"])\n        b = MyNode(cache_ctrl=[\"ram\"])\n        c = MyNode(cache_ctrl=[])\n\n        a.rem_cache(key=\"*\")\n        b.rem_cache(key=\"*\")\n        c.rem_cache(key=\"*\")\n\n        # normal property should be called every time\n        assert a.my_property_called == 0\n        assert a.my_property == 10\n        assert a.my_property_called == 1\n        assert a.my_property == 10\n        assert a.my_property == 10\n        assert a.my_property_called == 3\n\n        assert b.my_property_called == 0\n        assert b.my_property == 10\n        assert b.my_property_called == 1\n        assert b.my_property == 10\n        assert b.my_property == 10\n        assert b.my_property_called == 3\n\n        assert c.my_property_called == 0\n        assert c.my_property == 10\n        assert c.my_property_called == 1\n        assert c.my_property == 10\n        assert c.my_property == 10\n        assert c.my_property_called == 3\n\n        # cached property should only be called when it is accessed\n        assert a.my_cached_property_called == 0\n        assert a.my_cached_property == 20\n        assert a.my_cached_property_called == 1\n        assert a.my_cached_property == 20\n        assert a.my_cached_property == 20\n        assert a.my_cached_property_called == 1\n\n        assert b.my_cached_property_called == 0\n        assert b.my_cached_property == 20\n        assert b.my_cached_property_called == 1\n        assert b.my_cached_property == 20\n        assert b.my_cached_property == 20\n        assert b.my_cached_property_called == 1\n\n        assert c.my_cached_property_called == 0\n        assert c.my_cached_property == 20\n        assert c.my_cached_property_called == 1\n        assert c.my_cached_property == 20\n        assert c.my_cached_property == 20\n        assert c.my_cached_property_called == 1\n\n        # cache_ctrl cached property should only be called in the first node that accessses it\n        assert a.my_cache_ctrl_property_called == 0\n        assert a.my_cache_ctrl_property == 30\n        assert a.my_cache_ctrl_property_called == 1\n        assert a.my_cache_ctrl_property == 30\n        assert a.my_cache_ctrl_property == 30\n        assert a.my_cache_ctrl_property_called == 1\n\n        assert b.my_cache_ctrl_property_called == 0\n        assert b.my_cache_ctrl_property == 30\n        assert b.my_cache_ctrl_property_called == 0\n        assert b.my_cache_ctrl_property == 30\n        assert b.my_cache_ctrl_property == 30\n        assert b.my_cache_ctrl_property_called == 0\n\n        # but only if a cache_ctrl exists for the Node\n        assert c.my_cache_ctrl_property_called == 0\n        assert c.my_cache_ctrl_property == 30\n        assert c.my_cache_ctrl_property_called == 1\n        assert c.my_cache_ctrl_property == 30\n        assert c.my_cache_ctrl_property == 30\n        assert c.my_cache_ctrl_property_called == 1",
            "def test_cached_property_expires(self):",
            "class MyNode(podpac.Node):\n            expires_tomorrow_called = 0\n            expired_yesterday_called = 0\n\n            @cached_property(use_cache_ctrl=True, expires=\"1,D\")",
            "def expires_tomorrow(self):\n                self.expires_tomorrow_called += 1\n                return 10\n\n            @cached_property(use_cache_ctrl=True, expires=\"-1,D\")",
            "def expired_yesterday(self):\n                self.expired_yesterday_called += 1\n                return 20\n\n        a = MyNode(cache_ctrl=[\"ram\"])\n        b = MyNode(cache_ctrl=[\"ram\"])\n\n        # not expired, b uses cached version\n        assert a.expires_tomorrow_called == 0\n        assert b.expires_tomorrow_called == 0\n\n        assert a.expires_tomorrow == 10\n        assert a.expires_tomorrow == 10\n        assert b.expires_tomorrow == 10\n        assert b.expires_tomorrow == 10\n\n        assert a.expires_tomorrow_called == 1\n        assert b.expires_tomorrow_called == 0  # cache was used!\n\n        # expired, b can't use cached version\n        assert a.expired_yesterday_called == 0\n        assert b.expired_yesterday_called == 0\n\n        assert a.expired_yesterday == 20\n        assert a.expired_yesterday == 20\n        assert b.expired_yesterday == 20\n        assert b.expired_yesterday == 20\n\n        assert a.expired_yesterday_called == 1  # note the expiration only applies to fetching from the cache\n        assert b.expired_yesterday_called == 1  # cache was not used!",
            "def test_invalid_argument(self):\n        with pytest.raises(TypeError, match=\"cached_property decorator does not accept keyword argument\"):\n            cached_property(other=True)\n\n        with pytest.raises(TypeError, match=\"cached_property decorator does not accept any positional arguments\"):\n            cached_property(True)",
            "class TestInd2Slice(object):",
            "def test_slice(self):\n        assert ind2slice((slice(1, 4),)) == (slice(1, 4),)",
            "def test_integer(self):\n        assert ind2slice((1,)) == (1,)",
            "def test_integer_array(self):\n        assert ind2slice(([1, 2, 4],)) == (slice(1, 5),)",
            "def test_boolean_array(self):\n        assert ind2slice(([False, True, True, False, True, False],)) == (slice(1, 5),)",
            "def test_stepped(self):\n        assert ind2slice(([1, 3, 5],)) == (slice(1, 7, 2),)\n        assert ind2slice(([False, True, False, True, False, True],)) == (slice(1, 7, 2),)",
            "def test_multiindex(self):\n        I = (slice(1, 4), 1, [1, 2, 4], [False, True, False], [1, 3, 5])\n        assert ind2slice(I) == (slice(1, 4), 1, slice(1, 5), 1, slice(1, 7, 2))",
            "def test_nontuple(self):\n        assert ind2slice(slice(1, 4)) == slice(1, 4)\n        assert ind2slice(1) == 1\n        assert ind2slice([1, 2, 4]) == slice(1, 5)\n        assert ind2slice([False, True, True, False, True, False]) == slice(1, 5)\n        assert ind2slice([1, 3, 5]) == slice(1, 7, 2)",
            "class AnotherOne(podpac.algorithm.Algorithm):",
            "def algorithm(self, inputs, coordinates):\n        return self.create_output_array(coordinates, data=1)",
            "class TestNodeProber(object):\n    coords = podpac.Coordinates([podpac.clinspace(0, 2, 3, \"lat\"), podpac.clinspace(0, 2, 3, \"lon\")])\n    one = podpac.data.Array(\n        source=np.ones((3, 3)), coordinates=coords, style=podpac.style.Style(name=\"one_style\", units=\"o\")\n    )\n    two = podpac.data.Array(\n        source=np.ones((3, 3)) * 2, coordinates=coords, style=podpac.style.Style(name=\"two_style\", units=\"t\")\n    )\n    arange = podpac.algorithm.Arange()\n    nan = podpac.data.Array(source=np.ones((3, 3)) * np.nan, coordinates=coords)\n    another_one = AnotherOne()",
            "def test_single_prober(self):\n        expected = {\n            \"Array\": {\n                \"active\": True,\n                \"value\": 1,\n                \"units\": \"o\",\n                \"inputs\": [],\n                \"name\": \"one_style\",\n                \"node_hash\": self.one.hash,\n            }\n        }\n        out = probe_node(self.one, lat=1, lon=1)\n        assert out == expected",
            "def test_serial_prober(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n            a = podpac.algorithm.Arithmetic(one=self.one, eqn=\"one * 2\")\n            b = podpac.algorithm.Arithmetic(a=a, eqn=\"a*3\", style=podpac.style.Style(name=\"six_style\", units=\"m\"))\n            expected = {\n                \"Array\": {\n                    \"active\": True,\n                    \"value\": 1.0,\n                    \"units\": \"o\",\n                    \"inputs\": [],\n                    \"name\": \"one_style\",\n                    \"node_hash\": self.one.hash,\n                },\n                \"Arithmetic\": {\n                    \"active\": True,\n                    \"value\": 2.0,\n                    \"units\": \"\",\n                    \"inputs\": [\"Array\"],\n                    \"name\": \"Arithmetic\",\n                    \"node_hash\": a.hash,\n                },\n                \"Arithmetic_1\": {\n                    \"active\": True,\n                    \"value\": 6.0,\n                    \"units\": \"m\",\n                    \"inputs\": [\"Arithmetic\"],\n                    \"name\": \"six_style\",\n                    \"node_hash\": b.hash,\n                },\n            }\n            out = probe_node(b, lat=1, lon=1)\n            assert out == expected",
            "def test_parallel_prober(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n            a = podpac.algorithm.Arithmetic(one=self.one, two=self.two, eqn=\"one * two\")\n            expected = {\n                \"Array\": {\n                    \"active\": True,\n                    \"value\": 1.0,\n                    \"units\": \"o\",\n                    \"inputs\": [],\n                    \"name\": \"one_style\",\n                    \"node_hash\": self.one.hash,\n                },\n                \"Array_1\": {\n                    \"active\": True,\n                    \"value\": 2.0,\n                    \"units\": \"t\",\n                    \"inputs\": [],\n                    \"name\": \"two_style\",\n                    \"node_hash\": self.two.hash,\n                },\n                \"Arithmetic\": {\n                    \"active\": True,\n                    \"value\": 2.0,\n                    \"units\": \"\",\n                    \"inputs\": [\"Array\", \"Array_1\"],\n                    \"name\": \"Arithmetic\",\n                    \"node_hash\": a.hash,\n                },\n            }\n            out = probe_node(a, lat=1, lon=1)\n            assert out == expected",
            "def test_composited_prober(self):\n        a = podpac.compositor.OrderedCompositor(sources=[self.one, self.arange])\n        expected = {\n            \"Array\": {\n                \"active\": True,\n                \"value\": 1.0,\n                \"units\": \"o\",\n                \"inputs\": [],\n                \"name\": \"one_style\",\n                \"node_hash\": self.one.hash,\n            },\n            \"Arange\": {\n                \"active\": False,\n                \"value\": 0.0,\n                \"units\": \"\",\n                \"inputs\": [],\n                \"name\": \"Arange\",\n                \"node_hash\": self.arange.hash,\n            },\n            \"OrderedCompositor\": {\n                \"active\": True,\n                \"value\": 1.0,\n                \"units\": \"\",\n                \"inputs\": [\"Array\", \"Arange\"],\n                \"name\": \"OrderedCompositor\",\n                \"node_hash\": a.hash,\n            },\n        }\n        out = probe_node(a, lat=1, lon=1)\n        assert out == expected\n\n        a = podpac.compositor.OrderedCompositor(sources=[self.nan, self.two])\n        expected = {\n            \"Array\": {\n                \"active\": False,\n                \"value\": \"nan\",\n                \"units\": \"\",\n                \"inputs\": [],\n                \"name\": \"Array\",\n                \"node_hash\": self.nan.hash,\n            },\n            \"Array_1\": {\n                \"active\": True,\n                \"value\": 2.0,\n                \"units\": \"t\",\n                \"inputs\": [],\n                \"name\": \"two_style\",\n                \"node_hash\": self.two.hash,\n            },\n            \"OrderedCompositor\": {\n                \"active\": True,\n                \"value\": 2.0,\n                \"units\": \"\",\n                \"inputs\": [\"Array\", \"Array_1\"],\n                \"name\": \"OrderedCompositor\",\n                \"node_hash\": a.hash,\n            },\n        }\n        out = probe_node(a, lat=1, lon=1)\n        for k in out:\n            if np.isnan(out[k][\"value\"]):\n                out[k][\"value\"] = \"nan\"\n        assert out == expected\n\n        a = podpac.compositor.OrderedCompositor(sources=[self.nan, self.one, self.another_one])\n        expected = {\n            \"Array\": {\n                \"active\": False,\n                \"value\": \"nan\",\n                \"units\": \"\",\n                \"inputs\": [],\n                \"name\": \"Array\",\n                \"node_hash\": self.nan.hash,\n            },\n            \"Array_1\": {\n                \"active\": True,\n                \"value\": 1.0,\n                \"units\": \"o\",\n                \"inputs\": [],\n                \"name\": \"one_style\",\n                \"node_hash\": self.one.hash,\n            },\n            \"AnotherOne\": {\n                \"active\": False,\n                \"value\": 1.0,\n                \"units\": \"\",\n                \"inputs\": [],\n                \"name\": \"AnotherOne\",\n                \"node_hash\": self.another_one.hash,\n            },\n            \"OrderedCompositor\": {\n                \"active\": True,\n                \"value\": 1.0,\n                \"units\": \"\",\n                \"inputs\": [\"Array\", \"Array_1\", \"AnotherOne\"],\n                \"name\": \"OrderedCompositor\",\n                \"node_hash\": a.hash,\n            },\n        }\n        out = probe_node(a, lat=1, lon=1)\n        for k in out:\n            if np.isnan(out[k][\"value\"]):\n                out[k][\"value\"] = \"nan\"\n        assert out == expected",
            "def test_composited_prober_nested(self):\n        a = podpac.compositor.OrderedCompositor(\n            sources=[self.one, self.arange], style=podpac.style.Style(name=\"composited\", units=\"c\")\n        )\n        expected = {\n            \"name\": \"composited\",\n            \"value\": \"1.0 c\",\n            \"active\": True,\n            \"node_id\": a.hash,\n            \"params\": {},\n            \"inputs\": {\n                \"inputs\": [\n                    {\n                        \"name\": \"one_style\",\n                        \"value\": \"1.0 o\",\n                        \"active\": True,\n                        \"node_id\": self.one.hash,\n                        \"params\": {},\n                        \"inputs\": {},\n                    },\n                    {\n                        \"name\": \"Arange\",\n                        \"value\": \"0.0\",\n                        \"active\": False,\n                        \"node_id\": self.arange.hash,\n                        \"params\": {},\n                        \"inputs\": {},\n                    },\n                ]\n            },\n        }\n        out = probe_node(a, lat=1, lon=1, nested=True)\n        assert out == expected"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/test/test_common_test_utils.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestMakeCoordinates(object):",
            "def test_default_creation(self):\n        # Just make sure it runs\n        coords = ctu.make_coordinate_combinations()\n        assert len(coords) > 0\n        assert len(coords) == 168",
            "def test_custom_creation_no_stack(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([2, 3, 4, 5, 6], name=\"lon\")\n        alt = ArrayCoordinates1d([6, 7, 8, 9, 10, 11, 12], name=\"alt\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-02-01\"], name=\"time\")\n        coords = ctu.make_coordinate_combinations(lat=lat, lon=lon, alt=alt, time=time)\n        assert len(coords) > 0\n        assert len(coords) == 48",
            "def test_custom_creation_latlon_stack(self):\n        alt = ArrayCoordinates1d([6, 7, 8, 9, 10, 11, 12], name=\"alt\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-02-01\"], name=\"time\")\n        coords = ctu.make_coordinate_combinations(alt=alt, time=time)\n        assert len(coords) > 0\n        assert len(coords) == 70",
            "def test_custom_creation_mixed_type_1d(self):\n        lat = ArrayCoordinates1d([0.0, 1.0, 2.0, 4.0], name=\"lat\")\n        coords = ctu.make_coordinate_combinations(lat=lat)\n        assert len(coords) > 0\n        assert len(coords) == 84"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/datasource.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nGeneric Data Source Class\n\nDataSource is the root class for all other podpac defined data sources,\nincluding user defined data sources.\n\"\"\"",
            "\"\"\"\n        This method must be defined by the data source implementing the DataSource class.\n        When data source nodes are evaluated, this method is called with request coordinates and coordinate indexes.\n        The implementing method can choose which input provides the most efficient method of getting data\n        (i.e via coordinates or via the index of the coordinates).\n\n        Coordinates and coordinate indexes may be strided or subsets of the\n        source data, but all coordinates and coordinate indexes will match 1:1 with the subset data.\n\n        This method may return a numpy array, an xarray DaraArray, or a podpac UnitsDataArray.\n        If a numpy array or xarray DataArray is returned, :meth:`podpac.data.DataSource.evaluate` will\n        cast the data into a `UnitsDataArray` using the requested source coordinates.\n        If a podpac UnitsDataArray is passed back, the :meth:`podpac.data.DataSource.evaluate`\n        method will not do any further processing.\n        The inherited Node method `create_output_array` can be used to generate the template UnitsDataArray\n        in your DataSource.\n        See :meth:`podpac.Node.create_output_array` for more details.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            The coordinates that need to be retrieved from the data source using the coordinate system of the data\n            source\n        coordinates_index : List\n            A list of slices or a boolean array that give the indices of the data that needs to be retrieved from\n            the data source. The values in the coordinate_index will vary depending on the `coordinate_index_type`\n            defined for the data source.\n\n        Returns\n        --------\n        np.ndarray, xr.DataArray, :class:`podpac.UnitsDataArray`\n            A subset of the returned data. If a numpy array or xarray DataArray is returned,\n            the data will be cast into  UnitsDataArray using the returned data to fill values\n            at the requested source coordinates.\n        \"\"\"",
            "\"\"\"\n        Returns a Coordinates object that describes the coordinates of the data source.\n\n        In most cases, this method is defined by the data source implementing the DataSource class.\n        If method is not implemented by the data source, it will try to return ``self.coordinates``\n        if ``self.coordinates`` is not None.\n\n        Otherwise, this method will raise a NotImplementedError.\n\n        Returns\n        --------\n        :class:`podpac.Coordinates`\n           The coordinates describing the data source array.\n\n        Notes\n        ------\n        Need to pay attention to:\n        - the order of the dimensions\n        - the stacking of the dimension\n        - the type of coordinates\n\n        Coordinates should be non-nan and non-repeating for best compatibility\n        \"\"\"",
            "\"\"\"\n        Interpolation definition for the data source.\n        By default, the interpolation method is set to `podpac.settings[\"DEFAULT_INTERPOLATION\"]` which defaults to 'nearest'` for all dimensions.\n        \"\"\"",
            "\"\"\"\n        {interpolation}\n\n        If input is a string, it must match one of the interpolation shortcuts defined in\n        :attr:`podpac.data.INTERPOLATION_SHORTCUTS`. The interpolation method associated\n        with this string will be applied to all dimensions at the same time.\n\n        If input is a dict or list of dict, the dict or dict elements must adhere to the following format:\n\n        The key ``'method'`` defining the interpolation method name.\n        If the interpolation method is not one of :attr:`podpac.data.INTERPOLATION_SHORTCUTS`, a\n        second key ``'interpolators'`` must be defined with a list of\n        :class:`podpac.interpolators.Interpolator` classes to use in order of uages.\n        The dictionary may contain an option ``'params'`` key which contains a dict of parameters to pass along to\n        the :class:`podpac.interpolators.Interpolator` classes associated with the interpolation method.\n\n        The dict may contain the key ``'dims'`` which specifies dimension names (i.e. ``'time'`` or ``('lat', 'lon')`` ).\n        If the dictionary does not contain a key for all unstacked dimensions of the source coordinates, the\n        :attr:`podpac.data.INTERPOLATION_DEFAULT` value will be used.\n        All dimension keys must be unstacked even if the underlying coordinate dimensions are stacked.\n        Any extra dimensions included but not found in the source coordinates will be ignored.\n\n        The dict may contain a key ``'params'`` that can be used to configure the :class:`podpac.interpolators.Interpolator` classes associated with the interpolation method.\n\n        If input is a :class:`podpac.data.Interpolation` class, this Interpolation\n        class will be used without modification.\n        \"\"\"",
            "\"\"\"Base node for any data obtained directly from a single source.\n\n    Parameters\n    ----------\n    source : Any\n        The location of the source. Depending on the child node this can be a filepath,\n        numpy array, or dictionary as a few examples.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    nan_vals : List, optional\n        List of values from source data that should be interpreted as 'no data' or 'nans'\n    coordinate_index_type : str, optional\n        Type of index to use for data source. Possible values are ``['slice', 'numpy', 'xarray']``\n        Default is 'numpy', which allows a tuple of integer indices.\n    cache_coordinates : bool\n        Whether to cache coordinates using the podpac ``cache_ctrl``. Default False.\n    cache_output : bool\n        Should the node's output be cached? If not provided or None, uses default based on\n        settings[\"CACHE_DATASOURCE_OUTPUT_DEFAULT\"]. If True, outputs will be cached and retrieved from cache. If False,\n        outputs will not be cached OR retrieved from cache (even if they exist in cache).\n\n    Notes\n    -----\n    Custom DataSource Nodes must implement the :meth:`get_data` and :meth:`get_coordinates` methods.\n    \"\"\"",
            "\"\"\"{coordinates}\"\"\"",
            "\"\"\"datasource dims.\"\"\"",
            "\"\"\"datasource udims.\"\"\"",
            "\"\"\"datasource crs.\"\"\"",
            "\"\"\"Wrapper for `self.get_data` with pre and post processing\n\n        Returns\n        -------\n        podpac.core.units.UnitsDataArray\n            Returns UnitsDataArray with coordinates defined by _requested_source_coordinates\n\n        Raises\n        ------\n        TypeError\n            Raised if unknown data is passed by from self.get_data\n        NotImplementedError\n            Raised if get_data is not implemented by data source subclass\n\n        \"\"\"",
            "\"\"\"\n        Get source data, without interpolation.\n\n        Arguments\n        ---------\n        bounds : dict\n            Dictionary of bounds by dimension, optional.\n            Keys must be dimension names, and values are (min, max) tuples, e.g. ``{'lat': (10, 20)}``.\n\n        Returns\n        -------\n        data : UnitsDataArray\n            Source data\n        \"\"\"",
            "\"\"\"\n        Wraps the super Node.eval method in order to cache with the correct coordinates.\n\n        The output is independent of the crs or any extra dimensions, so this transforms and removes extra dimensions\n        before caching in the super eval method.\n        \"\"\"",
            "\"\"\"Evaluates this node using the supplied coordinates.\n\n        The coordinates are mapped to the requested coordinates, interpolated if necessary, and set to\n        `_requested_source_coordinates` with associated index `_requested_source_coordinates_index`. The requested\n        source coordinates and index are passed to `get_data()` returning the source data at the\n        coordinatesset to `_requested_source_data`. Finally `_requested_source_data` is interpolated\n        using the `interpolate` method and set to the `output` attribute of the node.\n\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n\n            An exception is raised if the requested coordinates are missing dimensions in the DataSource.\n            Extra dimensions in the requested coordinates are dropped.\n        output : :class:`podpac.UnitsDataArray`, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            Cannot evaluate these coordinates\n        \"\"\"",
            "\"\"\"\n        Get the available coordinates for the Node. For a DataSource, this is just the coordinates.\n\n        Returns\n        -------\n        coords_list : list\n            singleton list containing the coordinates (Coordinates object)\n        \"\"\"",
            "\"\"\"Get the full available coordinate bounds for the Node.\n\n        Arguments\n        ---------\n        crs : str\n            Desired CRS for the bounds. Use 'source' to use the native source crs.\n            If not specified, podpac.settings[\"DEFAULT_CRS\"] is used. Optional.\n\n        Returns\n        -------\n        bounds : dict\n            Bounds for each dimension. Keys are dimension names and values are tuples (min, max).\n        crs : str\n            The crs for the bounds.\n        \"\"\"",
            "\"\"\"{get_data}\n\n        Raises\n        ------\n        NotImplementedError\n            This needs to be implemented by derived classes\n        \"\"\"",
            "\"\"\"{get_coordinates}\n\n        Raises\n        ------\n        NotImplementedError\n            This needs to be implemented by derived classes\n        \"\"\"",
            "\"\"\"Set the coordinates. Used by Compositors as an optimization.\n\n        Arguments\n        ---------\n        coordinates : :class:`podpac.Coordinates`\n            Coordinates to set. Usually these are coordinates that are shared across compositor sources.\n\n        NOTE: This is only currently used by SMAPCompositor. It should potentially be moved to the SMAPSource.\n        \"\"\"",
            "\"\"\"\n        Select the boundary for the given the coordinates index. Only non-uniform boundary arrays need to be indexed.\n\n        Arguments\n        ---------\n        index : tuple\n            Coordinates index (e.g. coordinates_index)\n\n        Returns\n        -------\n        boundary : dict\n            Indexed boundary. Uniform boundaries are unchanged and non-uniform boundary arrays are indexed.\n        \"\"\""
        ],
        "code_snippets": [
            "class DataSource(Node):\n    \"\"\"Base node for any data obtained directly from a single source.\n\n    Parameters\n    ----------\n    source : Any\n        The location of the source. Depending on the child node this can be a filepath,\n        numpy array, or dictionary as a few examples.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    nan_vals : List, optional\n        List of values from source data that should be interpreted as 'no data' or 'nans'\n    coordinate_index_type : str, optional\n        Type of index to use for data source. Possible values are ``['slice', 'numpy', 'xarray']``\n        Default is 'numpy', which allows a tuple of integer indices.\n    cache_coordinates : bool\n        Whether to cache coordinates using the podpac ``cache_ctrl``. Default False.\n    cache_output : bool\n        Should the node's output be cached? If not provided or None, uses default based on\n        settings[\"CACHE_DATASOURCE_OUTPUT_DEFAULT\"]. If True, outputs will be cached and retrieved from cache. If False,\n        outputs will not be cached OR retrieved from cache (even if they exist in cache).\n\n    Notes\n    -----\n    Custom DataSource Nodes must implement the :meth:`get_data` and :meth:`get_coordinates` methods.\n    \"\"\"\n\n    nan_vals = tl.List().tag(attr=True)\n    nan_val = tl.Any(np.nan).tag(attr=True)\n    boundary = tl.Dict().tag(attr=True)\n\n    coordinate_index_type = tl.Enum(\n        [\"slice\", \"numpy\", \"xarray\"],\n        default_value=\"numpy\",\n    ).tag(attr=True)\n    cache_coordinates = tl.Bool(False)\n    cache_output = tl.Bool()\n\n    # privates\n    _coordinates = tl.Instance(Coordinates, allow_none=True, default_value=None, read_only=True)\n\n    # debug attributes\n    _requested_coordinates = tl.Instance(Coordinates, allow_none=True)\n    _requested_source_coordinates = tl.Instance(Coordinates, allow_none=True)\n    _requested_source_coordinates_index = tl.Instance(tuple, allow_none=True)\n    _requested_source_boundary = tl.Instance(dict, allow_none=True)\n    _requested_source_data = tl.Instance(UnitsDataArray, allow_none=True)\n    _evaluated_coordinates = tl.Instance(Coordinates, allow_none=True)\n\n    @tl.validate(\"boundary\")",
            "def _validate_boundary(self, d):\n        val = d[\"value\"]\n        for dim, boundary in val.items():\n            if dim not in VALID_DIMENSION_NAMES:\n                raise ValueError(\"Invalid dimension '%s' in boundary\" % dim)\n            if np.array(boundary).ndim == 0:\n                try:\n                    delta = make_coord_delta(boundary)\n                except ValueError:\n                    raise ValueError(\n                        \"Invalid boundary for dimension '%s' ('%s' is not a valid coordinate delta)\" % (dim, boundary)\n                    )\n\n                if np.array(delta).astype(float) < 0:\n                    raise ValueError(\"Invalid boundary for dimension '%s' (%s < 0)\" % (dim, delta))\n\n            if np.array(boundary).ndim == 1:\n                make_coord_delta_array(boundary)\n                raise NotImplementedError(\"Non-centered boundary not yet supported for dimension '%s'\" % dim)\n\n            if np.array(boundary).ndim == 2:\n                for elem in boundary:\n                    make_coord_delta_array(elem)\n                raise NotImplementedError(\"Non-uniform boundary not yet supported for dimension '%s'\" % dim)\n\n        return val\n\n    @tl.default(\"cache_output\")",
            "def _cache_output_default(self):\n        return settings[\"CACHE_DATASOURCE_OUTPUT_DEFAULT\"]\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def coordinates(self):",
            "def dims(self):",
            "def udims(self):",
            "def _crs(self):",
            "def _get_data(self, rc, rci):\n        \"\"\"Wrapper for `self.get_data` with pre and post processing\n\n        Returns\n        -------\n        podpac.core.units.UnitsDataArray\n            Returns UnitsDataArray with coordinates defined by _requested_source_coordinates\n\n        Raises\n        ------\n        TypeError\n            Raised if unknown data is passed by from self.get_data\n        NotImplementedError\n            Raised if get_data is not implemented by data source subclass\n\n        \"\"\"\n        # get data from data source at requested source coordinates and requested source coordinates index\n        data = self.get_data(rc, rci)\n\n        # convert data into UnitsDataArray depending on format\n        # TODO: what other processing needs to happen here?\n        if isinstance(data, UnitsDataArray):\n            udata_array = data\n        elif isinstance(data, xr.DataArray):\n            # TODO: check order of coordinates here\n            udata_array = self.create_output_array(rc, data=data.data)\n        elif isinstance(data, np.ndarray):\n            udata_array = self.create_output_array(rc, data=data)\n        else:\n            raise TypeError(\n                \"Unknown data type passed back from \"\n                + \"{}.get_data(): {}. \".format(type(self).__name__, type(data))\n                + \"Must be one of numpy.ndarray, xarray.DataArray, or podpac.UnitsDataArray\"\n            )\n\n        # extract single output, if necessary\n        # subclasses should extract single outputs themselves if possible, but this provides a backup\n        if \"output\" in udata_array.dims and self.output is not None:\n            udata_array = udata_array.sel(output=self.output)\n\n        # fill nan_vals in data array\n        udata_array.data[np.isin(udata_array.data, self.nan_vals)] = self.nan_val\n\n        return udata_array\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def get_source_data(self, bounds={}):\n        \"\"\"\n        Get source data, without interpolation.\n\n        Arguments\n        ---------\n        bounds : dict\n            Dictionary of bounds by dimension, optional.\n            Keys must be dimension names, and values are (min, max) tuples, e.g. ``{'lat': (10, 20)}``.\n\n        Returns\n        -------\n        data : UnitsDataArray\n            Source data\n        \"\"\"\n\n        coords, I = self.coordinates.select(bounds, return_index=True)\n        return self._get_data(coords, I)",
            "def eval(self, coordinates, **kwargs):\n        \"\"\"\n        Wraps the super Node.eval method in order to cache with the correct coordinates.\n\n        The output is independent of the crs or any extra dimensions, so this transforms and removes extra dimensions\n        before caching in the super eval method.\n        \"\"\"\n\n        # check for missing dimensions\n        for c in self.coordinates.values():\n            if isinstance(c, Coordinates1d):\n                if c.name not in coordinates.udims:\n                    raise ValueError(\"Cannot evaluate these coordinates, missing dim '%s'\" % c.name)\n            elif isinstance(c, StackedCoordinates):\n                if all(dim not in coordinates.udims for dim in c.udims):\n                    raise ValueError(\"Cannot evaluate these coordinates, missing at least one dim in '%s'\" % c.name)\n\n        # store original requested coordinates\n        requested_coordinates = coordinates\n        # This is needed for the interpolation mixin to avoid floating-point discrepancies\n        # between the requested coordinates and the evaluated coordinates\n        self._requested_coordinates = requested_coordinates\n\n        # remove extra dimensions\n        extra = [\n            c.name\n            for c in coordinates.values()\n            if (isinstance(c, Coordinates1d) and c.name not in self.udims)\n            or (isinstance(c, StackedCoordinates) and all(dim not in self.udims for dim in c.dims))\n        ]\n        coordinates = coordinates.drop(extra)\n\n        # transform coordinates into native crs if different\n        if coordinates.crs.lower() != self._crs.lower():\n            coordinates = coordinates.transform(self._crs)\n\n        # note: super().eval (not self._eval)\n        # This call already sub-selects an 'output' if specified\n        output = super().eval(coordinates, **kwargs)\n\n        # transform back to requested coordinates, if necessary\n        if coordinates.crs.lower() != requested_coordinates.crs.lower():\n            # need to use the already-selected output, if it exists\n            try:\n                outputs = output[\"output\"].data.tolist()\n                if isinstance(outputs, str):\n                    # this will pass outputs=None to the create function, which is what we want in this case\n                    # which is when it is a single output (not a dim)\n                    outputs = []\n            except KeyError:\n                # 'output' does not exist in the data, so outputs should be empty\n                outputs = []\n            except Exception as e:\n                outputs = self.outputs\n            coords = Coordinates.from_xarray(output, crs=output.attrs.get(\"crs\", None))\n            # the coords.transform in the next line can cause floating point discrepancies between\n            # the requested coordinates and the output coordinates. This is handled in the\n            # InterpolationMixin using self._requested_coordinates\n            output = self.create_output_array(\n                coords.transform(requested_coordinates.crs), data=output.data, outputs=outputs\n            )\n\n        return output\n\n    @common_doc(COMMON_DATA_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this node using the supplied coordinates.\n\n        The coordinates are mapped to the requested coordinates, interpolated if necessary, and set to\n        `_requested_source_coordinates` with associated index `_requested_source_coordinates_index`. The requested\n        source coordinates and index are passed to `get_data()` returning the source data at the\n        coordinatesset to `_requested_source_data`. Finally `_requested_source_data` is interpolated\n        using the `interpolate` method and set to the `output` attribute of the node.\n\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n\n            An exception is raised if the requested coordinates are missing dimensions in the DataSource.\n            Extra dimensions in the requested coordinates are dropped.\n        output : :class:`podpac.UnitsDataArray`, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            Cannot evaluate these coordinates\n        \"\"\"\n\n        log.debug(\"Evaluating {} data source\".format(self.__class__.__name__))\n\n        # Use the selector\n        if _selector is not None:\n            (rsc, rsci) = _selector(self.coordinates, coordinates, index_type=self.coordinate_index_type)\n        else:\n            # get source coordinates that are within the requested coordinates bounds\n            (rsc, rsci) = self.coordinates.intersect(coordinates, outer=True, return_index=True)\n\n        # if requested coordinates and coordinates do not intersect, shortcut with nan UnitsDataArary\n        if rsc.size == 0:\n            if output is None:\n                output = self.create_output_array(rsc)\n                if \"output\" in output.dims and self.output is not None:\n                    output = output.sel(output=self.output)\n            else:\n                output[:] = np.nan\n\n            if settings[\"DEBUG\"]:\n                self._evaluated_coordinates = coordinates\n                self._requested_source_coordinates = rsc\n                self._requested_source_coordinates_index = rsci\n                self._requested_source_boundary = None\n                self._requested_source_data = None\n                self._output = output\n\n            return output\n\n        # get data from data source\n        rsd = self._get_data(rsc, rsci)\n\n        if output is None:\n            # if requested_coordinates.crs.lower() != coordinates.crs.lower():\n            #     if rsc.shape == rsd.shape:\n            #         rsd = self.create_output_array(rsc, data=rsd.data)\n            #     else:\n            #         crds = Coordinates.from_xarray(rsd, crs=data.attrs.get(\"crs\", None))\n            #         rsd = self.create_output_array(crds.transform(rsc.crs), data=rsd.data)\n            output = rsd\n        else:\n            output.data[:] = rsd.data\n\n        # get indexed boundary\n        rsb = self._get_boundary(rsci)\n        output.attrs[\"boundary_data\"] = rsb\n        output.attrs[\"bounds\"] = self.coordinates.bounds\n\n        # save output to private for debugging\n        if settings[\"DEBUG\"]:\n            self._evaluated_coordinates = coordinates\n            self._requested_source_coordinates = rsc\n            self._requested_source_coordinates_index = rsci\n            self._requested_source_boundary = rsb\n            self._requested_source_data = rsd\n            self._output = output\n\n        return output",
            "def find_coordinates(self):\n        \"\"\"\n        Get the available coordinates for the Node. For a DataSource, this is just the coordinates.\n\n        Returns\n        -------\n        coords_list : list\n            singleton list containing the coordinates (Coordinates object)\n        \"\"\"\n\n        return [self.coordinates]",
            "def get_bounds(self, crs=\"default\"):\n        \"\"\"Get the full available coordinate bounds for the Node.\n\n        Arguments\n        ---------\n        crs : str\n            Desired CRS for the bounds. Use 'source' to use the native source crs.\n            If not specified, podpac.settings[\"DEFAULT_CRS\"] is used. Optional.\n\n        Returns\n        -------\n        bounds : dict\n            Bounds for each dimension. Keys are dimension names and values are tuples (min, max).\n        crs : str\n            The crs for the bounds.\n        \"\"\"\n\n        if crs == \"default\":\n            crs = settings[\"DEFAULT_CRS\"]\n        elif crs == \"source\":\n            crs = self.coordinates.crs\n\n        return self.coordinates.transform(crs).bounds, crs\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):\n        \"\"\"{get_data}\n\n        Raises\n        ------\n        NotImplementedError\n            This needs to be implemented by derived classes\n        \"\"\"\n        raise NotImplementedError\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_coordinates(self):\n        \"\"\"{get_coordinates}\n\n        Raises\n        ------\n        NotImplementedError\n            This needs to be implemented by derived classes\n        \"\"\"\n        raise NotImplementedError",
            "def set_coordinates(self, coordinates, force=False):\n        \"\"\"Set the coordinates. Used by Compositors as an optimization.\n\n        Arguments\n        ---------\n        coordinates : :class:`podpac.Coordinates`\n            Coordinates to set. Usually these are coordinates that are shared across compositor sources.\n\n        NOTE: This is only currently used by SMAPCompositor. It should potentially be moved to the SMAPSource.\n        \"\"\"\n\n        if force or not self.trait_is_defined(\"_coordinates\"):\n            self.set_trait(\"_coordinates\", coordinates)",
            "def _get_boundary(self, index):\n        \"\"\"\n        Select the boundary for the given the coordinates index. Only non-uniform boundary arrays need to be indexed.\n\n        Arguments\n        ---------\n        index : tuple\n            Coordinates index (e.g. coordinates_index)\n\n        Returns\n        -------\n        boundary : dict\n            Indexed boundary. Uniform boundaries are unchanged and non-uniform boundary arrays are indexed.\n        \"\"\"\n\n        if index is None:\n            return self.boundary\n\n        boundary = {}\n        for c, I in zip(self.coordinates.values(), index):\n            for dim in c.dims:\n                if dim not in self.boundary:\n                    pass\n                elif np.array(self.boundary[dim]).ndim == 2:\n                    boundary[dim] = np.array(self.boundary[dim][I])\n                else:\n                    boundary[dim] = self.boundary[dim]\n        return boundary"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/zarr_source.py",
        "comments": [
            "//\"):",
            "//\")",
            "// chunks[i])) for i, s in enumerate(index)])",
            "//\" + p for p in self.s3.ls(path)]"
        ],
        "docstrings": [
            "\"\"\"Create a DataSource node using zarr.\n\n    Attributes\n    ----------\n    source : str\n        Path to the Zarr archive\n    file_mode : str, optional\n        Default is 'r'. The mode used to open the Zarr archive. Options are r, r+, w, w- or x, a.\n    dataset : zarr.Group\n        The h5py file object used to read the file\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    data_key : str, int\n        data key, default 'data'\n    lat_key : str, int\n        latitude coordinates key, default 'lat'\n    lon_key : str, int\n        longitude coordinates key, default 'lon'\n    time_key : str, int\n        time coordinates key, default 'time'\n    alt_key : str, int\n        altitude coordinates key, default 'alt'\n    crs : str\n        Coordinate reference system of the coordinates\n    cf_time : bool\n        decode CF datetimes\n    cf_units : str\n        units, when decoding CF datetimes\n    cf_calendar : str\n        calendar, when decoding CF datetimes\n\n    See Also\n    --------\n    Zarr : Interpolated Zarr Datasource for general use.\n    \"\"\"",
            "\"\"\"\n        Test to see if a chunk exists for a particular slice.\n        Note: Only the start of the index is used.\n\n        Parameters\n        -----------\n        index: tuple(slice), optional\n            Default is None. A tuple of slices indicating the data that the users wants to access\n        chunk_str: str, optional\n            Default is None. A string equivalent to the filename of the chunk (.e.g. \"1.0.5\")\n        data_key: str, optional\n            Default is None. The data_key for the zarr array that will be queried.\n        chunks: list, optional\n            Defaut is None. The chunk structure of the zarr array. If not provided will use self.dataset[data_key].chunks\n        list_dir: list, optional\n            A list of existing paths -- used in lieu of 'exist' calls\n        \"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"Zarr Datasource with Interpolation.\"\"\""
        ],
        "code_snippets": [
            "class ZarrRaw(S3Mixin, FileKeysMixin, BaseFileSource):\n    \"\"\"Create a DataSource node using zarr.\n\n    Attributes\n    ----------\n    source : str\n        Path to the Zarr archive\n    file_mode : str, optional\n        Default is 'r'. The mode used to open the Zarr archive. Options are r, r+, w, w- or x, a.\n    dataset : zarr.Group\n        The h5py file object used to read the file\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    data_key : str, int\n        data key, default 'data'\n    lat_key : str, int\n        latitude coordinates key, default 'lat'\n    lon_key : str, int\n        longitude coordinates key, default 'lon'\n    time_key : str, int\n        time coordinates key, default 'time'\n    alt_key : str, int\n        altitude coordinates key, default 'alt'\n    crs : str\n        Coordinate reference system of the coordinates\n    cf_time : bool\n        decode CF datetimes\n    cf_units : str\n        units, when decoding CF datetimes\n    cf_calendar : str\n        calendar, when decoding CF datetimes\n\n    See Also\n    --------\n    Zarr : Interpolated Zarr Datasource for general use.\n    \"\"\"\n\n    file_mode = tl.Unicode(default_value=\"r\").tag(readonly=True)\n    coordinate_index_type = \"slice\"\n    _consolidated = False\n\n    def _get_store(self):\n        if self.source.startswith(\"s3:",
            "def _get_store(self):\n        if self.source.startswith(\"s3:",
            "def chunk_exists(self, index=None, chunk_str=None, data_key=None, chunks=None, list_dir=[]):\n        \"\"\"\n        Test to see if a chunk exists for a particular slice.\n        Note: Only the start of the index is used.\n\n        Parameters\n        -----------\n        index: tuple(slice), optional\n            Default is None. A tuple of slices indicating the data that the users wants to access\n        chunk_str: str, optional\n            Default is None. A string equivalent to the filename of the chunk (.e.g. \"1.0.5\")\n        data_key: str, optional\n            Default is None. The data_key for the zarr array that will be queried.\n        chunks: list, optional\n            Defaut is None. The chunk structure of the zarr array. If not provided will use self.dataset[data_key].chunks\n        list_dir: list, optional\n            A list of existing paths -- used in lieu of 'exist' calls\n        \"\"\"\n\n        if not data_key:\n            data_key = \"\"\n\n        if not chunks:\n            if data_key:\n                chunks = self.dataset[data_key].chunks\n            else:\n                chunks = self.dataset.chunks\n\n        if index:\n            chunk_str = \".\".join([str(int(s.start",
            "def list_dir(self, data_key=None):\n        za = self.dataset\n        if data_key:\n            za = za[data_key]\n        else:\n            data_key = \"\"\n\n        path = os.path.join(self.source, data_key)\n        if self.source.startswith(\"s3:\"):\n            path = path.replace(\"\\\\\", \"/\")\n            ld = [\"s3:",
            "def dataset(self):\n        store = self._get_store()\n        try:\n            # import zarr.open\n            # import zarr.open_consolidated\n            if self.file_mode == \"r\":\n                try:\n                    self._consolidated = True\n                    return zarr_open_consolidated(store)\n                except KeyError:\n                    pass  # No consolidated metadata available\n            self._consolidated = False\n            return zarr_open(store, mode=self.file_mode)\n        except ValueError:\n            raise ValueError(\"No Zarr store found at path '%s'\" % self.source)\n\n    # -------------------------------------------------------------------------\n    # public api methods\n    # -------------------------------------------------------------------------\n\n    @cached_property",
            "def dims(self):\n        if not isinstance(self.data_key, list):\n            key = self.data_key\n        else:\n            key = self.data_key[0]\n        try:\n            return self.dataset[key].attrs[\"_ARRAY_DIMENSIONS\"]\n        except:\n            lookup = {self.lat_key: \"lat\", self.lon_key: \"lon\", self.alt_key: \"alt\", self.time_key: \"time\"}\n            return [lookup[key] for key in self.dataset if key in lookup]",
            "def _add_keys(self, base_keys):\n        keys = base_keys.copy()\n        for bk in base_keys:\n            try:\n                new_keys = [bk + \"/\" + k for k in self.dataset[bk].keys()]\n                keys.extend(new_keys)\n\n                # Remove the group key\n                keys.pop(keys.index(bk))\n            except AttributeError:\n                pass\n        return keys\n\n    @cached_property",
            "def keys(self):\n        keys = list(self.dataset.keys())\n        full_keys = self._add_keys(keys)\n        while keys != full_keys:\n            keys = full_keys.copy()\n            full_keys = self._add_keys(keys)\n\n        return full_keys\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "class Zarr(InterpolationMixin, ZarrRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/pydap_source.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nPyDap DataSource\n\"\"\"",
            "\"\"\"Create a DataSource from an OpenDAP server feed.\n\n    Attributes\n    ----------\n    data_key : str\n        Pydap 'key' for the data to be retrieved from the server. Datasource may have multiple keys, so this key\n        determines which variable is returned from the source.\n    dataset : pydap.model.DatasetType\n        The open pydap dataset. This is provided for troubleshooting.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    source : str\n        URL of the OpenDAP server.\n\n    See Also\n    --------\n    PyDAP : Interpolated OpenDAP datasource for general use.\n    \"\"\"",
            "\"\"\"{get_coordinates}\n\n        Raises\n        ------\n        NotImplementedError\n            PyDAP cannot create coordinates. A child class must implement this method.\n        \"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"The list of available keys from the OpenDAP dataset.\n\n        Returns\n        -------\n        List\n            The list of available keys from the OpenDAP dataset. Any of these keys can be set as self.data_key\n        \"\"\"",
            "\"\"\"OpenDAP datasource with interpolation.\"\"\""
        ],
        "code_snippets": [
            "class PyDAPRaw(authentication.RequestsSessionMixin, DataSource):\n    \"\"\"Create a DataSource from an OpenDAP server feed.\n\n    Attributes\n    ----------\n    data_key : str\n        Pydap 'key' for the data to be retrieved from the server. Datasource may have multiple keys, so this key\n        determines which variable is returned from the source.\n    dataset : pydap.model.DatasetType\n        The open pydap dataset. This is provided for troubleshooting.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    source : str\n        URL of the OpenDAP server.\n\n    See Also\n    --------\n    PyDAP : Interpolated OpenDAP datasource for general use.\n    \"\"\"\n\n    source = tl.Unicode().tag(attr=True, required=True)\n    data_key = tl.Unicode().tag(attr=True, required=True)\n    server_throttle_sleep_time = tl.Float(\n        default_value=0.001, help=\"Some server have a throttling time for requests per period. \"\n    ).tag(attr=True)\n    server_throttle_retries = tl.Int(default_value=100, help=\"Number of retries for a throttled server.\").tag(attr=True)\n\n    # list of attribute names, used by __repr__ and __str__ to display minimal info about the node\n    _repr_keys = [\"source\"]\n    coordinate_index_type = \"slice\"\n\n    # hostname for RequestsSession is source. Try parsing off netloc\n    @tl.default(\"hostname\")",
            "def _hostname(self):\n        try:\n            return requests.utils.urlparse(self.source).netloc\n        except:\n            return self.source\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_coordinates(self):\n        \"\"\"{get_coordinates}\n\n        Raises\n        ------\n        NotImplementedError\n            PyDAP cannot create coordinates. A child class must implement this method.\n        \"\"\"\n        raise NotImplementedError(\"PyDAP cannot create coordinates. A child class must implement this method.\")\n\n    @cached_property",
            "def dataset(self):\n        # auth session\n        try:\n            return self._open_url()\n        except HTTPError as e:\n            # I need the 500 because pydap re-raises HTTPError wihout setting the code\n            if not (e.code != 400 or e.code != 300 or e.code != 500):\n                raise e\n            # Check Url (probably inefficient..., but worth a try to get authenticated)\n            try:\n                self.session.get(self.source + \".dds\")\n                return self._open_url()\n            except HTTPError as e:\n                if e.code != 400:\n                    raise e\n                _logger.exception(\"Error opening PyDap url '%s'\" % self.source)\n                raise HTTPError(\"Could not open PyDap url '%s'.\\nCheck login credentials.\" % self.source)",
            "def _open_url(self):\n        return pydap.client.open_url(self.source, session=self.session)\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "def keys(self):\n        \"\"\"The list of available keys from the OpenDAP dataset.\n\n        Returns\n        -------\n        List\n            The list of available keys from the OpenDAP dataset. Any of these keys can be set as self.data_key\n        \"\"\"\n        return self.dataset.keys()",
            "class PyDAP(InterpolationMixin, PyDAPRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/array_source.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nArray Datasource\n\"\"\"",
            "\"\"\"Create a DataSource from an array -- this node is mostly meant for small experiments\n\n    Attributes\n    ----------\n    source : np.ndarray\n        Numpy array containing the source data\n    coordinates : podpac.Coordinates\n        The coordinates of the source data\n\n    Notes\n    ------\n    `coordinates` need to supplied by the user when instantiating this node.\n\n    See Also\n    --------\n    Array : Interpolated array datasource.\n\n    This Node is not meant for large arrays, and cause issues with caching. As such, this Node override the default\n    cache behavior as having no cache -- its data is in RAM already and caching is not helpful.\n\n    Example\n    ---------\n    >>> # Create a time series of 10 32x34 images with R-G-B channels\n    >>> import numpy as np\n    >>> import podpac\n    >>> data = np.random.rand(10, 32, 34, 3)\n    >>> coords = podpac.Coordinates([podpac.clinspace(1, 10, 10, 'time'),\n                                     podpac.clinspace(1, 32, 32, 'lat'),\n                                     podpac.clinspace(1, 34, 34, 'lon')])\n    >>> node = podpac.data.Array(source=data, coordinates=coords, outputs=['R', 'G', 'B'])\n    >>> output = node.eval(coords)\n    \"\"\"",
            "\"\"\"Returns the shape of :attr:`self.array`\n\n        Returns\n        -------\n        tuple\n            Shape of :attr:`self.array`\n        \"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"Not needed.\"\"\"",
            "\"\"\"Array datasource with interpolation.\"\"\""
        ],
        "code_snippets": [
            "class ArrayRaw(NoCacheMixin, DataSource):\n    \"\"\"Create a DataSource from an array -- this node is mostly meant for small experiments\n\n    Attributes\n    ----------\n    source : np.ndarray\n        Numpy array containing the source data\n    coordinates : podpac.Coordinates\n        The coordinates of the source data\n\n    Notes\n    ------\n    `coordinates` need to supplied by the user when instantiating this node.\n\n    See Also\n    --------\n    Array : Interpolated array datasource.\n\n    This Node is not meant for large arrays, and cause issues with caching. As such, this Node override the default\n    cache behavior as having no cache -- its data is in RAM already and caching is not helpful.\n\n    Example\n    ---------\n    >>> # Create a time series of 10 32x34 images with R-G-B channels\n    >>> import numpy as np\n    >>> import podpac\n    >>> data = np.random.rand(10, 32, 34, 3)\n    >>> coords = podpac.Coordinates([podpac.clinspace(1, 10, 10, 'time'),\n                                     podpac.clinspace(1, 32, 32, 'lat'),\n                                     podpac.clinspace(1, 34, 34, 'lon')])\n    >>> node = podpac.data.Array(source=data, coordinates=coords, outputs=['R', 'G', 'B'])\n    >>> output = node.eval(coords)\n    \"\"\"\n\n    source = ArrayTrait().tag(attr=True, required=True)\n    coordinates = tl.Instance(Coordinates).tag(attr=True, required=True)\n\n    _repr_keys = [\"shape\"]\n\n    @tl.validate(\"source\")",
            "def _validate_source(self, d):\n        try:\n            d[\"value\"].astype(float)\n        except:\n            raise ValueError(\"Array 'source' data must be numerical\")\n        return d[\"value\"]",
            "def _first_init(self, **kwargs):\n        # If the coordinates were supplied explicitly, they may need to be deserialized.\n        if isinstance(kwargs.get(\"coordinates\"), OrderedDict):\n            kwargs[\"coordinates\"] = Coordinates.from_definition(kwargs[\"coordinates\"])\n        elif isinstance(kwargs.get(\"coordinates\"), string_types):\n            kwargs[\"coordinates\"] = Coordinates.from_json(kwargs[\"coordinates\"])\n\n        return kwargs\n\n    @property",
            "def shape(self):\n        \"\"\"Returns the shape of :attr:`self.array`\n\n        Returns\n        -------\n        tuple\n            Shape of :attr:`self.array`\n        \"\"\"\n        return self.source.shape\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "def set_coordinates(self, value):",
            "class Array(InterpolationMixin, ArrayRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/ogr.py",
        "comments": [],
        "docstrings": [
            "\"\"\" \"\"\"",
            "\"\"\"\n        Not available for OGR nodes.\n\n        Arguments\n        ---------\n        bounds : dict\n            Dictionary of bounds by dimension, optional.\n            Keys must be dimension names, and values are (min, max) tuples, e.g. ``{'lat': (10, 20)}``.\n\n        raises\n        ------\n        AttributeError : Cannot get source data for OGR datasources\n        \"\"\"",
            "\"\"\"\n        Not available for OGR nodes.\n\n        raises\n        ------\n        coord_list : list\n            list of available coordinates (Coordinates objects)\n        \"\"\""
        ],
        "code_snippets": [
            "class OGRRaw(Node):",
            "def _validate_driver(self, d):\n        ogr.GetDriverByName(d[\"value\"])\n        return d[\"value\"]\n\n    @tl.validate(\"source\")",
            "def _validate_source(self, d):\n        if not os.path.exists(d[\"value\"]):\n            raise ValueError(\"OGR source not found '%s'\" % d[\"value\"])\n        return d[\"value\"]\n\n    @cached_property",
            "def datasource(self):\n        driver = ogr.GetDriverByName(self.driver)\n        return driver.Open(self.source, 0)\n\n    @cached_property",
            "def extents(self):\n        layer = self.datasource.GetLayerByName(self.layer)\n        return layer.GetExtent()",
            "def get_source_data(self, bounds={}):\n        \"\"\"\n        Not available for OGR nodes.\n\n        Arguments\n        ---------\n        bounds : dict\n            Dictionary of bounds by dimension, optional.\n            Keys must be dimension names, and values are (min, max) tuples, e.g. ``{'lat': (10, 20)}``.\n\n        raises\n        ------\n        AttributeError : Cannot get source data for OGR datasources\n        \"\"\"\n\n        raise AttributeError(\n            \"Cannot get source data for OGR datasources. \"\n            \"The source data is a vector-based shapefile without a native resolution.\"\n        )",
            "def find_coordinates(self):\n        \"\"\"\n        Not available for OGR nodes.\n\n        raises\n        ------\n        coord_list : list\n            list of available coordinates (Coordinates objects)\n        \"\"\"\n\n        raise AttributeError(\n            \"Cannot get available coordinates for OGR datasources. \"\n            \"The source data is a vector-based shapefile without native coordinates.\"\n        )\n\n    @common_doc(COMMON_NODE_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        if \"lat\" not in coordinates.udims or \"lon\" not in coordinates.udims:\n            raise RuntimeError(\"OGR source requires lat and lon dims\")\n\n        requested_coordinates = coordinates\n        coordinates = coordinates.udrop([\"time\", \"alt\"], ignore_missing=True)\n\n        if coordinates.size == 1 or \"lat_lon\" in coordinates or \"lon_lat\" in coordinates:\n            # point or points\n            eps = 1e-6\n            data = np.empty(coordinates.size)\n            for i, (lat, lon) in enumerate(zip(coordinates[\"lat\"].coordinates, coordinates[\"lon\"].coordinates)):\n                geotransform = [lon - eps / 2.0, eps, 0.0, lat - eps / 2.0, 0.0, -1.0 * eps]\n                data[i] = self._get_data(1, 1, geotransform)\n            data = data.reshape(coordinates.shape)\n\n        else:\n            # resample non-uniform coordinates if necessary\n            if not coordinates[\"lat\"].is_uniform:\n                coordinates[\"lat\"] = clinspace(\n                    coordinates[\"lat\"].bounds[0], coordinates[\"lat\"].bounds[1], coordinates[\"lat\"].size, name=\"lat\"\n                )\n            if not coordinates[\"lon\"].is_uniform:\n                coordinates[\"lon\"] = clinspace(\n                    coordinates[\"lon\"].bounds[0], coordinates[\"lon\"].bounds[1], coordinates[\"lon\"].size, name=\"lon\"\n                )\n\n            # evaluate uniform grid\n            data = self._get_data(coordinates[\"lon\"].size, coordinates[\"lat\"].size, coordinates.geotransform)\n\n        if output is None:\n            output = self.create_output_array(coordinates, data=data)\n        else:\n            output.data[:] = data\n\n        # nan values\n        output.data[np.isin(output.data, self.nan_vals)] = self.nan_val\n\n        if settings[\"DEBUG\"]:\n            self._requested_coordinates = requested_coordinates\n            self._evaluated_coordinates = coordinates\n\n        return output",
            "def _get_data(self, xsize, ysize, geotransform):\n        nan_val = 0\n\n        # create target datasource\n        driver = gdal.GetDriverByName(\"MEM\")\n        target = driver.Create(\"\", xsize, ysize, gdal.GDT_Float64)\n        target.SetGeoTransform(geotransform)\n        band = target.GetRasterBand(1)\n        band.SetNoDataValue(nan_val)\n        band.Fill(nan_val)\n\n        # rasterize\n        layer = self.datasource.GetLayerByName(self.layer)\n        gdal.RasterizeLayer(target, [1], layer, options=[\"ATTRIBUTE=%s\" % self.attribute])\n\n        data = band.ReadAsArray(buf_type=gdal.GDT_Float64).copy()\n        data[data == nan_val] = np.nan\n        return data",
            "class OGR(InterpolationMixin, OGRRaw):\n    interpolation = \"nearest\""
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/dataset_source.py",
        "comments": [
            "//, http://, ftp://, and s3:// transport protocols are supported."
        ],
        "docstrings": [
            "\"\"\"Create a DataSource node using xarray.open_dataset.\n\n    Attributes\n    ----------\n    source : str\n        Path to the dataset file.\n        In addition to local paths, file://, http://, ftp://, and s3:// transport protocols are supported.\n    dataset : xarray.Dataset\n        Dataset object.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    data_key : str\n        data key, default 'data'\n    lat_key : str\n        latitude key, default 'lat'\n    lon_key : str\n        longitude key, default 'lon'\n    time_key : str\n        time key, default 'time'\n    alt_key : str\n        altitude key, default 'alt'\n    crs : str\n        Coordinate reference system of the coordinates\n    selection : dict\n        Extra dimension(s) selection. Select one coordinate by index for each extra dimension.\n        This is necessary when the data contains dimensions other than 'lat', 'lon', 'time', and 'alt'.\n        For example, with dims `('lat', 'lon', 'channel')`, use `{{'channel': 1}}`.\n    infer_podpac_coords: bool\n        If True, load the coordinates from the dataset coords directly. Default is False.\n        This is particularly useful if the file was saved using PODPAC.\n\n    See Also\n    --------\n    Dataset : Interpolated xarray dataset source for general use.\n    \"\"\"",
            "\"\"\"dataset coordinate dims\"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"{get_coordinates}\"\"\"",
            "\"\"\"xarray dataset source with interpolation.\"\"\""
        ],
        "code_snippets": [
            "class DatasetRaw(FileKeysMixin, LoadFileMixin, BaseFileSource):\n    \"\"\"Create a DataSource node using xarray.open_dataset.\n\n    Attributes\n    ----------\n    source : str\n        Path to the dataset file.\n        In addition to local paths, file:",
            "def open_dataset(self, fp):\n        return xr.open_dataset(fp, decode_cf=self.decode_cf)",
            "def close_dataset(self):\n        super(DatasetRaw, self).close_dataset()\n        self.dataset.close()\n\n    @cached_property",
            "def dims(self):",
            "def keys(self):\n        return list(self.dataset.keys())\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "def get_coordinates(self):",
            "class Dataset(InterpolationMixin, DatasetRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/ogc.py",
        "comments": [
            "//myhost/mywcs?SERVICE=WCS&REQUEST=GetCoverage&IDENTIFIER=TuMYrRQ4&VERSION=1.1.0&BOUNDINGBOX=-180,-90,180,90&TIME=2792-06-01T00:00:00.0&FORMAT=cf-netcdf",
            "//www.ctps.org/geoserver/web/wicket/bookmarkable/org.geoserver.wcs.web.demo.WCSRequestBuilder;jsessionid=9E2AA99F95410C694D05BA609F25527C?0",
            "// s"
        ],
        "docstrings": [
            "\"\"\"\nOGC-compliant datasources over HTTP\n\"\"\"",
            "\"\"\"Request and return a coverage from the WCS as a file-like object\n        note: additional **kwargs helps with multi-version implementation\n        core keyword arguments should be supported cross version\n        example:\n        cvg=wcs.getCoverage(identifier=['TuMYrRQ4'], timeSequence=['2792-06-01T00:00:00.0'], bbox=(-112,36,-106,41),\n                            format='cf-netcdf')\n        is equivalent to:\n        http://myhost/mywcs?SERVICE=WCS&REQUEST=GetCoverage&IDENTIFIER=TuMYrRQ4&VERSION=1.1.0&BOUNDINGBOX=-180,-90,180,90&TIME=2792-06-01T00:00:00.0&FORMAT=cf-netcdf\n        \"\"\"",
            "\"\"\"\n    Access data from a WCS source.\n\n    Attributes\n    ----------\n    source : str\n        WCS server url\n    layer : str\n        layer name (required)\n    version : str\n        WCS version, passed through to all requests (default '1.0.0')\n    format : str\n        Data format, passed through to the GetCoverage requests (default 'geotiff')\n    crs : str\n        coordinate reference system, passed through to the GetCoverage requests (default 'EPSG:4326')\n    interpolation : str\n        Interpolation, passed through to the GetCoverage requests.\n    max_size : int\n        maximum request size, optional.\n        If provided, the coordinates will be tiled into multiple requests.\n    allow_mock_client : bool\n        Default is False. If True, a mock client will be used to make WCS requests. This allows returns\n        from servers with only partial WCS implementations.\n    username : str\n        Username for servers that require authentication\n    password : str\n        Password for servers that require authentication\n\n    See Also\n    --------\n    WCS : WCS datasource with podpac interpolation.\n    \"\"\"",
            "\"\"\"\n        Get the full WCS grid.\n        \"\"\"",
            "\"\"\"Evaluates this node using the supplied coordinates.\n\n        This method intercepts the DataSource._eval method in order to use the requested coordinates directly when\n        they are a uniform grid.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n\n            An exception is raised if the requested coordinates are missing dimensions in the DataSource.\n            Extra dimensions in the requested coordinates are dropped.\n        output : :class:`podpac.UnitsDataArray`, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            Cannot evaluate these coordinates\n        \"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"WCS datasource with podpac interpolation.\"\"\""
        ],
        "code_snippets": [
            "class MockWCSClient(tl.HasTraits):\n    source = tl.Unicode()\n    version = tl.Enum([\"1.0.0\"], default_value=\"1.0.0\")\n    headers = None\n    cookies = None\n    auth = tl.Any()\n\n    def getCoverage(\n        self,\n        identifier=None,\n        bbox=None,\n        time=None,\n        format=None,\n        crs=None,\n        width=None,\n        height=None,\n        resx=None,\n        resy=None,\n        resz=None,\n        parameter=None,\n        method=\"Get\",\n        timeout=30,\n        **kwargs\n    ):\n        \"\"\"Request and return a coverage from the WCS as a file-like object\n        note: additional **kwargs helps with multi-version implementation\n        core keyword arguments should be supported cross version\n        example:\n        cvg=wcs.getCoverage(identifier=['TuMYrRQ4'], timeSequence=['2792-06-01T00:00:00.0'], bbox=(-112,36,-106,41),\n                            format='cf-netcdf')\n        is equivalent to:\n        http:",
            "def getCoverage(\n        self,\n        identifier=None,\n        bbox=None,\n        time=None,\n        format=None,\n        crs=None,\n        width=None,\n        height=None,\n        resx=None,\n        resy=None,\n        resz=None,\n        parameter=None,\n        method=\"Get\",\n        timeout=30,\n        **kwargs\n    ):\n        \"\"\"Request and return a coverage from the WCS as a file-like object\n        note: additional **kwargs helps with multi-version implementation\n        core keyword arguments should be supported cross version\n        example:\n        cvg=wcs.getCoverage(identifier=['TuMYrRQ4'], timeSequence=['2792-06-01T00:00:00.0'], bbox=(-112,36,-106,41),\n                            format='cf-netcdf')\n        is equivalent to:\n        http:",
            "class WCSError(NodeException):\n    pass",
            "class WCSRaw(DataSource):\n    \"\"\"\n    Access data from a WCS source.\n\n    Attributes\n    ----------\n    source : str\n        WCS server url\n    layer : str\n        layer name (required)\n    version : str\n        WCS version, passed through to all requests (default '1.0.0')\n    format : str\n        Data format, passed through to the GetCoverage requests (default 'geotiff')\n    crs : str\n        coordinate reference system, passed through to the GetCoverage requests (default 'EPSG:4326')\n    interpolation : str\n        Interpolation, passed through to the GetCoverage requests.\n    max_size : int\n        maximum request size, optional.\n        If provided, the coordinates will be tiled into multiple requests.\n    allow_mock_client : bool\n        Default is False. If True, a mock client will be used to make WCS requests. This allows returns\n        from servers with only partial WCS implementations.\n    username : str\n        Username for servers that require authentication\n    password : str\n        Password for servers that require authentication\n\n    See Also\n    --------\n    WCS : WCS datasource with podpac interpolation.\n    \"\"\"\n\n    source = tl.Unicode().tag(attr=True, required=True)\n    layer = tl.Unicode().tag(attr=True, required=True)\n    version = tl.Unicode(default_value=\"1.0.0\").tag(attr=True)\n    interpolation = InterpolationTrait(default_value=None, allow_none=True).tag(attr=True)\n    allow_mock_client = tl.Bool(False).tag(attr=True)\n    username = tl.Unicode(allow_none=True)\n    password = tl.Unicode(allow_none=True)\n\n    format = tl.CaselessStrEnum([\"geotiff\", \"geotiff_byte\"], default_value=\"geotiff\")\n    crs = tl.Unicode(default_value=\"EPSG:4326\")\n    max_size = tl.Long(default_value=None, allow_none=True)\n    wcs_kwargs = tl.Dict(help=\"Additional query parameters sent to the WCS server\")\n\n    _repr_keys = [\"source\", \"layer\"]\n\n    _requested_coordinates = tl.Instance(Coordinates, allow_none=True)\n    _evaluated_coordinates = tl.Instance(Coordinates)\n    coordinate_index_type = \"slice\"\n\n    @property",
            "def auth(self):\n        if self.username and self.password:\n            return owslib_util.Authentication(username=self.username, password=self.password)\n        return None\n\n    @cached_property",
            "def client(self):\n        try:\n            return owslib_wcs.WebCoverageService(self.source, version=self.version, auth=self.auth)\n        except Exception as e:\n            if self.allow_mock_client:\n                logger.warning(\n                    \"The OWSLIB Client could not be used. Server endpoint likely does not implement GetCapabilities\"\n                    \"requests. Using Mock client instead. Error was {}\".format(e)\n                )\n                return MockWCSClient(source=self.source, version=self.version, auth=self.auth)\n            else:\n                raise e",
            "def get_coordinates(self):\n        \"\"\"\n        Get the full WCS grid.\n        \"\"\"\n\n        metadata = self.client.contents[self.layer]\n\n        # coordinates\n        bbox = metadata.boundingBoxWGS84\n        crs = \"EPSG:4326\"\n        logging.debug(\"WCS available boundingboxes: {}\".format(metadata.boundingboxes))\n        for bboxes in metadata.boundingboxes:\n            if bboxes[\"nativeSrs\"] == self.crs:\n                bbox = bboxes[\"bbox\"]\n                crs = self.crs\n                break\n\n        low = metadata.grid.lowlimits\n        high = metadata.grid.highlimits\n        xsize = int(high[0]) - int(low[0])\n        ysize = int(high[1]) - int(low[1])\n\n        # Based on https:",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this node using the supplied coordinates.\n\n        This method intercepts the DataSource._eval method in order to use the requested coordinates directly when\n        they are a uniform grid.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n\n            An exception is raised if the requested coordinates are missing dimensions in the DataSource.\n            Extra dimensions in the requested coordinates are dropped.\n        output : :class:`podpac.UnitsDataArray`, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            Cannot evaluate these coordinates\n        \"\"\"\n        # The mock client cannot figure out the real coordinates, so just duplicate the requested coordinates\n        if isinstance(self.client, MockWCSClient):\n            if not coordinates[\"lat\"].is_uniform or not coordinates[\"lon\"].is_uniform:\n                raise NotImplementedError(\n                    \"When using the Mock WCS client, the requested coordinates need to be uniform.\"\n                )\n            self.set_trait(\"_coordinates\", coordinates)\n            self.set_trait(\"crs\", coordinates.crs)\n\n        # remove extra dimensions\n        extra = [\n            c.name\n            for c in coordinates.values()\n            if (isinstance(c, Coordinates1d) and c.name not in self.coordinates.udims)\n            or (isinstance(c, StackedCoordinates) and all(dim not in self.coordinates.udims for dim in c.dims))\n        ]\n        coordinates = coordinates.drop(extra)\n\n        # the datasource does do this, but we need to do it here to correctly select the correct case\n        if self.coordinates.crs.lower() != coordinates.crs.lower():\n            coordinates = coordinates.transform(self.coordinates.crs)\n\n        # for a uniform grid, use the requested coordinates (the WCS server will interpolate)\n        if (\n            (\"lat\" in coordinates.dims and \"lon\" in coordinates.dims)\n            and (coordinates[\"lat\"].is_uniform or coordinates[\"lat\"].size == 1)\n            and (coordinates[\"lon\"].is_uniform or coordinates[\"lon\"].size == 1)\n        ):",
            "def selector(rsc, coordinates, index_type=None):\n                return coordinates, None\n\n            return super()._eval(coordinates, output=output, _selector=selector)\n\n        # for uniform stacked, unstack to use the requested coordinates (the WCS server will interpolate)\n        if (\n            (\"lat\" in coordinates.udims and coordinates.is_stacked(\"lat\"))\n            and (\"lon\" in coordinates.udims and coordinates.is_stacked(\"lon\"))\n            and (coordinates[\"lat\"].is_uniform or coordinates[\"lat\"].size == 1)\n            and (coordinates[\"lon\"].is_uniform or coordinates[\"lon\"].size == 1)\n        ):",
            "def selector(rsc, coordinates, index_type=None):\n                unstacked = coordinates.unstack()\n                unstacked = unstacked.drop(\"alt\", ignore_missing=True)  # if lat_lon_alt\n                return unstacked, None\n\n            udata = super()._eval(coordinates, output=None, _selector=selector)\n            data = udata.data.diagonal()  # get just the stacked data\n            if output is None:\n                output = self.create_output_array(coordinates, data=data)\n            else:\n                output.data[:] = data\n            return output\n\n        # otherwise, pass-through (podpac will select and interpolate)\n        return super()._eval(coordinates, output=output, _selector=_selector)",
            "def _get_data(self, coordinates, coordinates_index):",
            "def _get_chunk(self, coordinates):\n        if coordinates[\"lon\"].size == 1:\n            w = coordinates[\"lon\"].coordinates[0]\n            e = coordinates[\"lon\"].coordinates[0]\n        else:\n            w = coordinates[\"lon\"].start - coordinates[\"lon\"].step / 2.0\n            e = coordinates[\"lon\"].stop + coordinates[\"lon\"].step / 2.0\n\n        if coordinates[\"lat\"].size == 1:\n            s = coordinates[\"lat\"].coordinates[0]\n            n = coordinates[\"lat\"].coordinates[0]\n        else:\n            s = coordinates[\"lat\"].start - coordinates[\"lat\"].step / 2.0\n            n = coordinates[\"lat\"].stop + coordinates[\"lat\"].step / 2.0\n\n        width = coordinates[\"lon\"].size\n        height = coordinates[\"lat\"].size\n\n        kwargs = self.wcs_kwargs.copy()\n\n        if \"time\" in coordinates:\n            kwargs[\"time\"] = coordinates[\"time\"].coordinates.astype(str).tolist()\n\n        if isinstance(self.interpolation, str):\n            kwargs[\"interpolation\"] = self.interpolation\n\n        logger.info(\n            \"WCS GetCoverage (source=%s, layer=%s, bbox=%s, shape=%s, time=%s)\"\n            % (self.source, self.layer, (w, n, e, s), (width, height), kwargs.get(\"time\"))\n        )\n\n        crs = pyproj.CRS(coordinates.crs)\n        bbox = (min(w, e), min(s, n), max(e, w), max(n, s))\n        # Based on the spec I need the following line, but\n        # all my tests on other servers suggests I don't need this...\n        # if crs.axis_info[0].direction == \"north\":\n        #     bbox = (min(s, n), min(w, e), max(n, s), max(e, w))\n\n        response = self.client.getCoverage(\n            identifier=self.layer,\n            bbox=bbox,\n            width=width,\n            height=height,\n            crs=self.crs,\n            format=self.format,\n            version=self.version,\n            **kwargs\n        )\n        content = response.read()\n\n        # check for errors\n        xml = bs4.BeautifulSoup(content, \"lxml\")\n        error = xml.find(\"serviceexception\")\n        if error:\n            raise WCSError(error.text)\n\n        # get data using rasterio\n        with rasterio.MemoryFile() as mf:\n            mf.write(content)\n            try:\n                dataset = mf.open(driver=\"GTiff\")\n            except rasterio.RasterioIOError:\n                raise WCSError(\"Could not read file with contents:\", content)\n\n        if \"time\" in coordinates and coordinates[\"time\"].size > 1:\n            # this should be easy to do, I'm just not sure how the data comes back.\n            # is each time in a different band?\n            raise NotImplementedError(\"TODO\")\n\n        data = dataset.read().astype(float).squeeze()\n\n        # Need to fix the order of the data in the case of multiple bands\n        if len(data.shape) == 3:\n            data = data.transpose((1, 2, 0))\n\n        # Need to fix the data order. The request and response order is always the same in WCS, but not in PODPAC\n        if n > s:  # By default it returns the data upside down, so this is backwards\n            data = data[::-1]\n        if e < w:\n            data = data[:, ::-1]\n\n        return data\n\n    @classmethod",
            "def get_layers(cls, source=None):\n        if source is None:\n            source = cls.source\n        client = owslib_wcs.WebCoverageService(source)\n        return list(client.contents)",
            "class WCS(InterpolationMixin, WCSRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/rasterio_source.py",
        "comments": [
            "//\"):",
            "// overview), int(np.ceil(inds[0].max() / overview) + 1)),",
            "// overview), int(np.ceil(inds[1].max() / overview) + 1)),"
        ],
        "docstrings": [
            "\"\"\"Create a DataSource using rasterio.\n\n    Attributes\n    ----------\n    source : str, :class:`io.BytesIO`\n        Path to the data source\n    dataset : :class:`rasterio._io.RasterReader`\n        A reference to the datasource opened by rasterio\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    band : int\n        The 'band' or index for the variable being accessed in files such as GeoTIFFs. Use None for all bounds.\n    crs : str, optional\n        The coordinate reference system. Normally this will come directly from the file, but this allows users to\n        specify the crs in case this information is missing from the file.\n    aws_https: bool\n        Default is True. If False, will not use https when reading from AWS. This is useful for debugging when SSL certificates are invalid.\n    prefer_overviews: bool, optional\n        Default is False. If True, will pull data from an overview with the closest resolution (step size) matching the smallest resolution\n        in the request.\n    prefer_overviews_closest: bool, optional\n        Default is False. If True, will find the closest overview instead of the closest\n\n    See Also\n    --------\n    Rasterio : Interpolated rasterio datasource for general use.\n    \"\"\"",
            "\"\"\"Closes the file for the datasource\"\"\"",
            "\"\"\"{get_coordinates}\n\n        The default implementation tries to find the lat/lon coordinates based on dataset.affine.\n        It cannot determine the alt or time dimensions, so child classes may\n        have to overload this method.\n        \"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"The number of bands\"\"\"",
            "\"\"\"A description of each band contained in dataset.tags\n\n        Returns\n        -------\n        OrderedDict\n            Dictionary of band_number: band_description pairs. The band_description values are a dictionary, each\n            containing a number of keys -- depending on the metadata\n        \"\"\"",
            "\"\"\"An alternative view of band_descriptions based on the keys present in the metadata\n\n        Returns\n        -------\n        dict\n            Dictionary of metadata keys, where the values are the value of the key for each band.\n            For example, band_keys['TIME'] = ['2015', '2016', '2017'] for a dataset with three bands.\n        \"\"\"",
            "\"\"\"Return the bands that have a key equal to a specified value.\n\n        Parameters\n        ----------\n        key : str / list\n            Key present in the metadata of the band. Can be a single key, or a list of keys.\n        value : str / list\n            Value of the key that should be returned. Can be a single value, or a list of values\n\n        Returns\n        -------\n        np.ndarray\n            An array of band numbers that match the criteria\n        \"\"\"",
            "\"\"\"Rasterio datasource with interpolation.\"\"\""
        ],
        "code_snippets": [
            "class RasterioRaw(S3Mixin, BaseFileSource):\n    \"\"\"Create a DataSource using rasterio.\n\n    Attributes\n    ----------\n    source : str, :class:`io.BytesIO`\n        Path to the data source\n    dataset : :class:`rasterio._io.RasterReader`\n        A reference to the datasource opened by rasterio\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    band : int\n        The 'band' or index for the variable being accessed in files such as GeoTIFFs. Use None for all bounds.\n    crs : str, optional\n        The coordinate reference system. Normally this will come directly from the file, but this allows users to\n        specify the crs in case this information is missing from the file.\n    aws_https: bool\n        Default is True. If False, will not use https when reading from AWS. This is useful for debugging when SSL certificates are invalid.\n    prefer_overviews: bool, optional\n        Default is False. If True, will pull data from an overview with the closest resolution (step size) matching the smallest resolution\n        in the request.\n    prefer_overviews_closest: bool, optional\n        Default is False. If True, will find the closest overview instead of the closest\n\n    See Also\n    --------\n    Rasterio : Interpolated rasterio datasource for general use.\n    \"\"\"\n\n    band = tl.CInt(allow_none=True).tag(attr=True)\n    crs = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n\n    driver = tl.Unicode(allow_none=True, default_value=None)\n    coordinate_index_type = tl.Unicode()\n    aws_https = tl.Bool(True).tag(attr=True)\n    prefer_overviews = tl.Bool(False).tag(attr=True)\n    prefer_overviews_closest = tl.Bool(False).tag(attr=True)\n\n    @tl.default(\"coordinate_index_type\")",
            "def _default_coordinate_index_type(self):\n        if self.prefer_overviews:\n            return \"numpy\"\n        else:\n            return \"slice\"\n\n    @cached_property",
            "def dataset(self):\n        return self.open_dataset(self.source)\n\n    def open_dataset(self, source, overview_level=None):\n        envargs = {\"AWS_HTTPS\": self.aws_https}\n        kwargs = {}\n        if overview_level is not None:\n            kwargs = {\"overview_level\": overview_level}\n        if source.startswith(\"s3:",
            "def open_dataset(self, source, overview_level=None):\n        envargs = {\"AWS_HTTPS\": self.aws_https}\n        kwargs = {}\n        if overview_level is not None:\n            kwargs = {\"overview_level\": overview_level}\n        if source.startswith(\"s3:",
            "def _band_default(self):\n        if self.outputs is not None and self.output is not None:\n            return self.outputs.index(self.output)\n        elif self.outputs is None:\n            return 1\n        else:\n            return None  # All bands\n\n    # -------------------------------------------------------------------------\n    # public api methods\n    # -------------------------------------------------------------------------\n\n    @cached_property",
            "def nan_vals(self):\n        return np.unique(np.array(self.dataset.nodatavals).astype(self.dtype)).tolist()",
            "def close_dataset(self):",
            "def get_coordinates(self):\n        \"\"\"{get_coordinates}\n\n        The default implementation tries to find the lat/lon coordinates based on dataset.affine.\n        It cannot determine the alt or time dimensions, so child classes may\n        have to overload this method.\n        \"\"\"\n\n        # check to see if the coordinates are rotated used affine\n        affine = self.dataset.transform\n        validate_crs = True\n        if self.crs is not None:\n            crs = self.crs\n        elif isinstance(self.dataset.crs, rasterio.crs.CRS) and \"init\" in self.dataset.crs:\n            crs = self.dataset.crs[\"init\"].upper()\n            if self.dataset.crs.is_valid:\n                validate_crs = False\n        elif isinstance(self.dataset.crs, dict) and \"init\" in self.dataset.crs:\n            crs = self.dataset.crs[\"init\"].upper()\n            if self.dataset.crs.is_valid:\n                validate_crs = False\n        else:\n            try:\n                crs = pyproj.CRS(self.dataset.crs).to_wkt()\n            except pyproj.exceptions.CRSError:\n                raise RuntimeError(\"Unexpected rasterio crs '%s'\" % self.dataset.crs)\n\n        return Coordinates.from_geotransform(affine.to_gdal(), self.dataset.shape, crs, validate_crs)\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "def get_data_overviews(self, coordinates, coordinates_index):\n        # Figure out how much coarser the request is than the actual data\n        reduction_factor = np.inf\n        for c in [\"lat\", \"lon\"]:\n            crd = coordinates[c]\n            if crd.size == 1:\n                reduction_factor = 0\n                break\n            if isinstance(crd, UniformCoordinates1d):\n                min_delta = crd.step\n            elif isinstance(crd, ArrayCoordinates1d) and crd.is_monotonic:\n                min_delta = crd.deltas.min()\n            else:\n                raise NotImplementedError(\n                    \"The Rasterio node with prefer_overviews=True currently does not support request coordinates type {}\".format(\n                        coordinates\n                    )\n                )\n            reduction_factor = min(\n                reduction_factor, np.abs(min_delta / self.coordinates[c].step)  # self.coordinates is always uniform\n            )\n        # Find the overview that's closest to this reduction factor\n        if (reduction_factor < 2) or (len(self.overviews) == 0):  # Then we shouldn't use an overview\n            overview = 1\n            overview_level = None\n        else:\n            diffs = reduction_factor - np.array(self.overviews)\n            if self.prefer_overviews_closest:\n                diffs = np.abs(diffs)\n            else:\n                diffs[diffs < 0] = np.inf\n            overview_level = np.argmin(diffs)\n            overview = self.overviews[np.argmin(diffs)]\n\n        # Now read the data\n        inds = coordinates_index\n        if overview_level is None:\n            dataset = self.dataset\n        else:\n            dataset = self.open_dataset(self.source, overview_level)\n        try:\n            # read data within coordinates_index window at the resolution of the overview\n            # Rasterio will then automatically pull from the overview\n            window = (\n                ((inds[0].min()",
            "def overviews(self):\n        return self.dataset.overviews(self.band)\n\n    @property",
            "def tags(self):\n        return self.dataset.tags()\n\n    @property",
            "def subdatasets(self):\n        return self.dataset.subdatasets\n\n    @property",
            "def band_count(self):",
            "def band_descriptions(self):\n        \"\"\"A description of each band contained in dataset.tags\n\n        Returns\n        -------\n        OrderedDict\n            Dictionary of band_number: band_description pairs. The band_description values are a dictionary, each\n            containing a number of keys -- depending on the metadata\n        \"\"\"\n\n        return OrderedDict((i, self.dataset.tags(i + 1)) for i in range(self.band_count))\n\n    @cached_property",
            "def band_keys(self):\n        \"\"\"An alternative view of band_descriptions based on the keys present in the metadata\n\n        Returns\n        -------\n        dict\n            Dictionary of metadata keys, where the values are the value of the key for each band.\n            For example, band_keys['TIME'] = ['2015', '2016', '2017'] for a dataset with three bands.\n        \"\"\"\n\n        keys = {k for i in range(self.band_count) for k in self.band_descriptions[i]}  # set\n        return {k: [self.band_descriptions[i].get(k) for i in range(self.band_count)] for k in keys}",
            "def get_band_numbers(self, key, value):\n        \"\"\"Return the bands that have a key equal to a specified value.\n\n        Parameters\n        ----------\n        key : str / list\n            Key present in the metadata of the band. Can be a single key, or a list of keys.\n        value : str / list\n            Value of the key that should be returned. Can be a single value, or a list of values\n\n        Returns\n        -------\n        np.ndarray\n            An array of band numbers that match the criteria\n        \"\"\"\n        if not hasattr(key, \"__iter__\") or isinstance(key, string_types):\n            key = [key]\n\n        if not hasattr(value, \"__iter__\") or isinstance(value, string_types):\n            value = [value]\n\n        match = np.ones(self.band_count, bool)\n        for k, v in zip(key, value):\n            match = match & (np.array(self.band_keys[k]) == v)\n        matches = np.where(match)[0] + 1\n\n        return matches",
            "class Rasterio(InterpolationMixin, RasterioRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/h5py_source.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Create a DataSource node using h5py.\n\n    Attributes\n    ----------\n    source : str\n        Path to the h5py file\n    dataset : h5py.File\n        The h5py file object used to read the file\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    file_mode : str, optional\n        Default is 'r'. The mode used to open the HDF5 file. Options are r, r+, w, w- or x, a (see h5py.File).\n    data_key : str, int\n        data key, default 'data'\n    lat_key : str, int\n        latitude coordinates key, default 'lat'\n    lon_key : str, int\n        longitude coordinates key, default 'lon'\n    time_key : str, int\n        time coordinates key, default 'time'\n    alt_key : str, int\n        altitude coordinates key, default 'alt',\n    array_dims : list of str\n        dataset dims, default ['lat', 'lon', 'alt', time'], for each <dim>_key defined\n    crs : str\n        Coordinate reference system of the coordinates\n    cf_time : bool\n        decode CF datetimes\n    cf_units : str\n        units, when decoding CF datetimes\n    cf_calendar : str\n        calendar, when decoding CF datetimes\n\n    See Also\n    --------\n    H5PY : Interpolated h5py datasource for general use.\n    \"\"\"",
            "\"\"\"Closes the file.\"\"\"",
            "\"\"\"dataset coordinate dims\"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"Dataset or group key for which attributes will be summarized.\"\"\"",
            "\"\"\"h5py datasource with interpolation.\"\"\""
        ],
        "code_snippets": [
            "class H5PYRaw(FileKeysMixin, BaseFileSource):\n    \"\"\"Create a DataSource node using h5py.\n\n    Attributes\n    ----------\n    source : str\n        Path to the h5py file\n    dataset : h5py.File\n        The h5py file object used to read the file\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    file_mode : str, optional\n        Default is 'r'. The mode used to open the HDF5 file. Options are r, r+, w, w- or x, a (see h5py.File).\n    data_key : str, int\n        data key, default 'data'\n    lat_key : str, int\n        latitude coordinates key, default 'lat'\n    lon_key : str, int\n        longitude coordinates key, default 'lon'\n    time_key : str, int\n        time coordinates key, default 'time'\n    alt_key : str, int\n        altitude coordinates key, default 'alt',\n    array_dims : list of str\n        dataset dims, default ['lat', 'lon', 'alt', time'], for each <dim>_key defined\n    crs : str\n        Coordinate reference system of the coordinates\n    cf_time : bool\n        decode CF datetimes\n    cf_units : str\n        units, when decoding CF datetimes\n    cf_calendar : str\n        calendar, when decoding CF datetimes\n\n    See Also\n    --------\n    H5PY : Interpolated h5py datasource for general use.\n    \"\"\"\n\n    file_mode = tl.Unicode(default_value=\"r\").tag(readonly=True)\n    array_dims = tl.List(trait=tl.Unicode()).tag(readonly=True)\n\n    @cached_property",
            "def dataset(self):\n        return h5py.File(self.source, self.file_mode)",
            "def close_dataset(self):",
            "def dims(self):",
            "def keys(self):\n        return H5PY._find_h5py_keys(self.dataset)\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "def dataset_attrs(self, key=\"/\"):",
            "def _find_h5py_keys(obj, keys=[]):\n        # recursively find keys\n\n        if isinstance(obj, (h5py.Group, h5py.File)):\n            for k in obj.keys():\n                keys = H5PY._find_h5py_keys(obj[k], keys)\n        else:\n            keys.append(obj.name)\n            return keys\n        keys = sorted(list(set(keys)))\n        return keys",
            "class H5PY(InterpolationMixin, H5PYRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/csv_source.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Create a DataSource from a .csv file.\n\n    This class assumes that the data has a storage format such as:\n    header 1,   header 2,   header 3, ...\n    row1_data1, row1_data2, row1_data3, ...\n    row2_data1, row2_data2, row2_data3, ...\n\n    Attributes\n    ----------\n    source : str\n        Path to the csv file\n    header : int, None\n        Row number containing the column names, default 0. Use None for no header.\n    dataset : pd.DataFrame\n        Raw Pandas DataFrame used to read the data\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    data_key : str, int\n        data column number or column title, default 'data'\n    lat_key : str, int\n        latitude column number or column title, default 'lat'\n    lon_key : str, int\n        longitude column number or column title, default 'lon'\n    time_key : str, int\n        time column number or column title, default 'time'\n    alt_key : str, int\n        altitude column number or column title, default 'alt'\n    crs : str\n        Coordinate reference system of the coordinates\n\n    See Also\n    --------\n    CSV : Interpolated CSV file datasource for general use.\n    \"\"\"",
            "\"\"\"list of dataset coordinate dimensions\"\"\"",
            "\"\"\"available data keys\"\"\"",
            "\"\"\"available data keys\"\"\"",
            "\"\"\"{get_coordinates}\n\n        Note: CSV files have StackedCoordinates.\n        \"\"\"",
            "\"\"\"{get_data}\"\"\"",
            "\"\"\"CSV datasource with interpolation.\"\"\""
        ],
        "code_snippets": [
            "class CSVRaw(FileKeysMixin, LoadFileMixin, BaseFileSource):\n    \"\"\"Create a DataSource from a .csv file.\n\n    This class assumes that the data has a storage format such as:\n    header 1,   header 2,   header 3, ...\n    row1_data1, row1_data2, row1_data3, ...\n    row2_data1, row2_data2, row2_data3, ...\n\n    Attributes\n    ----------\n    source : str\n        Path to the csv file\n    header : int, None\n        Row number containing the column names, default 0. Use None for no header.\n    dataset : pd.DataFrame\n        Raw Pandas DataFrame used to read the data\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    data_key : str, int\n        data column number or column title, default 'data'\n    lat_key : str, int\n        latitude column number or column title, default 'lat'\n    lon_key : str, int\n        longitude column number or column title, default 'lon'\n    time_key : str, int\n        time column number or column title, default 'time'\n    alt_key : str, int\n        altitude column number or column title, default 'alt'\n    crs : str\n        Coordinate reference system of the coordinates\n\n    See Also\n    --------\n    CSV : Interpolated CSV file datasource for general use.\n    \"\"\"\n\n    header = tl.Any(default_value=0).tag(attr=True)\n    lat_key = tl.Union([tl.Unicode(), tl.Int()], default_value=\"lat\").tag(attr=True)\n    lon_key = tl.Union([tl.Unicode(), tl.Int()], default_value=\"lon\").tag(attr=True)\n    time_key = tl.Union([tl.Unicode(), tl.Int()], default_value=\"time\").tag(attr=True)\n    alt_key = tl.Union([tl.Unicode(), tl.Int()], default_value=\"alt\").tag(attr=True)\n    data_key = tl.Union([tl.Unicode(), tl.Int(), tl.List(trait=tl.Unicode()), tl.List(trait=tl.Int())]).tag(attr=True)\n\n    @tl.default(\"data_key\")",
            "def _default_data_key(self):\n        return super(CSVRaw, self)._default_data_key()\n\n    @tl.validate(\"data_key\")",
            "def _validate_data_key(self, d):\n        keys = d[\"value\"]\n        if not isinstance(keys, list):\n            keys = [d[\"value\"]]\n\n        if isinstance(keys[0], int):\n            for col in keys:\n                if col not in self.available_data_cols:\n                    raise ValueError(\"Invalid data_key %d, available columns are %s\" % (col, self.available_data_cols))\n        else:\n            for key in keys:\n                if key not in self.available_data_keys:\n                    raise ValueError(\"Invalid data_key '%s', available keys are %s\" % (key, self.available_data_keys))\n\n        return d[\"value\"]\n\n    @tl.default(\"outputs\")",
            "def _default_outputs(self):\n        if not isinstance(self.data_key, list):\n            return None\n        else:\n            return [self._get_key(elem) for elem in self.data_key]\n\n    # -------------------------------------------------------------------------\n    # public api methods\n    # -------------------------------------------------------------------------",
            "def open_dataset(self, f):\n        return pd.read_csv(f, parse_dates=True, infer_datetime_format=True, header=self.header)\n\n    @cached_property",
            "def dims(self):",
            "def keys(self):",
            "def available_data_keys(self):",
            "def available_data_cols(self):\n        return [self._get_col(key) for key in self.available_data_keys]\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_coordinates(self):\n        \"\"\"{get_coordinates}\n\n        Note: CSV files have StackedCoordinates.\n        \"\"\"\n\n        coords = super(CSVRaw, self).get_coordinates()\n        if len(coords) == 1:\n            return coords\n        stacked = StackedCoordinates(list(coords.values()))\n        return Coordinates([stacked], validate_crs=False, **coords.properties)\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_data(self, coordinates, coordinates_index):",
            "def _lookup_key(self, dim):\n        lookup = {\"lat\": self.lat_key, \"lon\": self.lon_key, \"alt\": self.alt_key, \"time\": self.time_key}\n        return self._get_key(lookup[dim])",
            "def _get_key(self, key):\n        return self.dataset.columns[key] if isinstance(key, int) else key",
            "def _get_col(self, key):\n        return key if isinstance(key, int) else self.dataset.columns.get_loc(key)",
            "class CSV(InterpolationMixin, CSVRaw):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/file_source.py",
        "comments": [
            "//\"):",
            "//\") or self.source.startswith(\"https://\"):",
            "//\"):",
            "//\"):"
        ],
        "docstrings": [
            "\"\"\"\nDatasources from files\n\"\"\"",
            "\"\"\"\n    Base class for data sources loaded from file.\n\n    Attributes\n    ----------\n    source : str\n        Path to the data source.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    dataset : Any\n        dataset object\n    \"\"\"",
            "\"\"\"Close opened resources. Subclasses should implement if appropriate.\"\"\"",
            "\"\"\"\n    Mixin to load and cache files using various transport protocols.\n\n    Attributes\n    ----------\n    cache_dataset : bool\n        Default is False. Whether to cache the dataset after loading (as an optimization).\n    \"\"\"",
            "\"\"\"TODO\"\"\"",
            "\"\"\"\n    Mixin to specify data and coordinates dimensions keys.\n\n    Attributes\n    ----------\n    lat_key : str\n        latitude key, default 'lat'\n    lon_key : str\n        longitude key, default 'lon'\n    time_key : str\n        time key, default 'time'\n    alt_key : str\n        altitude key, default 'alt'\n    data_key : str, list\n        data key, or list of data keys for multiple-output nodes\n    crs : str\n        Coordinate reference system of the coordinates.\n    cf_time : bool\n        decode CF datetimes\n    cf_units : str\n        units, when decoding CF datetimes\n    cf_calendar : str\n        calendar, when decoding CF datetimes\n    \"\"\"",
            "\"\"\"list of attribute names, used by __repr__ and __str__ to display minimal info about the node\"\"\"",
            "\"\"\"available data keys\"\"\"",
            "\"\"\"{get_coordinates}\"\"\""
        ],
        "code_snippets": [
            "class BaseFileSource(DataSource):\n    \"\"\"\n    Base class for data sources loaded from file.\n\n    Attributes\n    ----------\n    source : str\n        Path to the data source.\n    coordinates : :class:`podpac.Coordinates`\n        {coordinates}\n    dataset : Any\n        dataset object\n    \"\"\"\n\n    source = tl.Unicode().tag(attr=True, required=True)\n\n    # list of attribute names, used by __repr__ and __str__ to display minimal info about the node\n    _repr_keys = [\"source\"]\n\n    @tl.default(\"source\")",
            "def _default_source(self):\n        raise ValueError(\"%s 'source' required\" % self.__class__.__name__)\n\n    # -------------------------------------------------------------------------\n    # public api properties and methods\n    # -------------------------------------------------------------------------\n\n    @property",
            "def dataset(self):\n        raise NotImplementedError()",
            "def close_dataset(self):",
            "class LoadFileMixin(S3Mixin):\n    \"\"\"\n    Mixin to load and cache files using various transport protocols.\n\n    Attributes\n    ----------\n    cache_dataset : bool\n        Default is False. Whether to cache the dataset after loading (as an optimization).\n    \"\"\"\n\n    cache_dataset = tl.Bool(False)\n    dataset_expires = tl.Any()\n    _file = None\n\n    @tl.validate(\"dataset_expires\")",
            "def _validate_dataset_expires(self, d):\n        expiration_timestamp(d[\"value\"])\n        return d[\"value\"]\n\n    @cached_property",
            "def _dataset_caching_node(self):\n        # stub node containing only the source node attr\n        return BaseFileSource(source=self.source, cache_ctrl=self.cache_ctrl)\n\n    @cached_property",
            "def dataset(self):\n        # get from the cache\n        # use the _dataset_caching_node \"stub\" here because the only node attr we care about is the source\n        if self.cache_dataset and self._dataset_caching_node.has_cache(key=\"dataset\"):\n            data = self._dataset_caching_node.get_cache(key=\"dataset\")\n            self._file = BytesIO(data)\n            return self._open(self._file, cache=False)\n\n        # otherwise, open the file (and cache it if desired)\n        if self.source.startswith(\"s3:",
            "def _open(self, f, cache=True):\n        if self.cache_dataset and cache:\n            self._dataset_caching_node.put_cache(f.read(), key=\"dataset\", expires=self.dataset_expires)\n            f.seek(0)\n        return self.open_dataset(f)",
            "def open_dataset(self, f):",
            "def close_dataset(self):\n        if self._file is not None:\n            self._file.close()\n\n\n@common_doc(COMMON_DATA_DOC)",
            "class FileKeysMixin(tl.HasTraits):\n    \"\"\"\n    Mixin to specify data and coordinates dimensions keys.\n\n    Attributes\n    ----------\n    lat_key : str\n        latitude key, default 'lat'\n    lon_key : str\n        longitude key, default 'lon'\n    time_key : str\n        time key, default 'time'\n    alt_key : str\n        altitude key, default 'alt'\n    data_key : str, list\n        data key, or list of data keys for multiple-output nodes\n    crs : str\n        Coordinate reference system of the coordinates.\n    cf_time : bool\n        decode CF datetimes\n    cf_units : str\n        units, when decoding CF datetimes\n    cf_calendar : str\n        calendar, when decoding CF datetimes\n    \"\"\"\n\n    data_key = tl.Union([tl.Unicode(), tl.List(trait=tl.Unicode())]).tag(attr=True)\n    lat_key = tl.Unicode(default_value=\"lat\").tag(attr=True)\n    lon_key = tl.Unicode(default_value=\"lon\").tag(attr=True)\n    time_key = tl.Unicode(default_value=\"time\").tag(attr=True)\n    alt_key = tl.Unicode(default_value=\"alt\").tag(attr=True)\n    crs = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    cf_time = tl.Bool(default_value=False).tag(attr=True)\n    cf_units = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    cf_calendar = tl.Unicode(allow_none=True, default_value=None).tag(attr=True)\n    skip_validation = tl.Bool(False).tag(attr=True)\n\n    @property",
            "def _repr_keys(self):",
            "def _default_data_key(self):\n        if len(self.available_data_keys) == 1:\n            return self.available_data_keys[0]\n        else:\n            return self.available_data_keys\n\n    @tl.validate(\"data_key\")",
            "def _validate_data_key(self, d):\n        keys = d[\"value\"]\n        if self.skip_validation:\n            return keys\n        if not isinstance(keys, list):\n            keys = [d[\"value\"]]\n        for key in keys:\n            if key not in self.available_data_keys:\n                raise ValueError(\"Invalid data_key '%s', available keys are %s\" % (key, self.available_data_keys))\n        return d[\"value\"]\n\n    @tl.default(\"outputs\")",
            "def _default_outputs(self):\n        if not isinstance(self.data_key, list):\n            return None\n        else:\n            return self.data_key\n\n    @tl.validate(\"outputs\")",
            "def _validate_outputs(self, d):\n        value = d[\"value\"]\n        if self.skip_validation:\n            return value\n        if not isinstance(self.data_key, list):\n            if value is not None:\n                raise TypeError(\"outputs must be None for single-output nodes\")\n        else:\n            if value is None:\n                raise TypeError(\"outputs and data_key mismatch (outputs=None, data_key=%s)\" % self.data_key)\n            if len(value) != len(self.data_key):\n                raise ValueError(\"outputs and data_key size mismatch (%d != %d)\" % (len(value), len(self.data_key)))\n        return value\n\n    # -------------------------------------------------------------------------\n    # public api properties and methods\n    # -------------------------------------------------------------------------\n\n    @property",
            "def keys(self):\n        raise NotImplementedError\n\n    @property",
            "def dims(self):\n        raise NotImplementedError\n\n    @cached_property",
            "def available_data_keys(self):",
            "def _lookup_key(self, dim):\n        lookup = {\"lat\": self.lat_key, \"lon\": self.lon_key, \"alt\": self.alt_key, \"time\": self.time_key}\n        return lookup[dim]\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_coordinates(self):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/reprojection.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Create a DataSource with a different resolution from another Node. This can be used to bilinearly interpolated a\n    dataset after averaging over a larger area.\n\n    Attributes\n    ----------\n    source : Node\n        The source node\n    source_interpolation : str\n        Type of interpolation method to use for the source node\n    reprojected_coordinates : :class:`podpac.Coordinates`\n        Coordinates where the source node should be evaluated.\n    \"\"\"",
            "\"\"\"{get_coordinates}\"\"\"",
            "\"\"\"{get_data}\"\"\""
        ],
        "code_snippets": [
            "class ReprojectedSource(DataSource):\n    \"\"\"Create a DataSource with a different resolution from another Node. This can be used to bilinearly interpolated a\n    dataset after averaging over a larger area.\n\n    Attributes\n    ----------\n    source : Node\n        The source node\n    source_interpolation : str\n        Type of interpolation method to use for the source node\n    reprojected_coordinates : :class:`podpac.Coordinates`\n        Coordinates where the source node should be evaluated.\n    \"\"\"\n\n    source = NodeTrait().tag(attr=True, required=True)\n    source_interpolation = InterpolationTrait().tag(attr=True)\n    reprojected_coordinates = tl.Instance(Coordinates).tag(attr=True, required=True)\n\n    # list of attribute names, used by __repr__ and __str__ to display minimal info about the node\n    _repr_keys = [\"source\", \"interpolation\"]",
            "def _first_init(self, **kwargs):\n        warnings.warn(\n            \"ReprojectedSource has been replaced by the Reproject algorithm node \"\n            \"and will be removed in a future version of podpac.\",\n            DeprecationWarning,\n        )\n\n        if \"reprojected_coordinates\" in kwargs:\n            if isinstance(kwargs[\"reprojected_coordinates\"], dict):\n                kwargs[\"reprojected_coordinates\"] = Coordinates.from_definition(kwargs[\"reprojected_coordinates\"])\n            elif isinstance(kwargs[\"reprojected_coordinates\"], string_types):\n                kwargs[\"reprojected_coordinates\"] = Coordinates.from_json(kwargs[\"reprojected_coordinates\"])\n\n        return super(ReprojectedSource, self)._first_init(**kwargs)\n\n    @cached_property",
            "def eval_source(self):\n        if self.source_interpolation is not None and not self.source.has_trait(\"interpolation\"):\n            _logger.warning(\n                \"ReprojectedSource cannot set the 'source_interpolation'\"\n                \" since 'source' does not have an 'interpolation' \"\n                \" trait. \\n type(source): %s\\nsource: %s\" % (str(type(self.source)), str(self.source))\n            )\n\n        source = self.source\n        if (\n            self.source_interpolation is not None\n            and self.source.has_trait(\"interpolation\")\n            and self.source_interpolation != self.source.interpolation\n        ):\n            source = copy.deepcopy(source)\n            source.set_trait(\"interpolation\", self.source_interpolation)\n\n        return source\n\n    @common_doc(COMMON_DATA_DOC)",
            "def get_coordinates(self):",
            "def get_data(self, coordinates, coordinates_index):",
            "def base_ref(self):\n        return \"{}_reprojected\".format(self.source.base_ref)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_dataset.py",
        "comments": [],
        "docstrings": [
            "\"\"\"test xarray dataset source\"\"\""
        ],
        "code_snippets": [
            "class TestDataset(object):",
            "def test_init_and_close(self):\n        node = Dataset(source=self.source, time_key=\"day\")\n        node.close_dataset()",
            "def test_dims(self):\n        node = Dataset(source=self.source, time_key=\"day\")\n        assert np.all([d in [\"time\", \"lat\", \"lon\"] for d in node.dims])\n\n        # un-mapped keys\n        # node = Dataset(source=self.source)\n        # with pytest.raises(ValueError, match=\"Unexpected dimension\"):\n        #     node.dims",
            "def test_available_data_keys(self):\n        node = Dataset(source=self.source, time_key=\"day\")\n        assert node.available_data_keys == [\"data\", \"other\"]",
            "def test_coordinates(self):\n        # specify dimension keys\n        node = Dataset(source=self.source, time_key=\"day\")\n        nc = node.coordinates\n        # assert nc.dims == (\"time\", \"lat\", \"lon\")  # Xarray is free to change this order -- we don't care\n        np.testing.assert_array_equal(nc[\"lat\"].coordinates, self.lat)\n        np.testing.assert_array_equal(nc[\"lon\"].coordinates, self.lon)\n        np.testing.assert_array_equal(nc[\"time\"].coordinates, self.time)\n        node.close_dataset()",
            "def test_get_data(self):\n        # specify data key\n        node = Dataset(source=self.source, time_key=\"day\", data_key=\"data\")\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out.transpose(\"time\", \"lat\", \"lon\"), self.data)\n        node.close_dataset()\n\n        node = Dataset(source=self.source, time_key=\"day\", data_key=\"other\")\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out.transpose(\"time\", \"lat\", \"lon\"), self.other)\n        node.close_dataset()",
            "def test_get_data_array_indexing(self):\n        node = Dataset(source=self.source, time_key=\"day\", data_key=\"data\")\n        out = node.eval(node.coordinates.transpose(\"time\", \"lat\", \"lon\")[:, [0, 2]])\n        np.testing.assert_array_equal(out, self.data[:, [0, 2]])\n        node.close_dataset()",
            "def test_get_data_multiple(self):\n        node = Dataset(source=self.source, time_key=\"day\", data_key=[\"data\", \"other\"])\n        out = node.eval(node.coordinates.transpose(\"time\", \"lat\", \"lon\"))\n        assert out.dims == (\"time\", \"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"data\", \"other\"])\n        np.testing.assert_array_equal(out.sel(output=\"data\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"other\"), self.other)\n        node.close_dataset()\n\n        # single\n        node = Dataset(source=self.source, time_key=\"day\", data_key=[\"other\"])\n        out = node.eval(node.coordinates.transpose(\"time\", \"lat\", \"lon\"))\n        assert out.dims == (\"time\", \"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"other\"])\n        np.testing.assert_array_equal(out.sel(output=\"other\"), self.other)\n        node.close_dataset()\n\n        # alternate output names\n        node = Dataset(source=self.source, time_key=\"day\", data_key=[\"data\", \"other\"], outputs=[\"a\", \"b\"])\n        out = node.eval(node.coordinates.transpose(\"time\", \"lat\", \"lon\"))\n        assert out.dims == (\"time\", \"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"a\", \"b\"])\n        np.testing.assert_array_equal(out.sel(output=\"a\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"b\"), self.other)\n        node.close_dataset()\n\n        # default\n        node = Dataset(source=self.source, time_key=\"day\")\n        out = node.eval(node.coordinates.transpose(\"time\", \"lat\", \"lon\"))\n        assert out.dims == (\"time\", \"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"data\", \"other\"])\n        np.testing.assert_array_equal(out.sel(output=\"data\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"other\"), self.other)\n        node.close_dataset()",
            "def test_extra_dimension_selection(self):\n        # default\n        node = Dataset(source=self.source, time_key=\"day\", data_key=\"data\")\n        assert node.selection is None\n\n        # treat day as an \"extra\" dimension, and select the second day\n        node = Dataset(source=self.source, data_key=\"data\", selection={\"day\": 1})\n        assert np.all([d in [\"lat\", \"lon\"] for d in node.dims])\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out, self.data[1].T)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_wcs.py",
        "comments": [
            "//maps.isric.org/mapserv?map=/map/sand.map\""
        ],
        "docstrings": [
            "\"\"\"Mocked WCS client that handles three getCoverage cases that are needed by the tests below.\"\"\"",
            "\"\"\"Test node that uses the MockClient above.\"\"\"",
            "\"\"\"Test node that uses the MockClient above, and injects podpac interpolation.\"\"\""
        ],
        "code_snippets": [
            "class MockClient(object):",
            "def getCoverage(self, **kwargs):\n        if kwargs[\"width\"] == 100 and kwargs[\"height\"] == 100:\n            return BytesIO(\n                b\"II*\\x00\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x01\\x03\\x00\\x01\\x00\\x00\\x00d\\x00\\x00\\x00\\x01\\x01\\x03\\x00\\x01\\x00\\x00\\x00d\\x00\\x00\\x00\\x02\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x03\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x06\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x15\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1a\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\xfe\\x00\\x00\\x00\\x1b\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\x06\\x01\\x00\\x00\\x1c\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00(\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00=\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00B\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00C\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00D\\x01\\x04\\x00\\x01\\x00\\x00\\x00\\xe6\\x01\\x00\\x00E\\x01\\x04\\x00\\x01\\x00\\x00\\x00\\xad\\n\\x00\\x00S\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xd8\\x85\\x0c\\x00\\x10\\x00\\x00\\x00\\x0e\\x01\\x00\\x00\\xaf\\x87\\x03\\x00 \\x00\\x00\\x00\\x8e\\x01\\x00\\x00\\xb0\\x87\\x0c\\x00\\x02\\x00\\x00\\x00\\xce\\x01\\x00\\x00\\xb1\\x87\\x02\\x00\\x08\\x00\\x00\\x00\\xde\\x01\\x00\\x00\\x00\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00F\\x029\\x9f\\xa4\\xa1\\xe9?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00J\\x82\\x8fv\\xb0\\xa9`\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf26`\\xb3Gz\\xd3?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x9f\\xa0>%z7@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x01\\x00\\x01\\x00\\x00\\x00\\x07\\x00\\x00\\x04\\x00\\x00\\x01\\x00\\x02\\x00\\x01\\x04\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x08\\x00\\x00\\x01\\x00\\xe6\\x10\\x01\\x08\\xb1\\x87\\x07\\x00\\x00\\x00\\x06\\x08\\x00\\x00\\x01\\x00\\x8e#\\t\\x08\\xb0\\x87\\x01\\x00\\x01\\x00\\x0b\\x08\\xb0\\x87\\x01\\x00\\x00\\x00\\x88mt\\x96\\x1d\\xa4r@\\x00\\x00\\x00@\\xa6TXAWGS 84|\\x00x^\\xed\\x9bypU\\xd5\\x1d\\xc7\\xcfK$+/\\x84$$B00\\x01\\x92\\x82\\x18\\x13\\x08\\x892M\\x14B5\\xc4@\\x1d\\x86\\x01\\xa4\\x80\\xd0\\x8cc\\x19\\x1b\\xa0X\\x96\\nXp\\xa6S\\x8dC\\x85\\xa1VF@\\xa9X+\\xadE\\x8b\\x10Y\\x1ai\\xd3\\xa4\\x02\\x06\\xd4D\\x89\\xefE I\\x13\\xb2\\x02%{x\\xf0n\\xcfr\\x97s\\x97\\x87\\xa9\\xd3?r\\xcf\\xfb\\x9d?\\xf2\\xee;K\\xe6~?\\xbf\\xe5,\\xf7>\\x84|\\x17\\xc9\\x81\\xdb$\\xae\\x9d|\\xf7\\xa3\\xe2\\xe7\\xfa\\x07.\\xbf5VD\\xaf\\xf0s\\xfd\\x122\\x86?\\xad\\xf0\\x9bB\\xf5\\xf3\\xd9\\x0fY\\xea\\xef\\r\\x15\\x95\\x08\\xf6\\xffo\\x97\\x8fh\\x94\\x08YL\\xfa\\xad\\xec\\xcf\\xa2DD\\xfd\\xc4\\xb0z\\xfb[\\xeb\\xb7\\xaa\\x15\\x81\\xc7\\xc0\\xf4cDb\\x06\\x80E\\xfa\\xb3TJ\\xa2D@\\x02\\xd8\\xf3\\x8d\\xe9\\xcf2\\xff\\xd3~\\xe2\\x15\\x12\\xf8\\x03\\xd4/\\xa2|\\x9c\\xf9\\x88,C\\xfa\\xb3\\xb0\\xb4\\xe4\\x10\\xd1\\xf9\\x89t\\xd0o\\xb4>\\r\\tC\\x11\\xd9\\xfe\\x03\\xd1/;\\x8a\\x11\\x8b\\xfd\\xbfc\\xc3Z\\x888\\xd0\\xfb\\xb4\\xbeV\\xd0\\xf4OV5F\\xfd}!\\x08]iM#\\xfa\\xbb\\xc3\\x15\\n\\x82\\xae~\\x88,\\xa3~\\x84\\\\\\xc95Ig3\\xf2\\x8a\\xe7\\\\\\xfe\\xd2\\x9f\\xf5wF\\x8a\\xaf\\xdf\\xbc\\xf7\\xe9:\\xb2\\xe8\\xcd\\x158*>\\xfbW\\xcd\\x91\\x9e&j\\x7f\\x92#L3\\x82>?\\xd8\\xf4\\x9bi\\xef\\xe3\\xfa2`z\\x9c\\x9c\\x12\\x1e\\xbf0\\xebw\\xea\\xea@H\\xfd\\xa6\\xbd\\x8f\\xe48\\xbe\\x7fM\\xe6\\x9e\\xa7hV\\\\\\xb6q\\x92bVAW\\x7f\\xe6\\xf4\\xd7[5a\\xb82%\\x1c\\\\\\xc8y\\xb5\\xa8\\xf67g\\x7f\\xd4\\xb5\\xcf\\xf9c\\xbc\\x04\\xc4-\\xaf\\xad\\xd4\\x08\\x88\\x08\\xc0b\\xf6\\xc7\\x82\\xfdI\\xbfe\\xde\\xbe1\\x8cV\\xfflG\\x87Sh\\xfb[\\xf8>\\xd3\\xbb\\xe1%\\xf2Y\\xf4'\\xc7\\xb6<m}(^\\x00X\\xeb\\xbf\\x1aM!\\xe0\\xec\\xf7PX\\xfc^\\x92\\x04\\xe9|X\\xf8[\\xcd\\x17D\\xb9\\xb2\\x04P\\x9f@\\x05\\xff\\xe3!\\xf7\\x85s\\x01\\x0f\\xa3\\x19T,\\xa9\\xf1\\x13\\x07\\xe0\\xf4\\xefI\\x17Z?3\\xff\\x95Q&w>\\xfe(\\xa9j}\\xad2c\\x03kc+B\\xc1\\x1c\\xc0z\\xf6ChW!\\xfaO\\xe4\\x8d\\xbeXG\\x91t\\xf5\\xa7\\t\\x1c\\x1c\\xc1\\xf4[\\xec|\\xa9\\xd8\\xd2l\\xa6\\xf9\\xeb\\x1a\\xf7\\x17\\x8b\\xf2Du\\x00\\xeb\\x83\\x1f\\xac\\xf6\\xf3\\xb0$\\xa6\\xf9\\x99+?\\xfcj\\xea\\\"]x\\x88\\xe3\\x01r\\xeaO\\xa8\\xe7\\xfc\\x9b\\xbf\\xbc\\x1d\\x88>\\xae\\xa8\\xed)\\x8a\\xd9]\\xf7t\\xa2\\xbc!\\xc4\\xed\\xc2\\x00\\xf8_\\xf4w\\xa6\\xaa\\x13\\xa5h\\xfa5\\x93S\\x1b\\xd7\\xba\\x1fQk>\\xef\\x0f*n\\n\\xbf\\x9e?\\x17y\\x03\\xd4JQ\\xf4\\xfb\\xca\\xfd\\x8a\\xd0\\x86\\xd1\\xf8\\xeaX\\xcb\\xce\\xe0'C\\xbe\\x97\\x1a\\x82\\x83A)\\x82\\x00\\xa8\\xad\\x99\\xa5\\x99\\xde\\xe2\\xea\\xeb\\xaet\\x84\\xce\\xecN>z\\xf2\\x0fI=\\x8f\\xa2\\xfe`\\xb9\\x8f \\xf2\\xa5\\x0f\\xe7X\\x88&U\\xbf\\x7f\\x12/\\xfd'\\xdd\\xe7xn\\xad\\xfbA\\xfcu\\xda\\xe6\\x0b\\xcf\\x95Ow4\\xc7\\xb1\\xee\\xc2\\x9c\\x03\\xa9\\t\\xcd\\n\\xc3+\\xcb\\x87\\xbf\\xd1xq?B\\xef,\\x9e?\\xae\\xa9o\\x07\\xb7@\\x14\\xc4\\xfe\\xbe\\x96>\\x8c\\x86\\xaa\\xdf\\x9d4\\x7f\\xdc%\\x87\\x80\\xfau\\x8b\\x1fuvW\\x16~E\\xeb\\xd1\\xb6\\x86\\xec\\xe0\\x05\\x08m?6:>4+\\xe8\\x01\\xd5K\\x04\\xb1\\xbft\\xeb.+\\xc7G\\xf8\\xe4\\x97\\x14\\xcfo*vtV\\xdf\\x93\\xde\\xf4\\xe0\\xe4\\xa0a\\xe1\\xd3+_\\xc4uG\\x83~\\x80\\xff\\n\\xa2\\xff\\xc3|\\xed\\\\G\\x07\\xa2\\xd3\\xf9\\xf2:\\x9c\\x00?\\xea.\\xc8m\\x1ay\\xee\\xc4!\\xe7\\x9a\\xbd\\x19\\x11\\xfd\\x91\\x18\\xcb\\x99L\\xccL\\x10\\xf9R\\xf5DK\\xf3\\xa3\\x9a\\xe2\\xd5+&\\xad[\\xd0>\\xf3\\xaf\\x93\\x12\\xb7\\xee[\\xb1\\xaalJv[G\\xdd[O\\xbd\\xaet\\x17\\x03\\x80$\\x9fq\\x99!\\xb8\\xab\\\\\\x93\\xe7\\xbeZ\\\\\\x7f3z\\xd8\\xbc\\x8b\\xbd\\xae\\x0c)\\xbc>\\xba>,*l\\xf3\\x8d\\x08\\xda\\xd9\\x13d\\x1ec\\xbf\\x9a\\x01\\xe8\\x1f\\xe1\\xb4\\xd6?D\\x00\\x0f\\x90\\xd0\\xa9\\x19>\\x8cV\\xf2E\\xdf\\xd0\\xfc_\\x8e+\\x99\\xb5u6\\x9aR\\xbbvW\\xc6\\x98\\xb0\\xb2NOH6\\x1a?\\xbe\\xa74\\x97\\x9c\\x03\\n\\xa1\\x7f_\\x81\\x0f\\xfd'\\x1b\\x82\\xd2V?\\xfbq\\xc4\\xe6y\\xb1\\r\\t\\x15\\x8d\\x81S\\x93_,\\xfc\\xc5\\x07\\xcf\\xbc\\xe7\\xbc\\xd2\\xf7\\x13\\x84*\\xa6\\t\\xf1\\x0e\\xa4\\x84\\xceO\\xe1_\\xee\\x90Y\\x90u@\\xedXt\\xf8\\x8dxWr\\xab\\xb3z\\xc5\\x9f\\xa3\\xbb\\xcf\\xe4\\xe4\\x04_\\xcc\\x8c\\xbf\\xffLO\\xfbc\\xaeT\\x84\\xae\\xb6'\\x0b0\\x07\\xd0\\xc5\\xafv\\xa6\\xa1w\\x85\\xc6\\xca\\x03\\x89C\\x8b\\xefo\\xcd_:g\\xd3\\x03i\\xc1\\x0b\\x9bz\\x97\\xdfN)\\xfd\\xea\\xf2\\xae\\x9d\\xab\\xd80\\xfb\\x07\\x80\\x9f\\xeb\\xbf\\xe3\\xde\\x07\\xb5\\xfd{\\xfb\\x84\\xde\\xfeOF\\xaf\\x9c\\xd5\\xfc#\\xef\\xa9\\x99\\xa3Rn\\xa6uD\\xb4\\xf4y\\xafv\\x17$\\xe3\\xeda\\xfa\\xbd\\x82\\xd8_\\xf5\\xfa\\xc6x\\xf5\\x92\\xc4DWk\\xf8\\x86\\xd0\\xf2\\x88\\xd8\\xa0\\xcb)\\xb7\\xfe\\xf6^Y\\xf9!\\xf4\\xbasb]\\xc7R\\xb7+3\\x16\\x95\\xe4\\x90\\xc8\\xd1\\x07\\x8c\\xfd\\xbe\\xe9\\x1d\\xa0\\xf9n}.\\xb8\\x16U\\xb5\\xf7\\xe7\\xcb\\\"k\\xbf\\x7fdd\\xdf\\xd8\\xa0w\\xb1\\xbc\\x0fn%wx\\xb3\\x96\\xbd0\\xf6Z\\x94\\x10)@\\xa7\\xdf*\\x0f\\xae\\xaek\\xba\\xfb^o\\xeb\\x85\\x90\\xbf/^\\x8b\\xcf\\x81\\x10zy=\\xaai\\x17f\\x17\\xa8\\xb7?\\x99\\x0b\\r%\\xf7\\xf6\\xb5){\\n'\\x06\\x9en\\xf1\\x1e\\xc7M\\xec\\xed\\xaf\\xd2Lm\\xedk\\xeb\\x100\\xe4\\xbf\\xef\\xa0\\xdf\\xd6\\xf2\\x8dg?\\x16\\xcf?\\x17\\x84\\x95m]\\xf2\\xfc\\xf2\\xc4\\x8d\\x1fm\\x8c\\xcc#\\xfd\\xe9\\xb2W\\xc2\\xef\\x85\\xe0dI\\x02\\xc6\\xd6\\x00\\x8c\\xf3\\xdf\\xb9\\xa9z\\xf7\\xef\\xf4V\\xbc\\xd5~\\xb4\\xea>\\x84\\xe6\\x05\\xcei\\xde@\\xf5\\xd3\\x18\\xb8\\xde;J\\xea\\r\\xc5\\xfam-\\x1fu\\x87\\xe9\\xf5\\xbe?;TFr`)i9\\x1c\\xf8\\xd8\\xaf\\xa2\\xc8\\x9b_\\xb5\\xa7\\xca\\xe3_\\xa0\\xf1\\xcf\\x8a\\xa3G\\xf9\\r\\xa8\\xbd\\x010=J\\xe2?\\x9fJ\\x1f\\xef\\xe0\\x17]\\xba\\xea\\xe2#\\xd1\\xfb\\x07\\ng\\x9c\\xdf\\xb9\\x10?\\xf7]5\\xafb\\xdd\\x96\\x193\\xb1\\xfcw\\xc7\\x90\\xb3p\\x12\\x04*\\n\\x19\\x89-?\\xf4\\x01p'\\xfd\\xa9\\xa5B\\xeb\\xc7\\xaf\\xba\\xe8\\xcb\\xe9\\xb3\\xb1\\xc5\\xc1O\\xe4\\xac\\xccm+@uu'\\xda\\xf2\\x13\\x86\\x8f!\\xe1O-O\\x9c^FW\\x96eK\\xc3\\xb3\\x9b\\xd6\\x9b\\x9f\\x13\\xe2N\\xda\\xe2\\xf9&\\xdeu\\x0c=\\xfb\\xbc3\\xa0(\\xaai\\x0bBuCcHw\\x92\\xfd)\\x06m,\\xad\\xb2g\\x1e\\xb0\\x06\\xd0\\xe9D\\xeb\\x8bV\\xd6\\xafK\\xe9\\x18\\xfb\\xcf,\\xe4h\\x8b\\xc1b?\\x9dFS?\\x9d\\x00\\x98vm\\xb6T\\\\\\xc2~\\x9e`\\xad\\xbfm\\xdb\\xf8\\x96_\\x1f\\xec*\\xa8ZSB\\x9e\\xfe:\\xa4\\xd2\\xec\\x93\\xf8\\xc8\\x9f\\xea\\xa7v6\\x0cS\\xc0\\xd8O\\xff7qC\\xf1k.\\xf88\\x87+\\xf2O}\\xdc\\xf8\\xdd\\x97w\\x16\\xf3\\r\\xaa\\x9f\\xab\\xf2\\xbb\\x9c\\xf4\\x92\\xad\\x89d6\\xb6\\x820`\\xfdxk\\xc8B\\x9fK}x\\x87\\xac\\xe8\\xe7|\\xc3V\\xfa\\xe5P\\xf6\\x0c\\xe1\\xee\\x9a\\x18^.\\xdc\\xb31~\\xadg\\x1154%\\xd8/\\x07*J\\xfa\\xab\\xf9\\x18\\xd0~\\xecf6\\xa6\\x1c\\xeb&\\x04,-\\xda\\x8c\\x80t\\x82\\xbd\\xe4\\xf3\\xc7\\xf2\\x91\\x9b4\\xa97\\xc9\\xee\\xf6\\xf0\\\\\\x84N\\xb3}>7\\xd5\\xb1\\xec\\xcf\\x18\\xc8\\x03\\xe4V\\xe2\\x1f\\xf6{\\\"pv\\x1aQ\\xe1N\\xda\\xffx$\\x93\\xd3\\x1eS\\xed\\r\\x08\\xf0Ln\\x1bA\\xd2\\x1aB\\xb9\\xc7X=\\xb9\\xd6\\x85>m\\xd5\\n\\x9b\\x16\\x14\\xf3\\xdb\\x06\\x04\\xd3\\x80\\xf5/\\x97\\xa5|g\\xfdt[l3\\xf7'wL\\t\\xb8^\\xd9-\\xeb\\xef\\xf2\\xb8{\\x1fF\\x9e!4\\x06p\\x19wQ\\x9f\\xd9d\\xa3\\xcb\\xd1N\\x9c\\x82\\xd5\\xf0\\xca\\xf1?\\xb5\\t\\x08\\xc5\\x85\\x1b\\xef\\x8a\\xc3/:\\x131\\xde\\xcaKA)a\\xb7;&\\xc8\\xc2\\xd2>c\\x17\\xb2 E\\xbe\\xce\\xf7)(\\x1d\\x00\\xbb\\xcc\\x04\\xbc\\x8cK\\x89\\x08U\\xa6\\x10)\\xfb\\xc2\\x9e\\xc0\\xcb]\\x17>\\xe3G\\xcd\\xcbN\\xa8\\t\\x9f88\\x95\\x8a\\xae\\x0fg\\x9f\\xf4\\xaf\\xce?T\\xf7\\xe0:\\x0c\\xe2\\xcb\\xff\\xbb~\\x85\\x90M\\xfc\\x1f\\x9b\\x86  &\\xa4\\xbb\\x99\\x86\\x90\\x18t\\xf0\\x13\\xcf\\xab\\x9a\\xc9\\x96\\xbcM\\xafq\\x0f\\\"\\x89\\xf2R\\\"^\\xb15\\xab\\x94\\x87\\xc8\\t\\xc5\\x1e\\xfa\\x89(\\xde\\x05\\xb0\\x06\\xba\\xe2#\\xbf\\xfb\\xd7\\x17\\xe6\\xe4\\xfa\\xbeJ`\\xe8w?\\x94\\xa7q\\xf8 \\xfdN\\x96,\\xca\\xad\\xb1\\x07:H\\x9d\\x08\\xd5[&\\xf2H7\\xa5\\xab6\\x84v\\xa1\\x9eA\\x9a\\xe5\\x01:o\\x18\\xa4\\xba\\xe5\\xdbj\\x8ff\\xd3\\x98\\xfc\\xf5\\xd2=\\xfc.@\\xa7\\x9f\\n5\\xb8\\n\\xe3\\xa0\\x19_>\\x02!\\x01`\\x13\\xfbS\\xcbi\\xe6$\\x13\\x00\\xe7\\x10\\n\\x00E\\xb7h\\xfa9\\x976\\xbbiN\\t\\xad\\x93\\xfe2\\xdf\\xa7\\xe3\\xf3\\x838\\x83s\\xa1`\\xfe\\xb7\\x83\\xa9\\xc6\\x87~\\xfa>\\\\^\\xc8!z\\xab\\xd89\\xca\\xb2\\xd4\\x14\\xa1\\xbb{>%(\\xd1O\\x97}6Y\\xfbyN\\xce\\xd6y\\xbf/\\xd3\\x9cK\\xd5~\\xf1`\\xe8c\\x88\\x08J\\xc4o\\xf4+y_cBp\\xd8G?\\x9f\\xf9\\xf8\\xacGt\\xa9\\x1e\\xef\\x906\\xe5\\xcc\\x94\\x03\\x81D\\x83\\xd6\\\"\\xa7\\x05\\xa5Bm\\x90!\\xf8\\xf2\\xa6AT\\xef#\\xfe\\rA\\xfe\\xf6\\x12\\x83l:\\xe1[\\xa7\\x046\\xd4&\\x9b?\\x9d-M\\x8a\\xa8Bl\\xcb\\xf6\\x11\\xaaZN4\\x0b|\\xbe\\x82v\\x97\\xa9\\xd8g\\xf6Wn\\xd8\\xb79\\xa9R\\xd3\\xcc\\xaf[\\t\\x99\\x9c\\xc1\\xcf\\xf4\\x9bBz\\xf0\\xeb\\xa7\\xcf0\\xefX\\xc8\\xf1\\x1f\\t|\\xe4u\\x98c\\x83\\x8eT\\xe2\\x82\\xa6\\x03\\x9d\\x0f\\x0cz\\xfd\\xf4\\xa8\\xee\\x8e\\xf2i#\\x11\\xe5\\xf84\\x1d\\xbf\\xea\\xc2\\x92\\xba\\xaa\\x92\\x0e\\x97%\\x13\\xf9\\xdcTBk\\x07;\\x00e\\x99f`\\xa0\\x1c\\xf912LI\\xcb\\xfa\\xfd&P<=J\\x86\\xa6\\x08J\\xd4\\x16\\xf2\\x89\\xb9x\\x9b1\\x81-q\\xbe\\xf4\\xeb\\x1e\\x0e\\xc9\\xe6U\\xd0\\xc9\\x9e\\xa1\\xa4H[\\xe8\\xa7\\xeeO\\xfc\\xd6\\x1c\\x03|\\xd5\\xb6\\xcc\\\\\\x07z\\xa9e;\\ro\\xc5\\xce\\x84\\x15qo\\xd5\\xd8\\x9cw\\xc8\\x83\\x07\\xbb\\xf7\\x9b\\xfc\\x19*\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x00\\x10\\x00\\x02@\\x00\\x08\\x00\\x01 \\x00\\x04\\x80\\x80\\x9d\\t\\xfc\\x17b@\\xfb.\"\n            )\n        elif kwargs[\"width\"] == 10 and kwargs[\"height\"] == 100:\n            return BytesIO(\n                b\"II*\\x00\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x01\\x01\\x03\\x00\\x01\\x00\\x00\\x00d\\x00\\x00\\x00\\x02\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x03\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x06\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x15\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1a\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\xfe\\x00\\x00\\x00\\x1b\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\x06\\x01\\x00\\x00\\x1c\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00(\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00=\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00B\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00C\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00D\\x01\\x04\\x00\\x01\\x00\\x00\\x00\\xe6\\x01\\x00\\x00E\\x01\\x04\\x00\\x01\\x00\\x00\\x00P\\x01\\x00\\x00S\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xd8\\x85\\x0c\\x00\\x10\\x00\\x00\\x00\\x0e\\x01\\x00\\x00\\xaf\\x87\\x03\\x00 \\x00\\x00\\x00\\x8e\\x01\\x00\\x00\\xb0\\x87\\x0c\\x00\\x02\\x00\\x00\\x00\\xce\\x01\\x00\\x00\\xb1\\x87\\x02\\x00\\x08\\x00\\x00\\x00\\xde\\x01\\x00\\x00\\x00\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00G\\x029\\x9f\\xa4\\xa1\\xe9?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00J\\x82\\x8fv\\xb0\\xa9`\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf26`\\xb3Gz\\xd3?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x9f\\xa0>%z7@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x01\\x00\\x01\\x00\\x00\\x00\\x07\\x00\\x00\\x04\\x00\\x00\\x01\\x00\\x02\\x00\\x01\\x04\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x08\\x00\\x00\\x01\\x00\\xe6\\x10\\x01\\x08\\xb1\\x87\\x07\\x00\\x00\\x00\\x06\\x08\\x00\\x00\\x01\\x00\\x8e#\\t\\x08\\xb0\\x87\\x01\\x00\\x01\\x00\\x0b\\x08\\xb0\\x87\\x01\\x00\\x00\\x00\\x88mt\\x96\\x1d\\xa4r@\\x00\\x00\\x00@\\xa6TXAWGS 84|\\x00x^\\xed\\xd6\\xdb\\t\\x00 \\x08\\x00@m\\xff\\x91\\x83\\xa2!\\x14\\xa1s\\x00\\x1f\\xa7\\x1fF\\x08\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 P'\\x90q\\xea\\x92\\xcbL\\x80\\x00\\x01\\x02\\x04\\x08\\x10\\x18(\\x90\\x9f\\xbf?\\xe6\\x1fx\\x94\\x1d-\\xbd\\xc5\\xaf\\xcc\\xddQK\\r\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04\\x08\\x10 @\\x80\\x00\\x01\\x02\\x04J\\x05.\\t^\\x06\\x01\"\n            )\n        elif kwargs[\"width\"] == 100 and kwargs[\"height\"] == 2:\n            return BytesIO(\n                b\"II*\\x00\\x08\\x00\\x00\\x00\\x14\\x00\\x00\\x01\\x03\\x00\\x01\\x00\\x00\\x00d\\x00\\x00\\x00\\x01\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x03\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x06\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x15\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1a\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\xfe\\x00\\x00\\x00\\x1b\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\x06\\x01\\x00\\x00\\x1c\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00(\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00=\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00B\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00C\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00D\\x01\\x04\\x00\\x01\\x00\\x00\\x00\\xe6\\x01\\x00\\x00E\\x01\\x04\\x00\\x01\\x00\\x00\\x003\\x01\\x00\\x00S\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xd8\\x85\\x0c\\x00\\x10\\x00\\x00\\x00\\x0e\\x01\\x00\\x00\\xaf\\x87\\x03\\x00 \\x00\\x00\\x00\\x8e\\x01\\x00\\x00\\xb0\\x87\\x0c\\x00\\x02\\x00\\x00\\x00\\xce\\x01\\x00\\x00\\xb1\\x87\\x02\\x00\\x08\\x00\\x00\\x00\\xde\\x01\\x00\\x00\\x00\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x009\\x8eZ\\x0f\\xe6\\x84b?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xca?\\xd1\\xd4\\xfb\\x7ff\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00is2\\x94c?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xd8K\\xb6Z?\\xfdK\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x01\\x00\\x01\\x00\\x00\\x00\\x07\\x00\\x00\\x04\\x00\\x00\\x01\\x00\\x02\\x00\\x01\\x04\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x08\\x00\\x00\\x01\\x00\\xe6\\x10\\x01\\x08\\xb1\\x87\\x07\\x00\\x00\\x00\\x06\\x08\\x00\\x00\\x01\\x00\\x8e#\\t\\x08\\xb0\\x87\\x01\\x00\\x01\\x00\\x0b\\x08\\xb0\\x87\\x01\\x00\\x00\\x00\\x88mt\\x96\\x1d\\xa4r@\\x00\\x00\\x00@\\xa6TXAWGS 84|\\x00x^\\xed\\xd0\\x81\\x00\\x00\\x00\\x00\\x80\\xa0\\xfd\\xa9\\x17)\\x84\\n\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180P\\x03\\x00\\x0f\\x00\\x01\"\n            )\n        elif kwargs[\"width\"] == 1 and kwargs[\"height\"] == 1:\n            return BytesIO(\n                b\"II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x00\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x03\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x06\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x15\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x1a\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\n\\x01\\x00\\x00\\x1b\\x01\\x05\\x00\\x01\\x00\\x00\\x00\\x12\\x01\\x00\\x00\\x1c\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00(\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00=\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00B\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00C\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x00\\x01\\x00\\x00D\\x01\\x04\\x00\\x01\\x00\\x00\\x00\\xba\\x01\\x00\\x00E\\x01\\x04\\x00\\x01\\x00\\x00\\x003\\x01\\x00\\x00S\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0e\\x83\\x0c\\x00\\x03\\x00\\x00\\x00\\x1a\\x01\\x00\\x00\\x82\\x84\\x0c\\x00\\x06\\x00\\x00\\x002\\x01\\x00\\x00\\xaf\\x87\\x03\\x00 \\x00\\x00\\x00b\\x01\\x00\\x00\\xb0\\x87\\x0c\\x00\\x02\\x00\\x00\\x00\\xa2\\x01\\x00\\x00\\xb1\\x87\\x02\\x00\\x08\\x00\\x00\\x00\\xb2\\x01\\x00\\x00\\x00\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00H\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xa6\\xf3\\x16\\xbf\\x06\\xd6\\xdc?\\xa6n\\xf6\\xcd]=\\xc7?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xbd\\xc5\\xaf\\x815\\x87f\\xc0\\xfa\\xbdu\\xb2\\xc1\\x05U@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x00\\x07\\x00\\x00\\x04\\x00\\x00\\x01\\x00\\x02\\x00\\x01\\x04\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x08\\x00\\x00\\x01\\x00\\xe6\\x10\\x01\\x08\\xb1\\x87\\x07\\x00\\x00\\x00\\x06\\x08\\x00\\x00\\x01\\x00\\x8e#\\t\\x08\\xb0\\x87\\x01\\x00\\x01\\x00\\x0b\\x08\\xb0\\x87\\x01\\x00\\x00\\x00\\x88mt\\x96\\x1d\\xa4r@\\x00\\x00\\x00@\\xa6TXAWGS 84|\\x00x^\\xed\\xd0\\x81\\x00\\x00\\x00\\x00\\x80\\xa0\\xfd\\xa9\\x17)\\x84\\n\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180`\\xc0\\x80\\x01\\x03\\x06\\x0c\\x180P\\x03\\x00\\x0f\\x00\\x01\"\n            )",
            "class MockWCSRaw(WCSRaw):",
            "def client(self):\n        return MockClient()",
            "def get_coordinates(self):\n        return COORDS",
            "class MockWCS(WCS):",
            "def client(self):\n        return MockClient()",
            "def get_coordinates(self):\n        return COORDS",
            "class TestWCSRaw(object):",
            "def test_eval_grid(self):\n        c = COORDS\n\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 1256581.0",
            "def test_eval_grid_chunked(self):\n        c = COORDS\n\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\", max_size=1000)\n        output = node.eval(c)\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 150.0",
            "def test_eval_grid_point(self):\n        c = COORDS[50, 50]\n\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\", max_size=1000)\n        output = node.eval(c)\n        assert output.shape == (1, 1)\n        assert output.data.sum() == 0.0",
            "def test_eval_nonuniform(self):\n        c = COORDS[[0, 10, 99], [0, 99]]\n\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 1256581.0",
            "def test_eval_uniform_stacked(self):\n        c = podpac.Coordinates([[COORDS[\"lat\"], COORDS[\"lon\"]]], dims=[\"lat_lon\"])\n\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.shape == (100,)\n        # MPU Note: changed from 14350.0 to 12640.0 based on np.diag(node.eval(COORDS)).sum()\n        assert output.data.sum() == 12640.0",
            "def test_eval_extra_unstacked_dim(self):\n        c = podpac.Coordinates([\"2020-01-01\", COORDS[\"lat\"], COORDS[\"lon\"]], dims=[\"time\", \"lat\", \"lon\"])\n\n        node = MockWCSRaw()\n        output = node.eval(c)\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 1256581.0",
            "def test_eval_extra_stacked_dim(self):\n        c = podpac.Coordinates(\n            [[COORDS[\"lat\"][50], COORDS[\"lon\"][50], 10]],\n            dims=[\"lat_lon_alt\"],\n            crs=\"+proj=longlat +datum=WGS84 +no_defs +vunits=m\",\n        )\n\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\", max_size=1000)\n        output = node.eval(c)\n        assert output.shape == (1,)\n        assert output.data.sum() == 0.0",
            "def test_eval_missing_dim(self):\n        c = podpac.Coordinates([COORDS[\"lat\"]])\n\n        node = MockWCSRaw()\n        with pytest.raises(ValueError, match=\"Cannot evaluate these coordinates\"):\n            output = node.eval(c)",
            "def test_eval_transpose(self):\n        c = COORDS.transpose(\"lon\", \"lat\")\n        node = MockWCSRaw(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.dims == (\"lon\", \"lat\")\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 1256581.0",
            "def test_eval_other_crs(self):\n        c = COORDS.transform(\"EPSG:3395\")\n\n        node = MockWCSRaw()\n        output = node.eval(c)\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 1256581.0",
            "class TestWCS(object):",
            "def test_eval_grid(self):\n        c = COORDS\n\n        node = MockWCS(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.shape == (100, 100)\n        assert output.data.sum() == 1256581.0",
            "def test_eval_nonuniform(self):\n        c = COORDS[[0, 10, 99], [0, 99]]\n\n        node = MockWCS(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.shape == (3, 2)\n        assert output.data.sum() == 0",
            "def test_eval_uniform_stacked(self):\n        c = podpac.Coordinates([[COORDS[\"lat\"], COORDS[\"lon\"]]], dims=[\"lat_lon\"])\n\n        node = MockWCS(source=\"mock\", layer=\"mock\")\n        output = node.eval(c)\n        assert output.shape == (100,)\n        # MPU Note: changed from 14350.0 to 12640.0 based on np.diag(node.eval(COORDS)).sum()\n        assert output.data.sum() == 12640.0\n\n\n@pytest.mark.integration\nclass TestWCSIntegration(object):\n    source = \"https:",
            "class TestWCSIntegration(object):\n    source = \"https:",
            "def setup_class(cls):\n        cls.node1 = WCSRaw(source=cls.source, layer=\"sand_0-5cm_mean\", format=\"geotiff_byte\", max_size=16384)\n        cls.node2 = WCS(source=cls.source, layer=\"sand_0-5cm_mean\", format=\"geotiff_byte\", max_size=16384)",
            "def test_coordinates(self):\n        self.node1.coordinates",
            "def test_eval_grid(self):\n        c = COORDS\n        self.node1.eval(c)\n        self.node2.eval(c)",
            "def test_eval_point(self):\n        c = COORDS[50, 50]\n        self.node1.eval(c)\n        self.node2.eval(c)",
            "def test_eval_nonuniform(self):\n        c = podpac.Coordinates([[-131.3, -131.4, -131.6], [23.0, 23.1, 23.3]], dims=[\"lon\", \"lat\"])\n        self.node1.eval(c)\n        self.node2.eval(c)",
            "def test_eval_uniform_stacked(self):\n        c = podpac.Coordinates([[COORDS[\"lat\"][::4], COORDS[\"lon\"][::4]]], dims=[\"lat_lon\"])\n        self.node1.eval(c)\n        self.node2.eval(c)",
            "def test_eval_chunked(self):\n        node = WCSRaw(source=self.source, layer=\"sand_0-5cm_mean\", format=\"geotiff_byte\", max_size=4000)\n        o1 = node.eval(COORDS)",
            "def test_eval_other_crs(self):\n        c = COORDS.transform(\"EPSG:3395\")\n        self.node1.eval(c)\n        self.node2.eval(c)",
            "def test_get_layers(self):\n        # most basic\n        layers = WCSRaw.get_layers(self.source)\n        assert isinstance(layers, list)\n\n        # also works with nodes that have a builtin source",
            "class WCSWithSource(WCSRaw):\n            source = self.source\n\n        layers = WCSWithSource.get_layers()\n        assert isinstance(layers, list)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_datasource.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nTest podpac.core.data.datasource module\n\"\"\"",
            "\"\"\"evaluate node at coordinates\"\"\"",
            "\"\"\"evaluate node with coordinates that do not overlap\"\"\"",
            "\"\"\"mock selector that just strides by 2\"\"\"",
            "\"\"\"evaluate note with nan_vals\"\"\""
        ],
        "code_snippets": [
            "class MockDataSource(DataSource):\n    data = np.ones((11, 11))\n    data[0, 0] = 10\n    data[0, 1] = 1\n    data[1, 0] = 5\n    data[1, 1] = None",
            "def get_coordinates(self):\n        return Coordinates([clinspace(-25, 25, 11), clinspace(-25, 25, 11)], dims=[\"lat\", \"lon\"])",
            "def get_data(self, coordinates, coordinates_index):\n        return self.create_output_array(coordinates, data=self.data[coordinates_index])",
            "class MockDataSourceStacked(DataSource):\n    data = np.arange(11)",
            "def get_coordinates(self):\n        return Coordinates([clinspace((-25, -25), (25, 25), 11)], dims=[\"lat_lon\"])",
            "def get_data(self, coordinates, coordinates_index):\n        return self.create_output_array(coordinates, data=self.data[coordinates_index])",
            "class MockMultipleDataSource(DataSource):\n    outputs = [\"a\", \"b\", \"c\"]\n    coordinates = Coordinates([[0, 1, 2, 3], [10, 11]], dims=[\"lat\", \"lon\"])",
            "def get_data(self, coordinates, coordinates_index):\n        return self.create_output_array(coordinates, data=1)",
            "class TestDataDocs(object):",
            "def test_common_data_doc(self):\n        # all DATA_DOC keys should be in the COMMON_DATA_DOC\n        for key in DATA_DOC:\n            assert key in COMMON_DATA_DOC\n            assert COMMON_DATA_DOC[key] == DATA_DOC[key]\n\n        # DATA_DOC should overwrite COMMON_NODE_DOC keys\n        for key in COMMON_NODE_DOC:\n            assert key in COMMON_DATA_DOC\n\n            if key in DATA_DOC:\n                assert COMMON_DATA_DOC[key] != COMMON_NODE_DOC[key]\n            else:\n                assert COMMON_DATA_DOC[key] == COMMON_NODE_DOC[key]",
            "class TestDataSource(object):",
            "def test_init(self):\n        node = DataSource()",
            "def test_repr(self):\n        node = DataSource()\n        repr(node)",
            "def test_get_data_not_implemented(self):\n        node = DataSource()\n\n        with pytest.raises(NotImplementedError):\n            node.get_data(None, None)",
            "def test_get_coordinates_not_implemented(self):\n        node = DataSource()\n        with pytest.raises(NotImplementedError):\n            node.get_coordinates()",
            "def test_coordinates(self):\n        # not implemented\n        node = DataSource()\n        with pytest.raises(NotImplementedError):\n            node.coordinates\n\n        # make sure get_coordinates gets called only once",
            "class MyDataSource(DataSource):\n            get_coordinates_called = 0",
            "def get_coordinates(self):\n                self.get_coordinates_called += 1\n                return Coordinates([])\n\n        node = MyDataSource()\n        assert node.get_coordinates_called == 0\n        assert isinstance(node.coordinates, Coordinates)\n        assert node.get_coordinates_called == 1\n        assert isinstance(node.coordinates, Coordinates)\n        assert node.get_coordinates_called == 1\n\n        # can't set coordinates attribute\n        with pytest.raises(AttributeError, match=\"can't set attribute\"):\n            node.coordinates = Coordinates([])",
            "def test_dims(self):",
            "class MyDataSource(DataSource):",
            "def get_coordinates(self):\n                return Coordinates([0, 0], dims=[\"lat\", \"lon\"])\n\n        node = MyDataSource()\n        assert node.dims == (\"lat\", \"lon\")",
            "def test_cache_coordinates(self):",
            "class MyDataSource(DataSource):\n            get_coordinates_called = 0",
            "def get_coordinates(self):\n                self.get_coordinates_called += 1\n                return Coordinates([])\n\n        a = MyDataSource(cache_coordinates=True, cache_ctrl=[\"ram\"])\n        b = MyDataSource(cache_coordinates=True, cache_ctrl=[\"ram\"])\n        c = MyDataSource(cache_coordinates=False, cache_ctrl=[\"ram\"])\n        d = MyDataSource(cache_coordinates=True, cache_ctrl=[])\n\n        a.rem_cache(\"*\")\n        b.rem_cache(\"*\")\n        c.rem_cache(\"*\")\n        d.rem_cache(\"*\")\n\n        # get_coordinates called once\n        assert not a.has_cache(\"coordinates\")\n        assert a.get_coordinates_called == 0\n        assert isinstance(a.coordinates, Coordinates)\n        assert a.get_coordinates_called == 1\n        assert isinstance(a.coordinates, Coordinates)\n        assert a.get_coordinates_called == 1\n\n        # coordinates is cached to a, b, and c\n        assert a.has_cache(\"coordinates\")\n        assert b.has_cache(\"coordinates\")\n        assert c.has_cache(\"coordinates\")\n        assert not d.has_cache(\"coordinates\")\n\n        # b: use cache, get_coordinates not called\n        assert b.get_coordinates_called == 0\n        assert isinstance(b.coordinates, Coordinates)\n        assert b.get_coordinates_called == 0\n\n        # c: don't use cache, get_coordinates called\n        assert c.get_coordinates_called == 0\n        assert isinstance(c.coordinates, Coordinates)\n        assert c.get_coordinates_called == 1\n\n        # d: use cache but there is no ram cache for this node, get_coordinates is called\n        assert d.get_coordinates_called == 0\n        assert isinstance(d.coordinates, Coordinates)\n        assert d.get_coordinates_called == 1",
            "def test_set_coordinates(self):\n        node = MockDataSource()\n        node.set_coordinates(Coordinates([]))\n        assert node.coordinates == Coordinates([])\n        assert node.coordinates != node.get_coordinates()\n\n        # don't overwrite\n        node = MockDataSource()\n        node.coordinates\n        node.set_coordinates(Coordinates([]))\n        assert node.coordinates != Coordinates([])\n        assert node.coordinates == node.get_coordinates()",
            "def test_boundary(self):\n        # default\n        node = DataSource()\n        assert node.boundary == {}\n\n        # none\n        node = DataSource(boundary={})\n\n        # centered\n        node = DataSource(boundary={\"lat\": 0.25, \"lon\": 2.0})\n        node = DataSource(boundary={\"time\": \"1,D\"})\n\n        # box (not necessary centered)\n        with pytest.raises(NotImplementedError, match=\"Non-centered boundary not yet supported\"):\n            node = DataSource(boundary={\"lat\": [-0.2, 0.3], \"lon\": [-2.0, 2.0]})\n\n        with pytest.raises(NotImplementedError, match=\"Non-centered boundary not yet supported\"):\n            node = DataSource(boundary={\"time\": [\"-1,D\", \"2,D\"]})\n\n        # polygon\n        with pytest.raises(NotImplementedError, match=\"Non-centered boundary not yet supported\"):\n            node = DataSource(boundary={\"lat\": [0.0, -0.5, 0.0, 0.5], \"lon\": [-0.5, 0.0, 0.5, 0.0]})  # diamond\n\n        # array of boundaries (one for each coordinate)\n        with pytest.raises(NotImplementedError, match=\"Non-uniform boundary not yet supported\"):\n            node = DataSource(boundary={\"lat\": [[-0.1, 0.4], [-0.2, 0.3], [-0.3, 0.2]], \"lon\": 0.5})\n\n        with pytest.raises(NotImplementedError, match=\"Non-uniform boundary not yet supported\"):\n            node = DataSource(boundary={\"time\": [[\"-1,D\", \"1,D\"], [\"-2,D\", \"1,D\"]]})\n\n        # invalid\n        with pytest.raises(tl.TraitError):\n            node = DataSource(boundary=0.5)\n\n        with pytest.raises(ValueError, match=\"Invalid dimension\"):\n            node = DataSource(boundary={\"other\": 0.5})\n\n        with pytest.raises(TypeError, match=\"Invalid coordinate delta\"):\n            node = DataSource(boundary={\"lat\": {}})\n\n        with pytest.raises(ValueError, match=\"Invalid boundary\"):\n            node = DataSource(boundary={\"lat\": -0.25, \"lon\": 2.0})  # negative\n\n        with pytest.raises(ValueError, match=\"Invalid boundary\"):\n            node = DataSource(boundary={\"time\": \"-2,D\"})  # negative\n\n        with pytest.raises(ValueError, match=\"Invalid boundary\"):\n            node = DataSource(boundary={\"time\": \"2018-01-01\"})  # not a delta",
            "def test_invalid_nan_vals(self):\n        with pytest.raises(tl.TraitError):\n            DataSource(nan_vals={})\n\n        with pytest.raises(tl.TraitError):\n            DataSource(nan_vals=10)",
            "def test_find_coordinates(self):\n        node = MockDataSource()\n        l = node.find_coordinates()\n        assert isinstance(l, list)\n        assert len(l) == 1\n        assert l[0] == node.coordinates",
            "def test_evaluate_at_coordinates(self):",
            "def test_evaluate_at_coordinates_with_output(self):\n        node = MockDataSource()\n        output = node.create_output_array(node.coordinates)\n        node.eval(node.coordinates, output=output)\n\n        assert output.shape == (11, 11)\n        assert output[0, 0] == 10",
            "def test_evaluate_no_overlap(self):",
            "def test_evaluate_no_overlap_with_output(self):\n        # there is a shortcut if there is no intersect, so we test that here\n        node = MockDataSource()\n        coords = Coordinates([clinspace(30, 40, 10), clinspace(30, 40, 10)], dims=[\"lat\", \"lon\"])\n        output = UnitsDataArray.create(coords, data=1)\n        node.eval(coords, output=output)\n        np.testing.assert_equal(output.data, np.full(output.shape, np.nan))",
            "def test_evaluate_extra_dims(self):\n        # drop extra unstacked dimension",
            "class MyDataSource(DataSource):\n            coordinates = Coordinates([1, 11], dims=[\"lat\", \"lon\"])",
            "def get_data(self, coordinates, coordinates_index):\n                return self.create_output_array(coordinates)\n\n        node = MyDataSource()\n        coords = Coordinates([1, 11, \"2018-01-01\"], dims=[\"lat\", \"lon\", \"time\"])\n        output = node.eval(coords)\n        assert output.dims == (\"lat\", \"lon\")  # time dropped\n\n        # drop extra stacked dimension if none of its dimensions are needed",
            "class MyDataSource(DataSource):\n            coordinates = Coordinates([\"2018-01-01\"], dims=[\"time\"])",
            "def get_data(self, coordinates, coordinates_index):\n                return self.create_output_array(coordinates)\n\n        node = MyDataSource()\n        coords = Coordinates([[1, 11], \"2018-01-01\"], dims=[\"lat_lon\", \"time\"])\n        output = node.eval(coords)\n        assert output.dims == (\"time\",)  # lat_lon dropped\n\n        # TODO\n        # but don't drop extra stacked dimension if any of its dimensions are needed\n        # output = node.eval(Coordinates([[1, 11, '2018-01-01']], dims=['lat_lon_time']))\n        # assert output.dims == ('lat_lon_time') # lat and lon not dropped",
            "def test_evaluate_missing_dims(self):\n        # missing unstacked dimension\n        node = MockDataSource()\n\n        with pytest.raises(ValueError, match=\"Cannot evaluate these coordinates.*\"):\n            node.eval(Coordinates([1], dims=[\"lat\"]))\n        with pytest.raises(ValueError, match=\"Cannot evaluate these coordinates.*\"):\n            node.eval(Coordinates([11], dims=[\"lon\"]))\n        with pytest.raises(ValueError, match=\"Cannot evaluate these coordinates.*\"):\n            node.eval(Coordinates([\"2018-01-01\"], dims=[\"time\"]))\n\n        # missing any part of stacked dimension\n        node = MockDataSourceStacked()\n\n        with pytest.raises(ValueError, match=\"Cannot evaluate these coordinates.*\"):\n            node.eval(Coordinates([1], dims=[\"time\"]))",
            "def test_evaluate_crs_transform(self):\n        node = MockDataSource()\n\n        coords = node.coordinates.transform(\"EPSG:2193\")\n        out = node.eval(coords)\n\n        # test data\n        np.testing.assert_array_equal(out.data, node.data)",
            "def test_evaluate_selector(self):",
            "def selector(rsc, coordinates, index_type=None):",
            "def test_nan_vals(self):",
            "def test_get_data_np_array(self):",
            "class MockDataSourceReturnsArray(MockDataSource):",
            "def get_data(self, coordinates, coordinates_index):\n                return self.data[coordinates_index]\n\n        node = MockDataSourceReturnsArray()\n        output = node.eval(node.coordinates)\n\n        assert isinstance(output, UnitsDataArray)\n        assert node.coordinates[\"lat\"].coordinates[4] == output.coords[\"lat\"].values[4]",
            "def test_get_data_DataArray(self):",
            "class MockDataSourceReturnsDataArray(MockDataSource):",
            "def get_data(self, coordinates, coordinates_index):\n                return xr.DataArray(self.data[coordinates_index])\n\n        node = MockDataSourceReturnsDataArray()\n        output = node.eval(node.coordinates)\n\n        assert isinstance(output, UnitsDataArray)\n        assert node.coordinates[\"lat\"].coordinates[4] == output.coords[\"lat\"].values[4]",
            "def test_get_data_invalid(self):",
            "class MockDataSourceReturnsInvalid(MockDataSource):",
            "def get_data(self, coordinates, coordinates_index):\n                return self.data[coordinates_index].tolist()\n\n        node = MockDataSourceReturnsInvalid()\n        with pytest.raises(TypeError, match=\"Unknown data type\"):\n            output = node.eval(node.coordinates)",
            "def test_evaluate_debug_attributes(self):\n        with podpac.settings:\n            podpac.settings[\"DEBUG\"] = True\n\n            node = MockDataSource()\n\n            assert node._evaluated_coordinates is None\n            assert node._requested_coordinates is None\n            assert node._requested_source_coordinates is None\n            assert node._requested_source_coordinates_index is None\n            assert node._requested_source_boundary is None\n            assert node._requested_source_data is None\n\n            node.eval(node.coordinates)\n\n            assert node._evaluated_coordinates is not None\n            assert node._requested_coordinates is not None\n            assert node._requested_source_coordinates is not None\n            assert node._requested_source_coordinates_index is not None\n            assert node._requested_source_boundary is not None\n            assert node._requested_source_data is not None",
            "def test_evaluate_debug_attributes_no_overlap(self):\n        with podpac.settings:\n            podpac.settings[\"DEBUG\"] = True\n\n            node = MockDataSource()\n\n            assert node._evaluated_coordinates is None\n            assert node._requested_coordinates is None\n            assert node._requested_source_coordinates is None\n            assert node._requested_source_coordinates_index is None\n            assert node._requested_source_boundary is None\n            assert node._requested_source_data is None\n\n            coords = Coordinates([clinspace(-55, -45, 20), clinspace(-55, -45, 20)], dims=[\"lat\", \"lon\"])\n            node.eval(coords)\n\n            assert node._evaluated_coordinates is not None\n            assert node._requested_coordinates is not None\n            assert node._requested_source_coordinates is not None\n            assert node._requested_source_coordinates_index is not None\n            assert node._requested_source_boundary is None  # still none in this case\n            assert node._requested_source_data is None  # still none in this case",
            "def test_get_boundary(self):\n        # disable boundary validation (until non-centered and non-uniform boundaries are fully implemented)",
            "class MockDataSourceNoBoundaryValidation(MockDataSource):\n            @tl.validate(\"boundary\")",
            "def _validate_boundary(self, d):\n                return d[\"value\"]\n\n        index = (slice(3, 9, 2), [3, 4, 6])\n\n        # points\n        node = MockDataSourceNoBoundaryValidation(boundary={})\n        boundary = node._get_boundary(index)\n        assert boundary == {}\n\n        # uniform centered\n        node = MockDataSourceNoBoundaryValidation(boundary={\"lat\": 0.1, \"lon\": 0.2})\n        boundary = node._get_boundary(index)\n        assert boundary == {\"lat\": 0.1, \"lon\": 0.2}\n\n        # uniform polygon\n        node = MockDataSourceNoBoundaryValidation(boundary={\"lat\": [-0.1, 0.1], \"lon\": [-0.1, 0.0, 0.1]})\n        boundary = node._get_boundary(index)\n        assert boundary == {\"lat\": [-0.1, 0.1], \"lon\": [-0.1, 0.0, 0.1]}\n\n        # non-uniform\n        lat_boundary = np.vstack([-np.arange(11), np.arange(11)]).T\n        lon_boundary = np.vstack([-2 * np.arange(11), 2 * np.arange(11)]).T\n        node = MockDataSourceNoBoundaryValidation(boundary={\"lat\": lat_boundary, \"lon\": lon_boundary})\n        boundary = node._get_boundary(index)\n        np.testing.assert_array_equal(boundary[\"lat\"], lat_boundary[index[0]])\n        np.testing.assert_array_equal(boundary[\"lon\"], lon_boundary[index[1]])",
            "def test_get_boundary_stacked(self):\n        # disable boundary validation (until non-centered and non-uniform boundaries are fully implemented)",
            "class MockDataSourceStackedNoBoundaryValidation(MockDataSourceStacked):\n            @tl.validate(\"boundary\")",
            "def _validate_boundary(self, d):\n                return d[\"value\"]\n\n        index = (slice(3, 9, 2),)\n\n        # points\n        node = MockDataSourceStackedNoBoundaryValidation(boundary={})\n        boundary = node._get_boundary(index)\n        assert boundary == {}\n\n        # uniform centered\n        node = MockDataSourceStackedNoBoundaryValidation(boundary={\"lat\": 0.1, \"lon\": 0.1})\n        boundary = node._get_boundary(index)\n        assert boundary == {\"lat\": 0.1, \"lon\": 0.1}\n\n        # uniform polygon\n        node = MockDataSourceStackedNoBoundaryValidation(boundary={\"lat\": [-0.1, 0.1], \"lon\": [-0.1, 0.0, 0.1]})\n        boundary = node._get_boundary(index)\n        assert boundary == {\"lat\": [-0.1, 0.1], \"lon\": [-0.1, 0.0, 0.1]}\n\n        # non-uniform\n        lat_boundary = np.vstack([-np.arange(11), np.arange(11)]).T\n        lon_boundary = np.vstack([-2 * np.arange(11), 2 * np.arange(11)]).T\n        node = MockDataSourceStackedNoBoundaryValidation(boundary={\"lat\": lat_boundary, \"lon\": lon_boundary})\n        boundary = node._get_boundary(index)\n        np.testing.assert_array_equal(boundary[\"lat\"], lat_boundary[index])\n        np.testing.assert_array_equal(boundary[\"lon\"], lon_boundary[index])",
            "def test_eval_get_cache_extra_dims(self):\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n\n            node = podpac.data.Array(\n                source=np.ones((3, 4)),\n                coordinates=podpac.Coordinates([range(3), range(4)], [\"lat\", \"lon\"]),\n                cache_ctrl=[\"ram\"],\n            )\n            coords1 = podpac.Coordinates([range(3), range(4), \"2012-05-19\"], [\"lat\", \"lon\", \"time\"])\n            coords2 = podpac.Coordinates([range(3), range(4), \"2019-09-10\"], [\"lat\", \"lon\", \"time\"])\n\n            # retrieve from cache on the second evaluation\n            node.eval(coords1)\n            assert not node._from_cache\n\n            node.eval(coords1)\n            assert node._from_cache\n\n            # also retrieve from cache with different time coordinates\n            node.eval(coords2)\n            assert node._from_cache",
            "def test_eval_get_cache_transform_crs(self):\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n\n            node = podpac.core.data.array_source.Array(\n                source=np.ones((3, 4)),\n                coordinates=podpac.Coordinates([range(3), range(4)], [\"lat\", \"lon\"], crs=\"EPSG:4326\"),\n                cache_ctrl=[\"ram\"],\n            )\n\n            # retrieve from cache on the second evaluation\n            node.eval(node.coordinates)\n            assert not node._from_cache\n\n            node.eval(node.coordinates)\n            assert node._from_cache\n\n            # also retrieve from cache with different crs\n            node.eval(node.coordinates.transform(\"EPSG:4326\"))\n            assert node._from_cache",
            "def test_get_source_data(self):\n        node = podpac.data.Array(\n            source=np.ones((3, 4)),\n            coordinates=podpac.Coordinates([range(3), range(4)], [\"lat\", \"lon\"]),\n        )\n\n        data = node.get_source_data()\n        np.testing.assert_array_equal(data, node.source)",
            "def test_get_source_data_with_bounds(self):\n        node = podpac.data.Array(\n            source=np.ones((3, 4)),\n            coordinates=podpac.Coordinates([range(3), range(4)], [\"lat\", \"lon\"]),\n        )\n\n        data = node.get_source_data({\"lon\": (1.5, 4.5)})\n        np.testing.assert_array_equal(data, node.source[:, 2:])",
            "def test_get_bounds(self):\n        node = podpac.data.Array(\n            source=np.ones((3, 4)),\n            coordinates=podpac.Coordinates([range(3), range(4)], [\"lat\", \"lon\"], crs=\"EPSG:2193\"),\n        )\n\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CRS\"] = \"EPSG:4326\"\n\n            # specify crs\n            bounds, crs = node.get_bounds(crs=\"EPSG:3857\")\n            expected = {\n                \"lat\": (-13291827.558247397, -13291815.707967814),\n                \"lon\": (9231489.26794932, 9231497.142754894),\n            }\n            for k in expected:\n                np.testing.assert_almost_equal(bounds[k], expected[k])\n            assert crs == \"EPSG:3857\"\n\n            # native/source crs\n            bounds, crs = node.get_bounds(crs=\"source\")\n            assert bounds == {\"lat\": (0, 2), \"lon\": (0, 3)}\n            assert crs == \"EPSG:2193\"\n\n            # default crs\n            bounds, crs = node.get_bounds()\n            assert bounds == {\n                \"lat\": (-75.81365382984804, -75.81362774074242),\n                \"lon\": (82.92787904584206, 82.92794978642414),\n            }\n            assert crs == \"EPSG:4326\"",
            "class TestDataSourceWithMultipleOutputs(object):",
            "def test_evaluate_no_overlap_with_output_extract_output(self):",
            "class MockMultipleDataSource(DataSource):\n            outputs = [\"a\", \"b\", \"c\"]\n            coordinates = Coordinates([[0, 1, 2, 3], [10, 11]], dims=[\"lat\", \"lon\"])",
            "def get_data(self, coordinates, coordinates_index):\n                return self.create_output_array(coordinates, data=1)\n\n        node = MockMultipleDataSource(output=\"a\")\n        coords = Coordinates([clinspace(-55, -45, 20), clinspace(-55, -45, 20)], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)\n\n        assert np.all(np.isnan(output))",
            "def test_evaluate_extract_output(self):\n        # don't extract when no output field is requested\n        node = MockMultipleDataSource()\n        o = node.eval(node.coordinates)\n        assert o.shape == (4, 2, 3)\n        np.testing.assert_array_equal(o.dims, [\"lat\", \"lon\", \"output\"])\n        np.testing.assert_array_equal(o[\"output\"], [\"a\", \"b\", \"c\"])\n        np.testing.assert_array_equal(o, 1)\n\n        # do extract when an output field is requested\n        node = MockMultipleDataSource(output=\"b\")\n\n        o = node.eval(node.coordinates)  # get_data case\n        assert o.shape == (4, 2)\n        np.testing.assert_array_equal(o.dims, [\"lat\", \"lon\"])\n        np.testing.assert_array_equal(o, 1)\n\n        # no intersection case\n        o = node.eval(Coordinates([[100, 200], [1000, 2000, 3000]], dims=[\"lat\", \"lon\"]))\n        assert o.shape == (0, 0)\n        np.testing.assert_array_equal(o.dims, [\"lat\", \"lon\"])\n        np.testing.assert_array_equal(o, np.nan)",
            "def test_evaluate_output_already_extracted(self):\n        # should still work if the node has already extracted it",
            "class ExtractedMultipleDataSource(MockMultipleDataSource):",
            "def get_data(self, coordinates, coordinates_index):\n                out = self.create_output_array(coordinates, data=1)\n                return out.sel(output=self.output)\n\n        node = ExtractedMultipleDataSource(output=\"b\")\n        o = node.eval(node.coordinates)\n        assert o.shape == (4, 2)\n        np.testing.assert_array_equal(o.dims, [\"lat\", \"lon\"])\n        np.testing.assert_array_equal(o, 1)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_h5py.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestH5PY(object):\n    source = os.path.join(os.path.dirname(__file__), \"assets/h5raster.hdf5\")",
            "def test_init(self):\n        node = H5PY(source=self.source, data_key=\"/data/init\", lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        node.close_dataset()",
            "def test_dims(self):\n        node = H5PY(source=self.source, data_key=\"/data/init\", lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        assert node.dims == [\"lat\", \"lon\"]\n        node.close_dataset()",
            "def test_available_data_keys(self):\n        node = H5PY(source=self.source, data_key=\"/data/init\", lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        assert node.available_data_keys == [\"/data/init\"]\n        node.close_dataset()",
            "def test_coordinates(self):\n        node = H5PY(source=self.source, data_key=\"/data/init\", lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        nc = node.coordinates\n        assert node.coordinates.shape == (3, 4)\n        np.testing.assert_array_equal(node.coordinates[\"lat\"].coordinates, [45.1, 45.2, 45.3])\n        np.testing.assert_array_equal(node.coordinates[\"lon\"].coordinates, [-100.1, -100.2, -100.3, -100.4])\n        node.close_dataset()",
            "def test_data(self):\n        node = H5PY(source=self.source, data_key=\"/data/init\", lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        o = node.eval(node.coordinates)\n        np.testing.assert_array_equal(o.data.ravel(), np.arange(12))\n        node.close_dataset()\n\n        # default\n        node = H5PY(source=self.source, lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        o = node.eval(node.coordinates)\n        np.testing.assert_array_equal(o.data.ravel(), np.arange(12))\n        node.close_dataset()",
            "def test_data_multiple(self):\n        node = H5PY(\n            source=self.source,\n            data_key=[\"/data/init\", \"/data/init\"],\n            outputs=[\"a\", \"b\"],\n            lat_key=\"/coords/lat\",\n            lon_key=\"/coords/lon\",\n        )\n        o = node.eval(node.coordinates)\n        assert o.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(o[\"output\"], [\"a\", \"b\"])\n        np.testing.assert_array_equal(o.sel(output=\"a\").data.ravel(), np.arange(12))\n        np.testing.assert_array_equal(o.sel(output=\"b\").data.ravel(), np.arange(12))\n        node.close_dataset()",
            "def test_dataset_attrs(self):\n        node = H5PY(source=self.source, data_key=\"/data/init\", lat_key=\"/coords/lat\", lon_key=\"/coords/lon\")\n        assert node.dataset_attrs() == {}\n        assert node.dataset_attrs(\"data\") == {\"test\": \"test\"}\n        assert node.dataset_attrs(\"coords/lat\") == {\"unit\": \"degrees\"}\n        assert node.dataset_attrs(\"coords/lon\") == {\"unit\": \"degrees\"}\n        assert node.dataset_attrs(\"coords\") == {\"crs\": \"EPSG:4326s\"}\n        node.close_dataset()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_file_source.py",
        "comments": [
            "//modis-pds/MCD43A4.006/00/08/2020018/MCD43A4.A2020018.h00v08.006.2020027031229_meta.json\"",
            "//speedtest.tele2.net/1KB.zip\")",
            "//httpstat.us/200\")",
            "///%s\" % path)",
            "///%s\" % path, cache_dataset=True)",
            "///%s\" % path)",
            "///%s\" % path, cache_dataset=True, dataset_expires=\"1,D\")",
            "///%s\" % path, cache_dataset=True, dataset_expires=\"-1,D\")"
        ],
        "docstrings": [],
        "code_snippets": [
            "class TestBaseFileSource(object):",
            "def test_source_required(self):\n        node = BaseFileSource()\n        with pytest.raises(ValueError, match=\"'source' required\"):\n            node.source",
            "def test_dataset_not_implemented(self):\n        node = BaseFileSource(source=\"mysource\")\n        with pytest.raises(NotImplementedError):\n            node.dataset",
            "def test_close(self):\n        node = BaseFileSource(source=\"mysource\")\n        node.close_dataset()",
            "def test_repr_str(self):\n        node = BaseFileSource(source=\"mysource\")\n        assert \"source=\" in repr(node)\n        assert \"source=\" in str(node)\n\n\n# ---------------------------------------------------------------------------------------------------------------------\n# LoadFileMixin\n# ---------------------------------------------------------------------------------------------------------------------",
            "class MockLoadFile(LoadFileMixin, BaseFileSource):",
            "def open_dataset(self, f):\n        return None",
            "class TestLoadFile(object):",
            "def test_open_dataset_not_implemented(self):\n        node = LoadFileMixin()\n        with pytest.raises(NotImplementedError):\n            node.open_dataset(None)",
            "def test_local(self):\n        path = os.path.join(os.path.dirname(__file__), \"assets/points-single.csv\")\n        node = MockLoadFile(source=path)\n        node.dataset\n\n    @pytest.mark.aws\n    def test_s3(self):\n        # TODO replace this with a better public s3 fileobj for testing\n        path = \"s3:",
            "def test_s3(self):\n        # TODO replace this with a better public s3 fileobj for testing\n        path = \"s3:",
            "def test_ftp(self):\n        node = MockLoadFile(source=\"ftp:",
            "def test_http(self):\n        node = MockLoadFile(source=\"https:",
            "def test_file(self):\n        path = os.path.join(os.path.dirname(__file__), \"assets/points-single.csv\")\n        node = MockLoadFile(source=\"file:",
            "def test_cache_dataset(self):\n        path = os.path.join(os.path.dirname(__file__), \"assets/points-single.csv\")\n\n        with podpac.settings:\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n            node = MockLoadFile(source=\"file:",
            "def test_dataset_expires(self):\n        path = os.path.join(os.path.dirname(__file__), \"assets/points-single.csv\")\n\n        with podpac.settings:\n            # not expired\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n            node = MockLoadFile(source=\"file:",
            "class MockFileKeys(FileKeysMixin, BaseFileSource):\n    source = \"mock-single\"\n    dataset = {\"lat\": LAT, \"lon\": LON, \"time\": TIME, \"alt\": ALT, \"data\": DATA}\n    keys = [\"lat\", \"lon\", \"time\", \"alt\", \"data\"]\n    dims = [\"lat\", \"lon\", \"time\", \"alt\"]",
            "class MockFileKeysMultipleAvailable(FileKeysMixin, BaseFileSource):\n    source = \"mock-multiple\"\n    dataset = {\"lat\": LAT, \"lon\": LON, \"time\": TIME, \"alt\": ALT, \"data\": DATA, \"other\": OTHER}\n    keys = [\"lat\", \"lon\", \"time\", \"alt\", \"data\", \"other\"]\n    dims = [\"lat\", \"lon\", \"time\", \"alt\"]",
            "class MockFileKeysEmpty(FileKeysMixin, BaseFileSource):\n    source = \"mock-empty\"\n    dataset = {\"lat\": LAT, \"lon\": LON, \"time\": TIME, \"alt\": ALT}\n    keys = [\"lat\", \"lon\", \"time\", \"alt\"]\n    dims = [\"lat\", \"lon\", \"time\", \"alt\"]",
            "class TestFileKeys(object):",
            "def test_not_implemented(self):",
            "class MySource(FileKeysMixin, BaseFileSource):\n            pass\n\n        node = MySource(source=\"mysource\")\n\n        with pytest.raises(NotImplementedError):\n            node.keys\n\n        with pytest.raises(NotImplementedError):\n            node.dims",
            "def test_available_data_keys(self):\n        node = MockFileKeys()\n        assert node.available_data_keys == [\"data\"]\n\n        node = MockFileKeysMultipleAvailable()\n        assert node.available_data_keys == [\"data\", \"other\"]\n\n        node = MockFileKeysEmpty()\n        with pytest.raises(ValueError, match=\"No data keys found\"):\n            node.available_data_keys",
            "def test_data_key(self):\n        node = MockFileKeys()\n        assert node.data_key == \"data\"\n\n        node = MockFileKeys(data_key=\"data\")\n        assert node.data_key == \"data\"\n\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = MockFileKeys(data_key=\"misc\")",
            "def test_data_key_multiple_outputs(self):\n        node = MockFileKeysMultipleAvailable()\n        assert node.data_key == [\"data\", \"other\"]\n\n        node = MockFileKeysMultipleAvailable(data_key=[\"other\", \"data\"])\n        assert node.data_key == [\"other\", \"data\"]\n\n        node = MockFileKeysMultipleAvailable(data_key=\"other\")\n        assert node.data_key == \"other\"\n\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = MockFileKeysMultipleAvailable(data_key=[\"data\", \"misc\"])\n\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = MockFileKeysMultipleAvailable(data_key=\"misc\")",
            "def test_no_outputs(self):\n        node = MockFileKeys(data_key=\"data\")\n        assert node.outputs == None\n\n        node = MockFileKeysMultipleAvailable(data_key=\"data\")\n        assert node.outputs == None\n\n        with pytest.raises(TypeError, match=\"outputs must be None for single-output nodes\"):\n            node = MockFileKeys(data_key=\"data\", outputs=[\"a\"])\n\n        with pytest.raises(TypeError, match=\"outputs must be None for single-output nodes\"):\n            node = MockFileKeysMultipleAvailable(data_key=\"data\", outputs=[\"a\"])\n\n        with pytest.raises(TypeError, match=\"outputs must be None for single-output nodes\"):\n            node = MockFileKeys(outputs=[\"a\"])",
            "def test_outputs(self):\n        # for multi-output nodes, use the dataset's keys by default\n        node = MockFileKeys(data_key=[\"data\"])\n        assert node.outputs == [\"data\"]\n\n        node = MockFileKeysMultipleAvailable(data_key=[\"data\", \"other\"])\n        assert node.outputs == [\"data\", \"other\"]\n\n        node = MockFileKeysMultipleAvailable(data_key=[\"data\"])\n        assert node.outputs == [\"data\"]\n\n        # alternate outputs names can be specified\n        node = MockFileKeys(data_key=[\"data\"], outputs=[\"a\"])\n        assert node.outputs == [\"a\"]\n\n        node = MockFileKeysMultipleAvailable(data_key=[\"data\", \"other\"], outputs=[\"a\", \"b\"])\n        assert node.outputs == [\"a\", \"b\"]\n\n        node = MockFileKeysMultipleAvailable(data_key=[\"data\"], outputs=[\"a\"])\n        assert node.outputs == [\"a\"]\n\n        node = MockFileKeysMultipleAvailable(outputs=[\"a\", \"b\"])\n        assert node.outputs == [\"a\", \"b\"]\n\n        # but the outputs and data_key must match\n        with pytest.raises(TypeError, match=\"outputs and data_key mismatch\"):\n            node = MockFileKeysMultipleAvailable(data_key=[\"data\"], outputs=None)\n\n        with pytest.raises(ValueError, match=\"outputs and data_key size mismatch\"):\n            node = MockFileKeysMultipleAvailable(data_key=[\"data\"], outputs=[\"a\", \"b\"])\n\n        with pytest.raises(ValueError, match=\"outputs and data_key size mismatch\"):\n            node = MockFileKeysMultipleAvailable(data_key=[\"data\", \"other\"], outputs=[\"a\"])",
            "def test_coordinates(self):\n        node = MockFileKeys()\n        nc = node.coordinates\n        assert nc.dims == (\"lat\", \"lon\", \"time\", \"alt\")\n        np.testing.assert_array_equal(nc[\"lat\"].coordinates, LAT)\n        np.testing.assert_array_equal(nc[\"lon\"].coordinates, LON)\n        np.testing.assert_array_equal(nc[\"time\"].coordinates, TIME)\n        np.testing.assert_array_equal(nc[\"alt\"].coordinates, ALT)",
            "def test_repr_str(self):\n        node = MockFileKeys()\n\n        assert \"source=\" in repr(node)\n        assert \"data_key=\" not in repr(node)\n\n        assert \"source=\" in str(node)\n        assert \"data_key=\" not in str(node)",
            "def test_repr_str_multiple_outputs(self):\n        node = MockFileKeysMultipleAvailable()\n\n        assert \"source=\" in repr(node)\n        assert \"data_key=\" not in repr(node)\n\n        assert \"source=\" in str(node)\n        assert \"data_key=\" not in str(node)\n\n        node = MockFileKeysMultipleAvailable(data_key=\"data\")\n\n        assert \"source=\" in repr(node)\n        assert \"data_key=\" in repr(node)\n\n        assert \"source=\" in str(node)\n        assert \"data_key=\" in str(node)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_pydap.py",
        "comments": [
            "//demo.opendap.org\"",
            "//demo.opendap.org\""
        ],
        "docstrings": [
            "\"\"\"mock pydap data source\"\"\"",
            "\"\"\"test pydap datasource\"\"\"",
            "\"\"\"test return of dataset keys\"\"\"",
            "\"\"\"test session attribute and traitlet default\"\"\"",
            "\"\"\"test get_data function of pydap\"\"\""
        ],
        "code_snippets": [
            "class MockPyDAP(PyDAP):",
            "def get_coordinates(self):\n        return Coordinates([clinspace(-25, 25, 11), clinspace(-25, 25, 11)], dims=[\"lat\", \"lon\"])",
            "def _open_url(self):\n        base = pydap.model.BaseType(name=\"key\", data=self.data)\n        dataset = pydap.model.DatasetType(name=\"dataset\")\n        dataset[\"key\"] = base\n        return dataset",
            "class TestPyDAP(object):",
            "def test_init(self):\n        node = PyDAP(source=\"mysource\", data_key=\"key\")",
            "def test_coordinates_not_implemented(self):\n        node = PyDAP(source=\"mysource\", data_key=\"key\")\n        with pytest.raises(NotImplementedError):\n            node.coordinates",
            "def test_keys(self):",
            "def test_session(self):",
            "def test_dataset(self):\n        node = MockPyDAP()\n        assert isinstance(node.dataset, pydap.model.DatasetType)",
            "def test_url_error(self):\n        node = PyDAP(source=\"mysource\")\n        with pytest.raises(Exception):\n            node.dataset",
            "def test_get_data(self):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_array.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Test Array datasource class\"\"\"",
            "\"\"\"defined get_data function\"\"\""
        ],
        "code_snippets": [
            "class TestArray(object):",
            "def test_data_array(self):\n        node = Array(source=self.data, coordinates=self.coordinates)",
            "def test_data_list(self):\n        # list is coercable to array\n        node = Array(source=[0, 1, 1], coordinates=self.coordinates)",
            "def test_invalid_data(self):\n        with pytest.raises(ValueError, match=\"Array 'source' data must be numerical\"):\n            node = Array(source=[\"a\", \"b\"], coordinates=self.coordinates)",
            "def test_get_data(self):",
            "def test_get_data_multiple(self):\n        data = np.random.rand(11, 11, 2)\n        node = Array(source=data, coordinates=self.coordinates, outputs=[\"a\", \"b\"])\n        output = node.eval(self.coordinates)\n        assert isinstance(output, UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(output[\"output\"], [\"a\", \"b\"])\n        np.testing.assert_array_equal(output.sel(output=\"a\"), data[:, :, 0])\n        np.testing.assert_array_equal(output.sel(output=\"b\"), data[:, :, 1])\n\n        node = Array(source=data, coordinates=self.coordinates, outputs=[\"a\", \"b\"], output=\"b\")\n        output = node.eval(self.coordinates)\n        assert isinstance(output, UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, data[:, :, 1])",
            "def test_coordinates(self):\n        node = Array(source=self.data, coordinates=self.coordinates)\n        assert node.coordinates\n\n        node = Array(source=self.data)\n        with pytest.raises(tl.TraitError):\n            node.coordinates",
            "def test_no_cache(self):\n        node = Array()\n        assert len(node.cache_ctrl._cache_stores) == 0"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_ogr.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestOGRRaw(object):\n    source = \"TODO\"\n    driver = \"ESRI Shapefile\"\n    layer = \"TODO\"\n    attribute = \"TODO\"",
            "def test_extents(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        node.extents",
            "def test_eval_uniform(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([podpac.clinspace(43, 44, 10), podpac.clinspace(-73, -72, 10)], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)",
            "def test_eval_point(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([43.7, -72.3], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)",
            "def test_eval_stacked(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([[[43, 43.5, 43.7], [-72.0, -72.5, -72.7]]], dims=[\"lat_lon\"])\n        output = node.eval(coords)",
            "def test_eval_nonuniform(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([[43, 43.5, 43.7], [-72.0, -72.5, -72.7]], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)\n\n        # coordinates are resampled to be uniform\n        np.testing.assert_array_equal(output[\"lat\"], [43, 43.35, 43.7])\n        np.testing.assert_array_equal(output[\"lon\"], [-72.7, -72.35, -72])",
            "def test_eval_extra_dims(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates(\n            [podpac.clinspace(43, 44, 10), podpac.clinspace(-73, -72, 10), \"2018-01-01\"], dims=[\"lat\", \"lon\", \"time\"]\n        )\n        output = node.eval(coords)",
            "def test_eval_missing_dims(self):\n        node = OGRRaw(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([\"2018-01-01\"], dims=[\"time\"])\n        with pytest.raises(RuntimeError, match=\"OGR source requires lat and lon dims\"):\n            output = node.eval(coords)\n\n\n@pytest.mark.skip(reason=\"No test file available yet\")",
            "class TestOGR(object):\n    source = \"TODO\"\n    driver = \"ESRI Shapefile\"\n    layer = \"TODO\"\n    attribute = \"TODO\"",
            "def test_eval_uniform(self):\n        node = OGR(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([podpac.clinspace(43, 44, 10), podpac.clinspace(-73, -72, 10)], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)",
            "def test_eval_point(self):\n        node = OGR(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([43.7, -72.3], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)",
            "def test_eval_stacked(self):\n        node = OGR(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([[[43, 43.5, 43.7], [-72.0, -72.5, -72.7]]], dims=[\"lat_lon\"])\n        output = node.eval(coords)",
            "def test_eval_nonuniform(self):\n        node = OGR(source=self.source, driver=self.driver, layer=self.layer, attribute=self.attribute)\n        coords = podpac.Coordinates([[43, 43.5, 43.7], [-72.0, -72.5, -72.7]], dims=[\"lat\", \"lon\"])\n        output = node.eval(coords)\n\n        # coordinates are interpolated back to the requested coordinates\n        np.testing.assert_array_equal(output[\"lat\"], coords[\"lat\"].coordinates)\n        np.testing.assert_array_equal(output[\"lon\"], coords[\"lon\"].coordinates)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_reprojected_source.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Test Reprojected Source\n    TODO: this needs to be reworked with real examples\n    \"\"\"",
            "\"\"\"test basic init of class\"\"\"",
            "\"\"\"test coordinates\"\"\"",
            "\"\"\"test get data from reprojected source\"\"\"",
            "\"\"\"test get data from reprojected source\"\"\"",
            "\"\"\"test base ref\"\"\""
        ],
        "code_snippets": [
            "class TestReprojectedSource(object):\n\n    \"\"\"Test Reprojected Source\n    TODO: this needs to be reworked with real examples\n    \"\"\"\n\n    data = np.random.rand(11, 11)\n    coordinates = Coordinates([clinspace(-25, 25, 11), clinspace(-25, 25, 11)], dims=[\"lat\", \"lon\"])\n    reprojected_coordinates = Coordinates([clinspace(-25, 50, 11), clinspace(-25, 50, 11)], dims=[\"lat\", \"lon\"])",
            "def test_init(self):",
            "def test_coordinates(self):",
            "def test_get_data(self):",
            "def test_source_interpolation(self):",
            "def test_interpolation_warning(self):\n        with pytest.warns(DeprecationWarning):\n            node = ReprojectedSource(source=Arange(), reprojected_coordinates=self.coordinates)\n        output = node.eval(node.coordinates)",
            "def test_base_ref(self):",
            "def test_deserialize_reprojected_coordinates(self):\n        with pytest.warns(DeprecationWarning):\n            node1 = ReprojectedSource(source=Node(), reprojected_coordinates=self.reprojected_coordinates)\n        with pytest.warns(DeprecationWarning):\n            node2 = ReprojectedSource(source=Node(), reprojected_coordinates=self.reprojected_coordinates.definition)\n        with pytest.warns(DeprecationWarning):\n            node3 = ReprojectedSource(source=Node(), reprojected_coordinates=self.reprojected_coordinates.json)\n\n        assert node1.reprojected_coordinates == self.reprojected_coordinates\n        assert node2.reprojected_coordinates == self.reprojected_coordinates\n        assert node3.reprojected_coordinates == self.reprojected_coordinates"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_rasterio.py",
        "comments": [],
        "docstrings": [
            "\"\"\"test rasterio data source\"\"\"",
            "\"\"\"test basic init of class\"\"\"",
            "\"\"\"test dataset attribute and trait default\"\"\"",
            "\"\"\"test default coordinates implementations\"\"\"",
            "\"\"\"test default get_data method\"\"\"",
            "\"\"\"test band descriptions methods\"\"\"",
            "\"\"\"test band count method\"\"\"",
            "\"\"\"test band keys methods\"\"\"",
            "\"\"\"test band numbers methods\"\"\""
        ],
        "code_snippets": [
            "class TestRasterio(object):",
            "def test_init(self):",
            "def test_dataset(self):",
            "def test_coordinates(self):",
            "def test_get_data(self):",
            "def test_band_count(self):",
            "def test_band_descriptions(self):",
            "def test_band_keys(self):",
            "def test_get_band_numbers(self):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_zarr.py",
        "comments": [
            "//podpac-internal-test/drought_parameters.zarr\""
        ],
        "docstrings": [],
        "code_snippets": [
            "class TestZarr(object):\n    path = os.path.join(os.path.dirname(__file__), \"assets\", \"zarr\")",
            "def test_local(self):\n        node = Zarr(source=self.path, data_key=\"a\")\n        node.close_dataset()",
            "def test_local_invalid_path(self):\n        with pytest.raises(ValueError, match=\"No Zarr store found\"):\n            node = Zarr(source=\"/does/not/exist\", data_key=\"a\")",
            "def test_dims(self):\n        node = Zarr(source=self.path)\n        assert node.dims == [\"lat\", \"lon\"]",
            "def test_available_data_keys(self):\n        node = Zarr(source=self.path)\n        assert node.available_data_keys == [\"a\", \"b\"]",
            "def test_coordinates(self):\n        node = Zarr(source=self.path, data_key=\"a\")\n        assert node.coordinates == Coordinates([[0, 1, 2], [10, 20, 30, 40]], dims=[\"lat\", \"lon\"])",
            "def test_eval(self):\n        coords = Coordinates([0, 10], dims=[\"lat\", \"lon\"])\n\n        a = Zarr(source=self.path, data_key=\"a\")\n        assert a.eval(coords)[0, 0] == 0.0\n\n        b = Zarr(source=self.path, data_key=\"b\")\n        assert b.eval(coords)[0, 0] == 1.0",
            "def test_eval_multiple(self):\n        coords = Coordinates([0, 10], dims=[\"lat\", \"lon\"])\n\n        z = Zarr(source=self.path, data_key=[\"a\", \"b\"])\n        out = z.eval(coords)\n        assert out.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"a\", \"b\"])\n        assert out.sel(output=\"a\")[0, 0] == 0.0\n        assert out.sel(output=\"b\")[0, 0] == 1.0\n\n        # single output key\n        z = Zarr(source=self.path, data_key=[\"a\"])\n        out = z.eval(coords)\n        assert out.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"a\"])\n        assert out.sel(output=\"a\")[0, 0] == 0.0\n\n        # alternate output names\n        z = Zarr(source=self.path, data_key=[\"a\", \"b\"], outputs=[\"A\", \"B\"])\n        out = z.eval(coords)\n        assert out.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"A\", \"B\"])\n        assert out.sel(output=\"A\")[0, 0] == 0.0\n        assert out.sel(output=\"B\")[0, 0] == 1.0\n\n        # default\n        z = Zarr(source=self.path)\n        out = z.eval(coords)\n        assert out.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"a\", \"b\"])\n        assert out.sel(output=\"a\")[0, 0] == 0.0\n        assert out.sel(output=\"b\")[0, 0] == 1.0\n\n    @pytest.mark.skip\n    def test_s3(self):\n        # This file no longer exists\n        path = \"s3:",
            "def test_s3(self):\n        # This file no longer exists\n        path = \"s3:"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/data/test/test_csv.py",
        "comments": [],
        "docstrings": [
            "\"\"\"test csv data source\"\"\""
        ],
        "code_snippets": [
            "class TestCSV(object):",
            "def test_init(self):\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")",
            "def test_close(self):\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")",
            "def test_get_dims(self):\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.dims == [\"lat\", \"lon\", \"time\", \"alt\"]\n\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.dims == [\"lat\", \"lon\", \"time\", \"alt\"]",
            "def test_available_data_keys(self):\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.available_data_keys == [\"data\"]\n\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.available_data_keys == [\"data\", \"other\"]\n\n        node = CSVRaw(source=self.source_no_data, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        with pytest.raises(ValueError, match=\"No data keys found\"):\n            node.available_data_keys",
            "def test_data_key(self):\n        # default\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.data_key == \"data\"\n\n        # specify\n        node = CSVRaw(source=self.source_single, data_key=\"data\", alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.data_key == \"data\"\n\n        # invalid\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(source=self.source_single, data_key=\"misc\", alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")",
            "def test_data_key_col(self):\n        # specify column\n        node = CSVRaw(source=self.source_single, data_key=4, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.data_key == 4\n\n        # invalid (out of range)\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(source=self.source_single, data_key=5, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n\n        # invalid (dimension key)\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(source=self.source_single, data_key=0, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")",
            "def test_data_key_multiple_outputs(self):\n        # default\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.data_key == [\"data\", \"other\"]\n\n        # specify multiple\n        node = CSVRaw(\n            source=self.source_multiple, data_key=[\"other\", \"data\"], alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\"\n        )\n        assert node.data_key == [\"other\", \"data\"]\n\n        # specify one\n        node = CSVRaw(source=self.source_multiple, data_key=\"other\", alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.data_key == \"other\"\n\n        # specify multiple: invalid item\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(\n                source=self.source_multiple, data_key=[\"data\", \"misc\"], alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\"\n            )\n\n        # specify one: invalid\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(source=self.source_multiple, data_key=\"misc\", alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")",
            "def test_data_key_col_multiple_outputs(self):\n        # specify multiple\n        node = CSVRaw(source=self.source_multiple, data_key=[4, 5], alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        assert node.data_key == [4, 5]\n        assert node.outputs == [\"data\", \"other\"]\n\n        # specify one\n        node = CSVRaw(source=self.source_multiple, data_key=4, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n\n        assert node.data_key == 4\n        assert node.outputs is None\n\n        # specify multiple: invalid item\n        with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(source=self.source_multiple, data_key=[4, 6], alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n\n            # specify one: invalid with pytest.raises(ValueError, match=\"Invalid data_key\"):\n            node = CSVRaw(source=self.source_multiple, data_key=6, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")",
            "def test_coordinates(self):\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        nc = node.coordinates\n        assert nc.dims == (\"lat_lon_time_alt\",)\n        np.testing.assert_array_equal(nc[\"lat\"].coordinates, self.lat)\n        np.testing.assert_array_equal(nc[\"lon\"].coordinates, self.lon)\n        np.testing.assert_array_equal(nc[\"time\"].coordinates, self.time)\n        np.testing.assert_array_equal(nc[\"alt\"].coordinates, self.alt)\n\n        # one dim (unstacked)\n        node = CSVRaw(source=self.source_one_dim)\n        nc = node.coordinates\n        assert nc.dims == (\"time\",)",
            "def test_get_data(self):\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", data_key=\"data\", crs=\"+proj=merc +vunits=m\")\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out, self.data)\n\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", data_key=\"data\", crs=\"+proj=merc +vunits=m\")\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out, self.data)\n\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", data_key=\"other\", crs=\"+proj=merc +vunits=m\")\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out, self.other)\n\n        # default\n        node = CSVRaw(source=self.source_single, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        out = node.eval(node.coordinates)\n        np.testing.assert_array_equal(out, self.data)",
            "def test_get_data_multiple(self):\n        # multiple data keys\n        node = CSVRaw(\n            source=self.source_multiple, alt_key=\"altitude\", data_key=[\"data\", \"other\"], crs=\"+proj=merc +vunits=m\"\n        )\n        out = node.eval(node.coordinates)\n        assert out.dims == (\"lat_lon_time_alt\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"data\", \"other\"])\n        np.testing.assert_array_equal(out.sel(output=\"data\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"other\"), self.other)\n\n        # single data key\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", data_key=[\"data\"], crs=\"+proj=merc +vunits=m\")\n        out = node.eval(node.coordinates)\n        assert out.dims == (\"lat_lon_time_alt\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"data\"])\n        np.testing.assert_array_equal(out.sel(output=\"data\"), self.data)\n\n        # alternate output names\n        node = CSVRaw(\n            source=self.source_multiple,\n            alt_key=\"altitude\",\n            data_key=[\"data\", \"other\"],\n            outputs=[\"a\", \"b\"],\n            crs=\"+proj=merc +vunits=m\",\n        )\n        out = node.eval(node.coordinates)\n        assert out.dims == (\"lat_lon_time_alt\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"a\", \"b\"])\n        np.testing.assert_array_equal(out.sel(output=\"a\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"b\"), self.other)\n\n        # default\n        node = CSVRaw(source=self.source_multiple, alt_key=\"altitude\", crs=\"+proj=merc +vunits=m\")\n        out = node.eval(node.coordinates)\n        assert out.dims == (\"lat_lon_time_alt\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"data\", \"other\"])\n        np.testing.assert_array_equal(out.sel(output=\"data\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"other\"), self.other)",
            "def test_cols(self):\n        node = CSVRaw(\n            source=self.source_multiple,\n            lat_key=0,\n            lon_key=1,\n            time_key=2,\n            alt_key=3,\n            data_key=5,\n            crs=\"+proj=merc +vunits=m\",\n        )\n\n        # coordinates\n        nc = node.coordinates\n        assert nc.dims == (\"lat_lon_time_alt\",)\n        np.testing.assert_array_equal(nc[\"lat\"].coordinates, self.lat)\n        np.testing.assert_array_equal(nc[\"lon\"].coordinates, self.lon)\n        np.testing.assert_array_equal(nc[\"time\"].coordinates, self.time)\n        np.testing.assert_array_equal(nc[\"alt\"].coordinates, self.alt)\n\n        # eval\n        out = node.eval(nc)\n        np.testing.assert_array_equal(out, self.other)",
            "def test_cols_multiple(self):\n        node = CSVRaw(\n            source=self.source_multiple,\n            lat_key=0,\n            lon_key=1,\n            time_key=2,\n            alt_key=3,\n            data_key=[4, 5],\n            outputs=[\"a\", \"b\"],\n            crs=\"+proj=merc +vunits=m\",\n        )\n\n        # native coordinantes\n        nc = node.coordinates\n        assert nc.dims == (\"lat_lon_time_alt\",)\n        np.testing.assert_array_equal(nc[\"lat\"].coordinates, self.lat)\n        np.testing.assert_array_equal(nc[\"lon\"].coordinates, self.lon)\n        np.testing.assert_array_equal(nc[\"time\"].coordinates, self.time)\n        np.testing.assert_array_equal(nc[\"alt\"].coordinates, self.alt)\n\n        # eval\n        out = node.eval(nc)\n        assert out.dims == (\"lat_lon_time_alt\", \"output\")\n        np.testing.assert_array_equal(out[\"output\"], [\"a\", \"b\"])\n        np.testing.assert_array_equal(out.sel(output=\"a\"), self.data)\n        np.testing.assert_array_equal(out.sel(output=\"b\"), self.other)",
            "def test_header(self):\n        node = CSVRaw(\n            source=self.source_no_header,\n            lat_key=0,\n            lon_key=1,\n            time_key=2,\n            alt_key=3,\n            data_key=4,\n            header=None,\n            crs=\"+proj=merc +vunits=m\",\n        )\n\n        # native coordinantes\n        nc = node.coordinates\n        assert nc.dims == (\"lat_lon_time_alt\",)\n        np.testing.assert_array_equal(nc[\"lat\"].coordinates, self.lat)\n        np.testing.assert_array_equal(nc[\"lon\"].coordinates, self.lon)\n        np.testing.assert_array_equal(nc[\"time\"].coordinates, self.time)\n        np.testing.assert_array_equal(nc[\"alt\"].coordinates, self.alt)\n\n        # eval\n        out = node.eval(nc)\n        np.testing.assert_array_equal(out, self.data)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/compositor.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nCompositor Summary\n\"\"\"",
            "\"\"\"A base class for compositor nodes.\n\n    Attributes\n    ----------\n    sources : list\n        Source nodes.\n    source_coordinates : :class:`podpac.Coordinates`\n        Coordinates that make each source unique. Must the same size as ``sources`` and single-dimensional. Optional.\n    multithreading : bool, optional\n        Default is False. If True, will always evaluate the compositor in serial, ignoring any MULTITHREADING settings\n\n    Notes\n    -----\n    Developers of compositor subclasses nodes need to implement the `composite` method.\n\n    Multitheading::\n      * When MULTITHREADING is False, the compositor stops evaluated sources once the output is completely filled.\n      * When MULTITHREADING is True, the compositor must evaluate every source.\n        The result is the same, but note that because of this, disabling multithreading could sometimes be faster,\n        especially if the number of threads is low.\n      * NASA data servers seem to have a hard limit of 10 simultaneous requests, so a max of 10 threads is recommend\n        for most use-cases.\n    \"\"\"",
            "\"\"\"Select and prepare sources based on requested coordinates.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            Coordinates to evaluate at compositor sources\n        _selector : :class:`podpac.core.interpolation.selectors.Selector`\n            Selector used to sub-select sources based on the interpolation scheme\n\n        Returns\n        -------\n        sources : :class:`np.ndarray`\n            Array of sources\n\n        Notes\n        -----\n         * If :attr:`source_coordinates` is defined, only sources that intersect the requested coordinates are selected.\n        \"\"\"",
            "\"\"\"Implements the rules for compositing multiple sources together. Must be implemented by child classes.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        data_arrays : generator\n            Evaluated data, in the same order as the sources. Yields a UnitsDataArray.\n        result : UnitDataArray, optional\n            An optional pre-filled array may be supplied, otherwise the output will be allocated.\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"",
            "\"\"\"Summary\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            Coordinates to evaluate at compositor sources\n\n        Yields\n        ------\n        :class:`podpac.core.units.UnitsDataArray`\n            Output from source node eval method\n        \"\"\"",
            "\"\"\"\n        Wraps the super Node.eval method in order to cache with the correct coordinates.\n\n        The output is independent of any extra dimensions, so this removes extra dimensions before caching in the\n        super eval method.\n        \"\"\"",
            "\"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"",
            "\"\"\"\n        Get the available coordinates for the Node.\n\n        Returns\n        -------\n        coords_list : list\n            available coordinates from all of the sources.\n        \"\"\"",
            "\"\"\"list of attribute names, used by __repr__ and __str__ to display minimal info about the node\"\"\""
        ],
        "code_snippets": [
            "class BaseCompositor(Node):\n    \"\"\"A base class for compositor nodes.\n\n    Attributes\n    ----------\n    sources : list\n        Source nodes.\n    source_coordinates : :class:`podpac.Coordinates`\n        Coordinates that make each source unique. Must the same size as ``sources`` and single-dimensional. Optional.\n    multithreading : bool, optional\n        Default is False. If True, will always evaluate the compositor in serial, ignoring any MULTITHREADING settings\n\n    Notes\n    -----\n    Developers of compositor subclasses nodes need to implement the `composite` method.\n\n    Multitheading::\n      * When MULTITHREADING is False, the compositor stops evaluated sources once the output is completely filled.\n      * When MULTITHREADING is True, the compositor must evaluate every source.\n        The result is the same, but note that because of this, disabling multithreading could sometimes be faster,\n        especially if the number of threads is low.\n      * NASA data servers seem to have a hard limit of 10 simultaneous requests, so a max of 10 threads is recommend\n        for most use-cases.\n    \"\"\"\n\n    sources = tl.List(trait=NodeTrait()).tag(attr=True, required=True)\n    source_coordinates = tl.Instance(Coordinates, allow_none=True, default_value=None).tag(attr=True)\n    multithreading = tl.Bool(False)\n\n    @tl.default(\"multithreading\")",
            "def _default_multithreading(self):\n        return settings[\"MULTITHREADING\"]\n\n    dims = tl.List(trait=Dimension()).tag(attr=True)\n    auto_outputs = tl.Bool(False)\n\n    # debug traits\n    _eval_sources = tl.Any()\n\n    @tl.validate(\"sources\")",
            "def _validate_sources(self, d):\n        sources = d[\"value\"]\n\n        n = np.sum([source.outputs is None for source in sources])\n        if not (n == 0 or n == len(sources)):\n            raise ValueError(\n                \"Cannot composite standard sources with multi-output sources. \"\n                \"The sources must all be standard single-output nodes or all multi-output nodes.\"\n            )\n\n        return sources\n\n    @tl.validate(\"source_coordinates\")",
            "def _validate_source_coordinates(self, d):\n        if d[\"value\"] is None:\n            return None\n\n        if d[\"value\"].ndim != 1:\n            raise ValueError(\"Invalid source_coordinates, invalid ndim (%d != 1)\" % d[\"value\"].ndim)\n\n        if d[\"value\"].size != len(self.sources):\n            raise ValueError(\n                \"Invalid source_coordinates, source and source_coordinates size mismatch (%d != %d)\"\n                % (d[\"value\"].size, len(self.sources))\n            )\n\n        return d[\"value\"]\n\n    @tl.default(\"outputs\")",
            "def _default_outputs(self):\n        if not self.auto_outputs:\n            return None\n\n        # autodetect outputs from sources\n        if all(source.outputs is None for source in self.sources):\n            outputs = None\n\n        elif all(source.outputs is not None and source.output is None for source in self.sources):\n            outputs = []\n            for source in self.sources:\n                for output in source.outputs:\n                    if output not in outputs:\n                        outputs.append(output)\n\n            if len(outputs) == 0:\n                outputs = None\n\n        else:\n            raise RuntimeError(\n                \"Compositor sources were not validated correctly. \"\n                \"Cannot composite standard sources with multi-output sources.\"\n            )\n\n        return outputs",
            "def select_sources(self, coordinates, _selector=None):\n        \"\"\"Select and prepare sources based on requested coordinates.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            Coordinates to evaluate at compositor sources\n        _selector : :class:`podpac.core.interpolation.selectors.Selector`\n            Selector used to sub-select sources based on the interpolation scheme\n\n        Returns\n        -------\n        sources : :class:`np.ndarray`\n            Array of sources\n\n        Notes\n        -----\n         * If :attr:`source_coordinates` is defined, only sources that intersect the requested coordinates are selected.\n        \"\"\"\n\n        # select intersecting sources, if possible\n        if self.source_coordinates is None:\n            sources = self.sources\n        else:\n            try:\n                if _selector is not None:\n                    _, I = _selector(self.source_coordinates, coordinates, index_type=\"numpy\")\n                else:\n                    _, I = self.source_coordinates.intersect(coordinates, outer=True, return_index=True)\n            except:\n                # Likely non-monotonic coordinates\n                _, I = self.source_coordinates.intersect(coordinates, outer=False, return_index=True)\n            i = I[0]\n            sources = np.array(self.sources)[i].tolist()\n\n        return sources",
            "def composite(self, coordinates, data_arrays, result=None):\n        \"\"\"Implements the rules for compositing multiple sources together. Must be implemented by child classes.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        data_arrays : generator\n            Evaluated data, in the same order as the sources. Yields a UnitsDataArray.\n        result : UnitDataArray, optional\n            An optional pre-filled array may be supplied, otherwise the output will be allocated.\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"\n\n        raise NotImplementedError()",
            "def iteroutputs(self, coordinates, _selector=None):\n        \"\"\"Summary\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            Coordinates to evaluate at compositor sources\n\n        Yields\n        ------\n        :class:`podpac.core.units.UnitsDataArray`\n            Output from source node eval method\n        \"\"\"\n\n        # get sources, potentially downselected\n        sources = self.select_sources(coordinates, _selector)\n\n        if settings[\"DEBUG\"]:\n            self._eval_sources = sources\n\n        if len(sources) == 0:\n            yield self.create_output_array(coordinates)\n            return\n\n        if self.multithreading:\n            n_threads = thread_manager.request_n_threads(len(sources))\n            if n_threads == 1:\n                thread_manager.release_n_threads(n_threads)\n        else:\n            n_threads = 0\n\n        if self.multithreading and n_threads > 1:\n            # evaluate nodes in parallel using thread pool\n            self._multi_threaded = True\n            pool = thread_manager.get_thread_pool(processes=n_threads)\n            outputs = pool.map(lambda src: src.eval(coordinates, _selector=_selector), sources)\n            pool.close()\n            thread_manager.release_n_threads(n_threads)\n            for output in outputs:\n                yield output\n\n        else:\n            # evaluate nodes serially\n            self._multi_threaded = False\n            for src in sources:\n                yield src.eval(coordinates, _selector=_selector)\n\n    @common_doc(COMMON_COMPOSITOR_DOC)",
            "def eval(self, coordinates, **kwargs):\n        \"\"\"\n        Wraps the super Node.eval method in order to cache with the correct coordinates.\n\n        The output is independent of any extra dimensions, so this removes extra dimensions before caching in the\n        super eval method.\n        \"\"\"\n\n        super_coordinates = coordinates\n\n        # remove extra dimensions\n        if self.dims:\n            extra = [\n                c.name\n                for c in coordinates.values()\n                if (isinstance(c, Coordinates1d) and c.name not in self.dims)\n                or (isinstance(c, StackedCoordinates) and all(dim not in self.dims for dim in c.dims))\n            ]\n            super_coordinates = super_coordinates.drop(extra)\n\n        # note: super().eval (not self._eval)\n        output = super().eval(super_coordinates, **kwargs)\n\n        if settings[\"DEBUG\"]:\n            self._requested_coordinates = coordinates\n\n        return output\n\n    @common_doc(COMMON_COMPOSITOR_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"\n\n        self._evaluated_coordinates = coordinates\n        outputs = self.iteroutputs(coordinates, _selector)\n        output = self.composite(coordinates, outputs, output)\n        return output",
            "def find_coordinates(self):\n        \"\"\"\n        Get the available coordinates for the Node.\n\n        Returns\n        -------\n        coords_list : list\n            available coordinates from all of the sources.\n        \"\"\"\n\n        return [coords for source in self.sources for coords in source.find_coordinates()]\n\n    @property",
            "def _repr_keys(self):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/ordered_compositor.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Compositor that combines sources based on their order in self.sources.\n\n    The sources should generally be interpolated before being composited (i.e. not raw datasources).\n\n    Attributes\n    ----------\n    sources : list\n        Source nodes, in order of preference. Later sources are only used where earlier sources do not provide data.\n    source_coordinates : :class:`podpac.Coordinates`\n        Coordinates that make each source unique. Must the same size as ``sources`` and single-dimensional. Optional.\n    multithreading : bool, optional\n        Default is True. If True, will always evaluate the compositor in serial, ignoring any MULTITHREADING settings\n\n    \"\"\"",
            "\"\"\"Composites data_arrays in order that they appear. Once a request contains no nans, the result is returned.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        data_arrays : generator\n            Evaluated source data, in the same order as the sources.\n        result : podpac.UnitsDataArray, optional\n            {eval_output}\n\n        Returns\n        -------\n        {eval_return} This composites the sources together until there are no nans or no more sources.\n        \"\"\""
        ],
        "code_snippets": [
            "class OrderedCompositor(BaseCompositor):\n    \"\"\"Compositor that combines sources based on their order in self.sources.\n\n    The sources should generally be interpolated before being composited (i.e. not raw datasources).\n\n    Attributes\n    ----------\n    sources : list\n        Source nodes, in order of preference. Later sources are only used where earlier sources do not provide data.\n    source_coordinates : :class:`podpac.Coordinates`\n        Coordinates that make each source unique. Must the same size as ``sources`` and single-dimensional. Optional.\n    multithreading : bool, optional\n        Default is True. If True, will always evaluate the compositor in serial, ignoring any MULTITHREADING settings\n\n    \"\"\"\n\n    multithreading = tl.Bool(False)\n\n    @common_doc(COMMON_COMPOSITOR_DOC)",
            "def composite(self, coordinates, data_arrays, result=None):\n        \"\"\"Composites data_arrays in order that they appear. Once a request contains no nans, the result is returned.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        data_arrays : generator\n            Evaluated source data, in the same order as the sources.\n        result : podpac.UnitsDataArray, optional\n            {eval_output}\n\n        Returns\n        -------\n        {eval_return} This composites the sources together until there are no nans or no more sources.\n        \"\"\"\n\n        if result is None:\n            result = self.create_output_array(coordinates)\n        else:\n            result[:] = np.nan\n\n        mask = UnitsDataArray.create(coordinates, outputs=self.outputs, data=0, dtype=bool)\n        for data in data_arrays:\n            if self.outputs is None:\n                try:\n                    data = data.transpose(*result.dims)\n                except ValueError:\n                    raise NodeException(\n                        \"Cannot evaluate compositor with requested dims %s. \"\n                        \"The compositor source dims are %s. \"\n                        \"Specify the compositor 'dims' attribute to ignore extra requested dims.\"\n                        % (coordinates.dims, data.dims)\n                    )\n                self._composite(result, data, mask)\n            else:\n                for name in data[\"output\"]:\n                    self._composite(result.sel(output=name), data.sel(output=name), mask.sel(output=name))\n\n            # stop if the results are full\n            if np.all(mask):\n                break\n\n        return result\n\n    @staticmethod",
            "def _composite(result, data, mask):\n        source_mask = np.isfinite(data.data)\n        b = ~mask & source_mask\n        result.data[b.data] = data.data[b.data]\n        mask |= source_mask"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/tile_compositor.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Compositor that combines tiled sources.\n\n    The requested data does not need to be interpolated by the sources before being composited\n\n    Attributes\n    ----------\n    sources : list\n        Source nodes (tiles). The sources should not be overlapping.\n    source_coordinates : :class:`podpac.Coordinates`\n        Coordinates that make each source unique. Must the same size as ``sources`` and single-dimensional. Optional.\n    \"\"\"",
            "\"\"\"Composites data_arrays (tiles) into a single result.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        data_arrays : generator\n            Evaluated source data.\n        result : podpac.UnitsDataArray, optional\n            {eval_output}\n\n        Returns\n        -------\n        {eval_return} This composites tiled sources into a single result.\n        \"\"\"",
            "\"\"\"\n        Get composited source data, without interpolation.\n\n        Arguments\n        ---------\n        bounds : dict\n            Dictionary of bounds by dimension, optional.\n            Keys must be dimension names, and values are (min, max) tuples, e.g. ``{'lat': (10, 20)}``.\n\n        Returns\n        -------\n        data : UnitsDataArray\n            Source data\n        \"\"\""
        ],
        "code_snippets": [
            "class TileCompositorRaw(BaseCompositor):\n    \"\"\"Compositor that combines tiled sources.\n\n    The requested data does not need to be interpolated by the sources before being composited\n\n    Attributes\n    ----------\n    sources : list\n        Source nodes (tiles). The sources should not be overlapping.\n    source_coordinates : :class:`podpac.Coordinates`\n        Coordinates that make each source unique. Must the same size as ``sources`` and single-dimensional. Optional.\n    \"\"\"\n\n    @common_doc(COMMON_COMPOSITOR_DOC)",
            "def composite(self, coordinates, data_arrays, result=None):\n        \"\"\"Composites data_arrays (tiles) into a single result.\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n        data_arrays : generator\n            Evaluated source data.\n        result : podpac.UnitsDataArray, optional\n            {eval_output}\n\n        Returns\n        -------\n        {eval_return} This composites tiled sources into a single result.\n        \"\"\"\n\n        # TODO: Fix boundary information on the combined data arrays\n        res = next(data_arrays)\n        bounds = res.attrs.get(\"bounds\", {})\n        for arr in data_arrays:\n            res = res.combine_first(arr)\n\n            # combine_first overrides MultiIndex names, even if they match. Reset them here:\n            if int(xr.__version__.split(\".\")[0]) < 2022:  # no longer necessary in newer versions of xarray\n                for dim, index in res.indexes.items():\n                    if isinstance(index, pd.MultiIndex):\n                        res = res.reindex({dim: pd.MultiIndex.from_tuples(index.values, names=arr.indexes[dim].names)})\n\n            obounds = arr.attrs.get(\"bounds\", {})\n            bounds = {\n                k: (min(bounds[k][0], obounds[k][0]), max(bounds[k][1], obounds[k][1])) for k in bounds if k in obounds\n            }\n        res = UnitsDataArray(res)\n        if bounds:\n            res.attrs[\"bounds\"] = bounds\n        if \"geotransform\" in res.attrs:  # Really hard to get the geotransform right, handle it in Coordinates\n            del res.attrs[\"geotransform\"]\n        if result is not None:\n            result.data[:] = res.transpose(*result.dims).data\n            return result\n        return res",
            "def get_source_data(self, bounds={}):\n        \"\"\"\n        Get composited source data, without interpolation.\n\n        Arguments\n        ---------\n        bounds : dict\n            Dictionary of bounds by dimension, optional.\n            Keys must be dimension names, and values are (min, max) tuples, e.g. ``{'lat': (10, 20)}``.\n\n        Returns\n        -------\n        data : UnitsDataArray\n            Source data\n        \"\"\"\n\n        if any(not hasattr(source, \"get_source_data\") for source in self.sources):\n            raise ValueError(\n                \"Cannot get composited source data; all sources must have `get_source_data` implemented (such as nodes derived from a DataSource or TileCompositor node).\"\n            )\n\n        coords = None  # n/a\n        source_data_arrays = (source.get_source_data(bounds) for source in self.sources)  # generator\n        return self.composite(coords, source_data_arrays)",
            "class TileCompositor(InterpolationMixin, TileCompositorRaw):\n    pass"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/test/test_ordered_compositor.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestOrderedCompositor(object):",
            "def test_composite(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = False\n\n            acoords = podpac.Coordinates([[-1, 0, 1], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n            asource = np.ones(acoords.shape)\n            asource[0, :] = np.nan\n            a = Array(source=asource, coordinates=acoords, interpolation=\"bilinear\")\n\n            bcoords = podpac.Coordinates([[0, 1, 2, 3], [10, 20, 30, 40]], dims=[\"lat\", \"lon\"])\n            bsource = np.zeros(bcoords.shape)\n            bsource[:, 0] = np.nan\n            b = Array(source=bsource, coordinates=bcoords, interpolation=\"bilinear\")\n\n            coords = podpac.Coordinates([[0, 1, 2], [10, 20, 30, 40, 50]], dims=[\"lat\", \"lon\"])\n\n            node = OrderedCompositor(sources=[a, b])\n            expected = np.array(\n                [[1.0, 1.0, 1.0, 0.0, np.nan], [1.0, 1.0, 1.0, 0.0, np.nan], [np.nan, np.nan, 0.0, 0.0, np.nan]]\n            )\n            np.testing.assert_allclose(node.eval(coords), expected, equal_nan=True)\n\n            node = OrderedCompositor(sources=[b, a])\n            expected = np.array(\n                [[1.0, 1.0, 0.0, 0.0, np.nan], [1.0, 1.0, 0.0, 0.0, np.nan], [np.nan, np.nan, 0.0, 0.0, np.nan]]\n            )\n            np.testing.assert_allclose(node.eval(coords), expected, equal_nan=True)",
            "def test_composite_multithreaded(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 8\n\n            acoords = podpac.Coordinates([[-1, 0, 1], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n            asource = np.ones(acoords.shape)\n            asource[0, :] = np.nan\n            a = Array(source=asource, coordinates=acoords, interpolation=\"bilinear\")\n\n            bcoords = podpac.Coordinates([[0, 1, 2, 3], [10, 20, 30, 40]], dims=[\"lat\", \"lon\"])\n            bsource = np.zeros(bcoords.shape)\n            bsource[:, 0] = np.nan\n            b = Array(source=bsource, coordinates=bcoords, interpolation=\"bilinear\")\n\n            coords = podpac.Coordinates([[0, 1, 2], [10, 20, 30, 40, 50]], dims=[\"lat\", \"lon\"])\n\n            node = OrderedCompositor(sources=[a, b])\n            expected = np.array(\n                [[1.0, 1.0, 1.0, 0.0, np.nan], [1.0, 1.0, 1.0, 0.0, np.nan], [np.nan, np.nan, 0.0, 0.0, np.nan]]\n            )\n            np.testing.assert_allclose(node.eval(coords), expected, equal_nan=True)\n\n            node = OrderedCompositor(sources=[b, a])\n            expected = np.array(\n                [[1.0, 1.0, 0.0, 0.0, np.nan], [1.0, 1.0, 0.0, 0.0, np.nan], [np.nan, np.nan, 0.0, 0.0, np.nan]]\n            )\n            np.testing.assert_allclose(node.eval(coords), expected, equal_nan=True)",
            "def test_composite_short_circuit(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = False\n            podpac.settings[\"DEBUG\"] = True\n\n            coords = podpac.Coordinates([[0, 1], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n            a = Array(source=np.ones(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n            b = Array(source=np.zeros(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n            node = OrderedCompositor(sources=[a, b])\n            output = node.eval(coords)\n            np.testing.assert_array_equal(output, a.source)\n            assert node._eval_sources[0]._output is not None\n            assert node._eval_sources[1]._output is None",
            "def test_composite_short_circuit_multithreaded(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 8\n            podpac.settings[\"DEBUG\"] = True\n\n            coords = podpac.Coordinates([[0, 1], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n            n_threads_before = podpac.core.managers.multi_threading.thread_manager._n_threads_used\n            a = Array(source=np.ones(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n            b = Array(source=np.zeros(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n            node = OrderedCompositor(sources=[a, b], multithreading=True)\n            node2 = OrderedCompositor(sources=[a, b], multithreading=False)\n            output = node.eval(coords)\n            output2 = node2.eval(coords)\n            np.testing.assert_array_equal(output, a.source)\n            np.testing.assert_array_equal(output2, a.source)\n            assert node._multi_threaded == True\n            assert node2._multi_threaded == False\n            assert podpac.core.managers.multi_threading.thread_manager._n_threads_used == n_threads_before",
            "def test_composite_into_result(self):\n        coords = podpac.Coordinates([[0, 1], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n        a = Array(source=np.ones(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n        b = Array(source=np.zeros(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n        node = OrderedCompositor(sources=[a, b])\n        result = node.create_output_array(coords, data=np.random.random(coords.shape))\n        output = node.eval(coords, output=result)\n        np.testing.assert_array_equal(output, a.source)\n        np.testing.assert_array_equal(result, a.source)",
            "def test_composite_multiple_outputs(self):\n        node = OrderedCompositor(sources=[MULTI_0_XY, MULTI_1_XY], auto_outputs=True)\n        output = node.eval(COORDS)\n        assert output.dims == (\"lat\", \"lon\", \"time\", \"output\")\n        np.testing.assert_array_equal(output[\"output\"], [\"x\", \"y\"])\n        np.testing.assert_array_equal(output.sel(output=\"x\"), np.full(COORDS.shape, 0))\n        np.testing.assert_array_equal(output.sel(output=\"y\"), np.full(COORDS.shape, 0))\n\n        node = OrderedCompositor(sources=[MULTI_1_XY, MULTI_0_XY], auto_outputs=True)\n        output = node.eval(COORDS)\n        assert output.dims == (\"lat\", \"lon\", \"time\", \"output\")\n        np.testing.assert_array_equal(output[\"output\"], [\"x\", \"y\"])\n        np.testing.assert_array_equal(output.sel(output=\"x\"), np.full(COORDS.shape, 1))\n        np.testing.assert_array_equal(output.sel(output=\"y\"), np.full(COORDS.shape, 1))",
            "def test_composite_combine_multiple_outputs(self):\n        node = OrderedCompositor(sources=[MULTI_0_XY, MULTI_1_XY, MULTI_2_X, MULTI_3_Z], auto_outputs=True)\n        output = node.eval(COORDS)\n        assert output.dims == (\"lat\", \"lon\", \"time\", \"output\")\n        np.testing.assert_array_equal(output[\"output\"], [\"x\", \"y\", \"z\"])\n        np.testing.assert_array_equal(output.sel(output=\"x\"), np.full(COORDS.shape, 0))\n        np.testing.assert_array_equal(output.sel(output=\"y\"), np.full(COORDS.shape, 0))\n        np.testing.assert_array_equal(output.sel(output=\"z\"), np.full(COORDS.shape, 3))\n\n        node = OrderedCompositor(sources=[MULTI_3_Z, MULTI_2_X, MULTI_0_XY, MULTI_1_XY], auto_outputs=True)\n        output = node.eval(COORDS)\n        assert output.dims == (\"lat\", \"lon\", \"time\", \"output\")\n        np.testing.assert_array_equal(output[\"output\"], [\"z\", \"x\", \"y\"])\n        np.testing.assert_array_equal(output.sel(output=\"x\"), np.full(COORDS.shape, 2))\n        np.testing.assert_array_equal(output.sel(output=\"y\"), np.full(COORDS.shape, 0))\n        np.testing.assert_array_equal(output.sel(output=\"z\"), np.full(COORDS.shape, 3))\n\n        node = OrderedCompositor(sources=[MULTI_2_X, MULTI_4_YX], auto_outputs=True)\n        output = node.eval(COORDS)\n        assert output.dims == (\"lat\", \"lon\", \"time\", \"output\")\n        np.testing.assert_array_equal(output[\"output\"], [\"x\", \"y\"])\n        np.testing.assert_array_equal(output.sel(output=\"x\"), np.full(COORDS.shape, 2))\n        np.testing.assert_array_equal(output.sel(output=\"y\"), np.full(COORDS.shape, 4))",
            "def test_composite_stacked_unstacked(self):\n        anative = podpac.Coordinates([podpac.clinspace((0, 1), (1, 2), size=3)], dims=[\"lat_lon\"])\n        bnative = podpac.Coordinates([podpac.clinspace(-2, 3, 3), podpac.clinspace(-1, 4, 3)], dims=[\"lat\", \"lon\"])\n        a = Array(source=np.random.rand(3), coordinates=anative, interpolation=\"nearest\")\n        b = Array(source=np.random.rand(3, 3) + 2, coordinates=bnative, interpolation=\"nearest\")\n\n        coords = podpac.Coordinates([podpac.clinspace(-3, 4, 32), podpac.clinspace(-2, 5, 32)], dims=[\"lat\", \"lon\"])\n\n        node = OrderedCompositor(sources=[a, b])\n        o = node.eval(coords)\n        # Check that both data sources are being used in the interpolation\n        assert np.any(o.data >= 2)\n        assert np.any(o.data <= 1)",
            "def test_composite_extra_dims(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = False\n\n            coords = podpac.Coordinates([[0, 1], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n            a = Array(source=np.ones(coords.shape), coordinates=coords, interpolation=\"bilinear\")\n\n            extra = podpac.Coordinates([coords[\"lat\"], coords[\"lon\"], \"2020-01-01\"], dims=[\"lat\", \"lon\", \"time\"])\n\n            # dims not provided, eval fails with extra dims\n            node = OrderedCompositor(sources=[a])\n            np.testing.assert_array_equal(node.eval(coords), a.source)\n            with pytest.raises(podpac.NodeException, match=\"Cannot evaluate compositor with requested dims\"):\n                node.eval(extra)\n\n            # dims provided, remove extra dims\n            node = OrderedCompositor(sources=[a], dims=[\"lat\", \"lon\"])\n            np.testing.assert_array_equal(node.eval(coords), a.source)\n            np.testing.assert_array_equal(node.eval(extra), a.source)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/test/test_base_compositor.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class MockComposite(BaseCompositor):",
            "def composite(self, coordinates, outputs, result=None):\n        if result is None:\n            result = self.create_output_array(coordinates)\n        output = next(outputs)\n        try:\n            result[:] = output.transpose(*result.dims)\n        except ValueError:\n            raise podpac.NodeException(\"Cannot evaluate compositor with requested dims\")\n        return result",
            "class TestBaseCompositor(object):",
            "def test_init(self):\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        repr(node)",
            "def test_source_coordinates(self):\n        # none (default)\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        assert node.source_coordinates is None\n\n        # unstacked\n        node = BaseCompositor(\n            sources=[podpac.algorithm.Arange(), podpac.algorithm.SinCoords()],\n            source_coordinates=podpac.Coordinates([[0, 1]], dims=[\"time\"]),\n        )\n\n        # stacked\n        node = BaseCompositor(\n            sources=[podpac.algorithm.Arange(), podpac.algorithm.SinCoords()],\n            source_coordinates=podpac.Coordinates([[[0, 1], [10, 20]]], dims=[\"time_alt\"]),\n        )\n\n        # invalid size\n        with pytest.raises(ValueError, match=\"Invalid source_coordinates, source and source_coordinates size mismatch\"):\n            node = BaseCompositor(\n                sources=[podpac.algorithm.Arange(), podpac.algorithm.SinCoords()],\n                source_coordinates=podpac.Coordinates([[0, 1, 2]], dims=[\"time\"]),\n            )\n\n        with pytest.raises(ValueError, match=\"Invalid source_coordinates, source and source_coordinates size mismatch\"):\n            node = BaseCompositor(\n                sources=[podpac.algorithm.Arange(), podpac.algorithm.SinCoords()],\n                source_coordinates=podpac.Coordinates([[0, 1, 2]], dims=[\"time\"]),\n            )\n\n        # invalid ndims\n        with pytest.raises(ValueError, match=\"Invalid source_coordinates\"):\n            node = BaseCompositor(\n                sources=[podpac.algorithm.Arange(), podpac.algorithm.SinCoords()],\n                source_coordinates=podpac.Coordinates([[0, 1], [10, 20]], dims=[\"time\", \"alt\"]),\n            )",
            "def test_select_sources_default(self):\n        node = BaseCompositor(sources=[DataSource(), DataSource(), podpac.algorithm.Arange()])\n        sources = node.select_sources(podpac.Coordinates([[0, 10]], [\"time\"]))\n\n        assert isinstance(sources, list)\n        assert len(sources) == 3",
            "def test_select_sources_intersection(self):\n        source_coords = podpac.Coordinates([[0, 10]], [\"time\"])\n        node = BaseCompositor(sources=[DataSource(), DataSource()], source_coordinates=source_coords)\n\n        # select all\n        selected = node.select_sources(source_coords)\n        assert len(selected) == 2\n        assert selected[0] == node.sources[0]\n        assert selected[1] == node.sources[1]\n\n        # select first\n        c = podpac.Coordinates([podpac.clinspace(0, 1, 10), podpac.clinspace(0, 1, 11), 0], [\"lat\", \"lon\", \"time\"])\n        selected = node.select_sources(c)\n        assert len(selected) == 1\n        assert selected[0] == node.sources[0]\n\n        # select second\n        c = podpac.Coordinates([podpac.clinspace(0, 1, 10), podpac.clinspace(0, 1, 11), 10], [\"lat\", \"lon\", \"time\"])\n        selected = node.select_sources(c)\n        assert len(selected) == 1\n        assert selected[0] == node.sources[1]\n\n        # select none\n        c = podpac.Coordinates([podpac.clinspace(0, 1, 10), podpac.clinspace(0, 1, 11), 100], [\"lat\", \"lon\", \"time\"])\n        selected = node.select_sources(c)\n        assert len(selected) == 0",
            "def test_iteroutputs_empty(self):\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        outputs = node.iteroutputs(podpac.Coordinates([-1, -1, -1], dims=[\"lat\", \"lon\", \"time\"]))\n        np.testing.assert_array_equal(next(outputs), [[[np.nan]]])\n        np.testing.assert_array_equal(next(outputs), [[[np.nan]]])\n        np.testing.assert_array_equal(next(outputs), [[[np.nan]]])\n        with pytest.raises(StopIteration):\n            next(outputs)",
            "def test_iteroutputs_singlethreaded(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = False\n\n            node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n            outputs = node.iteroutputs(COORDS)\n            np.testing.assert_array_equal(next(outputs), LAT)\n            np.testing.assert_array_equal(next(outputs), LON)\n            np.testing.assert_array_equal(next(outputs), TIME)\n            with pytest.raises(StopIteration):\n                next(outputs)\n            assert node._multi_threaded == False",
            "def test_iteroutputs_multithreaded(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 8\n\n            n_threads_before = podpac.core.managers.multi_threading.thread_manager._n_threads_used\n            node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n            outputs = node.iteroutputs(COORDS)\n            np.testing.assert_array_equal(next(outputs), LAT)\n            np.testing.assert_array_equal(next(outputs), LON)\n            np.testing.assert_array_equal(next(outputs), TIME)\n            with pytest.raises(StopIteration):\n                next(outputs)\n            assert node._multi_threaded == True\n            assert podpac.core.managers.multi_threading.thread_manager._n_threads_used == n_threads_before",
            "def test_iteroutputs_n_threads_1(self):\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 1\n\n            n_threads_before = podpac.core.managers.multi_threading.thread_manager._n_threads_used\n            node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n            outputs = node.iteroutputs(COORDS)\n            np.testing.assert_array_equal(next(outputs), LAT)\n            np.testing.assert_array_equal(next(outputs), LON)\n            np.testing.assert_array_equal(next(outputs), TIME)\n            with pytest.raises(StopIteration):\n                next(outputs)\n            assert node._multi_threaded == False\n            assert podpac.core.managers.multi_threading.thread_manager._n_threads_used == n_threads_before",
            "def test_composite(self):\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        with pytest.raises(NotImplementedError):\n            node.composite(COORDS, iter(()))",
            "def test_eval(self):\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        with pytest.raises(NotImplementedError):\n            node.eval(COORDS)\n\n        node = MockComposite(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        output = node.eval(COORDS)\n        np.testing.assert_array_equal(output, LAT)",
            "def test_eval_extra_dims(self):\n        coords = COORDS.drop(\"time\")\n        a = Array(source=np.ones(coords.shape), coordinates=coords)\n        b = Array(source=np.zeros(coords.shape), coordinates=coords)\n\n        # no dims provided, evaluation fails with extra requested dims\n        node = MockComposite(sources=[a, b])\n        np.testing.assert_array_equal(node.eval(coords), a.source)\n        with pytest.raises(podpac.NodeException, match=\"Cannot evaluate compositor with requested dims\"):\n            node.eval(COORDS)\n\n        # dims provided, evaluation should succeed with extra requested dims\n        node = MockComposite(sources=[a, b], dims=[\"lat\", \"lon\"])\n        np.testing.assert_array_equal(node.eval(coords), a.source)\n        np.testing.assert_array_equal(node.eval(COORDS), a.source)\n\n        # drop stacked dimensions if none of its dimensions are needed\n        c = podpac.Coordinates(\n            [COORDS[\"lat\"], COORDS[\"lon\"], [COORDS[\"time\"], [10, 20]]], dims=[\"lat\", \"lon\", \"time_alt\"]\n        )\n        np.testing.assert_array_equal(node.eval(c), a.source)\n\n        # TODO\n        # but don't drop stacked dimensions if any of its dimensions are needed\n        # c = podpac.Coordinates([[COORDS['lat'], COORDS['lon'], np.arange(COORDS['lat'].size)]], dims=['lat_lon_alt'])\n        # np.testing.assert_array_equal(node.eval(c), np.ones(COORDS['lat'].size))\n\n        # dims can also be specified by the node",
            "class MockComposite2(MockComposite):\n            dims = [\"lat\", \"lon\"]\n\n        node = MockComposite2(sources=[a, b])\n        np.testing.assert_array_equal(node.eval(coords), a.source)\n        np.testing.assert_array_equal(node.eval(COORDS), a.source)",
            "def test_find_coordinates(self):\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n\n        coord_list = node.find_coordinates()\n        assert isinstance(coord_list, list)\n        assert len(coord_list) == 3",
            "def test_outputs(self):\n        # standard single-output\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME])\n        assert node.outputs is None\n\n        # even if the sources have multiple outputs, the default here is outputs\n        node = BaseCompositor(sources=[MULTI_0_XY, MULTI_1_XY])\n        assert node.outputs is None",
            "def test_auto_outputs(self):\n        # autodetect single-output\n        node = BaseCompositor(sources=[ARRAY_LAT, ARRAY_LON, ARRAY_TIME], auto_outputs=True)\n        assert node.outputs is None\n\n        # autodetect multi-output\n        node = BaseCompositor(sources=[MULTI_0_XY, MULTI_1_XY], auto_outputs=True)\n        assert node.outputs == [\"x\", \"y\"]\n\n        node = BaseCompositor(sources=[MULTI_0_XY, MULTI_3_Z], auto_outputs=True)\n        assert node.outputs == [\"x\", \"y\", \"z\"]\n\n        node = BaseCompositor(sources=[MULTI_3_Z, MULTI_0_XY], auto_outputs=True)\n        assert node.outputs == [\"z\", \"x\", \"y\"]\n\n        node = BaseCompositor(sources=[MULTI_0_XY, MULTI_4_YX], auto_outputs=True)\n        assert node.outputs == [\"x\", \"y\"]\n\n        # mixed\n        with pytest.raises(ValueError, match=\"Cannot composite standard sources with multi-output sources.\"):\n            node = BaseCompositor(sources=[MULTI_2_X, ARRAY_LAT], auto_outputs=True)\n\n        # no sources\n        node = BaseCompositor(sources=[], auto_outputs=True)\n        assert node.outputs is None",
            "def test_forced_invalid_sources(self):",
            "class MyCompositor(BaseCompositor):\n            sources = [MULTI_2_X, ARRAY_LAT]\n            auto_outputs = True\n\n        node = MyCompositor()\n        with pytest.raises(RuntimeError, match=\"Compositor sources were not validated correctly\"):\n            node.outputs"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/compositor/test/test_tiled_compositor.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestTileCompositor(object):",
            "def test_composition(self):\n        a = ArrayRaw(source=np.arange(5) + 100, coordinates=podpac.Coordinates([[0, 1, 2, 3, 4]], dims=[\"lat\"]))\n        b = ArrayRaw(source=np.arange(5) + 200, coordinates=podpac.Coordinates([[5, 6, 7, 8, 9]], dims=[\"lat\"]))\n        c = ArrayRaw(source=np.arange(5) + 300, coordinates=podpac.Coordinates([[10, 11, 12, 13, 14]], dims=[\"lat\"]))\n\n        node = TileCompositorRaw(sources=[a, b, c])\n\n        output = node.eval(podpac.Coordinates([[3.5, 4.5, 5.5]], dims=[\"lat\"]))\n        np.testing.assert_array_equal(output[\"lat\"], [3, 4, 5, 6])\n        np.testing.assert_array_equal(output, [103, 104, 200, 201])",
            "def test_interpolation(self):\n        a = ArrayRaw(source=np.arange(5) + 100, coordinates=podpac.Coordinates([[0, 1, 2, 3, 4]], dims=[\"lat\"]))\n        b = ArrayRaw(source=np.arange(5) + 200, coordinates=podpac.Coordinates([[5, 6, 7, 8, 9]], dims=[\"lat\"]))\n        c = ArrayRaw(source=np.arange(5) + 300, coordinates=podpac.Coordinates([[10, 11, 12, 13, 14]], dims=[\"lat\"]))\n\n        node = TileCompositor(sources=[a, b, c], interpolation=\"bilinear\")\n\n        output = node.eval(podpac.Coordinates([[3.5, 4.5, 5.5]], dims=[\"lat\"]))\n        np.testing.assert_array_equal(output[\"lat\"], [3.5, 4.5, 5.5])\n        np.testing.assert_array_equal(output, [103.5, 152.0, 200.5])",
            "def test_composition_stacked_multiindex_names(self):\n        a = ArrayRaw(\n            source=np.arange(5) + 100,\n            coordinates=podpac.Coordinates([[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]], dims=[\"lat_lon\"]),\n        )\n        b = ArrayRaw(\n            source=np.arange(5) + 200,\n            coordinates=podpac.Coordinates([[[5, 6, 7, 8, 9], [5, 6, 7, 8, 9]]], dims=[\"lat_lon\"]),\n        )\n\n        node = TileCompositorRaw(sources=[a, b])\n\n        output = node.eval(podpac.Coordinates([[[3, 4, 5, 6], [3, 4, 5, 6]]], dims=[\"lat_lon\"]))\n\n        # this is checking that the 'lat' and 'lon' multiindex names are still there\n        np.testing.assert_array_equal(output[\"lat\"], [3, 4, 5, 6])\n        np.testing.assert_array_equal(output[\"lon\"], [3, 4, 5, 6])\n        np.testing.assert_array_equal(output, [103, 104, 200, 201])",
            "def test_get_source_data(self):\n        a = ArrayRaw(source=np.arange(5) + 100, coordinates=podpac.Coordinates([[0, 1, 2, 3, 4]], dims=[\"lat\"]))\n        b = ArrayRaw(source=np.arange(5) + 200, coordinates=podpac.Coordinates([[5, 6, 7, 8, 9]], dims=[\"lat\"]))\n        c = ArrayRaw(source=np.arange(5) + 300, coordinates=podpac.Coordinates([[10, 11, 12, 13, 14]], dims=[\"lat\"]))\n\n        node = TileCompositorRaw(sources=[a, b, c])\n\n        data = node.get_source_data()\n        np.testing.assert_array_equal(data[\"lat\"], np.arange(15))\n        np.testing.assert_array_equal(data, np.hstack([source.source for source in node.sources]))\n\n        # with bounds\n        data = node.get_source_data({\"lat\": (2.5, 6.5)})\n        np.testing.assert_array_equal(data[\"lat\"], [3, 4, 5, 6])\n        np.testing.assert_array_equal(data, [103, 104, 200, 201])\n\n        # error\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n            d = podpac.algorithm.Arithmetic(eqn=\"a+2\", a=a)\n        node = TileCompositorRaw(sources=[a, b, c, d])\n        with pytest.raises(ValueError, match=\"Cannot get composited source data\"):\n            node.get_source_data()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/aws.py",
        "comments": [
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.create_budget",
            "//podpac.org)\").tag(readonly=True)",
            "//{}/{}\".format(self.function_source_bucket, self.function_source_dist_key)",
            "//{}/{}\".format(self.function_source_bucket, self.function_source_dependencies_key)",
            "//{}/{}\".format(self.function_s3_bucket, self.function_s3_dependencies_key)",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/apigateway.html#APIGateway.Client.put_integration",
            "//github.com/aws/aws-sdk-ruby/issues/1080",
            "//{}.execute-api.{}.amazonaws.com/{}/{}\".format(",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html",
            "//docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketPolicy.html#API_PutBucketPolicy_RequestSyntax",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_object",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lambda.html#Lambda.Client.get_function",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.put_role",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.create_role",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.get_role",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/apigateway.html#APIGateway.Client.create_rest_api",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.describe_budget",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.describe_budget",
            "//console.aws.amazon.com/billing/home#/preferences/tags and 'Activate' the following User Defined Cost Allocation tags: {}.\\nBudget tracking will not work if these User Defined Cost Allocation tags are not active.\\nBudget creation and usage updates may take 24 hours to take effect.\".format(",
            "//boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.describe_budget",
            "/*\".format(self.function_s3_bucket)],\n                }\n            ],\n        }\n\n    @tl.default(\"function_role_assume_policy_document\")\n    def _function_role_assume_policy_document_default(self):\n        # enable role to be run by lambda - this document is defined by AWS\n        return {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}\n            ],\n        }\n\n    @tl.default(\"function_role_tags\")\n    def _function_role_tags_default(self):\n        return self.function_tags\n\n    # s3 parameters\n    function_s3_bucket = tl.Unicode().tag(attr=True, readonly=True)  # see default below\n    function_s3_dependencies_key = tl.Unicode()  # see default below\n    function_s3_input = tl.Unicode()  # see default below\n    function_s3_output = tl.Unicode()  # see default below\n    function_s3_tags = tl.Dict()  # see default below\n    _bucket = tl.Dict(default_value=None, allow_none=True)  # raw response from AWS on \"get_\"\n\n    @tl.default(\"function_s3_bucket\")\n    def _function_s3_bucket_default(self):\n        return settings[\"S3_BUCKET_NAME\"] or \"podpac-autogen-{}\".format(\n            np.datetime64(\"now\").astype(int)\n        )  # must be globally unique\n\n    @tl.default(\"function_s3_input\")\n    def _function_s3_input_default(self):\n        if settings[\"FUNCTION_S3_INPUT\"] is None:\n            settings[\"FUNCTION_S3_INPUT\"] = \"input/\"\n\n        return settings[\"FUNCTION_S3_INPUT\"]\n\n    @tl.default(\"function_s3_output\")\n    def _function_s3_output_default(self):\n        if settings[\"FUNCTION_S3_OUTPUT\"] is None:\n            settings[\"FUNCTION_S3_OUTPUT\"] = \"output/\"\n\n        return settings[\"FUNCTION_S3_OUTPUT\"]\n\n    @tl.default(\"function_s3_tags\")\n    def _function_s3_tags_default(self):\n        return self.function_tags\n\n    @tl.default(\"function_s3_dependencies_key\")\n    def _function_s3_dependencies_key_default(self):\n        if settings[\"FUNCTION_DEPENDENCIES_KEY\"] is None:\n            settings[\"FUNCTION_DEPENDENCIES_KEY\"] = \"podpac_deps_{}.zip\".format(version.semver())\n\n        return settings[\"FUNCTION_DEPENDENCIES_KEY\"]\n\n    # api gateway parameters\n    function_api_name = tl.Unicode().tag(readonly=True)  # see default below\n    function_api_description = tl.Unicode().tag(readonly=True)  # see default below\n    function_api_version = tl.Unicode(default_value=\"{}\".format(version.semver())).tag(readonly=True)\n    function_api_tags = tl.Dict().tag(readonly=True)  # see default below\n    function_api_stage = tl.Unicode(default_value=\"prod\").tag(readonly=True)\n    function_api_endpoint = tl.Unicode(default_value=\"eval\").tag(readonly=True)\n    _function_api_id = tl.Unicode(default_value=None, allow_none=True)  # will create api if None\n    _function_api_url = tl.Unicode(default_value=None, allow_none=True)\n    _function_api_resource_id = tl.Unicode(default_value=None, allow_none=True)\n    _api = tl.Dict(default_value=None, allow_none=True)  # raw response from AWS on \"get_\"\n\n    @tl.default(\"function_api_name\")\n    def _function_api_name_default(self):\n        return \"{}-api\".format(self.function_name)\n\n    @tl.default(\"function_api_description\")\n    def _function_api_description_default(self):\n        return \"PODPAC Lambda REST API for {} function\".format(self.function_name)\n\n    @tl.default(\"function_api_tags\")\n    def _function_api_tags_default(self):\n        return self.function_tags\n\n    # budget parameters\n    function_budget_amount = tl.Float(allow_none=True).tag(readonly=True)  # see default below\n    function_budget_email = tl.Unicode(allow_none=True).tag(readonly=True)  # see default below\n    function_budget_name = tl.Unicode().tag(readonly=True)  # see default below\n    function_budget_currency = tl.Unicode(default_value=\"USD\").tag(readonly=True)\n    _budget = tl.Dict(default_value=None, allow_none=True)  # raw response from AWS on \"get_\"\n\n    @tl.default(\"function_budget_amount\")\n    def _function_budget_amount_default(self):\n        return settings[\"AWS_BUDGET_AMOUNT\"] or None\n\n    @tl.default(\"function_budget_email\")\n    def _function_budget_email_default(self):\n        return settings[\"AWS_BUDGET_EMAIL\"] or None\n\n    @tl.default(\"function_budget_name\")\n    def _function_budget_name_default(self):\n        return \"{}-budget\".format(self.function_name)\n\n    # podpac node parameters\n    source = tl.Instance(Node, allow_none=True).tag(attr=True)\n    source_output_format = tl.Unicode(default_value=\"netcdf\")\n    source_output_name = tl.Unicode()\n    node_attrs = tl.Dict()\n    download_result = tl.Bool(True).tag(attr=True)\n    force_compute = tl.Bool().tag(attr=True)\n    eval_settings = tl.Dict().tag(attr=True)\n    eval_timeout = tl.Float(610).tag(attr=True)\n\n    @tl.default(\"source_output_name\")\n    def _source_output_name_default(self):\n        return self.source.__class__.__name__\n\n    @tl.default(\"force_compute\")\n    def _force_compute_default(self):\n        if settings[\"FUNCTION_FORCE_COMPUTE\"] is None:\n            settings[\"FUNCTION_FORCE_COMPUTE\"] = False\n\n        return settings[\"FUNCTION_FORCE_COMPUTE\"]\n\n    @tl.default(\"eval_settings\")\n    def _eval_settings_default(self):\n        return settings.copy()\n\n    @property\n    def pipeline(self):\n        \"\"\"\n        The pipeline of this manager is the aggregation of the source node definition and the output.\n        \"\"\"\n        d = OrderedDict()\n        d[\"pipeline\"] = self.source.definition\n        if self.node_attrs:\n            out_node = next(reversed(d[\"pipeline\"].keys()))\n            d[\"pipeline\"][out_node][\"attrs\"].update(self.node_attrs)\n        d[\"output\"] = {\"format\": self.source_output_format}\n        d[\"settings\"] = self.eval_settings\n        return d\n\n    @common_doc(COMMON_DOC)\n    def eval(self, coordinates, output=None, selector=None):\n        \"\"\"\n        Evaluate the source node on the AWS Lambda Function at the given coordinates\n        \"\"\"\n        if self.source is None:\n            raise ValueError(\"'source' node must be defined to eval\")\n\n        if self.function_eval_trigger == \"eval\":\n            return self._eval_invoke(coordinates, output)\n        elif self.function_eval_trigger == \"S3\":\n            return self._eval_s3(coordinates, output)\n        elif self.function_eval_trigger == \"APIGateway\":\n            raise NotImplementedError(\"APIGateway trigger not yet implemented through eval\")\n        else:\n            raise ValueError(\"Function trigger is not one of 'eval', 'S3', or 'APIGateway'\")\n\n    def build(self):\n        \"\"\"Build Lambda function and associated resources on AWS\n        to run PODPAC pipelines\n        \"\"\"\n\n        # TODO: move towards an architecture where the \"create_\" functions repair on each build\n        # and skip when the resources already exist\n\n        # see if current setup is valid, if so just return\n        valid = self.validate()\n        if valid:\n            _log.debug(\"Current cloud resources will support this PODPAC lambda function\")\n            return\n\n        # TODO: how much \"importing\" do we want to do? Currently, this will see if the cloud resource is available\n        # and if so, assume that it will work with the function setup\n\n        # self.validate updates current properties (self.function, self.role, self.bucket, self.api)\n        # create default role if it doesn't exist\n        if self._role is None:\n            self.create_role()\n\n            # after creating a role, you need to wait ~10 seconds before its active and will work with the lambda function\n            # this is not cool\n            time.sleep(10)\n\n        # create function\n        if self._function is None:\n            self.create_function()\n\n            # after creating a role, you need to wait ~5 seconds before its active and will return an arn\n            # this is also not cool\n            time.sleep(5)\n\n        # TODO: check to make sure function and role work together\n\n        # create API gateway\n        if self._api is None:\n            self.create_api()\n\n        # TODO: check to make sure function and API work together\n\n        # create S3 bucket - this will skip pieces that already exist\n        self.create_bucket()\n\n        # create budget, if defined\n        if self.function_budget_amount is not None:\n            self.create_budget()\n\n        # check to see if setup is valid after creation\n        # TODO: remove this in favor of something more granular??\n        self.validate(raise_exceptions=True)\n\n        _log.info(\"Successfully built AWS resources to support function {}\".format(self.function_name))\n\n    def validate(self, raise_exceptions=False):\n        \"\"\"\n        Validate cloud resources and interoperability of resources for\n        PODPAC usage\n\n        Parameters\n        ----------\n        raise_exceptions : bool, optional\n            Raise validation errors when encountered\n        \"\"\"\n\n        # TODO: I don't know if this is the right architecture to handle validation\n        # perhaps we just want to improve the \"create_\" methods to be self-healing\n\n        def _raise(msg):\n            _log.debug(msg)\n            if raise_exceptions:\n                raise Exception(msg)\n            else:\n                return False\n\n        # get currently defined resources\n        self.get_role()\n        self.get_function()\n        self.get_api()\n        self.get_bucket()\n        self.get_budget()\n\n        # check that each resource has a valid configuration\n        if not self.validate_role():\n            return _raise(\"Failed to validate role\")\n\n        if not self.validate_function():\n            return _raise(\"Failed to validate function\")\n\n        if not self.validate_bucket():\n            return _raise(\"Failed to validate bucket\")\n\n        if not self.validate_api():\n            return _raise(\"Failed to validate API\")\n\n        if not self.validate_budget():\n            return _raise(\"Failed to validate budget\")\n\n        # check that the integration of resources is correct\n\n        # check that role_arn is the same as function configured role\n        if self._function[\"Configuration\"][\"Role\"] != self._function_role_arn:\n            return _raise(\"Function role ARN is not the same as role ARN for {}\".format(self.function_role_name))\n\n        # if it makes it to the end, its valid\n        self._function_valid = True\n        return True\n\n    def delete(self, confirm=False):\n        \"\"\"Remove all cloud resources associated with function\n\n        Parameters\n        ----------\n        confirm : bool, optional\n            Must pass in confirm paramter\n        \"\"\"\n        _log.info(\"Removing all cloud resources associated with this Lamba node\")\n\n        if confirm:\n            self.remove_triggers()\n            self.delete_function()\n            self.delete_role()\n            self.delete_api()\n            self.delete_bucket(delete_objects=True)\n            self.delete_budget()\n        else:\n            raise ValueError(\"You must pass confirm=True to delete all AWS resources\")\n\n    def describe(self):\n        \"\"\"Show a description of the Lambda Utilities\"\"\"\n        # TODO: change this to format strings when we deprecate py 2\n        status = \"(staged)\" if not self._function_valid else \"(built)\"\n\n        # source dist\n        if not self._function_valid:\n            source_dist = (\n                self.function_source_dist_zip\n                if self.function_source_dist_zip is not None\n                else \"s3://{}/{}\".format(self.function_source_bucket, self.function_source_dist_key)\n            )\n        else:\n            source_dist = self._function_code_sha256\n\n        # source deps\n        if not self._function_valid:\n            source_deps = (\n                self.function_source_dependencies_zip\n                if self.function_source_dependencies_zip is not None\n                else \"s3://{}/{}\".format(self.function_source_bucket, self.function_source_dependencies_key)\n            )\n        else:\n            source_deps = \"s3://{}/{}\".format(self.function_s3_bucket, self.function_s3_dependencies_key)\n\n        # only show API if built or if its proposed in triggers\n        if self._api is not None or (not self._function_valid and \"APIGatway\" in self.function_triggers):\n            api_output = \"\"\"\n    API\n        Name: {function_api_name}\n        Description: {function_api_description}\n        ID: {_function_api_id}\n        Resource ID: {_function_api_resource_id}\n        Version: {function_api_version}\n        Tags: {function_api_tags}\n        Stage: {function_api_stage}\n        Endpoint: {function_api_endpoint}\n        URL: {_function_api_url}\n            \"\"\".format(\n                function_api_name=self.function_api_name,\n                function_api_description=self.function_api_description,\n                _function_api_id=self._function_api_id,\n                _function_api_resource_id=self._function_api_resource_id,\n                function_api_version=self.function_api_version,\n                function_api_tags=self.function_api_tags,\n                function_api_stage=self.function_api_stage,\n                function_api_endpoint=self.function_api_endpoint,\n                _function_api_url=self._function_api_url,\n            )\n        else:\n            api_output = \"\"\n\n        # only show budget if its defined\n        if self.function_budget_amount is not None:\n            budget_output = \"\"\"\n    Budget\n        Name: {function_budget_name}\n        Amount: {function_budget_amount}\n        Currency: {function_budget_currency}\n        E-mail: {function_budget_email}\n        Spent: {function_budget_usage} {function_budget_usage_currency}\n            \"\"\".format(\n                function_budget_name=self.function_budget_name,\n                function_budget_amount=self.function_budget_amount,\n                function_budget_currency=self.function_budget_currency,\n                function_budget_email=self.function_budget_email,\n                function_budget_usage=self._budget[\"CalculatedSpend\"][\"ActualSpend\"][\"Amount\"]\n                if self._budget\n                else None,\n                function_budget_usage_currency=self._budget[\"CalculatedSpend\"][\"ActualSpend\"][\"Unit\"]\n                if self._budget\n                else None,\n            )\n        else:\n            budget_output = \"\"\n\n        output = \"\"\"\nLambda Node {status}\n    Function\n        Name: {function_name}\n        Description: {function_description}\n        ARN: {_function_arn}\n        Triggers: {function_triggers}\n        Handler: {function_handler}\n        Environment Variables: {function_env_variables}\n        Timeout: {function_timeout} seconds\n        Memory: {function_memory} MB\n        Tags: {function_tags}\n        Source Dist: {source_dist}\n        Source Dependencies: {source_deps}\n        Last Modified: {_function_last_modified}\n        Version: {_function_version}\n        Restrict Evaluation: {function_restrict_pipelines}\n\n    S3\n        Bucket: {function_s3_bucket}\n        Tags: {function_s3_tags}\n        Input Folder: {function_s3_input}\n        Output Folder: {function_s3_output}\n\n    Role\n        Name: {function_role_name}\n        Description: {function_role_description}\n        ARN: {_function_role_arn}\n        Policy Document: {function_role_policy_document}\n        Policy ARNs: {function_role_policy_arns}\n        Assume Policy Document: {function_role_assume_policy_document}\n        Tags: {function_role_tags}\n\n        {api_output}\n\n        {budget_output}\n        \"\"\".format(\n            status=status,\n            function_name=self.function_name,\n            function_description=self.function_description,\n            function_triggers=self.function_triggers,\n            function_handler=self.function_handler,\n            function_env_variables=self.function_env_variables,\n            function_timeout=self.function_timeout,\n            function_memory=self.function_memory,\n            function_tags=self.function_tags,\n            source_dist=source_dist,\n            source_deps=source_deps,\n            _function_arn=self._function_arn,\n            _function_last_modified=self._function_last_modified,\n            _function_version=self._function_version,\n            function_restrict_pipelines=self.function_restrict_pipelines,\n            function_s3_bucket=self.function_s3_bucket,\n            function_s3_tags=self.function_s3_tags,\n            function_s3_input=self.function_s3_input,\n            function_s3_output=self.function_s3_output,\n            function_role_name=self.function_role_name,\n            function_role_description=self.function_role_description,\n            function_role_policy_document=self.function_role_policy_document,\n            function_role_policy_arns=self.function_role_policy_arns,\n            function_role_assume_policy_document=self.function_role_assume_policy_document,\n            function_role_tags=self.function_role_tags,\n            _function_role_arn=self._function_role_arn,\n            api_output=api_output,\n            budget_output=budget_output,\n        )\n\n        print(output)\n\n    # Function\n    def create_function(self):\n        \"\"\"Build Lambda function on AWS\"\"\"\n        if self.function_name is None:\n            raise AttributeError(\"Function name is not defined\")\n\n        if self.function_allow_unsafe_eval:\n            _log.info(\"Lambda function will allow unsafe evaluation of Nodes with the current settings\")\n            self.function_env_variables[\"PODPAC_UNSAFE_EVAL\"] = settings[\"UNSAFE_EVAL_HASH\"]\n\n        if self.function_restrict_pipelines:\n            _log.info(\"Lambda function will only run for pipelines: {}\".format(self.function_restrict_pipelines))\n            self.function_env_variables[\"PODPAC_RESTRICT_PIPELINES\"] = json.dumps(self.function_restrict_pipelines)\n\n        # add special tag - value is hash, for lack of better value at this point\n        self.function_tags[\"_podpac_resource_hash\"] = self.hash\n\n        # if function already exists, this will return existing function\n        function = create_function(\n            self.session,\n            self.function_name,\n            self._function_role_arn,\n            self.function_handler,\n            self.function_description,\n            self.function_timeout,\n            self.function_memory,\n            self.function_env_variables,\n            self.function_tags,\n            self.function_source_dist_zip,\n            self.function_source_bucket,\n            self.function_source_dist_key,\n        )\n\n        # set class properties\n        self._set_function(function)\n\n    def update_function(self):\n        \"\"\"Update lambda function with new parameters\"\"\"\n        if self.function_name is None:\n            raise AttributeError(\"Function name is not defined\")\n\n        # if function already exists, this will return existing function\n        function = update_function(\n            self.session,\n            self.function_name,\n            self.function_source_dist_zip,\n            self.function_source_bucket,\n            self.function_source_dist_key,\n        )\n\n        # set class properties\n        self._set_function(function)\n\n    def get_function(self):\n        \"\"\"Get function definition from AWS\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_function`\n        \"\"\"\n        function = get_function(self.session, self.function_name)\n        self._set_function(function)\n\n        return function\n\n    def validate_function(self):\n        \"\"\"\n        Validate that function is configured properly\n\n        This should only be run after running `self.get_function()`\n        \"\"\"\n        # TOOD: implement\n\n        if self._function is None:\n            return False\n\n        return True\n\n    def delete_function(self):\n        \"\"\"Remove AWS Lambda function and associated resources on AWS\"\"\"\n\n        self.get_function()\n\n        delete_function(self.session, self.function_name)\n\n        # reset internals\n        self._function = None\n        self._function_arn = None\n        self._function_last_modified = None\n        self._function_version = None\n        self._function_code_sha256 = None\n\n    def add_trigger(self, statement_id, principle, source_arn):\n        \"\"\"Add trigger (permission) to lambda function\n\n        Parameters\n        ----------\n        statement_id : str\n            Specific identifier for trigger\n        principle : str\n            Principle identifier from AWS\n        source_arn : str\n            Source ARN for trigger\n        \"\"\"\n        add_function_trigger(self.session, self.function_name, statement_id, principle, source_arn)\n        self._function_triggers[statement_id] = source_arn\n\n    def remove_trigger(self, statement_id):\n        \"\"\"Remove trigger (permission) from lambda function\n\n        Parameters\n        ----------\n        statement_id : str\n            Specific identifier for trigger\n        \"\"\"\n\n        remove_function_trigger(self.session, self.function_name, statement_id)\n\n        # remove from local dict\n        del self._function_triggers[statement_id]\n\n    def remove_triggers(self):\n        \"\"\"\n        Remove all triggers from function\n        \"\"\"\n        triggers = deepcopy(self._function_triggers)  # to avoid changing the size of dict during iteration\n        for trigger in triggers:\n            self.remove_trigger(trigger)\n\n    # IAM Roles\n    def create_role(self):\n        \"\"\"Create IAM role to execute podpac lambda function\"\"\"\n\n        # add special tag - value is hash\n        self.function_role_tags[\"_podpac_resource_hash\"] = self.hash\n\n        role = create_role(\n            self.session,\n            self.function_role_name,\n            self.function_role_description,\n            self.function_role_policy_document,\n            self.function_role_policy_arns,\n            self.function_role_assume_policy_document,\n            self.function_role_tags,\n        )\n\n        self._set_role(role)\n\n    def get_role(self):\n        \"\"\"Get role definition from AWS\n\n        See :attr:`self.function_role_name` for role_name\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_role`\n        \"\"\"\n        role = get_role(self.session, self.function_role_name)\n        self._set_role(role)\n\n        return role\n\n    def validate_role(self):\n        \"\"\"\n        Validate that role will work with function.\n\n        This should only be run after running `self.get_role()`\n        \"\"\"\n        # TODO: add constraints\n\n        if self._role is None:\n            return False\n\n        # check role policy document\n        document_valid = False\n        valid_document = {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n            \"Action\": \"sts:AssumeRole\",\n        }\n        for s in self.function_role_assume_policy_document[\"Statement\"]:\n            if json.dumps(s) == json.dumps(valid_document):\n                document_valid = True\n\n        if not document_valid:\n            _log.error(\"Function role policy document does not allow lambda function to assume role\")\n            return False\n\n        return True\n\n    def delete_role(self):\n        \"\"\"Remove role from AWS resources\n\n        See :attr:`self.function_role_name` for role_name\n        \"\"\"\n        self.get_role()\n\n        if self.function_role_name is None:\n            _log.debug(\"No role name defined for this function\")\n            return\n\n        delete_role(self.session, self.function_role_name)\n\n        # reset members\n        self._role = None\n        self._function_role_arn = None\n\n        # TODO: handle defaults after deletion\n\n    # S3 Creation\n    def create_bucket(self):\n        \"\"\"Create S3 bucket to work with function\"\"\"\n        if self.function_name is None:\n            raise AttributeError(\"Function name must be defined when creating S3 bucket and trigger\")\n\n        if self._function_arn is None:\n            raise ValueError(\"Lambda function must be created before creating a bucket\")\n\n        if self._function_role_arn is None:\n            raise ValueError(\"Function role must be created before creating a bucket\")\n\n        # add special tags - value is hash\n        self.function_s3_tags[\"_podpac_resource_hash\"] = self.hash\n\n        # create bucket\n        bucket = create_bucket(\n            self.session, self.function_s3_bucket, bucket_policy=None, bucket_tags=self.function_s3_tags\n        )\n        self._set_bucket(bucket)\n\n        # after creating a bucket, you need to wait ~2 seconds before its active and can be uploaded to\n        # this is not cool\n        time.sleep(5)\n\n        # get reference to s3 client for session\n        s3 = self.session.client(\"s3\")\n\n        # add podpac deps to bucket for version\n        # see if the function depedencies exist in bucket\n        try:\n            s3.head_object(Bucket=self.function_s3_bucket, Key=self.function_s3_dependencies_key)\n        except botocore.exceptions.ClientError:\n\n            # copy from user supplied dependencies\n            if self.function_source_dependencies_zip is not None:\n                put_object(\n                    self.session,\n                    self.function_s3_bucket,\n                    self.function_s3_dependencies_key,\n                    file=self.function_source_dependencies_zip,\n                )\n\n            # copy resources from podpac dist\n            else:\n                s3resource = self.session.resource(\"s3\")\n                copy_source = {\"Bucket\": self.function_source_bucket, \"Key\": self.function_source_dependencies_key}\n                s3resource.meta.client.copy(copy_source, self.function_s3_bucket, self.function_s3_dependencies_key)\n\n        # Add S3 Function triggers, if they don't exist already\n        # TODO: add validition to see if trigger already exists\n        if \"S3\" in self.function_triggers:\n            # add permission to invoke call lambda - this feels brittle due to source_arn\n            statement_id = re.sub(\"[-_.]\", \"\", self.function_s3_bucket)\n            principle = \"s3.amazonaws.com\"\n            source_arn = \"arn:aws:s3:::{}\".format(self.function_s3_bucket)\n            self.add_trigger(statement_id, principle, source_arn)\n\n            # lambda integration on object creation events\n            s3.put_bucket_notification_configuration(\n                Bucket=self.function_s3_bucket,\n                NotificationConfiguration={\n                    \"LambdaFunctionConfigurations\": [\n                        {\n                            \"Id\": \"{}\".format(np.datetime64(\"now\").astype(int)),\n                            \"LambdaFunctionArn\": self._function_arn,\n                            \"Events\": [\"s3:ObjectCreated:*\"],\n                            \"Filter\": {\"Key\": {\"FilterRules\": [{\"Name\": \"prefix\", \"Value\": self.function_s3_input}]}},\n                        }\n                    ]\n                },\n            )\n        else:\n            _log.debug(\"Skipping S3 trigger because 'S3' not in the function triggers\")\n\n        return bucket\n\n    def get_bucket(self):\n        \"\"\"Get S3 Bucket for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_bucket`\n        \"\"\"\n        bucket = get_bucket(self.session, self.function_s3_bucket)\n        self._set_bucket(bucket)\n\n        return bucket\n\n    def validate_bucket(self):\n        \"\"\"\n        Validate that bucket will work with function.\n\n        This should only be run after running `self.get_bucket()`\n        \"\"\"\n        if self._bucket is None:\n            return False\n\n        s3 = self.session.client(\"s3\")\n\n        # make sure dependencies are in there\n        try:\n            s3.head_object(Bucket=self.function_s3_bucket, Key=self.function_s3_dependencies_key)\n        except botocore.exceptions.ClientError:\n            _log.error(\"Failed to find PODPAC dependencies in bucket\")\n            return False\n\n        # TODO: make sure trigger exists\n        if \"S3\" in self.function_triggers:\n            pass\n\n        return True\n\n    def delete_bucket(self, delete_objects=False):\n        \"\"\"Delete bucket associated with this function\n\n        Parameters\n        ----------\n        delete_objects : bool, optional\n            Delete all objects in the bucket while deleting bucket. Defaults to False.\n        \"\"\"\n\n        self.get_bucket()\n\n        # delete bucket\n        delete_bucket(self.session, self.function_s3_bucket, delete_objects=delete_objects)\n\n        # TODO: update manage attributes here?\n        self._bucket = None\n\n    # API Gateway\n    def create_api(self):\n        \"\"\"Create API Gateway API for lambda function\"\"\"\n\n        if \"APIGateway\" not in self.function_triggers:\n            _log.debug(\"Skipping API creation because 'APIGateway' not in the function triggers\")\n            return\n\n        if self.function_name is None:\n            raise AttributeError(\"Function name must be defined when creating API Gateway\")\n\n        if self._function_arn is None:\n            raise ValueError(\"Lambda function must be created before creating an API bucket\")\n\n        # add special tag - value is hash\n        self.function_api_tags[\"_podpac_resource_hash\"] = self.hash\n\n        # create api and resource\n        api = create_api(\n            self.session,\n            self.function_api_name,\n            self.function_api_description,\n            self.function_api_version,\n            self.function_api_tags,\n            self.function_api_endpoint,\n        )\n        self._set_api(api)\n\n        # lambda proxy integration - this feels pretty brittle due to uri\n        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/apigateway.html#APIGateway.Client.put_integration\n        aws_lambda_uri = \"arn:aws:apigateway:{}:lambda:path/2015-03-31/functions\".format(self.session.region_name)\n        uri = \"{}/{}/invocations\".format(aws_lambda_uri, self._function_arn)\n\n        apigateway = self.session.client(\"apigateway\")\n        apigateway.put_integration(\n            restApiId=api[\"id\"],\n            resourceId=api[\"resource\"][\"id\"],\n            httpMethod=\"ANY\",\n            integrationHttpMethod=\"POST\",\n            type=\"AWS_PROXY\",\n            uri=uri,\n            passthroughBehavior=\"WHEN_NO_MATCH\",\n            contentHandling=\"CONVERT_TO_TEXT\",\n            timeoutInMillis=29000,\n        )\n\n        # get responses back\n        apigateway.put_integration_response(\n            restApiId=api[\"id\"],\n            resourceId=api[\"resource\"][\"id\"],\n            httpMethod=\"ANY\",\n            statusCode=\"200\",\n            selectionPattern=\"\",  # bug, see https://github.com/aws/aws-sdk-ruby/issues/1080\n        )\n\n        # deploy the api. this has to happen after creating the integration\n        deploy_api(self.session, self._function_api_id, self.function_api_stage)\n\n        # add permission to invoke call lambda - this feels brittle due to source_arn\n        statement_id = api[\"id\"]\n        principle = \"apigateway.amazonaws.com\"\n        source_arn = \"arn:aws:execute-api:{}:{}:{}/*/",
            "/*\".format(\n            self.session.region_name, self.session.get_account_id(), api[\"id\"]\n        )\n        self.add_trigger(statement_id, principle, source_arn)\n\n    def get_api(self):\n        \"\"\"Get API Gateway definition for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_api`\n        \"\"\"\n        if \"APIGateway\" not in self.function_triggers:\n            _log.debug(\"Skipping API get because 'APIGateway' not in the function triggers\")\n            return None\n\n        api = get_api(self.session, self.function_api_name, self.function_api_endpoint)\n        self._set_api(api)\n\n        return api\n\n    def validate_api(self):\n        \"\"\"\n        Validate that API will work with function.\n\n        This should only be run after running `self.get_api()`\n        \"\"\"\n\n        if \"APIGateway\" not in self.function_triggers:\n            _log.debug(\"Skipping API validation because 'APIGateway' not in the function triggers\")\n            return True\n\n        # TOOD: implement\n        if self._api is None:\n            return False\n\n        return True\n\n    def delete_api(self):\n        \"\"\"Delete API Gateway for Function\"\"\"\n\n        self.get_api()\n\n        # remove API\n        delete_api(self.session, self.function_api_name)\n\n        # reset\n        self._api = None\n        self._function_api_id = None\n        self._function_api_url = None\n        self._function_api_resource_id = None\n\n    # Function budget\n    def create_budget(self):\n        \"\"\"\n        EXPERIMENTAL FEATURE\n        Create budget for lambda function based on node hash.\n        \"\"\"\n        # skip if no budget provided\n        if self.function_budget_amount is None:\n            _log.debug(\"Skipping Budget creation because function budget is not defined\")\n            return\n\n        _log.warning(\n            \"Creating an AWS Budget with PODPAC is an experimental feature. Please continue to monitor AWS usage costs seperately.\"\n        )\n\n        budget = create_budget(\n            self.session,\n            self.function_budget_amount,\n            self.function_budget_email,\n            budget_name=self.function_budget_name,\n            budget_currency=self.function_budget_currency,\n            budget_filter_tags={\"_podpac_resource_hash\": self.hash},\n        )\n\n        self._set_budget(budget)\n\n    def get_budget(self):\n        \"\"\"\n        EXPERIMENTAL FEATURE\n        Get budget definition for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_budget`\n        \"\"\"\n\n        # skip if no budget provided\n        if self.function_budget_amount is None:\n            _log.debug(\"Skipping Budget request because function budget is not defined\")\n            return None\n\n        budget = get_budget(self.session, self.function_budget_name)\n        self._set_budget(budget)\n\n        return budget\n\n    def validate_budget(self):\n        \"\"\"Validate budget definition for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_budget`\n        \"\"\"\n\n        # skip if no budget provided\n        if self.function_budget_amount is None:\n            _log.debug(\"Skipping Budget validation because function budget is not defined\")\n            return True\n\n        if self._budget is None:\n            return False\n\n        return True\n\n    def delete_budget(self):\n        \"\"\"Delete budget associated with function\"\"\"\n        self.get_budget()\n\n        # delete budget\n        delete_budget(self.session, self.function_budget_name)\n\n        # reset class attributes\n        self._budget = None\n\n    # Logs\n    def get_logs(self, limit=5, start=None, end=None):\n        \"\"\"Get Cloudwatch logs from lambda function execution\n\n        See :func:`podpac.managers.aws.get_logs`\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit logs to the most recent N logs\n        start : str, optional\n            Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(start))\n            Defaults to 1 hour prior to ``end``.\n        end : str, optional\n            Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(end))\n            Defaults to now.\n\n        Returns\n        -------\n        list\n            list of log events\n        \"\"\"\n        if self.function_name is None:\n            raise AttributeError(\"Function name must be defined to get logs\")\n\n        log_group_name = \"/aws/lambda/{}\".format(self.function_name)\n        return get_logs(self.session, log_group_name, limit=limit, start=start, end=end)\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # Internals\n    # -----------------------------------------------------------------------------------------------------------------\n\n    def _set_function(self, function):\n        \"\"\"Set function class members\n\n        Parameters\n        ----------\n        function : dict\n        \"\"\"\n        # update all class members with return\n        # this allows a new Lambda instance to fill in class members from input function_name\n        if function is not None:\n            self.set_trait(\"function_handler\", function[\"Configuration\"][\"Handler\"])\n            self.set_trait(\"function_description\", function[\"Configuration\"][\"Description\"])\n            self.set_trait(\"function_env_variables\", function[\"Configuration\"][\"Environment\"][\"Variables\"])\n            self.set_trait(\"function_timeout\", function[\"Configuration\"][\"Timeout\"])\n            self.set_trait(\"function_memory\", function[\"Configuration\"][\"MemorySize\"])\n            self.set_trait(\"function_tags\", function[\"tags\"])\n            self._function_arn = function[\"Configuration\"][\"FunctionArn\"]\n            self._function_last_modified = function[\"Configuration\"][\"LastModified\"]\n            self._function_version = function[\"Configuration\"][\"Version\"]\n            self._function_code_sha256 = function[\"Configuration\"][\n                \"CodeSha256\"\n            ]  # TODO: is this the best way to determine S3 source bucket and dist zip?\n\n            # store a copy of the whole response from AWS\n            self._function = function\n\n    def _set_role(self, role):\n        \"\"\"Set role class members\n\n        Parameters\n        ----------\n        role : dict\n        \"\"\"\n        if role is not None:\n            self.set_trait(\"function_role_name\", role[\"RoleName\"])\n            self.set_trait(\"function_role_description\", role[\"Description\"])\n            self.set_trait(\"function_role_assume_policy_document\", role[\"AssumeRolePolicyDocument\"])\n            self.set_trait(\"function_role_policy_arns\", role[\"policy_arns\"])\n            self.set_trait(\"function_role_policy_document\", role[\"policy_document\"])\n            self.set_trait(\"function_role_tags\", role[\"tags\"])\n            self._function_role_arn = role[\"Arn\"]\n\n            # store a copy of the whole response from AWS\n            self._role = role\n\n    def _set_bucket(self, bucket):\n        \"\"\"Set bucket class members\n\n        Parameters\n        ----------\n        bucket : dict\n        \"\"\"\n        if bucket is not None:\n            self.set_trait(\"function_s3_bucket\", bucket[\"name\"])\n            self.set_trait(\"function_s3_tags\", bucket[\"tags\"])\n\n            # store a copy of the whole response from AWS\n            self._bucket = bucket\n\n    def _set_api(self, api):\n        \"\"\"Set api class members\n\n        Parameters\n        ----------\n        api : dict\n        \"\"\"\n        if api is not None:\n            self.set_trait(\"function_api_name\", api[\"name\"])\n            self.set_trait(\"function_api_description\", api[\"description\"])\n            self.set_trait(\"function_api_version\", api[\"version\"])\n            self.set_trait(\"function_api_tags\", api[\"tags\"])\n            self._function_api_id = api[\"id\"]\n\n            if \"stage\" in api and api[\"stage\"] is not None:\n                self.set_trait(\"function_api_stage\", api[\"stage\"])\n\n            if \"resource\" in api and api[\"resource\"] is not None:\n                self._function_api_resource_id = api[\"resource\"][\"id\"]\n                self.set_trait(\"function_api_endpoint\", api[\"resource\"][\"pathPart\"])\n\n            # set api url\n            self._function_api_url = self._get_api_url()\n\n            # store a copy of the whole response from AWS\n            self._api = api\n\n    def _set_budget(self, budget):\n        \"\"\"Set budget class members\n\n        Parameters\n        ----------\n        budget : dict\n        \"\"\"\n        if budget is not None:\n            budget_filter_tags = {\"_podpac_resource_hash\": self.hash}\n\n            self.set_trait(\"function_budget_amount\", float(budget[\"BudgetLimit\"][\"Amount\"]))\n            self.set_trait(\"function_budget_name\", budget[\"BudgetName\"])\n            self.set_trait(\"function_budget_currency\", budget[\"BudgetLimit\"][\"Unit\"])\n\n            # TODO\n            # self.set_trait(\"function_budget_email\", budget[\"BudgetLimit\"][\"Amount\"]))\n\n            # store a copy of the whole response from AWS\n            self._budget = budget\n\n    def _create_eval_pipeline(self, coordinates):\n        \"\"\"shorthand to create pipeline on eval\"\"\"\n\n        # add coordinates to the pipeline\n        pipeline = self.pipeline  # contains \"pipeline\" and \"output\" keys\n        pipeline[\"coordinates\"] = json.loads(coordinates.json)\n\n        # TODO: should we move this to `self.pipeline`?\n        pipeline[\"settings\"] = self.eval_settings\n        pipeline[\"settings\"][\n            \"FUNCTION_DEPENDENCIES_KEY\"\n        ] = self.function_s3_dependencies_key  # overwrite in case this is specified explicitly by class\n        if self.output_format:\n            pipeline[\"output\"] = self.output_format\n\n        return pipeline\n\n    def _eval_invoke(self, coordinates, output=None):\n        \"\"\"eval node through invoke trigger\"\"\"\n\n        # create eval pipeline\n        pipeline = self._create_eval_pipeline(coordinates)\n\n        # create lambda client\n        config = botocore.config.Config(\n            read_timeout=self.eval_timeout, max_pool_connections=1001, retries={\"max_attempts\": 0}\n        )\n        awslambda = self.session.client(\"lambda\", config=config)\n\n        # pipeline payload\n        payload = bytes(json.dumps(pipeline, indent=4, cls=JSONEncoder).encode(\"UTF-8\"))\n\n        if self.download_result:\n            _log.debug(\"Evaluating pipeline via invoke synchronously\")\n            response = awslambda.invoke(\n                FunctionName=self.function_name,\n                LogType=\"Tail\",  # include the execution log in the response.\n                Payload=payload,\n            )\n        else:\n            # async invocation\n            _log.debug(\"Evaluating pipeline via invoke asynchronously\")\n            awslambda.invoke(\n                FunctionName=self.function_name,\n                InvocationType=\"Event\",\n                LogType=\"Tail\",  # include the execution log in the response.\n                Payload=payload,\n            )\n\n            return\n\n        _log.debug(\"Received response from lambda function\")\n\n        if \"FunctionError\" in response:\n            _log.error(\"Unhandled error from lambda function\")\n            # logs = base64.b64decode(response[\"LogResult\"]).decode(\"UTF-8\").split('\\n')\n            payload = json.loads(response[\"Payload\"].read().decode(\"UTF-8\"))\n            raise LambdaException(\n                \"Error in lambda function evaluation:\\n\\nError Type: {}\\nError Message: {}\\nStack Trace: {}\".format(\n                    payload[\"errorType\"], payload[\"errorMessage\"], \"\\n\".join(payload[\"stackTrace\"])\n                )\n            )\n\n        # After waiting, load the pickle file like this:\n        payload = response[\"Payload\"].read()\n        try:\n            self._output = UnitsDataArray.open(payload)\n        except ValueError:\n            # Not actually a data-array, returning a string instead\n            return payload.decode(\"utf-8\")\n        return self._output\n\n    def _eval_s3(self, coordinates, output=None):\n        \"\"\"Evaluate node through s3 trigger\"\"\"\n\n        _log.debug(\"Evaluating pipeline via S3\")\n\n        input_folder = \"{}{}\".format(self.function_s3_input, \"/\" if not self.function_s3_input.endswith(\"/\") else \"\")\n        output_folder = \"{}{}\".format(self.function_s3_output, \"/\" if not self.function_s3_output.endswith(\"/\") else \"\")\n\n        # create eval pipeline\n        pipeline = self._create_eval_pipeline(coordinates)\n        pipeline[\"settings\"][\"FUNCTION_FORCE_COMPUTE\"] = self.force_compute\n        pipeline[\"settings\"][\n            \"FUNCTION_S3_INPUT\"\n        ] = input_folder  # overwrite in case this is specified explicitly by class\n        pipeline[\"settings\"][\n            \"FUNCTION_S3_OUTPUT\"\n        ] = output_folder  # overwrite in case this is specified explicitly by class\n\n        # filename\n        filename = \"{folder}{output}_{source}_{coordinates}.{suffix}\".format(\n            folder=input_folder,\n            output=self.source_output_name,\n            source=self.source.hash,\n            coordinates=coordinates.hash,\n            suffix=\"json\",\n        )\n\n        # create s3 client\n        s3 = self.session.client(\"s3\")\n\n        # put pipeline into s3 bucket\n        s3.put_object(\n            Body=(bytes(json.dumps(pipeline, indent=4, cls=JSONEncoder).encode(\"UTF-8\"))),\n            Bucket=self.function_s3_bucket,\n            Key=filename,\n        )\n\n        _log.debug(\"Successfully put pipeline into S3 bucket\")\n\n        # wait for object to exist\n        if not self.download_result:\n            return\n\n        # TODO: handle the \"force_compute\" parameter\n        waiter = s3.get_waiter(\"object_exists\")\n        filename = \"{folder}{output}_{source}_{coordinates}.{suffix}\".format(\n            folder=output_folder,\n            output=self.source_output_name,\n            source=self.source.hash,\n            coordinates=coordinates.hash,\n            suffix=self.source_output_format,\n        )\n\n        _log.debug(\"Starting to wait for output\")\n        waiter.wait(Bucket=self.function_s3_bucket, Key=filename)\n\n        # After waiting, load the pickle file like this:\n        _log.debug(\"Received response from lambda function\")\n        response = s3.get_object(Key=filename, Bucket=self.function_s3_bucket)\n        body = response[\"Body\"].read()\n        self._output = UnitsDataArray.open(body)\n        return self._output\n\n    def _eval_api(self, coordinates, output=None):\n        # TODO: implement and pass in settings in the REST API\n        pass\n\n    def _get_api_url(self):\n        \"\"\"Generated API url\"\"\"\n        if (\n            self._function_api_id is not None\n            and self.function_api_stage is not None\n            and self.function_api_endpoint is not None\n        ):\n            return \"https://{}.execute-api.{}.amazonaws.com/{}/{}\".format(\n                self._function_api_id, self.session.region_name, self.function_api_stage, self.function_api_endpoint\n            )\n        else:\n            return None\n\n    def __repr__(self):\n        rep = \"{} {}\\n\".format(str(self.__class__.__name__), \"(staged)\" if not self._function_valid else \"(built)\")\n        rep += \"\\tName: {}\\n\".format(self.function_name)\n        rep += \"\\tSource: {}\\n\".format(self.source.__class__.__name__ if self.source is not None else \"\")\n        rep += \"\\tBucket: {}\\n\".format(self.function_s3_bucket)\n        rep += \"\\tTriggers: {}\\n\".format(self.function_triggers)\n        rep += \"\\tRole: {}\\n\".format(self.function_role_name)\n\n        # Bucket\n\n        # API\n        if \"APIGateway\" in self.function_triggers:\n            rep += \"\\tAPI: {}\\n\".format(self.function_api_name)\n            rep += \"\\tAPI Url: {}\\n\".format(self._function_api_url)\n\n        return rep\n\n\nclass Session(boto3.Session):\n    \"\"\"Wrapper for :class:`boto3.Session`\n    See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html\n\n    We wrap the Session class to provide access to the podpac settings for the\n    aws_access_key_id, aws_secret_access_key, and region_name and to check the credentials\n    on session creation.\n    \"\"\"\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None, region_name=None):\n        aws_access_key_id = settings[\"AWS_ACCESS_KEY_ID\"] if aws_access_key_id is None else aws_access_key_id\n        aws_secret_access_key = (\n            settings[\"AWS_SECRET_ACCESS_KEY\"] if aws_secret_access_key is None else aws_secret_access_key\n        )\n        region_name = settings[\"AWS_REGION_NAME\"] if region_name is None else region_name\n\n        super(Session, self).__init__(\n            aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name\n        )\n\n        try:\n            _ = self.get_account_id()\n        except botocore.exceptions.ClientError as e:\n            _log.error(\n                \"AWS credential check failed. Confirm aws access key id and aws secred access key are valid. Credential check exception: {}\".format(\n                    str(e)\n                )\n            )\n            raise ValueError(\n                \"AWS credential check failed. Confirm aws access key id and aws secred access key are valid.\"\n            )\n\n    def get_account_id(self):\n        \"\"\"Return the account ID assciated with this AWS session. The credentials will determine the account ID.\n\n        Returns\n        -------\n        str\n            account id associated with credentials\n        \"\"\"\n        return self.client(\"sts\").get_caller_identity()[\"Account\"]\n\n\n# -----------------------------------------------------------------------------------------------------------------\n# S3\n# -----------------------------------------------------------------------------------------------------------------\ndef create_bucket(session, bucket_name, bucket_region=None, bucket_policy=None, bucket_tags={}):\n    \"\"\"Create S3 bucket\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n    bucket_region : str, optional\n        Location constraint for bucket. Defaults to no location constraint\n    bucket_policy : dict, optional\n        Bucket policy document as dict. For parameters, see https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketPolicy.html#API_PutBucketPolicy_RequestSyntax\n    bucket_tags : dict, optional\n        Bucket tags\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_bucket`\n\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\"\n\n    bucket = get_bucket(session, bucket_name)\n\n    # TODO: add checks to make sure bucket parameters match\n    if bucket is not None:\n        _log.debug(\"S3 bucket '{}' already exists. Using existing bucket.\".format(bucket_name))\n        return bucket\n\n    if bucket_name is None:\n        raise ValueError(\"`bucket_name` is None in create_bucket\")\n\n    # add special podpac tags for billing id\n    bucket_tags[\"_podpac_resource\"] = \"true\"\n\n    # bucket configuration\n    bucket_config = {\"ACL\": \"private\", \"Bucket\": bucket_name}\n    if bucket_region is not None:\n        bucket_config[\"LocationConstraint\"] = bucket_region\n\n    _log.debug(\"Creating S3 bucket {}\".format(bucket_name))\n    s3 = session.client(\"s3\")\n\n    # create bucket\n    s3.create_bucket(**bucket_config)\n\n    # add tags\n    # for some reason the tags API is different here\n    tags = []\n    for key in bucket_tags.keys():\n        tags.append({\"Key\": key, \"Value\": bucket_tags[key]})\n\n    s3.put_bucket_tagging(Bucket=bucket_name, Tagging={\"TagSet\": tags})\n\n    # set bucket policy\n    if bucket_policy is not None:\n        s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(bucket_policy))\n\n    # get finalized bucket\n    bucket = get_bucket(session, bucket_name)\n    _log.debug(\"Successfully created S3 bucket '{}'\".format(bucket_name))\n\n    return bucket\n\n\ndef get_object(session, bucket_name, bucket_path):\n    \"\"\"Get an object from an S3 bucket\n\n    See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n    bucket_path : str\n        Path to object in bucket\n    \"\"\"\n\n    if bucket_name is None or bucket_path is None:\n        return None\n\n    _log.debug(\"Getting object {} from S3 bucket {}\".format(bucket_path, bucket_name))\n    s3 = session.client(\"s3\")\n\n    # see if the object exists\n    try:\n        s3.head_object(Bucket=bucket_name, Key=bucket_path)\n    except botocore.exceptions.ClientError:\n        return None\n\n    # get the object\n    return s3.get_object(Bucket=bucket_name, Key=bucket_path)\n\n\ndef put_object(session, bucket_name, bucket_path, file=None, object_acl=\"private\", object_metadata=None):\n    \"\"\"Simple wrapper to put an object in an S3 bucket\n\n    See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_object\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n    bucket_path : str\n        Path in bucket to put object\n    file : str | bytes, optional\n        Path to local object or b'bytes'. If none, this will create a directory in bucket.\n    object_acl : str, optional\n        Object ACL. Defaults to 'private'\n        One of: 'private'|'public-read'|'public-read-write'|'authenticated-read'|'aws-exec-read'|'bucket-owner-read'|'bucket-owner-full-control'\n    object_metadata : dict, optional\n        Metadata to add to object\n    \"\"\"\n\n    if bucket_name is None or bucket_path is None:\n        return None\n\n    _log.debug(\"Putting object {} into S3 bucket {}\".format(bucket_path, bucket_name))\n    s3 = session.client(\"s3\")\n\n    object_config = {\"ACL\": object_acl, \"Bucket\": bucket_name, \"Key\": bucket_path}\n\n    object_body = None\n    if isinstance(file, string_types):\n        with open(file, \"rb\") as f:\n            object_body = f.read()\n    else:\n        object_body = file\n\n    if object_body is not None:\n        object_config[\"Body\"] = object_body\n\n    if object_metadata is not None:\n        object_config[\"Metadata\"] = object_metadata\n\n    s3.put_object(**object_config)\n\n\ndef get_bucket(session, bucket_name):\n    \"\"\"Get S3 bucket parameters\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n\n    Returns\n    -------\n    dict\n        Bucket dict containing keys: \"name\", region\", \"policy\", \"tags\"\n    \"\"\"\n    if bucket_name is None:\n        return None\n\n    _log.debug(\"Getting S3 bucket {}\".format(bucket_name))\n    s3 = session.client(\"s3\")\n\n    # see if the bucket exists\n    try:\n        s3.head_bucket(Bucket=bucket_name)\n    except botocore.exceptions.ClientError:\n        return None\n\n    # init empty object\n    bucket = {\"name\": bucket_name}\n\n    # TODO: this is usually none, even though the bucket has a region. It could either be a bug\n    # in getting the region/LocationConstraint, or just misleading\n    # get location constraint. this will be None for no location constraint\n    bucket[\"region\"] = s3.get_bucket_location(Bucket=bucket_name)[\"LocationConstraint\"]\n\n    try:\n        bucket[\"policy\"] = s3.get_bucket_policy(Bucket=bucket_name)[\"Policy\"]\n    except botocore.exceptions.ClientError:\n        bucket[\"policy\"] = None\n\n    # reverse tags into dict\n    tags = {}\n    try:\n        tag_set = s3.get_bucket_tagging(Bucket=bucket_name)[\"TagSet\"]\n        for tag in tag_set:\n            tags[tag[\"Key\"]] = tag[\"Value\"]\n    except botocore.exceptions.ClientError:\n        pass\n\n    bucket[\"tags\"] = tags\n\n    return bucket\n\n\ndef delete_bucket(session, bucket_name, delete_objects=False):\n    \"\"\"Remove S3 bucket from AWS resources\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name to delete\n    delete_objects : bool, optional\n        Must be set to True if the bucket contains files. This helps avoid deleting buckets inadvertantly\n    \"\"\"\n    if bucket_name is None:\n        _log.error(\"`bucket_name` not defined in delete_bucket\")\n        return\n\n    # make sure bucket exists\n    bucket = get_bucket(session, bucket_name)\n    if bucket is None:\n        _log.debug(\"S3 bucket '{}' does not exist\".format(bucket_name))\n        return\n\n    _log.debug(\"Removing S3 bucket '{}'\".format(bucket_name))\n    s3 = session.client(\"s3\")\n\n    # need to remove all objects before it can be removed. Only do this if delete_objects is TRue\n    if delete_objects:\n        s3resource = session.resource(\"s3\")\n        bucket = s3resource.Bucket(bucket_name)\n        bucket.object_versions.delete()  # delete objects that are versioned\n        bucket.objects.all().delete()  # delete objects that are not versioned\n\n    # now delete bucket\n    s3.delete_bucket(Bucket=bucket_name)\n    _log.debug(\"Successfully removed S3 bucket '{}'\".format(bucket_name))\n\n\n# -----------------------------------------------------------------------------------------------------------------\n# Lambda\n# -----------------------------------------------------------------------------------------------------------------\ndef create_function(\n    session,\n    function_name,\n    function_role_arn,\n    function_handler,\n    function_description=\"PODPAC function\",\n    function_timeout=600,\n    function_memory=2048,\n    function_env_variables={},\n    function_tags={},\n    function_source_dist_zip=None,\n    function_source_bucket=None,\n    function_source_dist_key=None,\n):\n    \"\"\"Build Lambda function and associated resources on AWS\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    function_role_arn : str\n        Role ARN for the function.\n        Generate a role for lambda function execution with :func:`podpac.managers.aws.create_role`.\n        The \"Arn\" key in the output of this function can be used and this input.\n    function_handler : str\n        Handler module and method (i.e. \"module.method\")\n    function_description : str, optional\n        Function description\n    function_timeout : int, optional\n        Function timeout\n    function_memory : int, optional\n        Function memory limit\n    function_env_variables : dict, optional\n        Environment variables for function\n    function_tags : dict, optional\n        Function tags\n    function_source_dist_zip : str, optional\n        Path to .zip archive containg the function source.\n    function_source_bucket : str\n        S3 Bucket containing .zip archive of the function source. If defined, :attr:`function_source_dist_key` must be defined.\n    function_source_dist_key : str\n        If :attr:`function_source_bucket` is defined, this is the path to the .zip archive of the function source.\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_function`\n    \"\"\"\n\n    function = get_function(session, function_name)\n\n    # TODO: add checks to make sure role parameters match\n    if function is not None:\n        _log.debug(\"AWS lambda function '{}' already exists. Using existing function.\".format(function_name))\n        return function\n\n    if function_name is None:\n        raise ValueError(\"`function_name` is None in create_function\")\n\n    # add special podpac tags for billing id\n    function_tags[\"_podpac_resource\"] = \"true\"\n\n    _log.debug(\"Creating lambda function {}\".format(function_name))\n    awslambda = session.client(\"lambda\")\n\n    lambda_config = {\n        \"Runtime\": \"python3.7\",\n        \"FunctionName\": function_name,\n        \"Publish\": True,\n        \"Role\": function_role_arn,\n        \"Handler\": function_handler,\n        \"Code\": {},\n        \"Description\": function_description,\n        \"Timeout\": function_timeout,\n        \"MemorySize\": function_memory,\n        \"Environment\": {\"Variables\": function_env_variables},\n        \"Tags\": function_tags,\n    }\n\n    # read function from zip file\n    if function_source_dist_zip is not None:\n        raise NotImplementedError(\"Supplying a source dist zip from a local file is not yet supported\")\n        # TODO: this fails when the file size is over a certain limit\n        # with open(function_source_dist_zip, \"rb\") as f:\n        #     lambda_config[\"Code\"][\"ZipFile\"] = f.read()\n\n    # read function from S3 (Default)\n    elif function_source_bucket is not None and function_source_dist_key is not None:\n        lambda_config[\"Code\"][\"S3Bucket\"] = function_source_bucket\n        lambda_config[\"Code\"][\"S3Key\"] = function_source_dist_key\n\n    else:\n        raise ValueError(\"Function source is not defined\")\n\n    # create function\n    awslambda.create_function(**lambda_config)\n\n    # get function after created\n    function = get_function(session, function_name)\n\n    _log.debug(\"Successfully created lambda function '{}'\".format(function_name))\n    return function\n\n\ndef get_function(session, function_name):\n    \"\"\"Get function definition from AWS\n\n    Parameters\n    ----------\n    function_name : str\n        Function name\n\n    Returns\n    -------\n    dict\n        Dict returned from Boto3 get_function\n        Based on value returned by https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lambda.html#Lambda.Client.get_function\n        Adds \"tags\" key to list function tags\n        Returns None if no function role is found\n    \"\"\"\n    if function_name is None:\n        return None\n\n    _log.debug(\"Getting lambda function {}\".format(function_name))\n\n    awslambda = session.client(\"lambda\")\n    try:\n        function = awslambda.get_function(FunctionName=function_name)\n        del function[\"ResponseMetadata\"]  # remove response details from function\n    except awslambda.exceptions.ResourceNotFoundException as e:\n        _log.debug(\"Failed to get lambda function {} with exception: {}\".format(function_name, e))\n        return None\n\n    # get tags\n    try:\n        function[\"tags\"] = awslambda.list_tags(Resource=function[\"Configuration\"][\"FunctionArn\"])[\"Tags\"]\n    except botocore.exceptions.ClientError:\n        function[\"tags\"] = {}\n\n    return function\n\n\ndef update_function(\n    session, function_name, function_source_dist_zip=None, function_source_bucket=None, function_source_dist_key=None\n):\n    \"\"\"Update function on AWS\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    function_source_dist_zip : str, optional\n        Path to .zip archive containg the function source.\n    function_source_bucket : str\n        S3 Bucket containing .zip archive of the function source. If defined, :attr:`function_source_dist_key` must be defined.\n    function_source_dist_key : str\n        If :attr:`function_source_bucket` is defined, this is the path to the .zip archive of the function source.\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_function`\n    \"\"\"\n    function = get_function(session, function_name)\n\n    if function is None:\n        raise ValueError(\"AWS lambda function {} does not exist\".format(function_name))\n\n    _log.debug(\"Updating lambda function {} code\".format(function_name))\n    awslambda = session.client(\"lambda\")\n\n    lambda_config = {\"FunctionName\": function_name, \"Publish\": True}\n\n    # read function from S3 (Default)\n    if function_source_bucket is not None and function_source_dist_key is not None:\n        lambda_config[\"S3Bucket\"] = function_source_bucket\n        lambda_config[\"S3Key\"] = function_source_dist_key\n\n    # read function from zip file\n    elif function_source_dist_zip is not None:\n        raise NotImplementedError(\"Supplying a source dist zip from a local file is not yet supported\")\n        # TODO: this fails when the file size is over a certain limit\n        # with open(function_source_dist_zip, \"rb\") as f:\n        #     lambda_config[\"ZipFile\"] = f.read()\n\n    else:\n        raise ValueError(\"Function source is not defined\")\n\n    # create function\n    awslambda.update_function_code(**lambda_config)\n\n    # get function after created\n    function = get_function(session, function_name)\n\n    _log.debug(\"Successfully updated lambda function code '{}'\".format(function_name))\n    return function\n\n\ndef delete_function(session, function_name):\n    \"\"\"Remove AWS Lambda function\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Lambda function name\n    \"\"\"\n    if function_name is None:\n        _log.error(\"`function_name` not defined in delete_function\")\n        return\n\n    # make sure function exists\n    function = get_function(session, function_name)\n    if function is None:\n        _log.debug(\"Lambda function '{}' does not exist\".format(function_name))\n        return\n\n    _log.debug(\"Removing lambda function '{}'\".format(function_name))\n\n    awslambda = session.client(\"lambda\")\n    awslambda.delete_function(FunctionName=function_name)\n\n    _log.debug(\"Removed lambda function '{}'\".format(function_name))\n\n\ndef add_function_trigger(session, function_name, statement_id, principle, source_arn):\n    \"\"\"Add trigger (permission) to lambda function\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    statement_id : str\n        Specific identifier for trigger\n    principle : str\n        Principle identifier from AWS\n    source_arn : str\n        Source ARN for trigger\n    \"\"\"\n    if function_name is None or statement_id is None or principle is None or source_arn is None:\n        raise ValueError(\n            \"`function_name`, `statement_id`, `principle`, and `source_arn` are required to add function trigger\"\n        )\n\n    awslambda = session.client(\"lambda\")\n    awslambda.add_permission(\n        FunctionName=function_name,\n        StatementId=statement_id,\n        Action=\"lambda:InvokeFunction\",\n        Principal=principle,\n        SourceArn=source_arn,\n    )\n\n\ndef remove_function_trigger(session, function_name, statement_id):\n    \"\"\"Remove trigger (permission) from lambda function\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    statement_id : str\n        Specific identifier for trigger\n    \"\"\"\n    if function_name is None or statement_id is None:\n        _log.error(\"`api_id` or `statement_id` not defined in remove_function_trigger\")\n        return\n\n    awslambda = session.client(\"lambda\")\n    try:\n        awslambda.remove_permission(FunctionName=function_name, StatementId=statement_id)\n    except awslambda.exceptions.ResourceNotFoundException:\n        _log.warning(\"Failed to remove permission {} on function {}\".format(statement_id, function_name))\n\n\n# -----------------------------------------------------------------------------------------------------------------\n# IAM Roles\n# -----------------------------------------------------------------------------------------------------------------\n\n\ndef create_role(\n    session,\n    role_name,\n    role_description=\"PODPAC Role\",\n    role_policy_document=None,\n    role_policy_arns=[],\n    role_assume_policy_document=None,\n    role_tags={},\n):\n    \"\"\"Create IAM role\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_name : str\n        Role name to create\n    role_description : str, optional\n        Role description\n    role_policy_document : dict, optional\n        Role policy document allowing role access to AWS resources\n        See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.put_role\n    role_policy_arns : list, optional\n        List of role policy ARN's to attach to role\n    role_assume_policy_document : None, optional\n        Role policy document.\n        Defaults to trust policy allowing role to execute lambda functions.\n        See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.create_role\n    role_tags : dict, optional\n        Role tags\n\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_role`\n    \"\"\"\n\n    role = get_role(session, role_name)\n\n    # TODO: add checks to make sure role parameters match\n    if role is not None:\n        _log.debug(\"IAM role '{}' already exists. Using existing role.\".format(role_name))\n        return role\n\n    if role_name is None:\n        raise ValueError(\"`role_name` is None in create_role\")\n\n    # default role_assume_policy_document is lambda\n    if role_assume_policy_document is None:\n        role_assume_policy_document = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}\n            ],\n        }\n\n    # add special podpac tags for billing id\n    role_tags[\"_podpac_resource\"] = \"true\"\n\n    _log.debug(\"Creating IAM role {}\".format(role_name))\n    iam = session.client(\"iam\")\n\n    iam_config = {\n        \"RoleName\": role_name,\n        \"Description\": role_description,\n        \"AssumeRolePolicyDocument\": json.dumps(role_assume_policy_document),\n    }\n\n    # for some reason the tags API is different here\n    tags = []\n    for key in role_tags.keys():\n        tags.append({\"Key\": key, \"Value\": role_tags[key]})\n    iam_config[\"Tags\"] = tags\n\n    # create role\n    iam.create_role(**iam_config)\n\n    # add role policy document\n    if role_policy_document is not None:\n        policy_name = \"{}-policy\".format(role_name)\n        iam.put_role_policy(RoleName=role_name, PolicyName=policy_name, PolicyDocument=json.dumps(role_policy_document))\n\n    # attached role polcy ARNs\n    for policy in role_policy_arns:\n        iam.attach_role_policy(RoleName=role_name, PolicyArn=policy)\n\n    # get finalized role\n    role = get_role(session, role_name)\n    _log.debug(\"Successfully created IAM role '{}'\".format(role_name))\n\n    return role\n\n\ndef get_role(session, role_name):\n    \"\"\"Get role definition from AWS\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_name : str\n        Role name\n\n    Returns\n    -------\n    dict\n        Dict returned from AWS defining role.\n        Based on the 'Role' key in https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.get_role\n        Adds \"policy_document\" key to show inline policy document.\n        Adds \"policy_arns\" key to list attached policies.\n        Adds \"tags\" key to list function tags\n        Returns None if no role is found\n    \"\"\"\n    if role_name is None:\n        return None\n\n    _log.debug(\"Getting IAM role with name {}\".format(role_name))\n    iam = session.client(\"iam\")\n\n    try:\n        response = iam.get_role(RoleName=role_name)\n        role = response[\"Role\"]\n    except iam.exceptions.NoSuchEntityException as e:\n        _log.debug(\"Failed to get IAM role for name {} with exception: {}\".format(role_name, e))\n        return None\n\n    # get inline policies\n    try:\n        policy_name = \"{}-policy\".format(role_name)\n        response = iam.get_role_policy(RoleName=role_name, PolicyName=policy_name)\n        role[\"policy_document\"] = response[\"PolicyDocument\"]\n    except botocore.exceptions.ClientError:\n        role[\"policy_document\"] = None\n\n    # get attached policies\n    try:\n        response = iam.list_attached_role_policies(RoleName=role_name)\n        role[\"policy_arns\"] = [policy[\"PolicyArn\"] for policy in response[\"AttachedPolicies\"]]\n    except botocore.exceptions.ClientError:\n        role[\"policy_arns\"] = []\n\n    # get tags - reverse tags into dict\n    tags = {}\n    try:\n        response = iam.list_role_tags(RoleName=role_name)\n        for tag in response[\"Tags\"]:\n            tags[tag[\"Key\"]] = tag[\"Value\"]\n    except botocore.exceptions.ClientError:\n        pass\n\n    role[\"tags\"] = tags\n\n    return role\n\n\ndef get_role_name(session, role_arn):\n    \"\"\"\n    Get function role name based on role_arn\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_arn : str\n        Role arn\n\n    Returns\n    -------\n    str\n        Role name.\n        Returns None if no role name is found for role arn.\n    \"\"\"\n    if role_arn is None:\n        return None\n\n    iam = session.client(\"iam\")\n    roles = iam.list_roles()\n    role = [role for role in roles[\"Roles\"] if role[\"Arn\"] == role_arn]\n    role_name = role[0] if len(role) else None\n    return role_name\n\n\ndef delete_role(session, role_name):\n    \"\"\"Remove role from AWS resources\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_name : str\n        Role name to delete\n    \"\"\"\n    if role_name is None:\n        _log.error(\"`role_name` not defined in delete_role\")\n        return\n\n    # make sure function exists\n    role = get_role(session, role_name)\n    if role is None:\n        _log.debug(\"IAM role '{}' does not exist\".format(role_name))\n        return\n\n    _log.debug(\"Removing IAM role '{}'\".format(role_name))\n    iam = session.client(\"iam\")\n\n    # need to remove inline policies first, if they exist\n    try:\n        policy_name = \"{}-policy\".format(role_name)\n        iam.delete_role_policy(RoleName=role_name, PolicyName=policy_name)\n    except botocore.exceptions.ClientError:\n        pass\n\n    # need to detach policies first\n    response = iam.list_attached_role_policies(RoleName=role_name)\n    for policy in response[\"AttachedPolicies\"]:\n        iam.detach_role_policy(RoleName=role_name, PolicyArn=policy[\"PolicyArn\"])\n\n    iam.delete_role(RoleName=role_name)\n    _log.debug(\"Successfully removed IAM role '{}'\".format(role_name))\n\n\n# -----------------------------------------------------------------------------------------------------------------\n# API Gateway\n# -----------------------------------------------------------------------------------------------------------------\n\n\ndef create_api(\n    session, api_name=\"podpac-api\", api_description=\"PODPAC API\", api_version=None, api_tags={}, api_endpoint=\"eval\"\n):\n    \"\"\"Create API Gateway REST API\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_name : str\n        API Name\n    api_description : str, optional\n        API Description. Defaults to \"PODPAC API\"\n    api_version : str, optional\n        API Version. Defaults to PODPAC version.\n    api_tags : dict, optional\n        API tags. Defaults to {}.\n    api_endpoint : str, optional\n        API endpoint. Defaults to \"eval\".\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_api`\n    \"\"\"\n\n    # set version to podpac version, if None\n    api = get_api(session, api_name, api_endpoint)\n\n    # TODO: add checks to make sure api parameters match\n    if api is not None and (\"resource\" in api and api[\"resource\"] is not None):\n        _log.debug(\n            \"API '{}' and API resource {} already exist. Using existing API ID and resource.\".format(\n                api_name, api_endpoint\n            )\n        )\n        return api\n\n    # add special podpac tags for billing id\n    api_tags[\"_podpac_resource\"] = \"true\"\n\n    apigateway = session.client(\"apigateway\")\n\n    if api is None:\n        _log.debug(\"Creating API gateway with name {}\".format(api_name))\n\n        # set version default\n        if api_version is None:\n            api_version = version.semver()\n\n        api = apigateway.create_rest_api(\n            name=api_name,\n            description=api_description,\n            version=api_version,\n            binaryMediaTypes=[\"*/"
        ],
        "docstrings": [
            "\"\"\"\nLambda is `Node` manager, which executes the given `Node` on an AWS Lambda\nfunction.\n\"\"\"",
            "\"\"\"Exception during execution of a Lambda node\"\"\"",
            "\"\"\"A `Node` wrapper to evaluate source on AWS Lambda function\n\n\n    Attributes\n    ----------\n    aws_access_key_id : str, optional\n        Access key id from AWS credentials. If :attr:`session` is provided, this attribute will be ignored. Overrides :attr:`podpac.settings`.\n    aws_region_name : str, optional\n        Name of the AWS region. If :attr:`session` is provided, this attribute will be ignored. Overrides :attr:`podpac.settings`.\n    aws_secret_access_key : str\n        Access key value from AWS credentials. If :attr:`session` is provided, this attribute will be ignored. Overrides :attr:`podpac.settings`.\n    function_name : str, optional\n        Name of the lambda function to use or create. Defaults to :attr:`podpac.settings[\"FUNCTION_NAME\"]` or \"podpac-lambda-autogen\".\n    function_timeout : int, optional\n        Timeout of the lambda function, in seconds. Defaults to 600.\n    function_triggers : list of str, optional\n        Methods to trigger this function. May only include [\"eval\", \"S3\", \"APIGateway\"]. During the :meth:`self.build()` process, this list will determine which AWS resources are linked to Lambda function. Defaults to [\"eval\"].\n    function_role_name : str, optional\n        Name of the AWS role created for lambda function. Defaults to :attr:`podpac.settings[\"FUNCTION_ROLE_NAME\"]` or \"podpac-lambda-autogen\".\n    function_s3_bucket : str, optional\n        S3 bucket name to use with lambda function. Defaults to :attr:`podpac.settings[\"S3_BUCKET_NAME\"]` or \"podpac-autogen-<timestamp>\" with the timestamp to ensure uniqueness.\n    eval_settings : dict, optional\n        Default is podpac.settings. PODPAC settings that will be used to evaluate the Lambda function.\n    eval_timeout : float, optional\n        Default is None. The amount of time to wait for an eval to return. To get near asynchronous response, set this to a small number.\n\n    Other Attributes\n    ----------------\n    node_attrs : dict\n        Additional attributes passed on to the Lambda definition of the base node\n    download_result : Bool\n        Flag that indicated whether node should wait to download the data.\n    function_api_description : str, optional\n        Description for the AWS API Gateway resource\n    function_api_endpoint : str, optional\n        Endpoint path for API Gateway. Defaults to \"eval\".\n    function_api_name : str, optional\n        AWS resource name for the API Gateway. Defaults to :attr:`self.function_name` + \"-api\".\n    function_api_stage : str, optional\n        Stage name for the API gateway. Defaults to \"prod\".\n    function_api_tags : dict, optional\n        AWS Tags for API Gateway resource. Defaults to :attr:`self.function_tags`.\n    function_api_version : str, optional\n        API Gateway version. Defaults to :meth:`podpac.verions.semver()`.\n    function_description : str, optional\n        Description for the AWS Lambda function resource\n    function_env_variables : dict, optional\n        Environment variables to use within the lambda function.\n    function_eval_trigger : str, optional\n        Function trigger to use during node eval process. Must be on of \"eval\" (default), \"S3\", or \"APIGateway\".\n    function_handler : str, optional\n        Handler method in Lambda function. Defaults to \"handler.handler\".\n    function_memory : int, optional\n        Memory allocated for each Lambda function. Defaults to 2048 MB.\n    function_restrict_pipelines : list, optional\n        List of Node hashes (see :class:`podpac.Node.hash`).\n        Restricts lambda function evaluation to specific Node definitions.\n    function_role_assume_policy_document : dict, optional.\n        Assume policy document for role created. Defaults to allowing role to assume Lambda function.\n    function_role_description : str, optional\n        Description for the AWS role resource\n    function_role_policy_arns : list of str, optional\n        Managed role policy ARNs to attach to role.\n    function_role_policy_document : dict, optional\n        Inline role policies to put in role.\n    function_role_tags : dict, optional\n        AWS Tags for role resource. Defaults to :attr:`self.function_tags`.\n    function_s3_dependencies_key : str, optional\n        S3 path to copy and reference podpac dependencies. Defaults to \"podpac_deps_<semver>.zip\".\n    function_s3_input : str, optional\n        Folder in :attr:`self.function_s3_bucket` to store input pipelines when \"S3\" is included in :attr:`self.function_triggers`. Defaults to \"input\".\n    function_s3_output : str, optional\n        Folder in :attr:`self.function_s3_bucket` to watch for output when \"S3\" is included in :attr:`self.function_triggers`. Defaults to \"output\".\n    function_s3_tags : dict, optional\n        AWS Tags for S3 bucket resource. Defaults to :attr:`self.function_tags`.\n    function_source_bucket : str, optional\n        S3 Bucket to use for released podpac distribution during :meth:`self.build()` process. Defaults to \"podpac-dist\". This bucket is managed by the PODPAC distribution team.\n    function_source_dependencies_key : str, optional\n        S3 path within :attr:`self.function_source_bucket` to source podpac dependencies archive during :meth:`self.build()` process. Defaults to \"<semver>/podpac_deps.zip\".\n    function_source_dependencies_zip : str, optional\n        Override :attr:`self.function_source_dependencies_key` and upload custom source podpac dependencies archive to :attr:`self.function_s3_bucket` during :meth:`self.build()` process.\n    function_source_dist_key : str, optional\n        S3 path within :attr:`self.function_source_bucket` to source podpac dist archive during :meth:`self.build()` process. Defaults to \"<semver>/podpac_dist.zip\".\n    function_source_dist_zip : str, optional\n        Override :attr:`self.function_source_dist_key` and create lambda function using custom source podpac dist archive to :attr:`self.function_s3_bucket` during :meth:`self.build()` process.\n    function_tags : dict, optional\n        AWS Tags for Lambda function resource. Defaults to :attr:`podpac.settings[\"AWS_TAGS\"]` or {}.\n    function_budget_amount : float, optional\n        EXPERIMENTAL FEATURE\n        Monthly budget for function and associated AWS resources.\n        When usage reaches 80% of this amount, AWS will notify :attr:`function_budget_email`.\n        Defaults to :attr:`podpac.settings[\"AWS_BUDGET_AMOUNT\"]`.\n    function_budget_email : str, optional\n        EXPERIMENTAL FEATURE\n        Email to notify when usage reaches 80% of :attr:`function_budget_amount`.\n        Defaults to :attr:`podpac.settings[\"AWS_BUDGET_EMAIL\"]`.\n    function_budget_name : str, optional\n        EXPERIMENTAL FEATURE\n        Name for AWS budget\n    function_budget_currency : str, optional\n        EXPERIMENTAL FEATURE\n        Currency type for the :attr:`function_budget_amount`.\n        Defaults to \"USD\".\n        See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.create_budget\n        for currency (or Unit) options.\n    output_format : dict, optional\n        Definition for how output is saved after results are computed.\n    session : :class:`podpac.managers.aws.Session`\n        AWS Session to use for this node.\n    source : :class:`podpac.Node`\n        Node to be evaluated on the Lambda function.\n    source_output_format : str\n        Output format for the evaluated results of `source`\n    source_output_name : str\n        Output name for the evaluated results of `source`\n    \"\"\"",
            "\"\"\"\n        The pipeline of this manager is the aggregation of the source node definition and the output.\n        \"\"\"",
            "\"\"\"\n        Evaluate the source node on the AWS Lambda Function at the given coordinates\n        \"\"\"",
            "\"\"\"Build Lambda function and associated resources on AWS\n        to run PODPAC pipelines\n        \"\"\"",
            "\"\"\"\n        Validate cloud resources and interoperability of resources for\n        PODPAC usage\n\n        Parameters\n        ----------\n        raise_exceptions : bool, optional\n            Raise validation errors when encountered\n        \"\"\"",
            "\"\"\"Remove all cloud resources associated with function\n\n        Parameters\n        ----------\n        confirm : bool, optional\n            Must pass in confirm paramter\n        \"\"\"",
            "\"\"\"Show a description of the Lambda Utilities\"\"\"",
            "\"\"\"\n    API\n        Name: {function_api_name}\n        Description: {function_api_description}\n        ID: {_function_api_id}\n        Resource ID: {_function_api_resource_id}\n        Version: {function_api_version}\n        Tags: {function_api_tags}\n        Stage: {function_api_stage}\n        Endpoint: {function_api_endpoint}\n        URL: {_function_api_url}\n            \"\"\"",
            "\"\"\"\n    Budget\n        Name: {function_budget_name}\n        Amount: {function_budget_amount}\n        Currency: {function_budget_currency}\n        E-mail: {function_budget_email}\n        Spent: {function_budget_usage} {function_budget_usage_currency}\n            \"\"\"",
            "\"\"\"\nLambda Node {status}\n    Function\n        Name: {function_name}\n        Description: {function_description}\n        ARN: {_function_arn}\n        Triggers: {function_triggers}\n        Handler: {function_handler}\n        Environment Variables: {function_env_variables}\n        Timeout: {function_timeout} seconds\n        Memory: {function_memory} MB\n        Tags: {function_tags}\n        Source Dist: {source_dist}\n        Source Dependencies: {source_deps}\n        Last Modified: {_function_last_modified}\n        Version: {_function_version}\n        Restrict Evaluation: {function_restrict_pipelines}\n\n    S3\n        Bucket: {function_s3_bucket}\n        Tags: {function_s3_tags}\n        Input Folder: {function_s3_input}\n        Output Folder: {function_s3_output}\n\n    Role\n        Name: {function_role_name}\n        Description: {function_role_description}\n        ARN: {_function_role_arn}\n        Policy Document: {function_role_policy_document}\n        Policy ARNs: {function_role_policy_arns}\n        Assume Policy Document: {function_role_assume_policy_document}\n        Tags: {function_role_tags}\n\n        {api_output}\n\n        {budget_output}\n        \"\"\"",
            "\"\"\"Build Lambda function on AWS\"\"\"",
            "\"\"\"Update lambda function with new parameters\"\"\"",
            "\"\"\"Get function definition from AWS\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_function`\n        \"\"\"",
            "\"\"\"\n        Validate that function is configured properly\n\n        This should only be run after running `self.get_function()`\n        \"\"\"",
            "\"\"\"Remove AWS Lambda function and associated resources on AWS\"\"\"",
            "\"\"\"Add trigger (permission) to lambda function\n\n        Parameters\n        ----------\n        statement_id : str\n            Specific identifier for trigger\n        principle : str\n            Principle identifier from AWS\n        source_arn : str\n            Source ARN for trigger\n        \"\"\"",
            "\"\"\"Remove trigger (permission) from lambda function\n\n        Parameters\n        ----------\n        statement_id : str\n            Specific identifier for trigger\n        \"\"\"",
            "\"\"\"\n        Remove all triggers from function\n        \"\"\"",
            "\"\"\"Create IAM role to execute podpac lambda function\"\"\"",
            "\"\"\"Get role definition from AWS\n\n        See :attr:`self.function_role_name` for role_name\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_role`\n        \"\"\"",
            "\"\"\"\n        Validate that role will work with function.\n\n        This should only be run after running `self.get_role()`\n        \"\"\"",
            "\"\"\"Remove role from AWS resources\n\n        See :attr:`self.function_role_name` for role_name\n        \"\"\"",
            "\"\"\"Create S3 bucket to work with function\"\"\"",
            "\"\"\"Get S3 Bucket for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_bucket`\n        \"\"\"",
            "\"\"\"\n        Validate that bucket will work with function.\n\n        This should only be run after running `self.get_bucket()`\n        \"\"\"",
            "\"\"\"Delete bucket associated with this function\n\n        Parameters\n        ----------\n        delete_objects : bool, optional\n            Delete all objects in the bucket while deleting bucket. Defaults to False.\n        \"\"\"",
            "\"\"\"Create API Gateway API for lambda function\"\"\"",
            "\"\"\"Get API Gateway definition for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_api`\n        \"\"\"",
            "\"\"\"\n        Validate that API will work with function.\n\n        This should only be run after running `self.get_api()`\n        \"\"\"",
            "\"\"\"Delete API Gateway for Function\"\"\"",
            "\"\"\"\n        EXPERIMENTAL FEATURE\n        Create budget for lambda function based on node hash.\n        \"\"\"",
            "\"\"\"\n        EXPERIMENTAL FEATURE\n        Get budget definition for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_budget`\n        \"\"\"",
            "\"\"\"Validate budget definition for function\n\n        Returns\n        -------\n        dict\n            See :func:`podpac.managers.aws.get_budget`\n        \"\"\"",
            "\"\"\"Delete budget associated with function\"\"\"",
            "\"\"\"Get Cloudwatch logs from lambda function execution\n\n        See :func:`podpac.managers.aws.get_logs`\n\n        Parameters\n        ----------\n        limit : int, optional\n            Limit logs to the most recent N logs\n        start : str, optional\n            Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(start))\n            Defaults to 1 hour prior to ``end``.\n        end : str, optional\n            Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(end))\n            Defaults to now.\n\n        Returns\n        -------\n        list\n            list of log events\n        \"\"\"",
            "\"\"\"Set function class members\n\n        Parameters\n        ----------\n        function : dict\n        \"\"\"",
            "\"\"\"Set role class members\n\n        Parameters\n        ----------\n        role : dict\n        \"\"\"",
            "\"\"\"Set bucket class members\n\n        Parameters\n        ----------\n        bucket : dict\n        \"\"\"",
            "\"\"\"Set api class members\n\n        Parameters\n        ----------\n        api : dict\n        \"\"\"",
            "\"\"\"Set budget class members\n\n        Parameters\n        ----------\n        budget : dict\n        \"\"\"",
            "\"\"\"shorthand to create pipeline on eval\"\"\"",
            "\"\"\"eval node through invoke trigger\"\"\"",
            "\"\"\"Evaluate node through s3 trigger\"\"\"",
            "\"\"\"Generated API url\"\"\"",
            "\"\"\"Wrapper for :class:`boto3.Session`\n    See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html\n\n    We wrap the Session class to provide access to the podpac settings for the\n    aws_access_key_id, aws_secret_access_key, and region_name and to check the credentials\n    on session creation.\n    \"\"\"",
            "\"\"\"Return the account ID assciated with this AWS session. The credentials will determine the account ID.\n\n        Returns\n        -------\n        str\n            account id associated with credentials\n        \"\"\"",
            "\"\"\"Create S3 bucket\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n    bucket_region : str, optional\n        Location constraint for bucket. Defaults to no location constraint\n    bucket_policy : dict, optional\n        Bucket policy document as dict. For parameters, see https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketPolicy.html#API_PutBucketPolicy_RequestSyntax\n    bucket_tags : dict, optional\n        Bucket tags\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_bucket`\n\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\"",
            "\"\"\"Get an object from an S3 bucket\n\n    See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n    bucket_path : str\n        Path to object in bucket\n    \"\"\"",
            "\"\"\"Simple wrapper to put an object in an S3 bucket\n\n    See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_object\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n    bucket_path : str\n        Path in bucket to put object\n    file : str | bytes, optional\n        Path to local object or b'bytes'. If none, this will create a directory in bucket.\n    object_acl : str, optional\n        Object ACL. Defaults to 'private'\n        One of: 'private'|'public-read'|'public-read-write'|'authenticated-read'|'aws-exec-read'|'bucket-owner-read'|'bucket-owner-full-control'\n    object_metadata : dict, optional\n        Metadata to add to object\n    \"\"\"",
            "\"\"\"Get S3 bucket parameters\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name\n\n    Returns\n    -------\n    dict\n        Bucket dict containing keys: \"name\", region\", \"policy\", \"tags\"\n    \"\"\"",
            "\"\"\"Remove S3 bucket from AWS resources\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    bucket_name : str\n        Bucket name to delete\n    delete_objects : bool, optional\n        Must be set to True if the bucket contains files. This helps avoid deleting buckets inadvertantly\n    \"\"\"",
            "\"\"\"Build Lambda function and associated resources on AWS\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    function_role_arn : str\n        Role ARN for the function.\n        Generate a role for lambda function execution with :func:`podpac.managers.aws.create_role`.\n        The \"Arn\" key in the output of this function can be used and this input.\n    function_handler : str\n        Handler module and method (i.e. \"module.method\")\n    function_description : str, optional\n        Function description\n    function_timeout : int, optional\n        Function timeout\n    function_memory : int, optional\n        Function memory limit\n    function_env_variables : dict, optional\n        Environment variables for function\n    function_tags : dict, optional\n        Function tags\n    function_source_dist_zip : str, optional\n        Path to .zip archive containg the function source.\n    function_source_bucket : str\n        S3 Bucket containing .zip archive of the function source. If defined, :attr:`function_source_dist_key` must be defined.\n    function_source_dist_key : str\n        If :attr:`function_source_bucket` is defined, this is the path to the .zip archive of the function source.\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_function`\n    \"\"\"",
            "\"\"\"Get function definition from AWS\n\n    Parameters\n    ----------\n    function_name : str\n        Function name\n\n    Returns\n    -------\n    dict\n        Dict returned from Boto3 get_function\n        Based on value returned by https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lambda.html#Lambda.Client.get_function\n        Adds \"tags\" key to list function tags\n        Returns None if no function role is found\n    \"\"\"",
            "\"\"\"Update function on AWS\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    function_source_dist_zip : str, optional\n        Path to .zip archive containg the function source.\n    function_source_bucket : str\n        S3 Bucket containing .zip archive of the function source. If defined, :attr:`function_source_dist_key` must be defined.\n    function_source_dist_key : str\n        If :attr:`function_source_bucket` is defined, this is the path to the .zip archive of the function source.\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_function`\n    \"\"\"",
            "\"\"\"Remove AWS Lambda function\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Lambda function name\n    \"\"\"",
            "\"\"\"Add trigger (permission) to lambda function\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    statement_id : str\n        Specific identifier for trigger\n    principle : str\n        Principle identifier from AWS\n    source_arn : str\n        Source ARN for trigger\n    \"\"\"",
            "\"\"\"Remove trigger (permission) from lambda function\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    function_name : str\n        Function name\n    statement_id : str\n        Specific identifier for trigger\n    \"\"\"",
            "\"\"\"Create IAM role\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_name : str\n        Role name to create\n    role_description : str, optional\n        Role description\n    role_policy_document : dict, optional\n        Role policy document allowing role access to AWS resources\n        See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.put_role\n    role_policy_arns : list, optional\n        List of role policy ARN's to attach to role\n    role_assume_policy_document : None, optional\n        Role policy document.\n        Defaults to trust policy allowing role to execute lambda functions.\n        See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.create_role\n    role_tags : dict, optional\n        Role tags\n\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_role`\n    \"\"\"",
            "\"\"\"Get role definition from AWS\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_name : str\n        Role name\n\n    Returns\n    -------\n    dict\n        Dict returned from AWS defining role.\n        Based on the 'Role' key in https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.get_role\n        Adds \"policy_document\" key to show inline policy document.\n        Adds \"policy_arns\" key to list attached policies.\n        Adds \"tags\" key to list function tags\n        Returns None if no role is found\n    \"\"\"",
            "\"\"\"\n    Get function role name based on role_arn\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_arn : str\n        Role arn\n\n    Returns\n    -------\n    str\n        Role name.\n        Returns None if no role name is found for role arn.\n    \"\"\"",
            "\"\"\"Remove role from AWS resources\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    role_name : str\n        Role name to delete\n    \"\"\"",
            "\"\"\"Create API Gateway REST API\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_name : str\n        API Name\n    api_description : str, optional\n        API Description. Defaults to \"PODPAC API\"\n    api_version : str, optional\n        API Version. Defaults to PODPAC version.\n    api_tags : dict, optional\n        API tags. Defaults to {}.\n    api_endpoint : str, optional\n        API endpoint. Defaults to \"eval\".\n\n    Returns\n    -------\n    dict\n        See :func:`podpac.managers.aws.get_api`\n    \"\"\"",
            "\"\"\"Get API Gateway definition\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_name : str\n        API Name\n    api_endpoint : str, optional\n        API endpoint path, defaults to returning the first endpoint it finds\n\n    Returns\n    -------\n    dict\n        (Returns output of Boto3 API Gateway creation\n        Equivalent to https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/apigateway.html#APIGateway.Client.create_rest_api\n        Contains extra key \"resource\" with output of of Boto3 API Resource. Set to None if API Resource ID is not found)\n        Returns None if API Id is not found\n    \"\"\"",
            "\"\"\"Deploy API gateway definition\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_id : str\n        API ID. Generated during API creation or returned from :func:`podpac.manager.aws.get_api`\n    api_stage : str\n        API Stage\n    \"\"\"",
            "\"\"\"Delete API Gateway API\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_id : str\n        API ID. Generated during API Creation.\n    \"\"\"",
            "\"\"\"\n    EXPERIMENTAL FEATURE\n    Create a budget for podpac AWS resources based on tags.\n    By default, this creates a budget for all podpac created resources with the tag: {\"_podpac_resource\": \"true\"}\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    budget_amount : int\n        Budget amount\n    budget_email : str, optional\n        Notification e-mail for budget alerts.\n        If no e-mail is provided, the budget must be monitored through the AWS interface.\n    budget_name : str, optional\n        Budget name\n    budget_currency : str, optional\n        Currency unit for budget. Defaults to \"USD\".\n        See https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.describe_budget\n        for Unit types.\n    budget_threshold : float, optional\n        Percent of the budget at which an e-mail notification is sent.\n    budget_filter_tags : dict, optional\n        Create budget for specific set of resource tags.\n        By default, the budget is created for all podpac created resources.\n\n    Returns\n    -------\n    dict\n        Returns Boto3 budget description\n        Equivalent to https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.describe_budget\n    \"\"\"",
            "\"\"\"\n    EXPERIMENTAL FEATURE\n    Get a budget by name\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    budget_name : str\n        Budget name\n\n    Returns\n    -------\n    dict\n        Returns Boto3 budget description\n        Equivalent to https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/budgets.html#Budgets.Client.describe_budget\n        Returns None if budget is not found.\n    \"\"\"",
            "\"\"\"Delete a budget by name\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    budget_name : str\n        Budget name\n\n    Returns\n    -------\n    None\n    \"\"\"",
            "\"\"\"Get logs from cloudwatch from specific log groups\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    log_group_name : str\n        Log group name\n    limit : int, optional\n        Limit logs to the most recent N logs\n    start : str, optional\n        Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(start))\n        Defaults to 1 hour prior to ``end``.\n    end : str, optional\n        Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(end))\n        Defaults to now.\n\n    Returns\n    -------\n    list\n        list of log events\n    \"\"\""
        ],
        "code_snippets": [
            "class\n\ntry:\n    import boto3\n    import botocore\nexcept:",
            "class err:",
            "def __init__(self, *args, **kwargs):\n            raise ImportError(\"boto3 is not installed, please install to use this functionality.\")",
            "class boto3:\n        Session = err\n\n\n# Set up logging\n_log = logging.getLogger(__name__)\n\nCOMMON_DOC = COMMON_NODE_DOC.copy()",
            "class LambdaException(Exception):",
            "class Lambda(Node):\n    \"\"\"A `Node` wrapper to evaluate source on AWS Lambda function\n\n\n    Attributes\n    ----------\n    aws_access_key_id : str, optional\n        Access key id from AWS credentials. If :attr:`session` is provided, this attribute will be ignored. Overrides :attr:`podpac.settings`.\n    aws_region_name : str, optional\n        Name of the AWS region. If :attr:`session` is provided, this attribute will be ignored. Overrides :attr:`podpac.settings`.\n    aws_secret_access_key : str\n        Access key value from AWS credentials. If :attr:`session` is provided, this attribute will be ignored. Overrides :attr:`podpac.settings`.\n    function_name : str, optional\n        Name of the lambda function to use or create. Defaults to :attr:`podpac.settings[\"FUNCTION_NAME\"]` or \"podpac-lambda-autogen\".\n    function_timeout : int, optional\n        Timeout of the lambda function, in seconds. Defaults to 600.\n    function_triggers : list of str, optional\n        Methods to trigger this function. May only include [\"eval\", \"S3\", \"APIGateway\"]. During the :meth:`self.build()` process, this list will determine which AWS resources are linked to Lambda function. Defaults to [\"eval\"].\n    function_role_name : str, optional\n        Name of the AWS role created for lambda function. Defaults to :attr:`podpac.settings[\"FUNCTION_ROLE_NAME\"]` or \"podpac-lambda-autogen\".\n    function_s3_bucket : str, optional\n        S3 bucket name to use with lambda function. Defaults to :attr:`podpac.settings[\"S3_BUCKET_NAME\"]` or \"podpac-autogen-<timestamp>\" with the timestamp to ensure uniqueness.\n    eval_settings : dict, optional\n        Default is podpac.settings. PODPAC settings that will be used to evaluate the Lambda function.\n    eval_timeout : float, optional\n        Default is None. The amount of time to wait for an eval to return. To get near asynchronous response, set this to a small number.\n\n    Other Attributes\n    ----------------\n    node_attrs : dict\n        Additional attributes passed on to the Lambda definition of the base node\n    download_result : Bool\n        Flag that indicated whether node should wait to download the data.\n    function_api_description : str, optional\n        Description for the AWS API Gateway resource\n    function_api_endpoint : str, optional\n        Endpoint path for API Gateway. Defaults to \"eval\".\n    function_api_name : str, optional\n        AWS resource name for the API Gateway. Defaults to :attr:`self.function_name` + \"-api\".\n    function_api_stage : str, optional\n        Stage name for the API gateway. Defaults to \"prod\".\n    function_api_tags : dict, optional\n        AWS Tags for API Gateway resource. Defaults to :attr:`self.function_tags`.\n    function_api_version : str, optional\n        API Gateway version. Defaults to :meth:`podpac.verions.semver()`.\n    function_description : str, optional\n        Description for the AWS Lambda function resource\n    function_env_variables : dict, optional\n        Environment variables to use within the lambda function.\n    function_eval_trigger : str, optional\n        Function trigger to use during node eval process. Must be on of \"eval\" (default), \"S3\", or \"APIGateway\".\n    function_handler : str, optional\n        Handler method in Lambda function. Defaults to \"handler.handler\".\n    function_memory : int, optional\n        Memory allocated for each Lambda function. Defaults to 2048 MB.\n    function_restrict_pipelines : list, optional\n        List of Node hashes (see :class:`podpac.Node.hash`).\n        Restricts lambda function evaluation to specific Node definitions.\n    function_role_assume_policy_document : dict, optional.\n        Assume policy document for role created. Defaults to allowing role to assume Lambda function.\n    function_role_description : str, optional\n        Description for the AWS role resource\n    function_role_policy_arns : list of str, optional\n        Managed role policy ARNs to attach to role.\n    function_role_policy_document : dict, optional\n        Inline role policies to put in role.\n    function_role_tags : dict, optional\n        AWS Tags for role resource. Defaults to :attr:`self.function_tags`.\n    function_s3_dependencies_key : str, optional\n        S3 path to copy and reference podpac dependencies. Defaults to \"podpac_deps_<semver>.zip\".\n    function_s3_input : str, optional\n        Folder in :attr:`self.function_s3_bucket` to store input pipelines when \"S3\" is included in :attr:`self.function_triggers`. Defaults to \"input\".\n    function_s3_output : str, optional\n        Folder in :attr:`self.function_s3_bucket` to watch for output when \"S3\" is included in :attr:`self.function_triggers`. Defaults to \"output\".\n    function_s3_tags : dict, optional\n        AWS Tags for S3 bucket resource. Defaults to :attr:`self.function_tags`.\n    function_source_bucket : str, optional\n        S3 Bucket to use for released podpac distribution during :meth:`self.build()` process. Defaults to \"podpac-dist\". This bucket is managed by the PODPAC distribution team.\n    function_source_dependencies_key : str, optional\n        S3 path within :attr:`self.function_source_bucket` to source podpac dependencies archive during :meth:`self.build()` process. Defaults to \"<semver>/podpac_deps.zip\".\n    function_source_dependencies_zip : str, optional\n        Override :attr:`self.function_source_dependencies_key` and upload custom source podpac dependencies archive to :attr:`self.function_s3_bucket` during :meth:`self.build()` process.\n    function_source_dist_key : str, optional\n        S3 path within :attr:`self.function_source_bucket` to source podpac dist archive during :meth:`self.build()` process. Defaults to \"<semver>/podpac_dist.zip\".\n    function_source_dist_zip : str, optional\n        Override :attr:`self.function_source_dist_key` and create lambda function using custom source podpac dist archive to :attr:`self.function_s3_bucket` during :meth:`self.build()` process.\n    function_tags : dict, optional\n        AWS Tags for Lambda function resource. Defaults to :attr:`podpac.settings[\"AWS_TAGS\"]` or {}.\n    function_budget_amount : float, optional\n        EXPERIMENTAL FEATURE\n        Monthly budget for function and associated AWS resources.\n        When usage reaches 80% of this amount, AWS will notify :attr:`function_budget_email`.\n        Defaults to :attr:`podpac.settings[\"AWS_BUDGET_AMOUNT\"]`.\n    function_budget_email : str, optional\n        EXPERIMENTAL FEATURE\n        Email to notify when usage reaches 80% of :attr:`function_budget_amount`.\n        Defaults to :attr:`podpac.settings[\"AWS_BUDGET_EMAIL\"]`.\n    function_budget_name : str, optional\n        EXPERIMENTAL FEATURE\n        Name for AWS budget\n    function_budget_currency : str, optional\n        EXPERIMENTAL FEATURE\n        Currency type for the :attr:`function_budget_amount`.\n        Defaults to \"USD\".\n        See https:",
            "def _session_default(self):\n        # defaults to \"settings\" if None\n        return Session(\n            aws_access_key_id=self.aws_access_key_id,\n            aws_secret_access_key=self.aws_secret_access_key,\n            region_name=self.aws_region_name,\n        )\n\n    # general function parameters\n    function_eval_trigger = tl.Enum([\"eval\", \"S3\", \"APIGateway\"], default_value=\"eval\").tag(attr=True)\n\n    # lambda function parameters\n    function_name = tl.Unicode().tag(attr=True, readonly=True)  # see default below\n    function_triggers = tl.List(tl.Enum([\"eval\", \"S3\", \"APIGateway\"])).tag(readonly=True)\n    function_handler = tl.Unicode(default_value=\"handler.handler\").tag(readonly=True)\n    function_description = tl.Unicode(default_value=\"PODPAC Lambda Function (https:",
            "def outputs(self):\n        return self.source.outputs\n\n    @tl.default(\"function_name\")",
            "def _function_name_default(self):\n        if settings[\"FUNCTION_NAME\"] is None:\n            settings[\"FUNCTION_NAME\"] = \"podpac-lambda-autogen\"\n\n        return settings[\"FUNCTION_NAME\"]\n\n    @tl.default(\"function_triggers\")",
            "def _function_triggers_default(self):\n        if self.function_eval_trigger != \"eval\":\n            return [\"eval\", self.function_eval_trigger]\n        else:\n            return [\"eval\"]\n\n    @tl.default(\"function_source_dist_key\")",
            "def _function_source_dist_key_default(self):\n        v = version.version()\n        if \"+\" in v:\n            v = \"dev\"\n\n        return \"{}/podpac_dist.zip\".format(v)\n\n    @tl.default(\"function_source_dependencies_key\")",
            "def _function_source_dependencies_key_default(self):\n        v = version.version()\n        if \"+\" in v:\n            v = \"dev\"\n\n        return \"{}/podpac_deps.zip\".format(v)\n\n    @tl.default(\"function_tags\")",
            "def _function_tags_default(self):\n        return settings[\"AWS_TAGS\"] or {}\n\n    @tl.default(\"function_allow_unsafe_eval\")",
            "def _function_allow_unsafe_eval_default(self):\n        return \"UNSAFE_EVAL_HASH\" in self.eval_settings and isinstance(\n            self.eval_settings[\"UNSAFE_EVAL_HASH\"], string_types\n        )\n\n    # role parameters\n    function_role_name = tl.Unicode().tag(readonly=True)  # see default below\n    function_role_description = tl.Unicode(default_value=\"PODPAC Lambda Role\").tag(readonly=True)\n    function_role_policy_document = tl.Dict(allow_none=True).tag(readonly=True)  # see default below - can be none\n    function_role_policy_arns = tl.List(\n        default_value=[\n            \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n        ]  # allows read/write to cloudwatch\n    ).tag(readonly=True)\n    function_role_assume_policy_document = tl.Dict().tag(readonly=True)  # see default below\n    function_role_tags = tl.Dict().tag(readonly=True)  # see default below\n    _function_role_arn = tl.Unicode(default_value=None, allow_none=True)\n    _role = tl.Dict(default_value=None, allow_none=True)  # raw response from AWS on \"get_\"\n\n    @tl.default(\"function_role_name\")",
            "def _function_role_name_default(self):\n        if settings[\"FUNCTION_ROLE_NAME\"] is None:\n            settings[\"FUNCTION_ROLE_NAME\"] = \"podpac-lambda-autogen\"\n\n        return settings[\"FUNCTION_ROLE_NAME\"]\n\n    @tl.default(\"function_role_policy_document\")\n    def _function_role_policy_document_default(self):\n        # enable role to be run by lambda - this document is defined by AWS\n        return {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"s3:PutObject\",\n                        \"s3:GetObject\",\n                        \"s3:DeleteObject\",\n                        \"s3:ReplicateObject\",\n                        \"s3:ListBucket\",\n                        \"s3:ListMultipartUploadParts\",\n                        \"s3:ListBucketByTags\",\n                        \"s3:GetBucketTagging\",\n                        \"s3:ListBucketVersions\",\n                        \"s3:AbortMultipartUpload\",\n                        \"s3:GetObjectTagging\",\n                        \"s3:ListBucketMultipartUploads\",\n                        \"s3:GetBucketLocation\",\n                        \"s3:GetObjectVersion\",\n                    ],\n                    \"Resource\": [\"arn:aws:s3:::{}",
            "def _function_role_policy_document_default(self):\n        # enable role to be run by lambda - this document is defined by AWS\n        return {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"s3:PutObject\",\n                        \"s3:GetObject\",\n                        \"s3:DeleteObject\",\n                        \"s3:ReplicateObject\",\n                        \"s3:ListBucket\",\n                        \"s3:ListMultipartUploadParts\",\n                        \"s3:ListBucketByTags\",\n                        \"s3:GetBucketTagging\",\n                        \"s3:ListBucketVersions\",\n                        \"s3:AbortMultipartUpload\",\n                        \"s3:GetObjectTagging\",\n                        \"s3:ListBucketMultipartUploads\",\n                        \"s3:GetBucketLocation\",\n                        \"s3:GetObjectVersion\",\n                    ],\n                    \"Resource\": [\"arn:aws:s3:::{}",
            "def get_api(session, api_name, api_endpoint=None):\n    \"\"\"Get API Gateway definition\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_name : str\n        API Name\n    api_endpoint : str, optional\n        API endpoint path, defaults to returning the first endpoint it finds\n\n    Returns\n    -------\n    dict\n        (Returns output of Boto3 API Gateway creation\n        Equivalent to https:",
            "def deploy_api(session, api_id, api_stage):\n    \"\"\"Deploy API gateway definition\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_id : str\n        API ID. Generated during API creation or returned from :func:`podpac.manager.aws.get_api`\n    api_stage : str\n        API Stage\n    \"\"\"\n    if api_id is None or api_stage is None:\n        raise ValueError(\"`api_id` and `api_stage` must be defined to deploy API\")\n\n    _log.debug(\"Deploying API Gateway with ID {} and stage {}\".format(api_id, api_stage))\n\n    apigateway = session.client(\"apigateway\")\n    apigateway.create_deployment(\n        restApiId=api_id, stageName=api_stage, stageDescription=\"Deployment of PODPAC API\", description=\"PODPAC API\"\n    )\n    _log.debug(\"Deployed API Gateway with ID {} and stage {}\".format(api_id, api_stage))",
            "def delete_api(session, api_name):\n    \"\"\"Delete API Gateway API\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    api_id : str\n        API ID. Generated during API Creation.\n    \"\"\"\n    if api_name is None:\n        _log.error(\"`api_id` not defined in delete_api\")\n        return\n\n    # make sure api exists\n    api = get_api(session, api_name, None)\n    if api is None:\n        _log.debug(\"API Gateway '{}' does not exist\".format(api_name))\n        return\n\n    _log.debug(\"Removing API Gateway with ID {}\".format(api[\"id\"]))\n\n    apigateway = session.client(\"apigateway\")\n    apigateway.delete_rest_api(restApiId=api[\"id\"])\n\n    _log.debug(\"Successfully removed API Gateway with ID {}\".format(api[\"id\"]))\n\n\n# -----------------------------------------------------------------------------------------------------------------\n# Budget\n# -----------------------------------------------------------------------------------------------------------------\n\n# Budget",
            "def create_budget(\n    session,\n    budget_amount,\n    budget_email=None,\n    budget_name=\"podpac-resource-budget\",\n    budget_currency=\"USD\",\n    budget_threshold=80.0,\n    budget_filter_tags={\"_podpac_resource\": \"true\"},\n):\n    \"\"\"\n    EXPERIMENTAL FEATURE\n    Create a budget for podpac AWS resources based on tags.\n    By default, this creates a budget for all podpac created resources with the tag: {\"_podpac_resource\": \"true\"}\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    budget_amount : int\n        Budget amount\n    budget_email : str, optional\n        Notification e-mail for budget alerts.\n        If no e-mail is provided, the budget must be monitored through the AWS interface.\n    budget_name : str, optional\n        Budget name\n    budget_currency : str, optional\n        Currency unit for budget. Defaults to \"USD\".\n        See https:",
            "def get_budget(session, budget_name):\n    \"\"\"\n    EXPERIMENTAL FEATURE\n    Get a budget by name\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    budget_name : str\n        Budget name\n\n    Returns\n    -------\n    dict\n        Returns Boto3 budget description\n        Equivalent to https:",
            "def delete_budget(session, budget_name):\n    \"\"\"Delete a budget by name\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    budget_name : str\n        Budget name\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    budgets = session.client(\"budgets\")\n\n    try:\n        budgets.delete_budget(AccountId=session.get_account_id(), BudgetName=budget_name)\n    except budgets.exceptions.NotFoundException as e:\n        pass\n\n    _log.debug(\"Successfully removed budget with name '{}'\".format(budget_name))\n\n\n# -----------------------------------------------------------------------------------------------------------------\n# Cloudwatch Logs\n# -----------------------------------------------------------------------------------------------------------------",
            "def get_logs(session, log_group_name, limit=100, start=None, end=None):\n    \"\"\"Get logs from cloudwatch from specific log groups\n\n    Parameters\n    ----------\n    session : :class:`Session`\n        AWS Boto3 Session. See :class:`Session` for creation.\n    log_group_name : str\n        Log group name\n    limit : int, optional\n        Limit logs to the most recent N logs\n    start : str, optional\n        Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(start))\n        Defaults to 1 hour prior to ``end``.\n    end : str, optional\n        Datetime string. Must work as input to np.datetime64 (i.e np.datetime64(end))\n        Defaults to now.\n\n    Returns\n    -------\n    list\n        list of log events\n    \"\"\"\n\n    # default is now\n    if end is None:\n        end = np.datetime64(\"now\")\n    else:\n        end = np.datetime64(end)\n\n    # default is 1 hour prior to now\n    if start is None:\n        start = end - np.timedelta64(1, \"h\")\n    else:\n        start = np.datetime64(start)\n\n    # convert to float and add precision for comparison with AWS response\n    start = start.astype(float) * 1000\n    end = end.astype(float) * 1000\n\n    # get client\n    cloudwatchlogs = session.client(\"logs\")  # cloudwatch logs\n\n    try:\n        log_streams = cloudwatchlogs.describe_log_streams(\n            logGroupName=log_group_name, orderBy=\"LastEventTime\", descending=True\n        )\n    except cloudwatchlogs.exceptions.ResourceNotFoundException:\n        _log.debug(\"No log streams found for log group name: {}\".format(log_group_name))\n        return []\n\n    streams = [\n        s for s in log_streams[\"logStreams\"] if (s[\"firstEventTimestamp\"] < end and s[\"lastEventTimestamp\"] > start)\n    ]\n    logs = []\n    for stream in streams:\n        response = cloudwatchlogs.get_log_events(\n            logGroupName=log_group_name,\n            logStreamName=stream[\"logStreamName\"],\n            startTime=int(start),\n            endTime=int(end) + 1000,\n            limit=limit,\n        )\n        logs += response[\"events\"]\n\n    # sort logs\n    logs.sort(key=lambda k: k[\"timestamp\"])\n\n    # take only the last \"limit\"\n    logs = logs[-limit:]\n\n    # add time easier to read\n    for log in logs:\n        log[\"time\"] = \"{}.{:03d}\".format(\n            datetime.fromtimestamp(log[\"timestamp\"] / 1000).strftime(\"%Y-%m-%d %H:%M:%S\"), log[\"timestamp\"] % 1000\n        )\n\n    return logs"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/multi_process.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    Source node will be evaluated in another process, and it is blocking!\n    \"\"\""
        ],
        "code_snippets": [
            "def _f(definition, coords, q, outputkw):\n    try:\n        n = Node.from_json(definition)\n        c = Coordinates.from_json(coords)\n        o = n.eval(c)\n        o._pp_serialize()\n        _log.debug(\"o.shape: {}, output_format: {}\".format(o.shape, outputkw))\n        if outputkw:\n            _log.debug(\"Saving output results to output format {}\".format(outputkw))\n            o = o.to_format(outputkw[\"format\"], **outputkw.get(\"format_kwargs\"))\n        q.put(o)\n    except Exception as e:\n        q.put(str(e))",
            "class Process(Node):\n    \"\"\"\n    Source node will be evaluated in another process, and it is blocking!\n    \"\"\"\n\n    source = NodeTrait().tag(attr=True)\n    output_format = tl.Dict(None, allow_none=True).tag(attr=True)\n    timeout = tl.Int(None, allow_none=True)\n    block = tl.Bool(True)\n\n    @property",
            "def outputs(self):\n        return self.source.outputs",
            "def eval(self, coordinates, **kwargs):\n        output = kwargs.get(\"output\")\n        definition = self.source.json\n        coords = coordinates.json\n\n        q = Queue()\n        process = mpProcess(target=_f, args=(definition, coords, q, self.output_format))\n        process.daemon = True\n        _log.debug(\"Starting process.\")\n        process.start()\n        _log.debug(\"Retrieving data from queue.\")\n        o = q.get(timeout=self.timeout, block=self.block)\n        _log.debug(\"Joining.\")\n        process.join()  # This is blocking!\n        _log.debug(\"Closing.\")\n        if (sys.version_info.major + sys.version_info.minor / 10.0) >= 3.7:\n            process.close()  # New in version Python 3.7\n        if isinstance(o, str):\n            raise Exception(o)\n        if o is None:\n            return\n        o._pp_deserialize()\n        if output is not None:\n            output[:] = o.data[:]\n        else:\n            output = o\n\n        return output"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/multi_threading.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nModule for dealing with multi-threaded execution. \n\nThis is used to ensure that the total number of threads specified in the settings is not exceeded. \n\n\"\"\"",
            "\"\"\"This is a singleton class that keeps track of the total number of threads used in an application.\"\"\"",
            "\"\"\"Returns the number of threads allowed for a pool taking into account all other threads application, as\n        specified by podpac.settings[\"N_THREADS\"].\n\n        Parameters\n        -----------\n        n : int\n            Number of threads requested by operation\n\n        Returns\n        --------\n        int\n            Number of threads a pool may use. Note, this may be less than or equal to n, and may be 0.\n        \"\"\"",
            "\"\"\"This releases the number of threads specified.\n\n        Parameters\n        ------------\n        n : int\n            Number of threads to be released\n\n        Returns\n        --------\n        int\n            Number of threads available after releases 'n' threads\n        \"\"\"",
            "\"\"\"Creates a threadpool that can be used to run jobs in parallel.\n\n        Parameters\n        -----------\n        processes : int\n            The number of threads or workers that will be part of the pool\n\n        Returns\n        --------\n        multiprocessing.ThreadPool\n            An instance of the ThreadPool class\n        \"\"\""
        ],
        "code_snippets": [
            "class FakeLock(object):\n    _locked = False",
            "def acquire(self):\n        while self._locked:\n            time.sleep(0.01)\n\n        self._locked = True",
            "def release(self):\n        self._locked = False",
            "def __enter__(self):\n        self.acquire()",
            "def __exit__(self, type, value, traceback):\n        self.release()\n\n\ntry:\n    l = Lock()\nexcept OSError:\n    Lock = FakeLock",
            "class ThreadManager(object):",
            "def __new__(cls):\n        if ThreadManager.__instance is None:\n            ThreadManager.__instance = object.__new__(cls)\n        return ThreadManager.__instance",
            "def request_n_threads(self, n):\n        \"\"\"Returns the number of threads allowed for a pool taking into account all other threads application, as\n        specified by podpac.settings[\"N_THREADS\"].\n\n        Parameters\n        -----------\n        n : int\n            Number of threads requested by operation\n\n        Returns\n        --------\n        int\n            Number of threads a pool may use. Note, this may be less than or equal to n, and may be 0.\n        \"\"\"\n        with self._lock:\n            available = max(0, settings.get(\"N_THREADS\", DEFAULT_N_THREADS) - self._n_threads_used)\n            claimed = min(available, n)\n            self._n_threads_used += claimed\n            return claimed",
            "def release_n_threads(self, n):\n        \"\"\"This releases the number of threads specified.\n\n        Parameters\n        ------------\n        n : int\n            Number of threads to be released\n\n        Returns\n        --------\n        int\n            Number of threads available after releases 'n' threads\n        \"\"\"\n        with self._lock:\n            self._n_threads_used = max(0, self._n_threads_used - n)\n            available = max(0, settings.get(\"N_THREADS\", DEFAULT_N_THREADS) - self._n_threads_used)\n            return available",
            "def get_thread_pool(self, processes):\n        \"\"\"Creates a threadpool that can be used to run jobs in parallel.\n\n        Parameters\n        -----------\n        processes : int\n            The number of threads or workers that will be part of the pool\n\n        Returns\n        --------\n        multiprocessing.ThreadPool\n            An instance of the ThreadPool class\n        \"\"\"\n        return ThreadPool(processes=processes)\n\n\nthread_manager = ThreadManager()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/parallel.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nModule to help farm out computation to multiple workers and save the results in a zarr file.\n\"\"\"",
            "\"\"\"\n    This class launches the parallel node evaluations in separate threads. As such, the node does not need to return\n    immediately (i.e. does NOT have to be asynchronous). For asynchronous nodes\n    (i.e. aws.Lambda with download_result=False) use ParrallelAsync\n\n    Attributes\n    -----------\n    chunks: dict\n        Dictionary of dimensions and sizes that will be iterated over. If a dimension is not in this dictionary, the\n        size of the eval coordinates will be used for the chunk. In this case, it may not be possible to automatically\n        set the coordinates of missing dimensions in the final file.\n    fill_output: bool\n        Default is True. When True, the final results will be assembled and returned to the user. If False, the final\n        results should be written to a file by specifying the output_format in a Process or Lambda node.\n        See note below.\n    source: podpac.Node\n        The source dataset for the computation\n    number_of_workers: int\n        Default is 1. Number of parallel process workers at one time.\n    start_i: int, optional\n        Default is 0. Starting chunk. This allow you to restart a run without having to check/submit 1000's of workers\n        before getting back to where you were. Empty chunks make the submission slower.\n\n    Notes\n    ------\n    In some cases where the input and output coordinates of the source node is not the same (such as reduce nodes)\n    and fill_output is True, the user may need to specify 'output' as part of the eval call.\n    \"\"\"",
            "\"\"\"\n    This class launches the parallel node evaluations in threads up to n_workers, and expects the node.eval to return\n    quickly for parallel execution. This Node was written with aws.Lambda(eval_timeout=1.25<small>) Nodes in mind.\n\n    Users can implement the `check_worker_available` method or specify the `no_worker_exception` attribute, which is an\n    exception thrown if workers are not available.\n\n    Attributes\n    -----------\n    chunks: dict\n        Dictionary of dimensions and sizes that will be iterated over. If a dimension is not in this dictionary, the\n        size of the eval coordinates will be used for the chunk. In this case, it may not be possible to automatically\n        set the coordinates of missing dimensions in the final file.\n    fill_output: bool\n        Default is True. When True, the final results will be assembled and returned to the user. If False, the final\n        results should be written to a file by specifying the output_format in a Process or Lambda node.\n        See note below.\n    source: podpac.Node\n        The source dataset for the computation\n    sleep_time: float\n        Default is 1 second. Number of seconds to sleep between trying to submit new workers\n    no_worker_exception: Exception, optional\n        Default is .Exception class used to identify when a submission failed due to no available workers. The default\n        is chosen to work with the podpac.managers.Lambda node.\n    async_exception: Exception\n        Default is botocore.exceptions.ReadTimeoutException. This is an exception thrown by the async function in case\n        it time out waiting for a return. In our case, this is a success. The default is chosen to work with the\n        podpac.managers.Lambda node.\n    Notes\n    ------\n    In some cases where the input and output coordinates of the source node is not the same (such as reduce nodes)\n    and fill_output is True, the user may need to specify 'output' as part of the eval call.\n    \"\"\"",
            "\"\"\"\n    This class assumes that the node has a 'output_format' attribute\n    (currently the \"Lambda\" Node, and the \"Process\" Node)\n\n    Attributes\n    -----------\n    zarr_file: str\n        Path to the output zarr file that collects all of the computed results. This can reside on S3.\n    dataset: ZarrGroup\n        A handle to the zarr group pointing to the output file\n    fill_output: bool, optional\n        Default is False (unlike parent class). If True, will collect the output data and return it as an xarray.\n    init_file_mode: str, optional\n        Default is 'w'. Mode used for initializing the zarr file.\n    zarr_chunks: dict\n        Size of the chunks in the zarr file for each dimension\n    zarr_shape: dict, optional\n        Default is the {coordinated.dims: coordinates.shape}, where coordinates used as part of the eval call. This\n        does not need to be specified unless the Node modifies the input coordinates (as part of a Reduce operation,\n        for example). The result can be incorrect and requires care/checking by the user.\n    zarr_coordinates: podpac.Coordinates, optional\n        Default is None. If the node modifies the shape of the input coordinates, this allows users to set the\n        coordinates in the output zarr file. This can be incorrect and requires care by the user.\n    skip_existing: bool\n        Default is False. If true, this will check to see if the results already exist. And if so, it will not\n        submit a job for that particular coordinate evaluation. This assumes self.chunks == self.zar_chunks\n    list_dir: bool, optional\n        Default is False. If skip_existing is True, by default existing files are checked by asking for an 'exists' call.\n        If list_dir is True, then at the first opportunity a \"list_dir\" is performed on the directory and the results\n        are cached.\n    \"\"\""
        ],
        "code_snippets": [
            "class dum:\n        pass",
            "class mod:\n        ClientError = dum\n        ReadTimeoutError = dum",
            "class botocore:\n        exceptions = mod\n\n\n# Set up logging\n_log = logging.getLogger(__name__)",
            "class Parallel(Node):\n    \"\"\"\n    This class launches the parallel node evaluations in separate threads. As such, the node does not need to return\n    immediately (i.e. does NOT have to be asynchronous). For asynchronous nodes\n    (i.e. aws.Lambda with download_result=False) use ParrallelAsync\n\n    Attributes\n    -----------\n    chunks: dict\n        Dictionary of dimensions and sizes that will be iterated over. If a dimension is not in this dictionary, the\n        size of the eval coordinates will be used for the chunk. In this case, it may not be possible to automatically\n        set the coordinates of missing dimensions in the final file.\n    fill_output: bool\n        Default is True. When True, the final results will be assembled and returned to the user. If False, the final\n        results should be written to a file by specifying the output_format in a Process or Lambda node.\n        See note below.\n    source: podpac.Node\n        The source dataset for the computation\n    number_of_workers: int\n        Default is 1. Number of parallel process workers at one time.\n    start_i: int, optional\n        Default is 0. Starting chunk. This allow you to restart a run without having to check/submit 1000's of workers\n        before getting back to where you were. Empty chunks make the submission slower.\n\n    Notes\n    ------\n    In some cases where the input and output coordinates of the source node is not the same (such as reduce nodes)\n    and fill_output is True, the user may need to specify 'output' as part of the eval call.\n    \"\"\"\n\n    _repr_keys = [\"source\", \"number_of_workers\", \"chunks\"]\n    source = NodeTrait().tag(attr=True)\n    chunks = tl.Dict().tag(attr=True)\n    fill_output = tl.Bool(True).tag(attr=True)\n    number_of_workers = tl.Int(1).tag(attr=True)\n    _lock = Lock()\n    errors = tl.List()\n    start_i = tl.Int(0)",
            "def eval(self, coordinates, **kwargs):\n        output = kwargs.get(\"output\")\n        # Make a thread pool to manage queue\n        pool = ThreadPool(processes=self.number_of_workers)\n\n        if output is None and self.fill_output:\n            output = self.create_output_array(coordinates)\n\n        shape = []\n        for d in coordinates.dims:\n            if d in self.chunks:\n                shape.append(self.chunks[d])\n            else:\n                shape.append(coordinates[d].size)\n\n        results = []\n        #         inputs = []\n        i = 0\n        for coords, slc in coordinates.iterchunks(shape, True):\n            #             inputs.append(coords)\n            if i < self.start_i:\n                _log.debug(\"Skipping {} since it is less than self.start_i ({})\".format(i, self.start_i))\n                i += 1\n                continue\n\n            out = None\n            if self.fill_output and output is not None:\n                out = output[slc]\n            with self._lock:\n                _log.debug(\"Added {} to worker pool\".format(i))\n                _log.debug(\"Node eval with coords: {}, {}\".format(slc, coords))\n                results.append(pool.apply_async(self.eval_source, [coords, slc, out, i]))\n            i += 1\n\n        _log.info(\"Added all chunks to worker pool. Now waiting for results.\")\n        start_time = time.time()\n        for i, res in enumerate(results):\n            #             _log.debug('Waiting for results: {} {}'.format(i, inputs[i]))\n            dt = str(np.timedelta64(int(1000 * (time.time() - start_time)), \"ms\").astype(object))\n            _log.info(\"({}): Waiting for results: {} / {}\".format(dt, i + 1, len(results)))\n\n            # Try to get the results / wait for the results\n            try:\n                o, slc = res.get()\n            except Exception as e:\n                o = None\n                slc = None\n                self.errors.append((i, res, e))\n                dt = str(np.timedelta64(int(1000 * (time.time() - start_time)), \"ms\").astype(object))\n                _log.warning(\"({}) {} failed with exception {}\".format(dt, i, e))\n\n            dt = str(np.timedelta64(int(1000 * (time.time() - start_time)), \"ms\").astype(object))\n            _log.info(\"({}) Finished result: {} / {}\".format(time.time() - start_time, i + 1, len(results)))\n\n            # Fill output\n            if self.fill_output:\n                if output is None:\n                    missing_dims = [d for d in coordinates.dims if d not in self.chunks.keys()]\n                    coords = coordinates.drop(missing_dims)\n                    missing_coords = Coordinates.from_xarray(o).drop(list(self.chunks.keys()))\n                    coords = merge_dims([coords, missing_coords])\n                    coords = coords.transpose(*coordinates.dims)\n                    output = self.create_output_array(coords)\n                output[slc] = o\n\n        _log.info(\"Completed parallel execution.\")\n        pool.close()\n\n        return output",
            "def eval_source(self, coordinates, coordinates_index, out, i, source=None):\n        if source is None:\n            source = self.source\n            # Make a copy to prevent any possibility of memory corruption\n            source = Node.from_definition(source.definition)\n\n        _log.info(\"Submitting source {}\".format(i))\n        return (source.eval(coordinates, output=out), coordinates_index)",
            "class ParallelAsync(Parallel):\n    \"\"\"\n    This class launches the parallel node evaluations in threads up to n_workers, and expects the node.eval to return\n    quickly for parallel execution. This Node was written with aws.Lambda(eval_timeout=1.25<small>) Nodes in mind.\n\n    Users can implement the `check_worker_available` method or specify the `no_worker_exception` attribute, which is an\n    exception thrown if workers are not available.\n\n    Attributes\n    -----------\n    chunks: dict\n        Dictionary of dimensions and sizes that will be iterated over. If a dimension is not in this dictionary, the\n        size of the eval coordinates will be used for the chunk. In this case, it may not be possible to automatically\n        set the coordinates of missing dimensions in the final file.\n    fill_output: bool\n        Default is True. When True, the final results will be assembled and returned to the user. If False, the final\n        results should be written to a file by specifying the output_format in a Process or Lambda node.\n        See note below.\n    source: podpac.Node\n        The source dataset for the computation\n    sleep_time: float\n        Default is 1 second. Number of seconds to sleep between trying to submit new workers\n    no_worker_exception: Exception, optional\n        Default is .Exception class used to identify when a submission failed due to no available workers. The default\n        is chosen to work with the podpac.managers.Lambda node.\n    async_exception: Exception\n        Default is botocore.exceptions.ReadTimeoutException. This is an exception thrown by the async function in case\n        it time out waiting for a return. In our case, this is a success. The default is chosen to work with the\n        podpac.managers.Lambda node.\n    Notes\n    ------\n    In some cases where the input and output coordinates of the source node is not the same (such as reduce nodes)\n    and fill_output is True, the user may need to specify 'output' as part of the eval call.\n    \"\"\"\n\n    source = NodeTrait().tag(attr=True)\n    chunks = tl.Dict().tag(attr=True)\n    fill_output = tl.Bool(True).tag(attr=True)\n    sleep_time = tl.Float(1).tag(attr=True)\n    no_worker_exception = tl.Type(botocore.exceptions.ClientError).tag(attr=True)\n    async_exception = tl.Type(botocore.exceptions.ReadTimeoutError).tag(attr=True)",
            "def check_worker_available(self):\n        return True",
            "def eval_source(self, coordinates, coordinates_index, out, i, source=None):\n        if source is None:\n            source = self.source\n            # Make a copy to prevent any possibility of memory corruption\n            source = Node.from_definition(source.definition)\n\n        success = False\n        o = None\n        while not success:\n            if self.check_worker_available():\n                try:\n                    o = source.eval(coordinates, output=out)\n                    success = True\n                except self.async_exception:\n                    # This exception is fine and constitutes a success\n                    o = None\n                    success = True\n                except self.no_worker_exception as e:\n                    response = e.response\n                    if not (response and response.get(\"Error\", {}).get(\"Code\") == \"TooManyRequestsException\"):\n                        raise e  # Raise error again, not the right error\n                    _log.debug(\"Worker {} exception {}\".format(i, e))\n                    success = False\n                    time.sleep(self.sleep_time)\n            else:\n                _log.debug(\"Worker unavailable for {}\".format(i, e))\n                time.sleep(self.sleep_time)\n        _log.info(\"Submitting source {}\".format(i))\n        return (o, coordinates_index)",
            "class ZarrOutputMixin(tl.HasTraits):\n    \"\"\"\n    This class assumes that the node has a 'output_format' attribute\n    (currently the \"Lambda\" Node, and the \"Process\" Node)\n\n    Attributes\n    -----------\n    zarr_file: str\n        Path to the output zarr file that collects all of the computed results. This can reside on S3.\n    dataset: ZarrGroup\n        A handle to the zarr group pointing to the output file\n    fill_output: bool, optional\n        Default is False (unlike parent class). If True, will collect the output data and return it as an xarray.\n    init_file_mode: str, optional\n        Default is 'w'. Mode used for initializing the zarr file.\n    zarr_chunks: dict\n        Size of the chunks in the zarr file for each dimension\n    zarr_shape: dict, optional\n        Default is the {coordinated.dims: coordinates.shape}, where coordinates used as part of the eval call. This\n        does not need to be specified unless the Node modifies the input coordinates (as part of a Reduce operation,\n        for example). The result can be incorrect and requires care/checking by the user.\n    zarr_coordinates: podpac.Coordinates, optional\n        Default is None. If the node modifies the shape of the input coordinates, this allows users to set the\n        coordinates in the output zarr file. This can be incorrect and requires care by the user.\n    skip_existing: bool\n        Default is False. If true, this will check to see if the results already exist. And if so, it will not\n        submit a job for that particular coordinate evaluation. This assumes self.chunks == self.zar_chunks\n    list_dir: bool, optional\n        Default is False. If skip_existing is True, by default existing files are checked by asking for an 'exists' call.\n        If list_dir is True, then at the first opportunity a \"list_dir\" is performed on the directory and the results\n        are cached.\n    \"\"\"\n\n    zarr_file = tl.Unicode().tag(attr=True)\n    dataset = tl.Any()\n    zarr_node = NodeTrait()\n    zarr_data_key = tl.Union([tl.Unicode(), tl.List()])\n    fill_output = tl.Bool(False)\n    init_file_mode = tl.Unicode(\"a\").tag(attr=True)\n    zarr_chunks = tl.Dict(default_value=None, allow_none=True).tag(attr=True)\n    zarr_shape = tl.Dict(allow_none=True, default_value=None).tag(attr=True)\n    zarr_coordinates = tl.Instance(Coordinates, allow_none=True, default_value=None).tag(attr=True)\n    zarr_dtype = tl.Unicode(\"f4\")\n    skip_existing = tl.Bool(True).tag(attr=True)\n    list_dir = tl.Bool(False)\n    _list_dir = tl.List(allow_none=True, default_value=[])\n    _shape = tl.Tuple()\n    _chunks = tl.List()\n    aws_client_kwargs = tl.Dict()\n    aws_config_kwargs = tl.Dict()",
            "def eval(self, coordinates, **kwargs):\n        output = kwargs.get(\"output\")\n        if self.zarr_shape is None:\n            self._shape = coordinates.shape\n        else:\n            self._shape = tuple(self.zarr_shape.values())\n\n        # initialize zarr file\n        if self.zarr_chunks is None:\n            chunks = [self.chunks[d] for d in coordinates]\n        else:\n            chunks = [self.zarr_chunks[d] for d in coordinates]\n        self._chunks = chunks\n        zf, data_key, zn = self.initialize_zarr_array(self._shape, chunks)\n        self.dataset = zf\n        self.zarr_data_key = data_key\n        self.zarr_node = zn\n        zn.keys\n\n        # eval\n        _log.debug(\"Starting parallel eval.\")\n        missing_dims = [d for d in coordinates.dims if d not in self.chunks.keys()]\n        if self.zarr_coordinates is not None:\n            missing_dims = missing_dims + [d for d in self.zarr_coordinates.dims if d not in missing_dims]\n            set_coords = merge_dims([coordinates.drop(missing_dims), self.zarr_coordinates])\n        else:\n            set_coords = coordinates.drop(missing_dims)\n        set_coords.transpose(*coordinates.dims)\n\n        self.set_zarr_coordinates(set_coords, data_key)\n        if self.list_dir:\n            dk = data_key\n            if isinstance(dk, list):\n                dk = dk[0]\n            self._list_dir = self.zarr_node.list_dir(dk)\n\n        output = super(ZarrOutputMixin, self).eval(coordinates, output=output)\n\n        # fill in the coordinates, this is guaranteed to be correct even if the user messed up.\n        if output is not None:\n            self.set_zarr_coordinates(Coordinates.from_xarray(output), data_key)\n        else:\n            return zf\n\n        return output",
            "def set_zarr_coordinates(self, coordinates, data_key):\n        # Fill in metadata\n        for dk in data_key:\n            self.dataset[dk].attrs[\"_ARRAY_DIMENSIONS\"] = coordinates.dims\n        for d in coordinates.dims:\n            # TODO ADD UNITS AND TIME DECODING INFORMATION\n            self.dataset.create_dataset(d, shape=coordinates[d].size, overwrite=True)\n            self.dataset[d][:] = coordinates[d].coordinates",
            "def initialize_zarr_array(self, shape, chunks):\n        _log.debug(\"Creating Zarr file.\")\n        zn = Zarr(source=self.zarr_file, file_mode=self.init_file_mode, aws_client_kwargs=self.aws_client_kwargs)\n        if self.source.output or getattr(self.source, \"data_key\", None):\n            data_key = self.source.output\n            if data_key is None:\n                data_key = self.source.data_key\n            if not isinstance(data_key, list):\n                data_key = [data_key]\n            elif self.source.outputs:  # If someone restricted the outputs for this node, we need to know\n                data_key = [dk for dk in data_key if dk in self.source.outputs]\n        elif self.source.outputs:\n            data_key = self.source.outputs\n        else:\n            data_key = [\"data\"]\n\n        zf = zarr.open(zn._get_store(), mode=self.init_file_mode)\n\n        # Intialize the output zarr arrays\n        for dk in data_key:\n            try:\n                arr = zf.create_dataset(\n                    dk,\n                    shape=shape,\n                    chunks=chunks,\n                    fill_value=np.nan,\n                    dtype=self.zarr_dtype,\n                    overwrite=not self.skip_existing,\n                )\n            except ValueError:\n                pass  # Dataset already exists\n\n        # Recompute any cached properties\n        zn = Zarr(source=self.zarr_file, file_mode=self.init_file_mode, aws_client_kwargs=self.aws_client_kwargs)\n        return zf, data_key, zn",
            "def eval_source(self, coordinates, coordinates_index, out, i, source=None):\n        if source is None:\n            source = self.source\n\n        if self.skip_existing:  # This section allows previously computed chunks to be skipped\n            dk = self.zarr_data_key\n            if isinstance(dk, list):\n                dk = dk[0]\n            try:\n                exists = self.zarr_node.chunk_exists(\n                    coordinates_index, data_key=dk, list_dir=self._list_dir, chunks=self._chunks\n                )\n            except ValueError as e:  # This was needed in cases where a poor internet connection caused read errors\n                exists = False\n            if exists:\n                _log.info(\"Skipping {} (already exists)\".format(i))\n                return out, coordinates_index\n\n        # Make a copy to prevent any possibility of memory corruption\n        source = Node.from_definition(source.definition)\n        _log.debug(\"Creating output format.\")\n        output = dict(\n            format=\"zarr_part\",\n            format_kwargs=dict(\n                part=[[s.start, min(s.stop, self._shape[i]), s.step] for i, s in enumerate(coordinates_index)],\n                source=self.zarr_file,\n                mode=\"a\",\n            ),\n        )\n        _log.debug(\"Finished creating output format.\")\n\n        if source.has_trait(\"output_format\"):\n            source.set_trait(\"output_format\", output)\n        _log.debug(\"output: {}, coordinates.shape: {}\".format(output, coordinates.shape))\n        _log.debug(\"Evaluating node.\")\n\n        o, slc = super(ZarrOutputMixin, self).eval_source(coordinates, coordinates_index, out, i, source)\n\n        if not source.has_trait(\"output_format\"):\n            o.to_format(output[\"format\"], **output[\"format_kwargs\"])\n        return o, slc",
            "class ParallelOutputZarr(ZarrOutputMixin, Parallel):\n    pass",
            "class ParallelAsyncOutputZarr(ZarrOutputMixin, ParallelAsync):\n    pass"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/test/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/test/test_aws.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestAWS(object):\n    pass"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/test/test_multiprocess.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestProcess(object):",
            "def test_mp_results_the_same(self):\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node = Arange()\n        o_sp = node.eval(coords)\n\n        node_mp = Process(source=node)\n        o_mp = node_mp.eval(coords)\n\n        np.testing.assert_array_equal(o_sp.data, o_mp.data)",
            "def test_mp_results_outputs(self):\n        node = Arange(outputs=[\"a\", \"b\"])\n        node_mp = Process(source=node)\n        assert node.outputs == node_mp.outputs",
            "def test_mp_results_the_same_set_output(self):\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node = Arange()\n        o_sp = node.eval(coords)\n        output = o_sp.copy()\n        output[:] = np.nan\n\n        node_mp = Process(source=node)\n        o_mp = node_mp.eval(coords, output=output)\n\n        np.testing.assert_array_equal(o_sp, output)",
            "def test_f(self):\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node = Arange()\n        q = Queue()\n        _f(node.json, coords.json, q, {})\n        o = q.get()\n        np.testing.assert_array_equal(o, node.eval(coords))",
            "def test_f_fmt(self):\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node = Arange()\n        q = Queue()\n        _f(node.json, coords.json, q, {\"format\": \"dict\", \"format_kwargs\": {}})\n        o = q.get()\n        np.testing.assert_array_equal(o[\"data\"], node.eval(coords).to_dict()[\"data\"])"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/test/test_multithreading.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestFakeLock(object):",
            "def test_enter_exist_single_thread(self):\n        lock = FakeLock()\n        assert lock._locked == False\n        with lock:\n            assert lock._locked\n        assert lock._locked == False",
            "def test_fake_lock_multithreaded(self):\n        lock = FakeLock()",
            "def f(s):\n            print(\"In\", s)\n            with lock:\n                print(\"Locked\", s)\n                assert lock._locked\n                time.sleep(0.05)\n            print(\"Unlocked\", s)\n            assert lock._locked == False\n\n        if sys.version_info.major == 2:\n            t1 = Thread(target=lambda: f(\"thread\"))\n            t2 = Thread(target=lambda: f(\"thread\"))\n            t1.daemon = True\n            t2.daemon = True\n        else:\n            t1 = Thread(target=lambda: f(\"thread\"), daemon=True)\n            t2 = Thread(target=lambda: f(\"thread\"), daemon=True)\n        print(\"In Main Thread\")\n        f(\"main1\")\n        print(\"Starting Thread\")\n        t1.run()\n        t2.run()\n        f(\"main2\")",
            "class TestThreadManager(object):",
            "def test_request_release_threads_single_threaded(self):\n        with settings:\n            settings[\"N_THREADS\"] = 5\n            # Requests\n            n = thread_manager.request_n_threads(3)\n            assert n == 3\n            n = thread_manager.request_n_threads(3)\n            assert n == 2\n            n = thread_manager.request_n_threads(3)\n            assert n == 0\n\n            # releases\n            assert thread_manager._n_threads_used == 5\n            n = thread_manager.release_n_threads(3)\n            assert n == 3\n            n = thread_manager.release_n_threads(2)\n            assert n == 5\n            n = thread_manager.release_n_threads(50)\n            assert n == 5",
            "def test_request_release_threads_multi_threaded(self):",
            "def f(s):\n            print(\"In\", s)\n            n1 = thread_manager.release_n_threads(s)\n            time.sleep(0.05)\n            n2 = thread_manager.release_n_threads(s)\n            print(\"Released\", s)\n            assert n2 >= n1\n\n        with settings:\n            settings[\"N_THREADS\"] = 7\n\n            if sys.version_info.major == 2:\n                t1 = Thread(target=lambda: f(5))\n                t2 = Thread(target=lambda: f(6))\n                t1.daemon = True\n                t2.daemon = True\n            else:\n                t1 = Thread(target=lambda: f(5), daemon=True)\n                t2 = Thread(target=lambda: f(6), daemon=True)\n            f(1)\n            t1.run()\n            t2.run()\n            f(7)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/managers/test/test_parallel.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestParallel(object):",
            "def test_parallel_multi_thread_compute_fill_output(self):\n        node = CoordData(coord_name=\"time\")\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = Parallel(source=node, number_of_workers=2, chunks={\"time\": 2})\n        o = node.eval(coords)\n        o_p = node_p.eval(coords)\n\n        np.testing.assert_array_equal(o, o_p)",
            "def test_parallel_multi_thread_compute_fill_output2(self):\n        node = CoordData(coord_name=\"time\")\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = Parallel(source=node, number_of_workers=2, chunks={\"time\": 2})\n        o = node.eval(coords)\n        o_p = o.copy()\n        o_p[:] = np.nan\n        node_p.eval(coords, output=o_p)\n\n        np.testing.assert_array_equal(o, o_p)\n\n    @pytest.mark.skipif(sys.version < \"3.7\", reason=\"python < 3.7 cannot handle processes launched from threads\")",
            "def test_parallel_process(self):\n        node = Process(source=CoordData(coord_name=\"time\"))\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = Parallel(source=node, number_of_workers=2, chunks={\"time\": 2})\n        o = node.eval(coords)\n        o_p = o.copy()\n        o_p[:] = np.nan\n        node_p.eval(coords, output=o_p)\n        time.sleep(0.1)\n\n        np.testing.assert_array_equal(o, o_p)",
            "class TestParallelAsync(object):\n    @pytest.mark.skipif(sys.version < \"3.7\", reason=\"python < 3.7 cannot handle processes launched from threads\")",
            "def test_parallel_process_async(self):\n        node = Process(source=CoordData(coord_name=\"time\"))  # , block=False)\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = ParallelAsync(source=node, number_of_workers=2, chunks={\"time\": 2}, fill_output=False)\n        node_p.eval(coords)\n        time.sleep(0.1)\n        # Just try to make it run...",
            "class TestParallelOutputZarr(object):\n    @pytest.mark.skipif(sys.version < \"3.7\", reason=\"python < 3.7 cannot handle processes launched from threads\")",
            "def test_parallel_process_zarr(self):\n        # Can't use tempfile.TemporaryDirectory because multiple processess need access to dir\n        tmpdir = os.path.join(tempfile.gettempdir(), \"test_parallel_process_zarr.zarr\")\n\n        node = Process(source=CoordData(coord_name=\"time\"))  # , block=False)\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = ParallelOutputZarr(\n            source=node, number_of_workers=2, chunks={\"time\": 2}, fill_output=False, zarr_file=tmpdir\n        )\n        o_zarr = node_p.eval(coords)\n        time.sleep(0.1)\n        # print(o_zarr.info)\n        np.testing.assert_array_equal([1, 2, 3, 4, 5], o_zarr[\"data\"][:])\n\n        shutil.rmtree(tmpdir)\n\n    @pytest.mark.skipif(sys.version < \"3.7\", reason=\"python < 3.7 cannot handle processes launched from threads\")",
            "def test_parallel_process_zarr_async(self):\n        # Can't use tempfile.TemporaryDirectory because multiple processess need access to dir\n        tmpdir = os.path.join(tempfile.gettempdir(), \"test_parallel_process_zarr_async.zarr\")\n\n        node = Process(source=CoordData(coord_name=\"time\"))  # , block=False)\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = ParallelAsyncOutputZarr(\n            source=node, number_of_workers=5, chunks={\"time\": 2}, fill_output=False, zarr_file=tmpdir\n        )\n        o_zarr = node_p.eval(coords)\n        # print(o_zarr.info)\n        time.sleep(0.01)\n        np.testing.assert_array_equal([1, 2, 3, 4, 5], o_zarr[\"data\"][:])\n\n        shutil.rmtree(tmpdir)\n\n    @pytest.mark.skipif(sys.version < \"3.7\", reason=\"python < 3.7 cannot handle processes launched from threads\")",
            "def test_parallel_process_zarr_async_starti(self):\n        # Can't use tempfile.TemporaryDirectory because multiple processess need access to dir\n        tmpdir = os.path.join(tempfile.gettempdir(), \"test_parallel_process_zarr_async_starti.zarr\")\n\n        node = Process(source=CoordData(coord_name=\"time\"))  # , block=False)\n        coords = Coordinates([[1, 2, 3, 4, 5]], [\"time\"])\n        node_p = ParallelAsyncOutputZarr(\n            source=node, number_of_workers=5, chunks={\"time\": 2}, fill_output=False, zarr_file=tmpdir, start_i=1\n        )\n        o_zarr = node_p.eval(coords)\n        # print(o_zarr.info)\n        time.sleep(0.01)\n        np.testing.assert_array_equal([np.nan, np.nan, 3, 4, 5], o_zarr[\"data\"][:])\n\n        node_p = ParallelAsyncOutputZarr(\n            source=node, number_of_workers=5, chunks={\"time\": 2}, fill_output=False, zarr_file=tmpdir, start_i=0\n        )\n        o_zarr = node_p.eval(coords)\n        np.testing.assert_array_equal([1, 2, 3, 4, 5], o_zarr[\"data\"][:])\n\n        shutil.rmtree(tmpdir)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/xarray_interpolator.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nInterpolator implementations\n\"\"\"",
            "\"\"\"Xarray interpolation Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n\n    fill_nan: bool\n        Default is False. If True, nan values will be filled before interpolation.\n    fill_value: float,str\n        Default is None. The value that will be used to fill nan values. This can be a number, or \"extrapolate\", see `scipy.interpn`/`scipy/interp1d`\n    kwargs: dict\n        Default is {{\"bounds_error\": False}}. Additional values to pass to xarray's `interp` method.\n\n    \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\""
        ],
        "code_snippets": [
            "class XarrayInterpolator(Interpolator):\n    \"\"\"Xarray interpolation Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n\n    fill_nan: bool\n        Default is False. If True, nan values will be filled before interpolation.\n    fill_value: float,str\n        Default is None. The value that will be used to fill nan values. This can be a number, or \"extrapolate\", see `scipy.interpn`/`scipy/interp1d`\n    kwargs: dict\n        Default is {{\"bounds_error\": False}}. Additional values to pass to xarray's `interp` method.\n\n    \"\"\"\n\n    dims_supported = [\"lat\", \"lon\", \"alt\", \"time\"]\n    methods_supported = [\n        \"nearest\",\n        \"linear\",\n        \"bilinear\",\n        \"quadratic\",\n        \"cubic\",\n        \"zero\",\n        \"slinear\",\n        \"next\",\n        \"previous\",\n        \"splinef2d\",\n    ]\n\n    # defined at instantiation\n    method = tl.Unicode(default_value=\"nearest\")\n    fill_value = tl.Union([tl.Unicode(), tl.Float()], default_value=None, allow_none=True)\n    fill_nan = tl.Bool(False)\n\n    kwargs = tl.Dict({\"bounds_error\": False})",
            "def __repr__(self):\n        rep = super(XarrayInterpolator, self).__repr__()\n        # rep += '\\n\\tspatial_tolerance: {}\\n\\ttime_tolerance: {}'.format(self.spatial_tolerance, self.time_tolerance)\n        return rep\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        udims_subset = self._filter_udims_supported(udims)\n\n        # confirm that udims are in both source and eval coordinates\n        if self._dim_in(udims_subset, source_coordinates, unstacked=True):\n            for d in source_coordinates.udims:  # Cannot handle stacked dimensions\n                if source_coordinates.is_stacked(d):\n                    return tuple()\n            return udims_subset\n        else:\n            return tuple()\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        coords = {}\n        nn_coords = {}\n\n        for d in udims:\n            # Note: This interpolator cannot handle stacked source -- and this is handled in the can_interpolate function\n            if source_coordinates[d].size == 1:\n                # If the source only has a single coordinate, xarray will automatically throw an error asking for at least 2 coordinates\n                # So, we prevent this. Main problem is that this won't respect any tolerances.\n                new_dim = [dd for dd in eval_coordinates.dims if d in dd][0]\n                nn_coords[d] = xr.DataArray(\n                    eval_coordinates[d].coordinates,\n                    dims=[new_dim],\n                    coords=[eval_coordinates.xcoords[new_dim]],\n                )\n                continue\n            if (\n                not source_coordinates.is_stacked(d)\n                and eval_coordinates.is_stacked(d)\n                and len(eval_coordinates[d].shape) == 1\n            ):\n                # Handle case for stacked coordinates (i.e. along a curve)\n                new_dim = [dd for dd in eval_coordinates.dims if d in dd][0]\n                coords[d] = xr.DataArray(\n                    eval_coordinates[d].coordinates, dims=[new_dim], coords=[eval_coordinates.xcoords[new_dim]]\n                )\n            elif (\n                not source_coordinates.is_stacked(d)\n                and eval_coordinates.is_stacked(d)\n                and len(eval_coordinates[d].shape) > 1\n            ):\n                # Dependent coordinates (i.e. a warped coordinate system)\n                keep_coords = {k: v for k, v in eval_coordinates.xcoords.items() if k in eval_coordinates.xcoords[d][0]}\n                coords[d] = xr.DataArray(\n                    eval_coordinates[d].coordinates, dims=eval_coordinates.xcoords[d][0], coords=keep_coords\n                )\n            else:\n                # TODO: Check dependent coordinates\n                coords[d] = eval_coordinates[d].coordinates\n\n        kwargs = self.kwargs.copy()\n        kwargs.update({\"fill_value\": self.fill_value})\n\n        coords[\"kwargs\"] = kwargs\n\n        if self.method == \"bilinear\":\n            self.method = \"linear\"\n\n        if self.fill_nan:\n            for d in source_coordinates.dims:\n                if not np.any(np.isnan(source_data)):\n                    break\n                # use_coordinate=False allows for interpolation when dimension is not monotonically increasing\n                source_data = source_data.interpolate_na(method=self.method, dim=d, use_coordinate=False)\n\n        if nn_coords:\n            source_data = source_data.sel(method=\"nearest\", **nn_coords)\n\n        output_data = source_data.interp(method=self.method, **coords)\n\n        return output_data.transpose(*eval_coordinates.xdims)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/scipy_interpolator.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nInterpolator implementations\n\"\"\"",
            "\"\"\"Scipy Point Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n    \"\"\"",
            "\"\"\"\n        {interpolator_can_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\"",
            "\"\"\"Scipy Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n    \"\"\"",
            "\"\"\"\n        {interpolator_can_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\""
        ],
        "code_snippets": [
            "class ScipyPoint(Interpolator):\n    \"\"\"Scipy Point Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n    \"\"\"\n\n    methods_supported = [\"nearest\"]\n    method = tl.Unicode(default_value=\"nearest\")\n    dims_supported = [\"lat\", \"lon\"]\n\n    # TODO: implement these parameters for the method 'nearest'\n    spatial_tolerance = tl.Float(default_value=np.inf)\n    time_tolerance = tl.Union([tl.Unicode(), tl.Instance(np.timedelta64, allow_none=True)])\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_can_interpolate}\n        \"\"\"\n\n        # TODO: make this so we don't need to specify lat and lon together\n        # or at least throw a warning\n        if (\n            \"lat\" in udims\n            and \"lon\" in udims\n            and not self._dim_in([\"lat\", \"lon\"], source_coordinates)\n            and self._dim_in([\"lat\", \"lon\"], source_coordinates, unstacked=True)\n            and self._dim_in([\"lat\", \"lon\"], eval_coordinates, unstacked=True)\n        ):\n\n            return tuple([\"lat\", \"lon\"])\n\n        # otherwise return no supported dims\n        return tuple()\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n\n        order = \"lat_lon\" if \"lat_lon\" in source_coordinates.dims else \"lon_lat\"\n\n        # calculate tolerance\n        if isinstance(eval_coordinates[\"lat\"], UniformCoordinates1d):\n            dlat = eval_coordinates[\"lat\"].step\n        else:\n            dlat = (eval_coordinates[\"lat\"].bounds[1] - eval_coordinates[\"lat\"].bounds[0]) / (\n                eval_coordinates[\"lat\"].size - 1\n            )\n\n        if isinstance(eval_coordinates[\"lon\"], UniformCoordinates1d):\n            dlon = eval_coordinates[\"lon\"].step\n        else:\n            dlon = (eval_coordinates[\"lon\"].bounds[1] - eval_coordinates[\"lon\"].bounds[0]) / (\n                eval_coordinates[\"lon\"].size - 1\n            )\n\n        tol = np.linalg.norm([dlat, dlon]) * 8\n\n        if self._dim_in([\"lat\", \"lon\"], eval_coordinates):\n            pts = np.stack([source_coordinates[dim].coordinates for dim in source_coordinates[order].dims], axis=1)\n            if order == \"lat_lon\":\n                pts = pts[:, ::-1]\n            pts = KDTree(pts)\n            lon, lat = np.meshgrid(eval_coordinates[\"lon\"].coordinates, eval_coordinates[\"lat\"].coordinates)\n            dist, ind = pts.query(np.stack((lon.ravel(), lat.ravel()), axis=1), distance_upper_bound=tol)\n            mask = ind == source_data[order].size\n            ind[mask] = 0  # This is a hack to make the select on the next line work\n            # (the masked values are set to NaN on the following line)\n            vals = source_data[{order: ind}]\n            vals[mask] = np.nan\n            # make sure 'lat_lon' or 'lon_lat' is the first dimension\n            dims = [dim for dim in source_data.dims if dim != order]\n            vals = vals.transpose(order, *dims).data\n            shape = vals.shape\n            coords = [eval_coordinates[\"lat\"].coordinates, eval_coordinates[\"lon\"].coordinates]\n            coords += [source_coordinates[d].coordinates for d in dims]\n            vals = vals.reshape(eval_coordinates[\"lat\"].size, eval_coordinates[\"lon\"].size, *shape[1:])\n            vals = UnitsDataArray(vals, coords=coords, dims=[\"lat\", \"lon\"] + dims)\n            # and transpose back to the destination order\n            output_data.data[:] = vals.transpose(*output_data.dims).data[:]\n\n            return output_data\n\n        elif self._dim_in([\"lat\", \"lon\"], eval_coordinates, unstacked=True):\n            dst_order = \"lat_lon\" if \"lat_lon\" in eval_coordinates.dims else \"lon_lat\"\n            src_stacked = np.stack(\n                [source_coordinates[dim].coordinates for dim in source_coordinates[order].dims], axis=1\n            )\n            new_stacked = np.stack(\n                [eval_coordinates[dim].coordinates for dim in source_coordinates[order].dims], axis=1\n            )\n            pts = KDTree(src_stacked)\n            dist, ind = pts.query(new_stacked, distance_upper_bound=tol)\n            mask = ind == source_data[order].size\n            ind[mask] = 0\n            vals = source_data[{order: ind}]\n            vals[{order: mask}] = np.nan\n            dims = list(output_data.dims)\n            dims[dims.index(dst_order)] = order\n            output_data.data[:] = vals.transpose(*dims).data[:]\n\n            return output_data\n\n\n@common_doc(COMMON_INTERPOLATOR_DOCS)",
            "class ScipyGrid(ScipyPoint):\n    \"\"\"Scipy Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n    \"\"\"\n\n    methods_supported = [\"nearest\", \"bilinear\", \"cubic_spline\", \"spline_2\", \"spline_3\", \"spline_4\"]\n    method = tl.Unicode(default_value=\"nearest\")\n\n    # TODO: implement these parameters for the method 'nearest'\n    spatial_tolerance = tl.Float(default_value=np.inf)\n    time_tolerance = tl.Union([tl.Unicode(), tl.Instance(np.timedelta64, allow_none=True)], default_value=None)\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_can_interpolate}\n        \"\"\"\n\n        # TODO: make this so we don't need to specify lat and lon together\n        # or at least throw a warning\n        if (\n            \"lat\" in udims\n            and \"lon\" in udims\n            and self._dim_in([\"lat\", \"lon\"], source_coordinates)\n            and self._dim_in([\"lat\", \"lon\"], eval_coordinates, unstacked=True)\n        ):\n\n            return [\"lat\", \"lon\"]\n\n        # otherwise return no supported dims\n        return tuple()\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n\n        if self._dim_in([\"lat\", \"lon\"], eval_coordinates):\n            return self._interpolate_irregular_grid(\n                udims, source_coordinates, source_data, eval_coordinates, output_data, grid=True\n            )\n\n        elif self._dim_in([\"lat\", \"lon\"], eval_coordinates, unstacked=True):\n            eval_coordinates_us = eval_coordinates.unstack()\n            return self._interpolate_irregular_grid(\n                udims, source_coordinates, source_data, eval_coordinates_us, output_data, grid=False\n            )",
            "def _interpolate_irregular_grid(\n        self, udims, source_coordinates, source_data, eval_coordinates, output_data, grid=True\n    ):\n\n        if len(source_data.dims) > 2:\n            keep_dims = [\"lat\", \"lon\"]\n            return self._loop_helper(\n                self._interpolate_irregular_grid,\n                keep_dims,\n                udims,\n                source_coordinates,\n                source_data,\n                eval_coordinates,\n                output_data,\n                grid=grid,\n            )\n\n        s = []\n        if source_coordinates[\"lat\"].is_descending:\n            lat = source_coordinates[\"lat\"].coordinates[::-1]\n            s.append(slice(None, None, -1))\n        else:\n            lat = source_coordinates[\"lat\"].coordinates\n            s.append(slice(None, None))\n        if source_coordinates[\"lon\"].is_descending:\n            lon = source_coordinates[\"lon\"].coordinates[::-1]\n            s.append(slice(None, None, -1))\n        else:\n            lon = source_coordinates[\"lon\"].coordinates\n            s.append(slice(None, None))\n\n        data = source_data.data[tuple(s)]\n\n        # remove nan's\n        I, J = np.isfinite(lat), np.isfinite(lon)\n        coords_i = lat[I], lon[J]\n        coords_i_dst = [eval_coordinates[\"lon\"].coordinates, eval_coordinates[\"lat\"].coordinates]\n\n        # Swap order in case datasource uses lon,lat ordering instead of lat,lon\n        if source_coordinates.dims.index(\"lat\") > source_coordinates.dims.index(\"lon\"):\n            I, J = J, I\n            coords_i = coords_i[::-1]\n            coords_i_dst = coords_i_dst[::-1]\n        data = data[I, :][:, J]\n\n        if self.method in [\"bilinear\", \"nearest\"]:\n            f = RegularGridInterpolator(\n                coords_i, data, method=self.method.replace(\"bi\", \"\"), bounds_error=False, fill_value=np.nan\n            )\n            if grid:\n                x, y = np.meshgrid(*coords_i_dst)\n            else:\n                x, y = coords_i_dst\n            output_data.data[:] = f((y.ravel(), x.ravel())).reshape(output_data.shape)\n\n        # TODO: what methods is 'spline' associated with?\n        elif \"spline\" in self.method:\n            if self.method == \"cubic_spline\":\n                order = 3\n            else:\n                # TODO: make this a parameter\n                order = int(self.method.split(\"_\")[-1])\n\n            f = RectBivariateSpline(coords_i[0], coords_i[1], data, kx=max(1, order), ky=max(1, order))\n            output_data.data[:] = f(coords_i_dst[1], coords_i_dst[0], grid=grid).reshape(output_data.shape)\n\n        return output_data"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/none_interpolator.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nInterpolator implementations\n\"\"\"",
            "\"\"\"None Interpolation\"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_can_select}\n        \"\"\"",
            "\"\"\"\n        {interpolator_select}\n        \"\"\""
        ],
        "code_snippets": [
            "class NoneInterpolator(Interpolator):",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        udims_subset = self._filter_udims_supported(udims)\n\n        return udims_subset\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        # Note, some of the following code duplicates code in the Selector class.\n        # This duplication is for the sake of optimization\n\n        return source_data\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_can_select}\n        \"\"\"\n        if not (self.method == \"none\"):\n            return tuple()\n\n        udims_subset = self._filter_udims_supported(udims)\n        return udims_subset\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def select_coordinates(self, udims, source_coordinates, eval_coordinates, index_type=\"numpy\"):\n        \"\"\"\n        {interpolator_select}\n        \"\"\"\n        return source_coordinates.intersect(eval_coordinates, outer=False, return_index=True)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/interpolation_manager.py",
        "comments": [],
        "docstrings": [
            "\"\"\"str : Default interpolation method used when creating a new :class:`Interpolation` class \"\"\"",
            "\"\"\"list : list of available interpolator classes\"\"\"",
            "\"\"\"dict : Dictionary of a string interpolator name and associated interpolator class\"\"\"",
            "\"\"\"dict: Dictionary of string interpolation methods and associated interpolator classes\n   (i.e. ``'nearest': [NearestNeighbor, RasterioInterpolator, ScipyGrid]``) \"\"\"",
            "\"\"\"Load interpolators from :list:`INTERPOLATORS`\n\n    Defines :dict:`INTERPOLATORS_DICT`, and :dict:`INTERPOLATION_METHODS_DICT`\n    \"\"\"",
            "\"\"\"\n    Custom label for interpolation exceptions\n    \"\"\"",
            "\"\"\"Create an interpolation class to handle one interpolation method per unstacked dimension.\n    Used to interpolate data within a datasource.\n\n    Parameters\n    ----------\n    definition : str, tuple (str, list of podpac.core.data.interpolator.Interpolator), dict\n        Interpolation definition used to define interpolation methods for each definiton.\n        See :attr:`podpac.data.DataSource.interpolation` for more details.\n\n    Raises\n    ------\n    InterpolationException\n        Raised when definition parameter is improperly formatted\n\n    \"\"\"",
            "\"\"\"parse interpolation definitions into a tuple of (method, Interpolator)\n\n        Parameters\n        ----------\n        definition : str, dict\n            interpolation definition\n            See :attr:`podpac.data.DataSource.interpolation` for more details.\n\n        Returns\n        -------\n        dict\n            dict with keys 'method', 'interpolators', and 'params'\n\n        Raises\n        ------\n        InterpolationException\n        TypeError\n        \"\"\"",
            "\"\"\"Make sure interpolator is a subclass of Interpolator\n\n        Parameters\n        ----------\n        interpolator : any\n            input definition to validate\n\n        Raises\n        ------\n        TypeError\n            Raises a type error if interpolator is not a subclass of Interpolator\n        \"\"\"",
            "\"\"\"Set the list of interpolation definitions to the input dimension\n\n        Parameters\n        ----------\n        udims : tuple\n            tuple of dimensiosn to assign definition to\n        definition : dict\n            dict definition returned from _parse_interpolation_method\n        \"\"\"",
            "\"\"\"Create interpolator queue based on interpolation configuration and requested/native source_coordinates\n\n        Parameters\n        ----------\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n        select_method : function\n            method used to determine if interpolator can handle dimensions\n        strict : bool, optional\n            Raise an error if all dimensions can't be handled\n\n        Returns\n        -------\n        OrderedDict\n            Dict of (udims: Interpolator) to run in order\n\n        Raises\n        ------\n        InterpolationException\n            If `strict` is True, InterpolationException is raised when all dimensions cannot be handled\n        \"\"\"",
            "\"\"\"\n        Select a subset or coordinates if interpolator can downselect.\n\n        At this point in the execution process, podpac has selected a subset of source_coordinates that intersects\n        with the requested coordinates, dropped extra dimensions from requested coordinates, and confirmed\n        source coordinates are not missing any dimensions.\n\n        Parameters\n        ----------\n        source_coordinates : :class:`podpac.Coordinates`\n            Intersected source coordinates\n        eval_coordinates : :class:`podpac.Coordinates`\n            Requested coordinates to evaluate\n\n        Returns\n        -------\n        (:class:`podpac.Coordinates`, list)\n            Returns tuple with the first element subset of selected coordinates and the second element the indicies\n            of the selected coordinates\n        \"\"\"",
            "\"\"\"Interpolate data from requested coordinates to source coordinates\n\n        Parameters\n        ----------\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        source_data : podpac.core.units.UnitsDataArray\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n        output_data : podpac.core.units.UnitsDataArray\n            Description\n\n        Returns\n        -------\n        podpac.core.units.UnitDataArray\n            returns the new output UnitDataArray of interpolated data\n\n        Raises\n        ------\n        InterpolationException\n            Raises InterpolationException when interpolator definition can't support all the dimensions\n            of the requested coordinates\n        \"\"\""
        ],
        "code_snippets": [
            "def load_interpolators():\n    \"\"\"Load interpolators from :list:`INTERPOLATORS`\n\n    Defines :dict:`INTERPOLATORS_DICT`, and :dict:`INTERPOLATION_METHODS_DICT`\n    \"\"\"\n\n    # create empty arrays in INTEPROLATOR_METHODS\n    for method in INTERPOLATION_METHODS:\n        INTERPOLATION_METHODS_DICT[method] = []\n\n    # fill dictionaries with interpolator properties\n    for interpolator_class in INTERPOLATORS:\n        interpolator = interpolator_class()\n        INTERPOLATORS_DICT[interpolator.name] = interpolator_class\n\n        for method in INTERPOLATION_METHODS:\n            if method in interpolator.methods_supported:\n                INTERPOLATION_METHODS_DICT[method].append(interpolator_class)\n\n\n# load interpolators when module is first loaded\n# TODO does this really only load once?\n# TODO maybe move this whole section?\nload_interpolators()",
            "class InterpolationException(Exception):\n    \"\"\"\n    Custom label for interpolation exceptions\n    \"\"\"\n\n    pass",
            "class InterpolationManager(object):\n    \"\"\"Create an interpolation class to handle one interpolation method per unstacked dimension.\n    Used to interpolate data within a datasource.\n\n    Parameters\n    ----------\n    definition : str, tuple (str, list of podpac.core.data.interpolator.Interpolator), dict\n        Interpolation definition used to define interpolation methods for each definiton.\n        See :attr:`podpac.data.DataSource.interpolation` for more details.\n\n    Raises\n    ------\n    InterpolationException\n        Raised when definition parameter is improperly formatted\n\n    \"\"\"\n\n    definition = None\n    config = OrderedDict()  # container for interpolation methods for each dimension\n    _last_interpolator_queue = None  # container for the last run interpolator queue - useful for debugging\n    _last_select_queue = None  # container for the last run select queue - useful for debugging\n    _interpolation_params = None",
            "def __init__(self, definition=INTERPOLATION_DEFAULT):\n\n        self.definition = deepcopy(definition)\n        self.config = OrderedDict()\n        self._interpolation_params = {}\n\n        # if definition is None, set to default\n        if self.definition is None:\n            self.definition = INTERPOLATION_DEFAULT\n\n        # set each dim to interpolator definition\n        if isinstance(definition, (dict, list)):\n\n            # convert dict to list\n            if isinstance(definition, dict):\n                definition = [definition]\n\n            for interp_definition in definition:\n\n                # get interpolation method dict\n                method = self._parse_interpolation_method(interp_definition)\n\n                # specify dims\n                if \"dims\" in interp_definition:\n                    if isinstance(interp_definition[\"dims\"], list):\n                        udims = tuple(\n                            sorted(interp_definition[\"dims\"])\n                        )  # make sure the dims are always in the same order\n                    else:\n                        raise TypeError('The \"dims\" key of an interpolation definition must be a list')\n                else:\n                    udims = (\"default\",)\n\n                # make sure udims are not already specified in config\n                for config_dims in iter(self.config):\n                    if set(config_dims) & set(udims):\n                        raise InterpolationException(\n                            'Dimensions \"{}\" cannot be defined '.format(udims)\n                            + \"multiple times in interpolation definition {}\".format(interp_definition)\n                        )\n                # add all udims to definition\n                self.config = self._set_interpolation_method(udims, method)\n\n            # set default if its not been specified in the dict\n            if (\"default\",) not in self.config:\n                existing_dims = set(v for k in self.config.keys() for v in k)  # Default is NOT allowed to adjust these\n                name = (\"default\",)\n                if len(existing_dims) > 0:\n                    valid_dims = set(VALID_DIMENSION_NAMES)\n                    default_dims = valid_dims - existing_dims\n                    name = tuple(default_dims)\n\n                default_method = self._parse_interpolation_method(INTERPOLATION_DEFAULT)\n                self.config = self._set_interpolation_method(name, default_method)\n\n        elif isinstance(definition, string_types):\n            method = self._parse_interpolation_method(definition)\n            self.config = self._set_interpolation_method((\"default\",), method)\n\n        else:\n            raise TypeError(\n                '\"{}\" is not a valid interpolation definition type. '.format(definition)\n                + \"Interpolation definiton must be a string or list of dicts\"\n            )\n\n        # make sure ('default',) is always the last entry in config dictionary\n        if (\"default\",) in self.config:\n            default = self.config.pop((\"default\",))\n            self.config[(\"default\",)] = default",
            "def __repr__(self):\n        rep = str(self.__class__.__name__)\n        for udims in iter(self.config):\n            # rep += '\\n\\t%s:\\n\\t\\tmethod: %s\\n\\t\\tinterpolators: %s\\n\\t\\tparams: %s' % \\\n            rep += \"\\n\\t%s: %s, %s, %s\" % (\n                udims,\n                self.config[udims][\"method\"],\n                [i.__class__.__name__ for i in self.config[udims][\"interpolators\"]],\n                self.config[udims][\"params\"],\n            )\n\n        return rep",
            "def _parse_interpolation_method(self, definition):\n        \"\"\"parse interpolation definitions into a tuple of (method, Interpolator)\n\n        Parameters\n        ----------\n        definition : str, dict\n            interpolation definition\n            See :attr:`podpac.data.DataSource.interpolation` for more details.\n\n        Returns\n        -------\n        dict\n            dict with keys 'method', 'interpolators', and 'params'\n\n        Raises\n        ------\n        InterpolationException\n        TypeError\n        \"\"\"\n        if isinstance(definition, string_types):\n            if definition not in INTERPOLATION_METHODS:\n                raise InterpolationException(\n                    '\"{}\" is not a valid interpolation shortcut. '.format(definition)\n                    + \"Valid interpolation shortcuts: {}\".format(INTERPOLATION_METHODS)\n                )\n            return {\"method\": definition, \"interpolators\": INTERPOLATION_METHODS_DICT[definition], \"params\": {}}\n\n        elif isinstance(definition, dict):\n\n            # confirm method in dict\n            if \"method\" not in definition:\n                raise InterpolationException(\n                    \"{} is not a valid interpolation definition. \".format(definition)\n                    + 'Interpolation definition dict must contain key \"method\" string value'\n                )\n            else:\n                method_string = definition[\"method\"]\n\n            # if specifying custom method, user must include interpolators\n            if \"interpolators\" not in definition and method_string not in INTERPOLATION_METHODS:\n                raise InterpolationException(\n                    '\"{}\" is not a valid interpolation shortcut. '.format(method_string)\n                    + 'Specify list \"interpolators\" or change \"method\" '\n                    + \"to a valid interpolation shortcut: {}\".format(INTERPOLATION_METHODS)\n                )\n            elif \"interpolators\" not in definition:\n                interpolators = INTERPOLATION_METHODS_DICT[method_string]\n            else:\n                interpolators = definition[\"interpolators\"]\n\n            # default for params\n            if \"params\" in definition:\n                params = definition[\"params\"]\n            else:\n                params = {}\n\n            # confirm types\n            if not isinstance(method_string, string_types):\n                raise TypeError(\n                    \"{} is not a valid interpolation method. \".format(method_string)\n                    + \"Interpolation method must be a string\"\n                )\n\n            if not isinstance(interpolators, list):\n                raise TypeError(\n                    \"{} is not a valid interpolator definition. \".format(interpolators)\n                    + \"Interpolator definition must be of type list containing Interpolator\"\n                )\n\n            if not isinstance(params, dict):\n                raise TypeError(\n                    \"{} is not a valid interpolation params definition. \".format(params)\n                    + \"Interpolation params must be a dict\"\n                )\n\n            # handle when interpolator is a string (most commonly from a node definition)\n            for idx, interpolator_class in enumerate(interpolators):\n                if isinstance(interpolator_class, string_types):\n                    if interpolator_class in INTERPOLATORS_DICT.keys():\n                        interpolators[idx] = INTERPOLATORS_DICT[interpolator_class]\n                    else:\n                        raise TypeError(\n                            'Interpolator \"{}\" is not in the dictionary of valid '.format(interpolator_class)\n                            + \"interpolators: {}\".format(INTERPOLATORS_DICT)\n                        )\n\n            # validate interpolator class\n            for interpolator in interpolators:\n                self._validate_interpolator(interpolator)\n\n            # if all checks pass, return the definition\n            return {\"method\": method_string, \"interpolators\": interpolators, \"params\": params}\n\n        else:\n            raise TypeError(\n                '\"{}\" is not a valid Interpolator definition. '.format(definition)\n                + \"Interpolation definiton must be a string or dict.\"\n            )",
            "def _validate_interpolator(self, interpolator):\n        \"\"\"Make sure interpolator is a subclass of Interpolator\n\n        Parameters\n        ----------\n        interpolator : any\n            input definition to validate\n\n        Raises\n        ------\n        TypeError\n            Raises a type error if interpolator is not a subclass of Interpolator\n        \"\"\"\n        try:\n            valid = issubclass(interpolator, Interpolator)\n            if not valid:\n                raise TypeError()\n        except TypeError:\n            raise TypeError(\n                \"{} is not a valid interpolator type. \".format(interpolator)\n                + \"Interpolator must be of type {}\".format(Interpolator)\n            )",
            "def _set_interpolation_method(self, udims, definition):\n        \"\"\"Set the list of interpolation definitions to the input dimension\n\n        Parameters\n        ----------\n        udims : tuple\n            tuple of dimensiosn to assign definition to\n        definition : dict\n            dict definition returned from _parse_interpolation_method\n        \"\"\"\n\n        method = deepcopy(definition[\"method\"])\n        interpolators = deepcopy(definition[\"interpolators\"])\n        params = deepcopy(definition[\"params\"])\n\n        # instantiate interpolators\n        for (idx, interpolator) in enumerate(interpolators):\n            parms = {k: v for k, v in params.items() if hasattr(interpolator, k)}\n            interpolators[idx] = interpolator(method=method, **parms)\n\n        definition[\"interpolators\"] = interpolators\n\n        # Record parameters to make sure they are being captured\n        self._interpolation_params.update({k: False for k in params})\n\n        # set to interpolation configuration for dims\n        self.config[udims] = definition\n        return self.config",
            "def _select_interpolator_queue(self, source_coordinates, eval_coordinates, select_method, strict=False):\n        \"\"\"Create interpolator queue based on interpolation configuration and requested/native source_coordinates\n\n        Parameters\n        ----------\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n        select_method : function\n            method used to determine if interpolator can handle dimensions\n        strict : bool, optional\n            Raise an error if all dimensions can't be handled\n\n        Returns\n        -------\n        OrderedDict\n            Dict of (udims: Interpolator) to run in order\n\n        Raises\n        ------\n        InterpolationException\n            If `strict` is True, InterpolationException is raised when all dimensions cannot be handled\n        \"\"\"\n        source_dims = set(source_coordinates.udims)\n        handled_dims = set()\n\n        interpolator_queue = OrderedDict()\n\n        # go through all dims in config\n        for key in iter(self.config):\n\n            # if the key is set to (default,), it represents all the remaining dimensions that have not been handled\n            # __init__ makes sure that (default,) will always be the last key in on\n            if key == (\"default\",):\n                udims = tuple(sorted(source_dims - handled_dims))\n            else:\n                udims = key\n\n            # get configured list of interpolators for dim definition\n            interpolators = self.config[key][\"interpolators\"]\n\n            # iterate through interpolators recording which dims they support\n            for interpolator in interpolators:\n                # if all dims have been handled already, skip the rest\n                if not udims:\n                    break\n\n                # see which dims the interpolator can handle\n                if self.config[key][\"method\"] not in interpolator.methods_supported:\n                    can_handle = tuple()\n                else:\n                    can_handle = getattr(interpolator, select_method)(udims, source_coordinates, eval_coordinates)\n\n                # if interpolator can handle all udims\n                if not set(udims) - set(can_handle):\n\n                    # save union of dims that can be handled by this interpolator and already supported dims for next iteration\n                    handled_dims = handled_dims | set(can_handle)\n\n                    # set interpolator to work on that dimension in the interpolator_queue if dim has no interpolator\n                    if udims not in interpolator_queue:\n\n                        interpolator_queue[udims] = interpolator\n\n        # throw error if the source_dims don't encompass all the supported dims\n        # this should happen rarely because of default\n        if len(source_dims - handled_dims) > 0 and strict:\n            missing_dims = list(source_dims - handled_dims)\n            raise InterpolationException(\n                \"Dimensions {} \".format(missing_dims)\n                + \"can't be handled by interpolation definition:\\n {}\".format(self)\n            )\n\n        # TODO: adjust by interpolation cost\n        return interpolator_queue",
            "def select_coordinates(self, source_coordinates, eval_coordinates, index_type=\"numpy\"):\n        \"\"\"\n        Select a subset or coordinates if interpolator can downselect.\n\n        At this point in the execution process, podpac has selected a subset of source_coordinates that intersects\n        with the requested coordinates, dropped extra dimensions from requested coordinates, and confirmed\n        source coordinates are not missing any dimensions.\n\n        Parameters\n        ----------\n        source_coordinates : :class:`podpac.Coordinates`\n            Intersected source coordinates\n        eval_coordinates : :class:`podpac.Coordinates`\n            Requested coordinates to evaluate\n\n        Returns\n        -------\n        (:class:`podpac.Coordinates`, list)\n            Returns tuple with the first element subset of selected coordinates and the second element the indicies\n            of the selected coordinates\n        \"\"\"\n\n        # TODO: short circuit if source_coordinates contains eval_coordinates\n        # short circuit if source and eval coordinates are the same\n        if source_coordinates == eval_coordinates:\n            return source_coordinates, tuple([slice(0, None)] * len(source_coordinates.shape))\n\n        interpolator_queue = self._select_interpolator_queue(source_coordinates, eval_coordinates, \"can_select\")\n\n        self._last_select_queue = interpolator_queue\n\n        # For heterogeneous selections, we need to select and then recontruct each set of dimensions\n        selected_coords = {}\n        selected_coords_idx = {k: np.arange(source_coordinates[k].size) for k in source_coordinates.dims}\n        for udims in interpolator_queue:\n            interpolator = interpolator_queue[udims]\n            extra_dims = [d for d in source_coordinates.udims if d not in udims]\n            sc = source_coordinates.udrop(extra_dims)\n            # run interpolation. mutates selected coordinates and selected coordinates index\n            sel_coords, sel_coords_idx = interpolator.select_coordinates(\n                udims, sc, eval_coordinates, index_type=index_type\n            )\n            # Save individual 1-D coordinates for later reconstruction\n            for i, k in enumerate(sel_coords.dims):\n                selected_coords[k] = sel_coords[k]\n                selected_coords_idx[k] = sel_coords_idx[i]\n\n        # Reconstruct dimensions\n        for d in source_coordinates.dims:\n            if d not in selected_coords:  # Some coordinates may not have a selector when heterogeneous\n                selected_coords[d] = source_coordinates[d]\n            # np.ix_ call doesn't work with slices, and fancy numpy indexing does not work well with mixed slice/index\n            if isinstance(selected_coords_idx[d], slice) and index_type != \"slice\":\n                selected_coords_idx[d] = np.arange(source_coordinates[d].size)[selected_coords_idx[d]]\n\n        selected_coords = Coordinates(\n            [selected_coords[k] for k in source_coordinates.dims],\n            source_coordinates.dims,\n            crs=source_coordinates.crs,\n            validate_crs=False,\n        )\n        if index_type == \"numpy\":\n            npcoords = []\n            has_stacked = False\n            for k in source_coordinates.dims:\n                # Deal with nD stacked source coords (marked by coords being in tuple)\n                if isinstance(selected_coords_idx[k], tuple):\n                    has_stacked = True\n                    npcoords.extend([sci for sci in selected_coords_idx[k]])\n                else:\n                    npcoords.append(selected_coords_idx[k])\n            if has_stacked:\n                # When stacked coordinates are nD we cannot use the catchall of the next branch\n                selected_coords_idx2 = npcoords\n            else:\n                # This would not be needed if everything went as planned in\n                # interpolator.select_coordinates, but this is a catchall that works\n                # for 90% of the cases\n                selected_coords_idx2 = np.ix_(*[np.ravel(npc) for npc in npcoords])\n        elif index_type == \"xarray\":\n            selected_coords_idx2 = []\n            for i in selected_coords.dims:\n                # Deal with nD stacked source coords (marked by coords being in tuple)\n                if isinstance(selected_coords_idx[i], tuple):\n                    selected_coords_idx2.extend([xr.DataArray(sci, dims=[i]) for sci in selected_coords_idx[i]])\n                else:\n                    selected_coords_idx2.append(selected_coords_idx[i])\n            selected_coords_idx2 = tuple(selected_coords_idx2)\n        elif index_type == \"slice\":\n            selected_coords_idx2 = []\n            for i in selected_coords.dims:\n                # Deal with nD stacked source coords (marked by coords being in tuple)\n                if isinstance(selected_coords_idx[i], tuple):\n                    selected_coords_idx2.extend(selected_coords_idx[i])\n                else:\n                    if isinstance(selected_coords_idx[i], np.ndarray):\n                        # This happens when the interpolator_queue is empty, so we have to turn the\n                        # initialized coordinates into slices instead of numpy arrays\n                        selected_coords_idx2.append(\n                            slice(selected_coords_idx[i].min(), selected_coords_idx[i].max() + 1)\n                        )\n                    else:\n                        selected_coords_idx2.append(selected_coords_idx[i])\n\n            selected_coords_idx2 = tuple(selected_coords_idx2)\n        else:\n            raise ValueError(\"Unknown index_type '%s'\" % index_type)\n        return selected_coords, tuple(selected_coords_idx2)",
            "def interpolate(self, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"Interpolate data from requested coordinates to source coordinates\n\n        Parameters\n        ----------\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        source_data : podpac.core.units.UnitsDataArray\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n        output_data : podpac.core.units.UnitsDataArray\n            Description\n\n        Returns\n        -------\n        podpac.core.units.UnitDataArray\n            returns the new output UnitDataArray of interpolated data\n\n        Raises\n        ------\n        InterpolationException\n            Raises InterpolationException when interpolator definition can't support all the dimensions\n            of the requested coordinates\n        \"\"\"\n\n        # loop through multiple outputs if necessary\n        if \"output\" in output_data.dims:\n            for output in output_data.coords[\"output\"]:\n                output_data.sel(output=output)[:] = self.interpolate(\n                    source_coordinates,\n                    source_data.sel(output=output).drop(\"output\"),\n                    eval_coordinates,\n                    output_data.sel(output=output).drop(\"output\"),\n                )\n            return output_data\n\n        ## drop already-selected output variable\n        # if \"output\" in output_data.coords:\n        # source_data = source_data.drop(\"output\")\n        # output_data = output_data.drop(\"output\")\n\n        # short circuit if the source data and requested coordinates are of shape == 1\n        if source_data.size == 1 and eval_coordinates.size == 1:\n            output_data.data[:] = source_data.data.flatten()[0]\n            return output_data\n\n        # short circuit if source_coordinates contains eval_coordinates\n        # TODO handle stacked issubset of unstacked case\n        #      this case is currently skipped because of the set(eval_coordinates) == set(source_coordinates)))\n        if eval_coordinates.issubset(source_coordinates) and set(eval_coordinates) == set(source_coordinates):\n            if any(isinstance(c, StackedCoordinates) and c.ndim > 1 for c in eval_coordinates.values()):\n                # TODO AFFINE\n                # currently this is bypassing the short-circuit in the shaped stacked coordinates case\n                pass\n            else:\n                try:\n                    data = source_data.interp(output_data.coords, method=\"nearest\")\n                except (NotImplementedError, ValueError):\n                    try:\n                        data = source_data.sel(output_data.coords[output_data.dims])\n                    except KeyError:\n                        # Since the output is a subset of the original data,\n                        # we can just rely on xarray's broadcasting capability\n                        # to subselect data, as the final fallback\n                        output_data[:] = 0\n                        data = source_data + output_data\n\n                output_data.data[:] = data.transpose(*output_data.dims)\n                return output_data\n\n        interpolator_queue = self._select_interpolator_queue(\n            source_coordinates, eval_coordinates, \"can_interpolate\", strict=True\n        )\n\n        # for debugging purposes, save the last defined interpolator queue\n        self._last_interpolator_queue = interpolator_queue\n\n        # reset interpolation parameters\n        for k in self._interpolation_params:\n            self._interpolation_params[k] = False\n\n        # iterate through each dim tuple in the queue\n        dtype = output_data.dtype\n        attrs = source_data.attrs\n        for udims, interpolator in interpolator_queue.items():\n            # TODO move the above short-circuits into this loop\n            if all([ud not in source_coordinates.udims for ud in udims]):\n                # Skip this udim if it's not part of the source coordinates (can happen with default)\n                continue\n            # Check if parameters are being used\n            for k in self._interpolation_params:\n                self._interpolation_params[k] = hasattr(interpolator, k) or self._interpolation_params[k]\n\n            # interp_coordinates are essentially intermediate eval_coordinates\n            interp_dims = [dim for dim, c in source_coordinates.items() if set(c.dims).issubset(udims)]\n            other_dims = [dim for dim, c in eval_coordinates.items() if not set(c.dims).issubset(udims)]\n            interp_coordinates = merge_dims(\n                [source_coordinates.drop(interp_dims), eval_coordinates.drop(other_dims)], validate_crs=False\n            )\n            interp_data = UnitsDataArray.create(interp_coordinates, dtype=dtype)\n            interp_data = interpolator.interpolate(\n                udims, source_coordinates, source_data, interp_coordinates, interp_data\n            )\n\n            # prepare for the next iteration\n            source_data = interp_data.transpose(*interp_coordinates.xdims)\n            source_data.attrs = attrs\n            source_coordinates = interp_coordinates\n\n        output_data.data = interp_data.transpose(*output_data.dims)\n\n        # Throw warnings for unused parameters\n        for k in self._interpolation_params:\n            if self._interpolation_params[k]:\n                continue\n            _logger.warning(\"The interpolation parameter '{}' was ignored during interpolation.\".format(k))\n\n        return output_data",
            "def _fix_coordinates_for_none_interp(self, eval_coordinates, source_coordinates):\n        interpolator_queue = self._select_interpolator_queue(\n            source_coordinates, eval_coordinates, \"can_interpolate\", strict=True\n        )\n        if not any([isinstance(interpolator_queue[k], NoneInterpolator) for k in interpolator_queue]):\n            # Nothing to do, just return eval_coordinates\n            return eval_coordinates\n\n        # Likely need to fix the output, since the shape of output will\n        # not match the eval coordinates in most cases\n        new_dims = []\n        new_coords = []\n        covered_udims = []\n        for k in interpolator_queue:\n            if not isinstance(interpolator_queue[k], NoneInterpolator):\n                # Keep the eval_coordinates for these dimensions\n                for d in eval_coordinates.dims:\n                    ud = d.split(\"_\")\n                    for u in ud:\n                        if u in k:\n                            new_dims.append(d)\n                            new_coords.append(eval_coordinates[d])\n                            covered_udims.extend(ud)\n                            break\n            else:\n                for d in source_coordinates.dims:\n                    ud = d.split(\"_\")\n                    for u in ud:\n                        if u in k:\n                            new_dims.append(d)\n                            new_coords.append(source_coordinates[d])\n                            covered_udims.extend(ud)\n                            break\n        new_coordinates = Coordinates(new_coords, new_dims)\n        return new_coordinates",
            "class InterpolationTrait(tl.Union):\n    default_value = INTERPOLATION_DEFAULT\n\n    # .tag(attr=True, required=True, default = \"linear\")\n    def __init__(\n        self,\n        trait_types=[tl.Dict(), tl.List(), tl.Enum(INTERPOLATION_METHODS), tl.Instance(InterpolationManager)],\n        *args,\n        **kwargs\n    ):\n        super(InterpolationTrait, self).__init__(trait_types=trait_types, *args, **kwargs)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/rasterio_interpolator.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nInterpolator implementations\n\"\"\"",
            "\"\"\"Rasterio Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n    rasterio_interpolators : list of str\n        Interpolator methods available via rasterio\n    \"\"\"",
            "\"\"\"{interpolator_can_interpolate}\"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\""
        ],
        "code_snippets": [
            "class RasterioInterpolator(Interpolator):\n    \"\"\"Rasterio Interpolation\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n    rasterio_interpolators : list of str\n        Interpolator methods available via rasterio\n    \"\"\"\n\n    methods_supported = [\n        \"nearest\",\n        \"bilinear\",\n        \"cubic\",\n        \"cubic_spline\",\n        \"lanczos\",\n        \"average\",\n        \"mode\",\n        \"gauss\",\n        \"max\",\n        \"min\",\n        \"med\",\n        \"q1\",\n        \"q3\",\n    ]\n    method = tl.Unicode(default_value=\"nearest\")\n\n    dims_supported = [\"lat\", \"lon\"]\n\n    # TODO: implement these parameters for the method 'nearest'\n    spatial_tolerance = tl.Float(default_value=np.inf)\n    time_tolerance = tl.Union([tl.Unicode(), tl.Instance(np.timedelta64, allow_none=True)])\n\n    # TODO: support 'gauss' method?\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n\n        # TODO: handle when udims does not contain both lat and lon\n        # if the source data has more dims than just lat/lon is asked, loop over those dims and run the interpolation\n        # on those grids\n        if len(source_data.dims) > 2:\n            keep_dims = [\"lat\", \"lon\"]\n            return self._loop_helper(\n                self.interpolate, keep_dims, udims, source_coordinates, source_data, eval_coordinates, output_data\n            )\n\n        with rasterio.Env():\n            src_transform = transform.Affine.from_gdal(*source_coordinates.geotransform)\n            src_crs = rasterio.crs.CRS.from_proj4(source_coordinates.crs)\n            # Need to make sure array is c-contiguous\n            source = np.ascontiguousarray(source_data.data)\n\n            dst_transform = transform.Affine.from_gdal(*eval_coordinates.geotransform)\n            dst_crs = rasterio.crs.CRS.from_proj4(eval_coordinates.crs)\n            # Need to make sure array is c-contiguous\n            if not output_data.data.flags[\"C_CONTIGUOUS\"]:\n                destination = np.ascontiguousarray(output_data.data)\n            else:\n                destination = output_data.data\n\n            reproject(\n                source,\n                np.atleast_2d(destination.squeeze()),  # Needed for legacy compatibility\n                src_transform=src_transform,\n                src_crs=src_crs,\n                src_nodata=np.nan,\n                dst_transform=dst_transform,\n                dst_crs=dst_crs,\n                dst_nodata=np.nan,\n                resampling=getattr(Resampling, self.method),\n            )\n            output_data.data[:] = destination\n\n        return output_data"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/selector.py",
        "comments": [
            "// ncols, ci % ncols)"
        ],
        "docstrings": [
            "\"\"\"\n        Params\n        -------\n        method: str, list\n            Either a list of offsets or a type of selection\n        \"\"\"",
            "\"\"\"Sub-selects the source_coords based on the request_coords\n\n        Parameters\n        ------------\n        source_coords: :class:`podpac.Coordinates`\n            The coordinates of the source data\n        request_coords: :class:`podpac.Coordinates`\n            The coordinates of the request (user eval)\n        index_type: str, optional\n            Default is 'numpy'. Either \"numpy\", \"xarray\", or \"slice\". The returned index will be compatible with,\n            either \"numpy\" (default) or \"xarray\" objects, or any\n            object that works with tuples of slices (\"slice\")\n\n        Returns\n        --------\n        :class:`podpac.Coordinates`:\n            The sub-selected source coordinates\n        tuple(indices):\n            The indices that can be used to sub-select the source coordinates to produce the sub-selected coordinates.\n            This is useful for directly indexing into the data type.\n        \"\"\""
        ],
        "code_snippets": [
            "def _higher_precision_time_stack(coords0, coords1, dims):\n    crds0 = []\n    crds1 = []\n    lens = []\n    for d in dims:\n        c0, c1 = _higher_precision_time_coords1d(coords0[d], coords1[d])\n        crds0.append(c0)\n        crds1.append(c1)\n        lens.append(len(c1))\n    if np.all(np.array(lens) == lens[0]):\n        crds1 = np.stack(crds1, axis=0)\n\n    return np.stack(crds0, axis=0), crds1",
            "def _higher_precision_time_coords1d(coords0, coords1):\n    dtype0 = coords0.coordinates[0].dtype\n    dtype1 = coords1.coordinates[0].dtype\n    if not np.issubdtype(dtype0, np.datetime64) or not np.issubdtype(dtype1, np.datetime64):\n        return coords0.coordinates, coords1.coordinates\n    if dtype0 > dtype1:  # greater means higher precision (smaller unit)\n        dtype = dtype0\n    else:\n        dtype = dtype1\n    return coords0.coordinates.astype(dtype).astype(float), coords1.coordinates.astype(dtype).astype(float)",
            "def _index2slice(index):\n    if index.size == 0:\n        return slice(0, 0)\n    elif index.size == 1:\n        return slice(index[0], index[0] + 1)\n    else:\n        df = np.diff(index)\n        mn = np.min(index)\n        mx = np.max(index)\n        if np.all(df == df[0]):\n            return slice(mn, mx + 1, df[0])\n        else:\n            return slice(mn, mx + 1)",
            "class Selector(tl.HasTraits):\n    supported_methods = [\"nearest\", \"linear\", \"bilinear\", \"cubic\"]\n\n    method = tl.Tuple()\n    respect_bounds = tl.Bool(False)",
            "def __init__(self, method=None):\n        \"\"\"\n        Params\n        -------\n        method: str, list\n            Either a list of offsets or a type of selection\n        \"\"\"\n        if isinstance(method, str):\n            self.method = METHOD.get(method)\n        else:\n            self.method = method",
            "def select(self, source_coords, request_coords, index_type=\"numpy\"):\n        \"\"\"Sub-selects the source_coords based on the request_coords\n\n        Parameters\n        ------------\n        source_coords: :class:`podpac.Coordinates`\n            The coordinates of the source data\n        request_coords: :class:`podpac.Coordinates`\n            The coordinates of the request (user eval)\n        index_type: str, optional\n            Default is 'numpy'. Either \"numpy\", \"xarray\", or \"slice\". The returned index will be compatible with,\n            either \"numpy\" (default) or \"xarray\" objects, or any\n            object that works with tuples of slices (\"slice\")\n\n        Returns\n        --------\n        :class:`podpac.Coordinates`:\n            The sub-selected source coordinates\n        tuple(indices):\n            The indices that can be used to sub-select the source coordinates to produce the sub-selected coordinates.\n            This is useful for directly indexing into the data type.\n        \"\"\"\n        if source_coords.crs.lower() != request_coords.crs.lower():\n            request_coords = request_coords.transform(source_coords.crs)\n        coords = []\n        coords_inds = []\n        for coord1d in source_coords._coords.values():\n            ci = self._select1d(coord1d, request_coords, index_type)\n            ci = np.sort(np.unique(ci))\n            if len(coord1d.shape) == 2:  # Handle case of 2D-stacked coordinates\n                ncols = coord1d.shape[1]\n                ci = (ci",
            "def _select1d(self, source, request, index_type):\n        if isinstance(source, StackedCoordinates):\n            ci = self._select_stacked(source, request, index_type)\n        elif source.is_uniform:\n            ci = self._select_uniform(source, request, index_type)\n        else:\n            ci = self._select_nonuniform(source, request, index_type)\n        # else:\n        # _logger.info(\"Coordinates are not subselected for source {} with request {}\".format(source, request))\n        # return source, slice(0, None)\n        return ci",
            "def _merge_indices(self, indices, source_dims, request_dims):\n        # For numpy to broadcast correctly, we have to reshape each of the indices\n        reshape = np.ones(len(indices), int)\n        new_indices = []\n        for i in range(len(indices)):\n            reshape[:] = 1\n            reshape[i] = -1\n            if isinstance(indices[i], tuple):\n                # nD stacked coordinates\n                # This means the source has shape (N, M, ...)\n                # But the coordinates are stacked (i.e. lat_lon with shape N, M for the lon and lat parts)\n                new_indices.append(tuple([ind.reshape(*reshape) for ind in indices[i]]))\n            else:\n                new_indices.append(indices[i].reshape(*reshape))\n        return tuple(new_indices)",
            "def _select_uniform(self, source, request, index_type):\n        crds = request[source.name]\n        if crds.is_uniform and abs(crds.step) < abs(source.step) and not request.is_stacked(source.name):\n            start_ind = np.clip((crds.start - source.start) / source.step, 0, source.size)\n            end_ind = np.clip((crds.stop - source.start) / source.step, 0, source.size)\n            start = int(np.floor(max(0, min(start_ind, end_ind) + min(self.method) - 1e-6)))\n            stop = int(np.ceil(min(source.size, max(start_ind, end_ind) + max(self.method) + 1 + 1e-6)))\n            return np.arange(start, stop)\n\n        index = (crds.coordinates - source.start) / source.step\n        stop_ind = source.size - 1\n        if len(self.method) > 1:\n            flr_ceil = {-1: np.floor(index), 1: np.ceil(index)}\n        else:\n            # In this case, floating point error really matters, so we have to do a test\n            up = np.round(index - 1e-6)\n            down = np.round(index + 1e-6)\n            # When up and down do not agree, make sure both the indices that will be kept.\n            index_mid = down  # arbitrarily default to down when both satisfy criteria\n            Iup = up != down\n            Iup[Iup] = (up[Iup] >= 0) & (up[Iup] <= stop_ind) & ((up[Iup] > down.max()) | (up[Iup] < down.min()))\n            index_mid[Iup] = up[Iup]\n            flr_ceil = {0: index_mid}\n\n        inds = []\n        for m in self.method:\n            sign = np.sign(m)\n            base = flr_ceil[sign]\n            inds.append(base + sign * (sign * m - 1))\n\n        inds = np.stack(inds, axis=1).ravel().astype(int)\n        inds = inds[(inds >= 0) & (inds <= stop_ind)]\n        return inds",
            "def _select_nonuniform(self, source, request, index_type):\n        src, req = _higher_precision_time_coords1d(source, request[source.name])\n        ckdtree_source = cKDTree(src[:, None])\n        _, inds = ckdtree_source.query(req[:, None], k=len(self.method))\n        inds = inds[inds < source.coordinates.size]\n        return inds.ravel()",
            "def _select_stacked(self, source, request, index_type):\n        udims = [ud for ud in source.udims if ud in request.udims]\n\n        # if the udims are all stacked in the same stack as part of the request coordinates, then we can take a shortcut.\n        # Otherwise we have to evaluate each unstacked set of dimensions independently\n        indep_evals = [ud for ud in udims if not request.is_stacked(ud)]\n        # two udims could be stacked, but in different dim groups, e.g. source (lat, lon), request (lat, time), (lon, alt)\n        stacked = {d for d in request.dims for ud in udims if ud in d and request.is_stacked(ud)}\n\n        inds = np.array([])\n        # Parts of the below code is duplicated in NearestNeighborInterpolotor\n        src_coords, req_coords_diag = _higher_precision_time_stack(source, request, udims)\n        # For nD stacked coordinates we need to unravel the stacked dimension\n        ckdtree_source = cKDTree(src_coords.reshape(src_coords.shape[0], -1).T)\n        if (len(indep_evals) + len(stacked)) <= 1:\n            req_coords = req_coords_diag.T\n        elif (len(stacked) == 0) | (len(indep_evals) == 0 and len(stacked) == len(udims)):\n            req_coords = np.stack([i.ravel() for i in np.meshgrid(*req_coords_diag, indexing=\"ij\")], axis=1)\n        else:\n            # Rare cases? E.g. lat_lon_time_alt source to lon, time_alt, lat destination\n            sizes = [request[d].size for d in request.dims]\n            reshape = np.ones(len(request.dims), int)\n            coords = [None] * len(udims)\n            for i in range(len(udims)):\n                ii = [ii for ii in range(len(request.dims)) if udims[i] in request.dims[ii]][0]\n                reshape[:] = 1\n                reshape[ii] = -1\n                coords[i] = req_coords_diag[i].reshape(*reshape)\n                for j, d in enumerate(request.dims):\n                    if udims[i] in d:  # Then we don't need to repeat\n                        continue\n                    coords[i] = coords[i].repeat(sizes[j], axis=j)\n            req_coords = np.stack([i.ravel() for i in coords], axis=1)\n\n        _, inds = ckdtree_source.query(req_coords, k=len(self.method))\n        inds = inds[inds < source.coordinates.size]\n        inds = inds.ravel()\n        return inds"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/interpolator.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nAbstract class for all Interpolator implementations\n\nAttributes\n----------\nCOMMON_INTERPOLATOR_DOCS : dict\n    Documentation prototype for interpolators\n\"\"\"",
            "\"\"\"\n        method : str\n            Current interpolation method to use in Interpolator (i.e. 'nearest').\n            This attribute is set during node evaluation when a new :class:`Interpolation`\n            class is constructed. See the :class:`podpac.data.DataSource` `interpolation` attribute for\n            more information on specifying the interpolator method.\n        methods_supported : list\n            List of methods supported by the interpolator.\n            This attribute should be defined by the implementing :class:`Interpolator`.\n            See :attr:`podpac.data.INTERPOLATION_METHODS` for list of available method strings.\n        dims_supported : list\n            List of unstacked dimensions supported by the interpolator.\n            This attribute should be defined by the implementing :class:`Interpolator`.\n            Used by private convience method :meth:`_filter_udims_supported`.\n        \"\"\"",
            "\"\"\"\n        Attributes\n        ----------\n        method : str\n            Current interpolation method to use in Interpolator (i.e. 'nearest').\n            This attribute is set during node evaluation when a new :class:`Interpolation`\n            class is constructed. See the :class:`podpac.data.DataSource` `interpolation` attribute for\n            more information on specifying the interpolator method.\n        dims_supported : list\n            List of unstacked dimensions supported by the interpolator.\n            This attribute should be defined by the implementing :class:`Interpolator`.\n            Used by private convience method :meth:`_filter_udims_supported`.\n        spatial_tolerance : float\n            Default is inf. Maximum distance to the nearest coordinate in space.\n            Cooresponds to the unit of the space measurement.\n        time_tolerance : float\n            Default is inf. Maximum distance to the nearest coordinate in time coordinates.\n            Accepts p.timedelta64() (i.e. np.timedelta64(1, 'D') for a 1-Day tolerance)\n        alt_tolerance : float\n            Default is inf. Maximum distance to the nearest coordinate in altitude coordinates. Corresponds to the unit\n            of the altitude as part of the requested coordinates\n        spatial_scale : float\n            Default is 1. This only applies when the source has stacked dimensions with different units.\n            The spatial_scale defines the factor that lat, lon coordinates will be scaled by (coordinates are divided by spatial_scale)\n            to output a valid distance for the combined set of dimensions.\n        time_scale : float\n            Default is 1. This only applies when the source has stacked dimensions with different units.\n            The time_scale defines the factor that time coordinates will be scaled by (coordinates are divided by time_scale)\n            to output a valid distance for the combined set of dimensions.\n        alt_scale : float\n            Default is 1. This only applies when the source has stacked dimensions with different units.\n            The alt_scale defines the factor that alt coordinates will be scaled by (coordinates are divided by alt_scale)\n            to output a valid distance for the combined set of dimensions.\n        respect_bounds : bool\n            Default is True. If True, any requested dimension OUTSIDE of the bounds will be interpolated as 'nan'.\n            Otherwise, any point outside the bounds will have NN interpolation allowed.\n        remove_nan: bool\n            Default is False. If True, nan's in the source dataset will NOT be interpolated. This can be used if a value for the function\n            is needed at every point of the request. It is not helpful when computing statistics, where nan values will be explicitly\n            ignored. In that case, if remove_nan is True, nan values will take on the values of neighbors, skewing the statistical result.\n        use_selector: bool\n            Default is True. If True, a subset of the coordinates will be selected BEFORE the data of a dataset is retrieved. This\n            reduces the number of data retrievals needed for large datasets. In cases where remove_nan = True, the selector may select\n            only nan points, in which case the interpolation fails to produce non-nan data. This usually happens when requesting a single\n            point from a dataset that contains nans. As such, in these cases set use_selector = False to get a non-nan value.\n\n        \"\"\"",
            "\"\"\"\n        Evaluate if interpolator can downselect the source coordinates from the requested coordinates\n        for the unstacked dims supplied.\n        If not overwritten, this method returns an empty tuple (``tuple()``)\n\n        Parameters\n        ----------\n        udims : tuple\n            dimensions to select\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n\n        Returns\n        -------\n        tuple\n            Returns a tuple of dimensions that can be selected with this interpolator\n            If no dimensions can be selected, method should return an emtpy tuple\n        \"\"\"",
            "\"\"\"\n        Downselect coordinates with interpolator method\n\n        Parameters\n        ----------\n        udims : tuple\n            dimensions to select coordinates\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        source_coordinates_index : list\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n\n        Returns\n        -------\n        (:class:`podpac.Coordinates`, list)\n            returns the new down selected coordinates and the new associated index. These coordinates must exist\n            in the coordinates of the source data\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\"",
            "\"\"\"\n        Evaluate if this interpolation method can handle the requested coordinates and source_coordinates.\n        If not overwritten, this method returns an empty tuple (`tuple()`)\n\n        Parameters\n        ----------\n        udims : tuple\n            dimensions to interpolate\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n\n        Returns\n        -------\n        tuple\n            Returns a tuple of dimensions that can be interpolated with this interpolator\n            If no dimensions can be interpolated, method should return an emtpy tuple\n        \"\"\"",
            "\"\"\"\n        Interpolate data from requested coordinates to source coordinates.\n\n        Parameters\n        ----------\n        udims : tuple\n            dimensions to interpolate\n        source_coordinates : :class:`podpac.Coordinates`\n            Description\n        source_data : podpac.core.units.UnitsDataArray\n            Description\n        eval_coordinates : :class:`podpac.Coordinates`\n            Description\n        output_data : podpac.core.units.UnitsDataArray\n            Description\n\n        Raises\n        ------\n        NotImplementedError\n\n        Returns\n        -------\n        podpac.core.units.UnitDataArray\n            returns the updated output of interpolated data\n        \"\"\"",
            "\"\"\"dict : Common interpolate docs \"\"\"",
            "\"\"\"\n    Custom label for interpolator exceptions\n    \"\"\"",
            "\"\"\"Interpolation Method\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n\n    \"\"\"",
            "\"\"\"\n        Interpolator definition\n\n        Returns\n        -------\n        str\n            String name of interpolator.\n        \"\"\"",
            "\"\"\"\n        Interpolator definition\n\n        Returns\n        -------\n        str\n            String name of interpolator.\n        \"\"\"",
            "\"\"\"\n        Overwrite this method if a Interpolator needs to do any\n        additional initialization after the standard initialization.\n        \"\"\"",
            "\"\"\"Verify the dim exists on coordinates\n\n        Parameters\n        ----------\n        dim : str, list of str\n            Dimension or list of dimensions to verify\n        *coords :class:`podpac.Coordinates`\n            coordinates to evaluate\n        unstacked : bool, optional\n            True if you want to compare dimensions in unstacked form, otherwise compare dimensions however\n            they are defined on the DataSource. Defaults to False.\n\n        Returns\n        -------\n        Boolean\n            True if the dim is in all input coordinates\n        \"\"\"",
            "\"\"\"In cases where the interpolator can only handle a limited number of dimensions, loop over the extra ones\n        Parameters\n        ----------\n        func : callable\n            The interpolation function that should be called on the data subset. Should have the following arguments:\n            func(udims, source_coordinates, source_data, eval_coordinates, output_data)\n        interp_dims: list(str)\n            List of source dimensions that will be interpolator. The looped dimensions will be computed\n        udims: list(str)\n           The unstacked coordinates that this interpolator handles\n        source_coordinates: podpac.Coordinates\n            The coordinates of the source data\n        eval_coordinates: podpac.Coordinates\n            The user-requested or evaluated coordinates\n        output_data: podpac.UnitsDataArray\n            Container for the output of the interpolation function\n        \"\"\"",
            "\"\"\"\n        {interpolator_can_select}\n        \"\"\"",
            "\"\"\"\n        {interpolator_select}\n        \"\"\"",
            "\"\"\"\n        {interpolator_can_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\""
        ],
        "code_snippets": [
            "class InterpolatorException(Exception):\n    \"\"\"\n    Custom label for interpolator exceptions\n    \"\"\"\n\n    pass\n\n\n@common_doc(COMMON_INTERPOLATOR_DOCS)",
            "class Interpolator(tl.HasTraits):\n    \"\"\"Interpolation Method\n\n    Attributes\n    ----------\n    {interpolator_attributes}\n\n    \"\"\"\n\n    # defined by implementing Interpolator class\n    methods_supported = tl.List(tl.Unicode())\n    dims_supported = tl.List(tl.Unicode())\n\n    # defined at instantiation\n    method = tl.Unicode()\n\n    # Next are used for optimizing the interpolation pipeline\n    # If -1, it's cost is assume the same as a competing interpolator in the\n    # stack, and the determination is made based on the number of DOF before\n    # and after each interpolation step.\n    # cost_func = tl.CFloat(-1)  # The rough cost FLOPS/DOF to do interpolation\n    # cost_setup = tl.CFloat(-1)  # The rough cost FLOPS/DOF to set up the interpolator",
            "def __init__(self, **kwargs):\n\n        # Call traitlets constructor\n        super(Interpolator, self).__init__(**kwargs)\n\n        # check method\n        if len(self.methods_supported) and self.method not in self.methods_supported:\n            raise InterpolatorException(\"Method {} is not supported by Interpolator {}\".format(self.method, self.name))\n        self.init()",
            "def __repr__(self):\n        return \"{} ({})\".format(self.name, self.method)\n\n    @property",
            "def name(self):\n        \"\"\"\n        Interpolator definition\n\n        Returns\n        -------\n        str\n            String name of interpolator.\n        \"\"\"\n        return str(self.__class__.__name__)\n\n    @property",
            "def definition(self):\n        \"\"\"\n        Interpolator definition\n\n        Returns\n        -------\n        str\n            String name of interpolator.\n        \"\"\"\n        return self.name",
            "def init(self):\n        \"\"\"\n        Overwrite this method if a Interpolator needs to do any\n        additional initialization after the standard initialization.\n        \"\"\"\n        pass",
            "def _filter_udims_supported(self, udims):\n\n        # find the intersection between dims_supported and udims, return tuple of intersection\n        return tuple(set(self.dims_supported) & set(udims))",
            "def _dim_in(self, dim, *coords, **kwargs):\n        \"\"\"Verify the dim exists on coordinates\n\n        Parameters\n        ----------\n        dim : str, list of str\n            Dimension or list of dimensions to verify\n        *coords :class:`podpac.Coordinates`\n            coordinates to evaluate\n        unstacked : bool, optional\n            True if you want to compare dimensions in unstacked form, otherwise compare dimensions however\n            they are defined on the DataSource. Defaults to False.\n\n        Returns\n        -------\n        Boolean\n            True if the dim is in all input coordinates\n        \"\"\"\n\n        unstacked = kwargs.pop(\"unstacked\", False)\n\n        if isinstance(dim, six.string_types):\n            dim = [dim]\n        elif not isinstance(dim, (list, tuple)):\n            raise ValueError(\"`dim` input must be a str, list of str, or tuple of str\")\n\n        for coord in coords:\n            for d in dim:\n                if (unstacked and d not in coord.udims) or (not unstacked and d not in coord.dims):\n                    return False\n\n        return True",
            "def _loop_helper(\n        self, func, interp_dims, udims, source_coordinates, source_data, eval_coordinates, output_data, **kwargs\n    ):\n        \"\"\"In cases where the interpolator can only handle a limited number of dimensions, loop over the extra ones\n        Parameters\n        ----------\n        func : callable\n            The interpolation function that should be called on the data subset. Should have the following arguments:\n            func(udims, source_coordinates, source_data, eval_coordinates, output_data)\n        interp_dims: list(str)\n            List of source dimensions that will be interpolator. The looped dimensions will be computed\n        udims: list(str)\n           The unstacked coordinates that this interpolator handles\n        source_coordinates: podpac.Coordinates\n            The coordinates of the source data\n        eval_coordinates: podpac.Coordinates\n            The user-requested or evaluated coordinates\n        output_data: podpac.UnitsDataArray\n            Container for the output of the interpolation function\n        \"\"\"\n        loop_dims = [d for d in source_data.dims if d not in interp_dims]\n        if not loop_dims:  # Do the actual interpolation\n            return func(udims, source_coordinates, source_data, eval_coordinates, output_data, **kwargs)\n\n        dim = loop_dims[0]\n        for i in output_data.coords[dim]:\n            idx = {dim: i}\n\n            if not i.isin(source_data.coords[dim]):\n                # This case should have been properly handled in the interpolation_manager\n                raise InterpolatorException(\"Unexpected interpolation error\")\n\n            output_data.loc[idx] = self._loop_helper(\n                func,\n                interp_dims,\n                udims,\n                source_coordinates.drop(dim),\n                source_data.loc[idx],\n                eval_coordinates.drop(dim),\n                output_data.loc[idx],\n                **kwargs\n            )\n        return output_data\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_can_select}\n        \"\"\"\n        if not (self.method in Selector.supported_methods):\n            return tuple()\n\n        udims_subset = self._filter_udims_supported(udims)\n        return udims_subset\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def select_coordinates(self, udims, source_coordinates, eval_coordinates, index_type=\"numpy\"):\n        \"\"\"\n        {interpolator_select}\n        \"\"\"\n        selector = Selector(method=self.method)\n        return selector.select(source_coordinates, eval_coordinates, index_type=index_type)\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_can_interpolate}\n        \"\"\"\n        return tuple()\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        raise NotImplementedError"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/nearest_neighbor_interpolator.py",
        "comments": [
            "// ncols"
        ],
        "docstrings": [
            "\"\"\"\nInterpolator implementations\n\"\"\"",
            "\"\"\"Nearest Neighbor Interpolation\n\n    {nearest_neighbor_attributes}\n    \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\"",
            "\"\"\"\n        {interpolator_interpolate}\n        \"\"\"",
            "\"\"\"Nearest Neighbor (Preview) Interpolation\n\n    {nearest_neighbor_attributes}\n    \"\"\"",
            "\"\"\"\n        {interpolator_can_select}\n        \"\"\"",
            "\"\"\"\n        {interpolator_select}\n        \"\"\""
        ],
        "code_snippets": [
            "class NearestNeighbor(Interpolator):\n    \"\"\"Nearest Neighbor Interpolation\n\n    {nearest_neighbor_attributes}\n    \"\"\"\n\n    dims_supported = [\"lat\", \"lon\", \"alt\", \"time\"]\n    methods_supported = [\"nearest\"]\n\n    # defined at instantiation\n    method = tl.Unicode(default_value=\"nearest\")\n    spatial_tolerance = tl.Float(default_value=np.inf, allow_none=True)\n    time_tolerance = tl.Union([tl.Unicode(), tl.Instance(np.timedelta64, allow_none=True)])\n    alt_tolerance = tl.Float(default_value=np.inf, allow_none=True)\n\n    # spatial_scale only applies when the source is stacked with time or alt. The supplied value will be assigned a distance of \"1'\"\n    spatial_scale = tl.Float(default_value=1, allow_none=True)\n    # time_scale only applies when the source is stacked with lat, lon, or alt. The supplied value will be assigned a distance of \"1'\"\n    time_scale = tl.Union([tl.Unicode(), tl.Instance(np.timedelta64, allow_none=True)])\n    # alt_scale only applies when the source is stacked with lat, lon, or time. The supplied value will be assigned a distance of \"1'\"\n    alt_scale = tl.Float(default_value=1, allow_none=True)\n\n    respect_bounds = tl.Bool(True)\n    remove_nan = tl.Bool(False)\n    use_selector = tl.Bool(True)",
            "def __repr__(self):\n        rep = super(NearestNeighbor, self).__repr__()\n        # rep += '\\n\\tspatial_tolerance: {}\\n\\ttime_tolerance: {}'.format(self.spatial_tolerance, self.time_tolerance)\n        return rep\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        udims_subset = self._filter_udims_supported(udims)\n\n        return udims_subset",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n        selector = super().can_select(udims, source_coordinates, eval_coordinates)\n        if self.use_selector:\n            return selector\n        return ()\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n        \"\"\"\n        {interpolator_interpolate}\n        \"\"\"\n        # Note, some of the following code duplicates code in the Selector class.\n        # This duplication is for the sake of optimization",
            "def is_stacked(d):\n            return \"_\" in d\n\n        if hasattr(source_data, \"attrs\") and \"bounds\" in source_data.attrs:\n            bounds = source_data.attrs[\"bounds\"]\n            if \"time\" in bounds and bounds[\"time\"]:\n                if \"time\" in eval_coordinates.udims:\n                    bounds[\"time\"] = [\n                        self._atime_to_float(b, source_coordinates[\"time\"], eval_coordinates[\"time\"])\n                        for b in bounds[\"time\"]\n                    ]\n                else:\n                    bounds[\"time\"] = [\n                        self._atime_to_float(b, source_coordinates[\"time\"], source_coordinates[\"time\"])\n                        for b in bounds[\"time\"]\n                    ]\n\n        else:\n            bounds = None\n\n        if self.remove_nan:\n            # Eliminate nans from the source data. Note, this could turn a uniform griddted dataset into a stacked one\n            source_data, source_coordinates = self._remove_nans(source_data, source_coordinates)\n\n        data_index = []\n        for d in source_coordinates.dims:\n            # Make sure we're supposed to do nearest neighbor interpolation for this UDIM, otherwise skip this dimension\n            if len([dd for dd in d.split(\"_\") if dd in udims]) == 0:\n                index = self._resize_unstacked_index(np.arange(source_coordinates[d].size), d, eval_coordinates)\n                data_index.append(index)\n                continue\n            source = source_coordinates[d]\n            if is_stacked(d):\n                if bounds is not None:\n                    bound = np.stack([bounds[dd] for dd in d.split(\"_\")], axis=1)\n                else:\n                    bound = None\n                index = self._get_stacked_index(d, source, eval_coordinates, bound)\n\n                if len(source.shape) == 2:  # Handle case of 2D-stacked coordinates\n                    ncols = source.shape[1]\n                    index1 = index",
            "def _remove_nans(self, source_data, source_coordinates):\n        index = np.array(np.isnan(source_data), bool)\n        if not np.any(index):\n            return source_data, source_coordinates\n\n        data = source_data.data[~index]\n        coords = np.meshgrid(\n            *[source_coordinates[d.split(\"_\")[0]].coordinates for d in source_coordinates.dims], indexing=\"ij\"\n        )\n        repeat_shape = coords[0].shape\n        coords = [c[~index] for c in coords]\n\n        final_dims = [d.split(\"_\")[0] for d in source_coordinates.dims]\n        # Add back in any stacked coordinates\n        for i, d in enumerate(source_coordinates.dims):\n            dims = d.split(\"_\")\n            if len(dims) == 1:\n                continue\n            reshape = np.ones(len(coords), int)\n            reshape[i] = -1\n            repeats = list(repeat_shape)\n            repeats[i] = 1\n            for dd in dims[1:]:\n                crds = source_coordinates[dd].coordinates.reshape(*reshape)\n                for j, r in enumerate(repeats):\n                    crds = crds.repeat(r, axis=j)\n                coords.append(crds[~index])\n                final_dims.append(dd)\n\n        return data, Coordinates([coords], dims=[final_dims])",
            "def _get_tol(self, dim, source, request):\n        if dim in [\"lat\", \"lon\"]:\n            return self.spatial_tolerance\n        if dim == \"alt\":\n            return self.alt_tolerance\n        if dim == \"time\":\n            if self.time_tolerance == \"\":\n                return np.inf\n            return self._time_to_float(self.time_tolerance, source, request)\n        raise NotImplementedError()",
            "def _get_scale(self, dim, source_1d, request_1d):\n        if dim in [\"lat\", \"lon\"]:\n            return 1 / self.spatial_scale\n        if dim == \"alt\":\n            return 1 / self.alt_scale\n        if dim == \"time\":\n            if self.time_scale == \"\":\n                return 1.0\n            return 1 / self._time_to_float(self.time_scale, source_1d, request_1d)\n        raise NotImplementedError()",
            "def _time_to_float(self, time, time_source, time_request):\n        dtype0 = time_source.coordinates[0].dtype\n        dtype1 = time_request.coordinates[0].dtype\n        dtype = dtype0 if dtype0 > dtype1 else dtype1\n        time = make_coord_delta(time)\n        if isinstance(time, np.timedelta64):\n            time1 = (time + np.datetime64(\"2000\")).astype(dtype).astype(float) - (\n                np.datetime64(\"2000\").astype(dtype).astype(float)\n            )\n        return time1",
            "def _atime_to_float(self, time, time_source, time_request):\n        dtype0 = time_source.coordinates[0].dtype\n        dtype1 = time_request.coordinates[0].dtype\n        dtype = dtype0 if dtype0 > dtype1 else dtype1\n        time = make_coord_value(time)\n        if isinstance(time, np.datetime64):\n            time = time.astype(dtype).astype(float)\n        return time",
            "def _get_stacked_index(self, dim, source, request, bounds=None):\n        # The udims are in the order of the request so that the meshgrid calls will be in the right order\n        udims = [ud for ud in request.udims if ud in source.udims]\n\n        time_source = time_request = None\n        if \"time\" in udims:\n            time_source = source[\"time\"]\n            time_request = request[\"time\"]\n\n        tols = np.array([self._get_tol(d, time_source, time_request) for d in udims])[None, :]\n        scales = np.array([self._get_scale(d, time_source, time_request) for d in udims])[None, :]\n        tol = np.linalg.norm((tols * scales).squeeze())\n        src_coords, req_coords_diag = _higher_precision_time_stack(source, request, udims)\n        # We need to unwravel the nD stacked coordinates\n        ckdtree_source = cKDTree(src_coords.reshape(src_coords.shape[0], -1).T * scales)\n\n        # if the udims are all stacked in the same stack as part of the request coordinates, then we're done.\n        # Otherwise we have to evaluate each unstacked set of dimensions independently\n        # Note, part of this code is duplicated in the selector\n        indep_evals = [ud for ud in udims if not request.is_stacked(ud)]\n        # two udims could be stacked, but in different dim groups, e.g. source (lat, lon), request (lat, time), (lon, alt)\n        stacked = {d for d in request.dims for ud in udims if ud in d and request.is_stacked(ud)}\n\n        if (len(indep_evals) + len(stacked)) <= 1:  # output is stacked in the same way\n            # The ckdtree call below needs the lat/lon pairs in the last axis position\n            req_coords = np.moveaxis(req_coords_diag, 0, -1)\n        elif (len(stacked) == 0) | (len(indep_evals) == 0 and len(stacked) == len(udims)):\n            req_coords = np.stack([i.ravel() for i in np.meshgrid(*req_coords_diag, indexing=\"ij\")], axis=1)\n        else:\n            # Rare cases? E.g. lat_lon_time_alt source to lon, time_alt, lat destination\n            sizes = [request[d].size for d in request.dims]\n            reshape = np.ones(len(request.dims), int)\n            coords = [None] * len(udims)\n            for i in range(len(udims)):\n                ii = [ii for ii in range(len(request.dims)) if udims[i] in request.dims[ii]][0]\n                reshape[:] = 1\n                reshape[ii] = -1\n                coords[i] = req_coords_diag[i].reshape(*reshape)\n                for j, d in enumerate(request.dims):\n                    if udims[i] in d:  # Then we don't need to repeat\n                        continue\n                    coords[i] = coords[i].repeat(sizes[j], axis=j)\n            req_coords = np.stack([i.ravel() for i in coords], axis=1)\n\n        dist, index = ckdtree_source.query(req_coords * scales, k=1)\n\n        if self.respect_bounds:\n            if bounds is None:\n                bounds = np.stack(\n                    [\n                        src_coords.reshape(src_coords.shape[0], -1).T.min(0),\n                        src_coords.reshape(src_coords.shape[0], -1).T.max(0),\n                    ],\n                    axis=1,\n                )\n            # Fix order of bounds\n            bounds = bounds[:, [source.udims.index(dim) for dim in udims]]\n            index[np.any((req_coords > bounds[1]), axis=-1) | np.any((req_coords < bounds[0]), axis=-1)] = -1\n\n        if tol and tol != np.inf:\n            index[dist > tol] = -1\n\n        return index",
            "def _get_uniform_index(self, dim, source, request, bounds=None):\n        tol = self._get_tol(dim, source, request)\n\n        index = (request.coordinates - source.start) / source.step\n        rindex = np.around(index).astype(int)\n        stop_ind = int(source.size)\n        if self.respect_bounds:\n            rindex[(rindex < 0) | (rindex >= stop_ind)] = -1\n        else:\n            rindex = np.clip(rindex, 0, stop_ind - 1)\n        if tol and tol != np.inf:\n            if dim == \"time\":\n                step = self._time_to_float(source.step, source, request)\n            else:\n                step = source.step\n            rindex[np.abs(index - rindex) * np.abs(step) > tol] = -1\n\n        return rindex",
            "def _get_nonuniform_index(self, dim, source, request, bounds=None):\n        tol = self._get_tol(dim, source, request)\n\n        src, req = _higher_precision_time_coords1d(source, request)\n        ckdtree_source = cKDTree(src.reshape(-1, 1))\n        dist, index = ckdtree_source.query(req[:].reshape(-1, 1), k=1)\n        index[index == source.coordinates.size] = -1\n\n        if self.respect_bounds:\n            if bounds is None:\n                bounds = [src.min(), src.max()]\n            index[(req.ravel() > bounds[1]) | (req.ravel() < bounds[0])] = -1\n        if tol and tol != np.inf:\n            index[dist > tol] = -1\n\n        return index",
            "def _resize_unstacked_index(self, index, source_dim, request):\n        # When the request is stacked, and the stacked dimensions are n-dimensions where n > 1,\n        # Then len(request.shape) != len(request.dims), so it take s a little bit of footwork\n        # to get the correct shape for the index\n        reshape = np.array(request.shape)\n        i = 0\n        for dim in request.dims:\n            addnext = len(request[dim].shape)\n            if source_dim not in dim:\n                reshape[i : i + addnext] = 1\n            i += addnext\n        return index.reshape(*reshape)",
            "def _resize_stacked_index(self, index, source_dim, request):\n        reshape = np.array(request.shape)\n        i = 0\n        for dim in request.dims:\n            addnext = len(request[dim].shape)\n            d = dim.split(\"_\")\n            if not any([dd in source_dim for dd in d]):\n                reshape[i : i + addnext] = 1\n            i += addnext\n        return index.reshape(*reshape)\n\n\n@common_doc(COMMON_INTERPOLATOR_DOCS)",
            "class NearestPreview(NearestNeighbor):\n    \"\"\"Nearest Neighbor (Preview) Interpolation\n\n    {nearest_neighbor_attributes}\n    \"\"\"\n\n    methods_supported = [\"nearest_preview\"]\n    method = tl.Unicode(default_value=\"nearest_preview\")\n    spatial_tolerance = tl.Float(read_only=True, allow_none=True, default_value=None)\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n        \"\"\"\n        {interpolator_can_select}\n        \"\"\"\n        udims_subset = self._filter_udims_supported(udims)\n\n        # confirm that udims are in source and eval coordinates\n        # TODO: handle stacked coordinates\n        if self._dim_in(udims_subset, source_coordinates):\n            return udims_subset\n        else:\n            return tuple()\n\n    @common_doc(COMMON_INTERPOLATOR_DOCS)",
            "def select_coordinates(self, udims, source_coordinates, eval_coordinates, index_type=\"numpy\"):\n        \"\"\"\n        {interpolator_select}\n        \"\"\"\n        new_coords = []\n        new_coords_idx = []\n\n        source_coords, source_coords_index = source_coordinates.intersect(\n            eval_coordinates, outer=True, return_index=True\n        )\n\n        if source_coords.size == 0:\n            return source_coords, source_coords_index\n\n        # iterate over the source coordinate dims in case they are stacked\n        for src_dim, idx in zip(source_coords, source_coords_index):\n\n            # TODO: handle stacked coordinates\n            if isinstance(source_coords[src_dim], StackedCoordinates):\n                raise InterpolatorException(\"NearestPreview select does not yet support stacked dimensions\")\n\n            if src_dim in eval_coordinates.dims:\n                src_coords = source_coords[src_dim]\n                dst_coords = eval_coordinates[src_dim]\n\n                if isinstance(dst_coords, UniformCoordinates1d):\n                    dst_start = dst_coords.start\n                    dst_stop = dst_coords.stop\n                    dst_delta = dst_coords.step\n                else:\n                    dst_start = dst_coords.coordinates[0]\n                    dst_stop = dst_coords.coordinates[-1]\n                    with np.errstate(invalid=\"ignore\"):\n                        dst_delta = (dst_stop - dst_start) / (dst_coords.size - 1)\n\n                if isinstance(src_coords, UniformCoordinates1d):\n                    src_start = src_coords.start\n                    src_stop = src_coords.stop\n                    src_delta = src_coords.step\n                else:\n                    src_start = src_coords.coordinates[0]\n                    src_stop = src_coords.coordinates[-1]\n                    with np.errstate(invalid=\"ignore\"):\n                        src_delta = (src_stop - src_start) / (src_coords.size - 1)\n\n                ndelta = max(1, np.round(np.abs(dst_delta / src_delta)))\n                idx_offset = 0\n                if src_coords.size == 1:\n                    c = src_coords.copy()\n                else:\n                    c_test = UniformCoordinates1d(src_start, src_stop, ndelta * src_delta, **src_coords.properties)\n                    bounds = source_coordinates[src_dim].bounds\n                    # The delta/2 ensures the endpoint is included when there is a floating point rounding error\n                    # the delta/2 is more than needed, but does guarantee.\n                    src_stop = np.clip(src_stop + ndelta * src_delta / 2, bounds[0], bounds[1])\n                    c = UniformCoordinates1d(src_start, src_stop, ndelta * src_delta, **src_coords.properties)\n                    if c.size > c_test.size:  # need to adjust the index as well\n                        idx_offset = int(ndelta)\n\n                idx_start = idx.start if isinstance(idx, slice) else idx[0]\n                idx_stop = idx.stop if isinstance(idx, slice) else idx[-1]\n                if idx_stop is not None:\n                    idx_stop += idx_offset\n                idx = slice(idx_start, idx_stop, int(ndelta))\n            else:\n                c = source_coords[src_dim]\n\n            new_coords.append(c)\n            new_coords_idx.append(idx)\n\n        return Coordinates(new_coords, validate_crs=False), tuple(new_coords_idx)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/interpolation.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Node to used to interpolate from self.source.coordinates to the user-specified, evaluated coordinates.\n\n    Parameters\n    ----------\n    source : Any\n        The source node which will be interpolated\n    interpolation : str, dict, optional\n        Interpolation definition for the data source.\n        By default, the interpolation method is set to `podpac.settings[\"DEFAULT_INTERPOLATION\"]` which defaults to ``'nearest'`` for all dimensions.\n\n         If input is a string, it must match one of the interpolation shortcuts defined in\n        :attr:`podpac.data.INTERPOLATION_SHORTCUTS`. The interpolation method associated\n        with this string will be applied to all dimensions at the same time.\n\n        If input is a dict or list of dict, the dict or dict elements must adhere to the following format:\n\n        The key ``'method'`` defining the interpolation method name.\n        If the interpolation method is not one of :attr:`podpac.data.INTERPOLATION_SHORTCUTS`, a\n        second key ``'interpolators'`` must be defined with a list of\n        :class:`podpac.interpolators.Interpolator` classes to use in order of uages.\n        The dictionary may contain an option ``'params'`` key which contains a dict of parameters to pass along to\n        the :class:`podpac.interpolators.Interpolator` classes associated with the interpolation method.\n\n        The dict may contain the key ``'dims'`` which specifies dimension names (i.e. ``'time'`` or ``('lat', 'lon')`` ).\n        If the dictionary does not contain a key for all unstacked dimensions of the source coordinates, the\n        :attr:`podpac.data.INTERPOLATION_DEFAULT` value will be used.\n        All dimension keys must be unstacked even if the underlying coordinate dimensions are stacked.\n        Any extra dimensions included but not found in the source coordinates will be ignored.\n\n        The dict may contain a key ``'params'`` that can be used to configure the :class:`podpac.interpolators.Interpolator` classes associated with the interpolation method.\n\n        If input is a :class:`podpac.data.Interpolation` class, this Interpolation\n        class will be used without modification.\n    cache_output : bool\n        Should the node's output be cached? If not provided or None, uses default based on\n        settings[\"CACHE_DATASOURCE_OUTPUT_DEFAULT\"]. If True, outputs will be cached and retrieved from cache. If False,\n        outputs will not be cached OR retrieved from cache (even if they exist in cache).\n\n    Examples\n    -----\n    # To use bilinear interpolation for [lat,lon]  a specific interpolator for [time], and the default for [alt], use:\n    >>> interp_node = Interpolation(\n            source=some_node,\n            interpolation=interpolation = [\n                {\n                'method': 'bilinear',\n                'dims': ['lat', 'lon']\n                },\n                {\n                'method': [podpac.interpolators.NearestNeighbor],\n                'dims': ['time']\n                }\n            ]\n        )\n\n    \"\"\"",
            "\"\"\"Get the interpolation class currently set for this data source.\n\n        The DataSource ``interpolation`` property is used to define the\n        :class:`podpac.data.InterpolationManager` class that will handle interpolation for requested coordinates.\n\n        Returns\n        -------\n        :class:`podpac.data.InterpolationManager`\n            InterpolationManager class defined by DataSource `interpolation` definition\n        \"\"\"",
            "\"\"\"Return the interpolators selected for the previous node evaluation interpolation.\n        If the node has not been evaluated, or if interpolation was not necessary, this will return\n        an empty OrderedDict\n\n        Returns\n        -------\n        OrderedDict\n            Key are tuple of unstacked dimensions, the value is the interpolator used to interpolate these dimensions\n        \"\"\"",
            "\"\"\"Update _interpolation property\"\"\"",
            "\"\"\"Evaluates this node using the supplied coordinates.\n\n        The coordinates are mapped to the requested coordinates, interpolated if necessary, and set to\n        `_requested_source_coordinates` with associated index `_requested_source_coordinates_index`. The requested\n        source coordinates and index are passed to `get_data()` returning the source data at the\n        coordinatesset to `_requested_source_data`. Finally `_requested_source_data` is interpolated\n        using the `interpolate` method and set to the `output` attribute of the node.\n\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n\n            An exception is raised if the requested coordinates are missing dimensions in the DataSource.\n            Extra dimensions in the requested coordinates are dropped.\n        output : :class:`podpac.UnitsDataArray`, optional\n            {eval_output}\n        _selector :\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            Cannot evaluate these coordinates\n        \"\"\"",
            "\"\"\"\n        Get the available coordinates for the Node. For a DataSource, this is just the coordinates.\n\n        Returns\n        -------\n        coords_list : list\n            singleton list containing the coordinates (Coordinates object)\n        \"\"\"",
            "\"\"\"Get the full available coordinate bounds for the Node.\n\n        Arguments\n        ---------\n        crs : str\n            Desired CRS for the bounds. Use 'source' to use the native source crs.\n            If not specified, the default CRS in the podpac settings is used. Optional.\n\n        Returns\n        -------\n        bounds : dict\n            Bounds for each dimension. Keys are dimension names and values are tuples (hi, lo).\n        crs : str\n            The crs for the bounds.\n        \"\"\""
        ],
        "code_snippets": [
            "class InterpolationMixin(tl.HasTraits):\n    # interpolation = InterpolationTrait().tag(attr=True, required=False, default = \"nearesttt\")\n    interpolation = InterpolationTrait().tag(attr=True)\n\n    _interp_node = None\n\n    @property",
            "def _repr_keys(self):\n        return super()._repr_keys + [\"interpolation\"]",
            "def _eval(self, coordinates, output=None, _selector=None):\n        node = Interpolate(\n            interpolation=self.interpolation,\n            source_id=self.hash,\n            force_eval=True,\n            cache_ctrl=CacheCtrl([]),\n            style=self.style,\n        )\n        node._set_interpolation()\n        selector = node._interpolation.select_coordinates\n        node._source_xr = super()._eval(coordinates, _selector=selector)\n        self._interp_node = node\n        if isinstance(self, DataSource):\n            # This is required to ensure that the output coordinates\n            # match the requested coordinates to floating point precision\n            r = node.eval(self._requested_coordinates, output=output)\n        else:\n            r = node.eval(coordinates, output=output)\n        # Helpful for debugging\n        self._from_cache = node._from_cache\n        return r",
            "class Interpolate(Node):\n    \"\"\"Node to used to interpolate from self.source.coordinates to the user-specified, evaluated coordinates.\n\n    Parameters\n    ----------\n    source : Any\n        The source node which will be interpolated\n    interpolation : str, dict, optional\n        Interpolation definition for the data source.\n        By default, the interpolation method is set to `podpac.settings[\"DEFAULT_INTERPOLATION\"]` which defaults to ``'nearest'`` for all dimensions.\n\n         If input is a string, it must match one of the interpolation shortcuts defined in\n        :attr:`podpac.data.INTERPOLATION_SHORTCUTS`. The interpolation method associated\n        with this string will be applied to all dimensions at the same time.\n\n        If input is a dict or list of dict, the dict or dict elements must adhere to the following format:\n\n        The key ``'method'`` defining the interpolation method name.\n        If the interpolation method is not one of :attr:`podpac.data.INTERPOLATION_SHORTCUTS`, a\n        second key ``'interpolators'`` must be defined with a list of\n        :class:`podpac.interpolators.Interpolator` classes to use in order of uages.\n        The dictionary may contain an option ``'params'`` key which contains a dict of parameters to pass along to\n        the :class:`podpac.interpolators.Interpolator` classes associated with the interpolation method.\n\n        The dict may contain the key ``'dims'`` which specifies dimension names (i.e. ``'time'`` or ``('lat', 'lon')`` ).\n        If the dictionary does not contain a key for all unstacked dimensions of the source coordinates, the\n        :attr:`podpac.data.INTERPOLATION_DEFAULT` value will be used.\n        All dimension keys must be unstacked even if the underlying coordinate dimensions are stacked.\n        Any extra dimensions included but not found in the source coordinates will be ignored.\n\n        The dict may contain a key ``'params'`` that can be used to configure the :class:`podpac.interpolators.Interpolator` classes associated with the interpolation method.\n\n        If input is a :class:`podpac.data.Interpolation` class, this Interpolation\n        class will be used without modification.\n    cache_output : bool\n        Should the node's output be cached? If not provided or None, uses default based on\n        settings[\"CACHE_DATASOURCE_OUTPUT_DEFAULT\"]. If True, outputs will be cached and retrieved from cache. If False,\n        outputs will not be cached OR retrieved from cache (even if they exist in cache).\n\n    Examples\n    -----\n    # To use bilinear interpolation for [lat,lon]  a specific interpolator for [time], and the default for [alt], use:\n    >>> interp_node = Interpolation(\n            source=some_node,\n            interpolation=interpolation = [\n                {\n                'method': 'bilinear',\n                'dims': ['lat', 'lon']\n                },\n                {\n                'method': [podpac.interpolators.NearestNeighbor],\n                'dims': ['time']\n                }\n            ]\n        )\n\n    \"\"\"\n\n    source = NodeTrait(allow_none=True).tag(attr=True, required=True)\n    source_id = tl.Unicode(allow_none=True).tag(attr=True)\n    _source_xr = tl.Instance(UnitsDataArray, allow_none=True)  # This is needed for the Interpolation Mixin\n\n    interpolation = InterpolationTrait().tag(attr=True)\n    cache_output = tl.Bool()\n\n    # privates\n    _interpolation = tl.Instance(InterpolationManager)\n    _coordinates = tl.Instance(Coordinates, allow_none=True, default_value=None, read_only=True)\n\n    _requested_source_coordinates = tl.Instance(Coordinates)\n    _requested_source_coordinates_index = tl.Tuple()\n    _requested_source_data = tl.Instance(UnitsDataArray)\n    _evaluated_coordinates = tl.Instance(Coordinates)\n\n    @tl.default(\"style\")",
            "def _default_style(self):  # Pass through source style by default\n        if self.source is not None:\n            return self.source.style\n        else:\n            return super()._default_style()\n\n    # this adds a more helpful error message if user happens to try an inspect _interpolation before evaluate\n    @tl.default(\"_interpolation\")",
            "def _default_interpolation(self):\n        self._set_interpolation()\n        return self._interpolation\n\n    @tl.default(\"cache_output\")",
            "def _cache_output_default(self):\n        return settings[\"CACHE_NODE_OUTPUT_DEFAULT\"]\n\n    @tl.default(\"units\")",
            "def _use_source_units(self):\n        return getattr(self.source, \"units\", None)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def interpolation_class(self):\n        \"\"\"Get the interpolation class currently set for this data source.\n\n        The DataSource ``interpolation`` property is used to define the\n        :class:`podpac.data.InterpolationManager` class that will handle interpolation for requested coordinates.\n\n        Returns\n        -------\n        :class:`podpac.data.InterpolationManager`\n            InterpolationManager class defined by DataSource `interpolation` definition\n        \"\"\"\n\n        return self._interpolation\n\n    @property",
            "def interpolators(self):\n        \"\"\"Return the interpolators selected for the previous node evaluation interpolation.\n        If the node has not been evaluated, or if interpolation was not necessary, this will return\n        an empty OrderedDict\n\n        Returns\n        -------\n        OrderedDict\n            Key are tuple of unstacked dimensions, the value is the interpolator used to interpolate these dimensions\n        \"\"\"\n\n        if self._interpolation._last_interpolator_queue is not None:\n            return self._interpolation._last_interpolator_queue\n        else:\n            return OrderedDict()",
            "def _set_interpolation(self):",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this node using the supplied coordinates.\n\n        The coordinates are mapped to the requested coordinates, interpolated if necessary, and set to\n        `_requested_source_coordinates` with associated index `_requested_source_coordinates_index`. The requested\n        source coordinates and index are passed to `get_data()` returning the source data at the\n        coordinatesset to `_requested_source_data`. Finally `_requested_source_data` is interpolated\n        using the `interpolate` method and set to the `output` attribute of the node.\n\n\n        Parameters\n        ----------\n        coordinates : :class:`podpac.Coordinates`\n            {requested_coordinates}\n\n            An exception is raised if the requested coordinates are missing dimensions in the DataSource.\n            Extra dimensions in the requested coordinates are dropped.\n        output : :class:`podpac.UnitsDataArray`, optional\n            {eval_output}\n        _selector :\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            Cannot evaluate these coordinates\n        \"\"\"\n\n        _logger.debug(\"Evaluating {} data source\".format(self.__class__.__name__))\n\n        # store requested coordinates for debugging\n        if settings[\"DEBUG\"]:\n            self._original_requested_coordinates = coordinates\n\n        # store input coordinates to evaluated coordinates\n        self._evaluated_coordinates = deepcopy(coordinates)\n\n        # reset interpolation\n        self._set_interpolation()\n\n        selector = self._interpolation.select_coordinates\n\n        source_out = self._source_eval(self._evaluated_coordinates, selector)\n        source_coords = Coordinates.from_xarray(source_out)\n\n        # Drop extra coordinates\n        extra_dims = [d for d in coordinates.udims if d not in source_coords.udims]\n        coordinates = coordinates.udrop(extra_dims)\n\n        # Transform so that interpolation happens on the source data coordinate system\n        if source_coords.crs.lower() != coordinates.crs.lower():\n            coordinates = coordinates.transform(source_coords.crs)\n\n        # Fix source coordinates in the case where some dimension are not being interpolated\n        coordinates = self._interpolation._fix_coordinates_for_none_interp(coordinates, source_coords)\n\n        if output is None:\n            if \"output\" in source_out.dims:\n                self.set_trait(\"outputs\", source_out.coords[\"output\"].data.tolist())\n            output = self.create_output_array(coordinates)\n\n        if source_out.size == 0:  # short cut\n            return output\n\n        # interpolate data into output\n        output = self._interpolation.interpolate(source_coords, source_out, coordinates, output)\n\n        # if requested crs is differented than coordinates,\n        # fabricate a new output with the original coordinates and new values\n        if self._evaluated_coordinates.crs != coordinates.crs:\n            output = self.create_output_array(self._evaluated_coordinates.drop(extra_dims), data=output[:].values)\n\n        # save output to private for debugging\n        if settings[\"DEBUG\"]:\n            self._output = output\n            self._source_xr = source_out\n\n        return output",
            "def _source_eval(self, coordinates, selector, output=None):\n        if isinstance(self._source_xr, UnitsDataArray):\n            return self._source_xr\n        else:\n            return self.source.eval(coordinates, output=output, _selector=selector)",
            "def find_coordinates(self):\n        \"\"\"\n        Get the available coordinates for the Node. For a DataSource, this is just the coordinates.\n\n        Returns\n        -------\n        coords_list : list\n            singleton list containing the coordinates (Coordinates object)\n        \"\"\"\n\n        return self.source.find_coordinates()",
            "def get_bounds(self, crs=\"default\"):\n        \"\"\"Get the full available coordinate bounds for the Node.\n\n        Arguments\n        ---------\n        crs : str\n            Desired CRS for the bounds. Use 'source' to use the native source crs.\n            If not specified, the default CRS in the podpac settings is used. Optional.\n\n        Returns\n        -------\n        bounds : dict\n            Bounds for each dimension. Keys are dimension names and values are tuples (hi, lo).\n        crs : str\n            The crs for the bounds.\n        \"\"\"\n\n        return self.source.get_bounds(crs=crs)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/test/test_interpolator.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nTest interpolation methods\n\n\n\"\"\"",
            "\"\"\"Test abstract interpolator class\"\"\""
        ],
        "code_snippets": [
            "class TestInterpolator(object):",
            "def test_can_select(self):",
            "class CanAlwaysSelect(Interpolator):",
            "def can_select(self, udims, reqcoords, srccoords):\n                return udims",
            "class CanNeverSelect(Interpolator):",
            "def can_select(self, udims, reqcoords, srccoords):\n                return tuple()\n\n        interp = CanAlwaysSelect(method=\"method\")\n        can_select = interp.can_select((\"time\", \"lat\"), None, None)\n        assert \"lat\" in can_select and \"time\" in can_select\n\n        interp = CanNeverSelect(method=\"method\")\n        can_select = interp.can_select((\"time\", \"lat\"), None, None)\n        assert not can_select",
            "def test_dim_in(self):\n        interpolator = Interpolator(methods_supported=[\"test\"], method=\"test\")\n\n        coords = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n        assert interpolator._dim_in(\"lat\", coords)\n        assert interpolator._dim_in(\"lat\", coords, unstacked=True)\n        assert not interpolator._dim_in(\"time\", coords)\n\n        coords_two = Coordinates([clinspace(0, 10, 5)], dims=[\"lat\"])\n        assert interpolator._dim_in(\"lat\", coords, coords_two)\n        assert not interpolator._dim_in(\"lon\", coords, coords_two)\n\n        coords_three = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lat_lon\"])\n        assert not interpolator._dim_in(\"lat\", coords, coords_two, coords_three)\n        assert interpolator._dim_in(\"lat\", coords, coords_two, coords_three, unstacked=True)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/test/test_interpolation_manager.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nTest interpolation methods\n\n\n\"\"\"",
            "\"\"\"Test interpolation class and support methods\"\"\"",
            "\"\"\"TODO: Allow user to be missing rasterio and scipy\"\"\"",
            "\"\"\"test constructor\"\"\""
        ],
        "code_snippets": [
            "class TestInterpolation(object):",
            "def test_allow_missing_modules(self):",
            "def test_interpolation_methods(self):\n        assert len(set(INTERPOLATION_METHODS) & set(INTERPOLATION_METHODS_DICT.keys())) == len(INTERPOLATION_METHODS)",
            "def test_interpolator_init_type(self):",
            "def test_str_definition(self):\n        # should throw an error if string input is not one of the INTERPOLATION_METHODS\n        with pytest.raises(InterpolationException):\n            InterpolationManager(\"test\")\n\n        interp = InterpolationManager(\"nearest\")\n        assert interp.config[(\"default\",)]\n        assert isinstance(interp.config[(\"default\",)], dict)\n        assert interp.config[(\"default\",)][\"method\"] == \"nearest\"\n        assert isinstance(interp.config[(\"default\",)][\"interpolators\"][0], Interpolator)",
            "def test_dict_definition(self):\n\n        # should handle a default definition without any dimensions\n        interp = InterpolationManager({\"method\": \"nearest\", \"params\": {\"spatial_tolerance\": 1}})\n        assert isinstance(interp.config[(\"default\",)], dict)\n        assert interp.config[(\"default\",)][\"method\"] == \"nearest\"\n        assert isinstance(interp.config[(\"default\",)][\"interpolators\"][0], Interpolator)\n        assert interp.config[(\"default\",)][\"params\"] == {\"spatial_tolerance\": 1}\n\n        # handle string methods\n        interp = InterpolationManager({\"method\": \"nearest\", \"dims\": [\"lat\", \"lon\"]})\n        print(interp.config)\n        assert isinstance(interp.config[(\"lat\", \"lon\")], dict)\n        assert interp.config[(\"lat\", \"lon\")][\"method\"] == \"nearest\"\n        assert isinstance(interp.config[list(interp.config.keys())[-1]][\"interpolators\"][0], Interpolator)\n        assert interp.config[list(interp.config.keys())[-1]][\"params\"] == {}\n\n        # handle dict methods\n\n        # should throw an error if method is not in dict\n        with pytest.raises(InterpolationException):\n            InterpolationManager([{\"test\": \"test\", \"dims\": [\"lat\", \"lon\"]}])\n\n        # should throw an error if method is not a string\n        with pytest.raises(InterpolationException):\n            InterpolationManager([{\"method\": 5, \"dims\": [\"lat\", \"lon\"]}])\n\n        # should throw an error if method is not one of the INTERPOLATION_METHODS and no interpolators defined\n        with pytest.raises(InterpolationException):\n            InterpolationManager([{\"method\": \"myinter\", \"dims\": [\"lat\", \"lon\"]}])\n\n        # should throw an error if params is not a dict\n        with pytest.raises(TypeError):\n            InterpolationManager([{\"method\": \"nearest\", \"dims\": [\"lat\", \"lon\"], \"params\": \"test\"}])\n\n        # should throw an error if interpolators is not a list\n        with pytest.raises(TypeError):\n            InterpolationManager([{\"method\": \"nearest\", \"interpolators\": \"test\", \"dims\": [\"lat\", \"lon\"]}])\n\n        # should throw an error if interpolators are not Interpolator classes\n        with pytest.raises(TypeError):\n            InterpolationManager(\n                [{\"method\": \"nearest\", \"interpolators\": [NearestNeighbor, \"test\"], \"dims\": [\"lat\", \"lon\"]}]\n            )\n\n        # should throw an error if dimension is defined twice\n        with pytest.raises(InterpolationException):\n            InterpolationManager(\n                [{\"method\": \"nearest\", \"dims\": [\"lat\", \"lon\"]}, {\"method\": \"bilinear\", \"dims\": [\"lat\"]}]\n            )\n\n        # should throw an error if dimension is not a list\n        with pytest.raises(TypeError):\n            InterpolationManager([{\"method\": \"nearest\", \"dims\": \"lat\"}])\n\n        # should handle standard INTEPROLATION_SHORTCUTS\n        interp = InterpolationManager([{\"method\": \"nearest\", \"dims\": [\"lat\", \"lon\"]}])\n        assert isinstance(interp.config[(\"lat\", \"lon\")], dict)\n        assert interp.config[(\"lat\", \"lon\")][\"method\"] == \"nearest\"\n        assert isinstance(interp.config[(\"lat\", \"lon\")][\"interpolators\"][0], Interpolator)\n        assert interp.config[(\"lat\", \"lon\")][\"params\"] == {}\n\n        # should not allow custom methods if interpolators can't support\n        with pytest.raises(InterpolatorException):\n            interp = InterpolationManager(\n                [{\"method\": \"myinter\", \"interpolators\": [NearestNeighbor, NearestPreview], \"dims\": [\"lat\", \"lon\"]}]\n            )\n\n        # should allow custom methods if interpolators can support",
            "class MyInterp(Interpolator):\n            methods_supported = [\"myinter\"]\n\n        interp = InterpolationManager([{\"method\": \"myinter\", \"interpolators\": [MyInterp], \"dims\": [\"lat\", \"lon\"]}])\n        assert interp.config[(\"lat\", \"lon\")][\"method\"] == \"myinter\"\n        assert isinstance(interp.config[(\"lat\", \"lon\")][\"interpolators\"][0], MyInterp)\n\n        # should allow params to be set\n        interp = InterpolationManager(\n            [\n                {\n                    \"method\": \"myinter\",\n                    \"interpolators\": [MyInterp],\n                    \"params\": {\"spatial_tolerance\": 5},\n                    \"dims\": [\"lat\", \"lon\"],\n                }\n            ]\n        )\n\n        assert interp.config[(\"lat\", \"lon\")][\"params\"] == {\"spatial_tolerance\": 5}\n\n        # set default equal to empty tuple\n        interp = InterpolationManager([{\"method\": \"bilinear\", \"dims\": [\"lat\"]}])\n        assert interp.config[list(interp.config.keys())[-1]][\"method\"] == INTERPOLATION_DEFAULT\n\n        # use default with override if not all dimensions are supplied\n        interp = InterpolationManager([{\"method\": \"bilinear\", \"dims\": [\"lat\"]}, \"nearest\"])\n        assert interp.config[list(interp.config.keys())[-1]][\"method\"] == \"nearest\"\n\n        # make sure default is always the last key in the ordered config dict\n        interp = InterpolationManager([\"nearest\", {\"method\": \"bilinear\", \"dims\": [\"lat\"]}])\n        assert list(interp.config.keys())[-1] == (\"default\",)\n\n        # should sort the dims keys\n        interp = InterpolationManager([\"nearest\", {\"method\": \"bilinear\", \"dims\": [\"lon\", \"lat\"]}])\n        assert interp.config[(\"lat\", \"lon\")][\"method\"] == \"bilinear\"",
            "def test_init_interpolators(self):\n\n        # should set method\n        interp = InterpolationManager(\"nearest\")\n        assert interp.config[(\"default\",)][\"interpolators\"][0].method == \"nearest\"\n\n        # InterpolationManager init should init all interpolators in the list\n        interp = InterpolationManager([{\"method\": \"nearest\", \"params\": {\"spatial_tolerance\": 1}}])\n        assert interp.config[(\"default\",)][\"interpolators\"][0].spatial_tolerance == 1\n\n        # should throw TraitErrors defined by Interpolator\n        with pytest.raises(tl.TraitError):\n            InterpolationManager([{\"method\": \"nearest\", \"params\": {\"spatial_tolerance\": \"tol\"}}])\n\n        with pytest.raises(AttributeError):\n            assert interp.config[(\"default\",)][\"interpolators\"][0].myarg == \"tol\"",
            "def test_select_interpolator_queue(self):\n\n        reqcoords = Coordinates([[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]], dims=[\"lat\", \"lon\", \"time\", \"alt\"])\n        srccoords = Coordinates([[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]], dims=[\"lat\", \"lon\", \"time\", \"alt\"])\n\n        # create a few dummy interpolators that handle certain dimensions\n        # (can_select is defined by default to look at dims_supported)",
            "class TimeLat(Interpolator):\n            methods_supported = [\"myinterp\"]\n            dims_supported = [\"time\", \"lat\"]",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n                return self._filter_udims_supported(udims)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n                return self._filter_udims_supported(udims)",
            "class LatLon(Interpolator):\n            methods_supported = [\"myinterp\"]\n            dims_supported = [\"lat\", \"lon\"]",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n                return self._filter_udims_supported(udims)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n                return self._filter_udims_supported(udims)",
            "class Lon(Interpolator):\n            methods_supported = [\"myinterp\"]\n            dims_supported = [\"lon\"]",
            "def can_select(self, udims, source_coordinates, eval_coordinates):\n                return self._filter_udims_supported(udims)",
            "def can_interpolate(self, udims, source_coordinates, eval_coordinates):\n                return self._filter_udims_supported(udims)\n\n        # set up a strange interpolation definition\n        # we want to interpolate (lat, lon) first, then after (time, alt)\n        interp = InterpolationManager(\n            [\n                {\"method\": \"myinterp\", \"interpolators\": [LatLon, TimeLat], \"dims\": [\"lat\", \"lon\"]},\n                {\"method\": \"myinterp\", \"interpolators\": [TimeLat, Lon], \"dims\": [\"time\", \"alt\"]},\n            ]\n        )\n\n        # default = 'nearest', which will return NearestPreview for can_select\n        interpolator_queue = interp._select_interpolator_queue(srccoords, reqcoords, \"can_select\")\n        assert isinstance(interpolator_queue, OrderedDict)\n        assert isinstance(interpolator_queue[(\"lat\", \"lon\")], LatLon)\n        assert (\"time\", \"alt\") not in interpolator_queue and (\"alt\", \"time\") not in interpolator_queue\n\n        # should throw an error if strict is set and not all dimensions can be handled\n        with pytest.raises(InterpolationException):\n            interp_copy = deepcopy(interp)\n            interpolator_queue = interp_copy._select_interpolator_queue(srccoords, reqcoords, \"can_select\", strict=True)\n\n        # default = Nearest, which can handle all dims for can_interpolate\n        interpolator_queue = interp._select_interpolator_queue(srccoords, reqcoords, \"can_interpolate\")\n        assert isinstance(interpolator_queue, OrderedDict)\n        assert isinstance(interpolator_queue[(\"lat\", \"lon\")], LatLon)",
            "def test_select_coordinates(self):\n\n        reqcoords = Coordinates(\n            [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]], dims=[\"lat\", \"lon\", \"time\", \"alt\"], crs=\"+proj=merc +vunits=m\"\n        )\n        srccoords = Coordinates(\n            [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]], dims=[\"lat\", \"lon\", \"time\", \"alt\"], crs=\"+proj=merc +vunits=m\"\n        )\n\n        # create a few dummy interpolators that handle certain dimensions\n        # (can_select is defined by default to look at dims_supported)",
            "class TimeLat(Interpolator):\n            methods_supported = [\"myinterp\"]\n            dims_supported = [\"time\", \"lat\"]",
            "def select_coordinates(self, udims, srccoords, srccoords_idx, reqcoords):\n                return srccoords, srccoords_idx",
            "class LatLon(Interpolator):\n            methods_supported = [\"myinterp\"]\n            dims_supported = [\"lat\", \"lon\"]",
            "def select_coordinates(self, udims, srccoords, srccoords_idx, reqcoords):\n                return srccoords, srccoords_idx",
            "class Lon(Interpolator):\n            methods_supported = [\"myinterp\"]\n            dims_supported = [\"lon\"]",
            "def select_coordinates(self, udims, srccoords, srccoords_idx, reqcoords):\n                return srccoords, srccoords_idx\n\n        # set up a strange interpolation definition\n        # we want to interpolate (lat, lon) first, then after (time, alt)\n        interp = InterpolationManager(\n            [\n                {\"method\": \"myinterp\", \"interpolators\": [LatLon, TimeLat], \"dims\": [\"lat\", \"lon\"]},\n                {\"method\": \"myinterp\", \"interpolators\": [TimeLat, Lon], \"dims\": [\"time\", \"alt\"]},\n            ]\n        )\n\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n\n        assert len(coords) == len(srccoords)\n        assert len(coords[\"lat\"]) == len(srccoords[\"lat\"])\n        assert cidx == tuple([slice(0, None)] * 4)",
            "def test_interpolate(self):",
            "class TestInterp(Interpolator):\n            dims_supported = [\"lat\", \"lon\"]\n            methods_supported = [\"myinterp\"]",
            "def can_interpolate(self, udims, src, req):\n                return udims",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n                output_data = source_data\n                return output_data\n\n        # test basic functionality\n        reqcoords = Coordinates([[-0.5, 1.5, 3.5], [0.5, 2.5, 4.5]], dims=[\"lat\", \"lon\"])\n        srccoords = Coordinates([[0, 2, 4], [0, 3, 4]], dims=[\"lat\", \"lon\"])\n        srcdata = UnitsDataArray(\n            np.random.rand(3, 3), coords=[srccoords[c].coordinates for c in srccoords], dims=srccoords.dims\n        )\n        outdata = UnitsDataArray(\n            np.zeros(srcdata.shape), coords=[reqcoords[c].coordinates for c in reqcoords], dims=reqcoords.dims\n        )\n\n        interp = InterpolationManager({\"method\": \"myinterp\", \"interpolators\": [TestInterp], \"dims\": [\"lat\", \"lon\"]})\n        outdata = interp.interpolate(srccoords, srcdata, reqcoords, outdata)\n\n        assert np.all(outdata == srcdata)\n\n        # test if data is size 1",
            "class TestFakeInterp(Interpolator):\n            dims_supported = [\"lat\"]",
            "def interpolate(self, udims, source_coordinates, source_data, eval_coordinates, output_data):\n                return None\n\n        reqcoords = Coordinates([[1]], dims=[\"lat\"])\n        srccoords = Coordinates([[1]], dims=[\"lat\"])\n        srcdata = UnitsDataArray(\n            np.random.rand(1), coords=[srccoords[c].coordinates for c in srccoords], dims=srccoords.dims\n        )\n        outdata = UnitsDataArray(\n            np.zeros(srcdata.shape), coords=[reqcoords[c].coordinates for c in reqcoords], dims=reqcoords.dims\n        )\n\n        interp = InterpolationManager({\"method\": \"myinterp\", \"interpolators\": [TestFakeInterp], \"dims\": [\"lat\", \"lon\"]})\n        outdata = interp.interpolate(srccoords, srcdata, reqcoords, outdata)\n\n        assert np.all(outdata == srcdata)",
            "class TestHeterogenousInterpolation(object):\n    DATA = np.arange(64).reshape((4, 4, 4))\n    COORDS = Coordinates(\n        [[0, 1, 2, 3], [0, 1, 2, 3], [\"2020-01-01\", \"2020-01-05\", \"2020-01-09\", \"2020-01-13\"]],\n        dims=[\"lat\", \"lon\", \"time\"],\n    )\n    C1 = Coordinates([1.0, 1.0, \"2020-01-05\"], dims=[\"lat\", \"lon\", \"time\"])\n    C2 = Coordinates([1.2, 1.3, \"2020-01-05\"], dims=[\"lat\", \"lon\", \"time\"])\n    C3 = Coordinates([1.0, 1.0, \"2020-01-06\"], dims=[\"lat\", \"lon\", \"time\"])\n    C4 = Coordinates([1.2, 1.3, \"2020-01-06\"], dims=[\"lat\", \"lon\", \"time\"])\n    C = Coordinates([[1.0, 1.2], [1.0, 1.3], [\"2020-01-05\", \"2020-01-06\"]], dims=[\"lat\", \"lon\", \"time\"])\n\n    S1 = Coordinates([[1.0, 1.0], \"2020-01-05\"], dims=[\"lat_lon\", \"time\"])\n    S2 = Coordinates([[1.2, 1.3], \"2020-01-05\"], dims=[\"lat_lon\", \"time\"])\n    S3 = Coordinates([[1.0, 1.0], \"2020-01-06\"], dims=[\"lat_lon\", \"time\"])\n    S4 = Coordinates([[1.2, 1.3], \"2020-01-06\"], dims=[\"lat_lon\", \"time\"])\n    S = Coordinates(\n        [[[1.0, 1.2, 1.0, 1.2], [1.0, 1.3, 1.0, 1.3]], [\"2020-01-05\", \"2020-01-06\"]], dims=[\"lat_lon\", \"time\"]\n    )",
            "def test_nearest(self):\n        interpolation = \"nearest\"\n        node = podpac.data.Array(source=self.DATA, coordinates=self.COORDS, interpolation=interpolation)\n\n        assert node.eval(self.C1)[0, 0, 0] == 21.0\n        assert node.eval(self.C2)[0, 0, 0] == 21.0\n        assert node.eval(self.C3)[0, 0, 0] == 21.0\n        assert node.eval(self.C4)[0, 0, 0] == 21.0\n        assert np.all(node.eval(self.C) == 21.0)\n\n        assert node.eval(self.S1)[0, 0] == 21.0\n        assert node.eval(self.S2)[0, 0] == 21.0\n        assert node.eval(self.S3)[0, 0] == 21.0\n        assert node.eval(self.S4)[0, 0] == 21.0\n        np.testing.assert_array_equal(node.eval(self.S), [[21, 21], [21, 21], [21, 21], [21, 21]])",
            "def test_mixed(self):\n        interpolation = [{\"method\": \"nearest\", \"dims\": [\"time\"]}, {\"method\": \"bilinear\", \"dims\": [\"lat\", \"lon\"]}]\n        node = podpac.data.Array(source=self.DATA, coordinates=self.COORDS, interpolation=interpolation)\n\n        assert node.eval(self.C1)[0, 0, 0] == 21.0\n        assert node.eval(self.C2)[0, 0, 0] == 25.4\n        assert node.eval(self.C3)[0, 0, 0] == 21.0\n        assert node.eval(self.C4)[0, 0, 0] == 25.4\n        np.testing.assert_array_equal(node.eval(self.C), [[[21, 21], [22.2, 22.2]], [[24.2, 24.2], [25.4, 25.4]]])\n\n        assert node.eval(self.S1)[0, 0] == 21.0\n        assert node.eval(self.S2)[0, 0] == 25.4\n        assert node.eval(self.S3)[0, 0] == 21.0\n        assert node.eval(self.S4)[0, 0] == 25.4\n        np.testing.assert_array_equal(node.eval(self.S), [[21, 21], [25.4, 25.4], [21, 21], [25.4, 25.4]])\n\n        # other order\n        interpolation = [{\"method\": \"bilinear\", \"dims\": [\"lat\", \"lon\"]}, {\"method\": \"nearest\", \"dims\": [\"time\"]}]\n        node = podpac.data.Array(source=self.DATA, coordinates=self.COORDS, interpolation=interpolation)\n\n        assert node.eval(self.C1)[0, 0, 0] == 21.0\n        assert node.eval(self.C2)[0, 0, 0] == 25.4\n        assert node.eval(self.C3)[0, 0, 0] == 21.0\n        assert node.eval(self.C4)[0, 0, 0] == 25.4\n        np.testing.assert_array_equal(node.eval(self.C), [[[21, 21], [22.2, 22.2]], [[24.2, 24.2], [25.4, 25.4]]])\n\n        assert node.eval(self.S1)[0, 0] == 21.0\n        assert node.eval(self.S2)[0, 0] == 25.4\n        assert node.eval(self.S3)[0, 0] == 21.0\n        assert node.eval(self.S4)[0, 0] == 25.4\n        np.testing.assert_array_equal(node.eval(self.S), [[21, 21], [25.4, 25.4], [21, 21], [25.4, 25.4]])",
            "def test_mixed_linear_time(self):\n        interpolation = [{\"method\": \"bilinear\", \"dims\": [\"time\"]}, {\"method\": \"nearest\", \"dims\": [\"lat\", \"lon\"]}]\n        node = podpac.data.Array(source=self.DATA, coordinates=self.COORDS, interpolation=interpolation)\n\n        assert node.eval(self.C1)[0, 0, 0] == 21.0\n        assert node.eval(self.C2)[0, 0, 0] == 21.0\n        assert node.eval(self.C3)[0, 0, 0] == 21.25\n        assert node.eval(self.C4)[0, 0, 0] == 21.25\n        np.testing.assert_array_equal(node.eval(self.C), [[[21, 21.25], [21, 21.25]], [[21, 21.25], [21, 21.25]]])\n\n        assert node.eval(self.S1)[0, 0] == 21.0\n        assert node.eval(self.S2)[0, 0] == 21.0\n        assert node.eval(self.S3)[0, 0] == 21.25\n        assert node.eval(self.S4)[0, 0] == 21.25\n        np.testing.assert_array_equal(node.eval(self.S), [[21, 21.25], [21, 21.25], [21, 21.25], [21, 21.25]])",
            "def test_multiple_outputs_nearest(self):\n        interpolation = \"nearest\"\n        node = podpac.data.Array(\n            source=np.transpose([self.DATA, 2 * self.DATA], [1, 2, 3, 0]),\n            coordinates=self.COORDS,\n            interpolation=interpolation,\n            outputs=[\"a\", \"b\"],\n        )\n\n        np.testing.assert_array_equal(node.eval(self.C1)[0, 0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.C2)[0, 0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.C3)[0, 0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.C4)[0, 0, 0], [21.0, 2 * 21.0])\n\n        np.testing.assert_array_equal(node.eval(self.S1)[0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.S2)[0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.S3)[0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.S4)[0, 0], [21.0, 2 * 21.0])",
            "def test_multiple_outputs(self):\n        interpolation = [{\"method\": \"nearest\", \"dims\": [\"time\"]}, {\"method\": \"bilinear\", \"dims\": [\"lat\", \"lon\"]}]\n        node = podpac.data.Array(\n            source=np.transpose([self.DATA, 2 * self.DATA], [1, 2, 3, 0]),\n            coordinates=self.COORDS,\n            interpolation=interpolation,\n            outputs=[\"a\", \"b\"],\n        )\n\n        np.testing.assert_array_equal(node.eval(self.C1)[0, 0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.C2)[0, 0, 0], [25.4, 2 * 25.4])\n        np.testing.assert_array_equal(node.eval(self.C3)[0, 0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.C4)[0, 0, 0], [25.4, 2 * 25.4])\n        np.testing.assert_array_equal(\n            node.eval(self.C),\n            [\n                [[[21, 42], [21, 42]], [[22.2, 44.4], [22.2, 44.4]]],\n                [[[24.2, 48.4], [24.2, 48.4]], [[25.4, 50.8], [25.4, 50.8]]],\n            ],\n        )\n\n        np.testing.assert_array_equal(node.eval(self.S1)[0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.S2)[0, 0], [25.4, 2 * 25.4])\n        np.testing.assert_array_equal(node.eval(self.S3)[0, 0], [21.0, 2 * 21.0])\n        np.testing.assert_array_equal(node.eval(self.S4)[0, 0], [25.4, 2 * 25.4])\n        np.testing.assert_array_equal(\n            node.eval(self.S),\n            [[[21, 42], [21, 42]], [[25.4, 50.8], [25.4, 50.8]], [[21, 42], [21, 42]], [[25.4, 50.8], [25.4, 50.8]]],\n        )"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/test/test_interpolators.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nTest interpolation methods\n\n\n\"\"\"",
            "\"\"\"test interpolation functions\"\"\"",
            "\"\"\"regular interpolation using rasterio\"\"\"",
            "\"\"\"should handle descending\"\"\"",
            "\"\"\"test interpolation functions\"\"\"",
            "\"\"\"irregular interpolation\"\"\"",
            "\"\"\"irregular interpolation\"\"\"",
            "\"\"\"should handle descending\"\"\"",
            "\"\"\"should handle descending\"\"\"",
            "\"\"\"irregular interpolation\"\"\"",
            "\"\"\"interpolate point data to nearest neighbor with various coords_dst\"\"\"",
            "\"\"\"test interpolation functions\"\"\"",
            "\"\"\"irregular interpolation\"\"\"",
            "\"\"\"should handle descending\"\"\"",
            "\"\"\"should handle descending\"\"\"",
            "\"\"\"irregular interpolation\"\"\""
        ],
        "code_snippets": [
            "class MockArrayDataSource(InterpolationMixin, DataSource):\n    data = ArrayTrait().tag(attr=True)\n    coordinates = tl.Instance(Coordinates).tag(attr=True)",
            "def get_data(self, coordinates, coordinates_index):\n        return self.create_output_array(coordinates, data=self.data[coordinates_index])",
            "class MockArrayDataSourceXR(InterpolationMixin, DataSource):\n    data = ArrayTrait().tag(attr=True)\n    coordinates = tl.Instance(Coordinates).tag(attr=True)",
            "def get_data(self, coordinates, coordinates_index):\n        dataxr = self.create_output_array(self.coordinates, data=self.data)\n        return self.create_output_array(coordinates, data=dataxr[coordinates_index].data)",
            "class TestNone(object):",
            "def test_none_select(self):\n        reqcoords = Coordinates([[-0.5, 1.5, 3.5], [0.5, 2.5, 4.5]], dims=[\"lat\", \"lon\"])\n        srccoords = Coordinates([[-1, 0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], dims=[\"lat\", \"lon\"])\n\n        # test straight ahead functionality\n        interp = InterpolationManager(\"none\")\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        assert coords == srccoords[1:5, 1:-1]\n        assert srccoords[cidx] == coords\n\n        # test when selection is applied serially\n        interp = InterpolationManager([{\"method\": \"none\", \"dims\": [\"lat\"]}, {\"method\": \"none\", \"dims\": [\"lon\"]}])\n\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        assert coords == srccoords[1:5, 1:-1]\n        assert srccoords[cidx] == coords\n\n        # Test Case where rounding issues causes problem with endpoint\n        reqcoords = Coordinates([[0, 2, 4], [0, 2, 4]], dims=[\"lat\", \"lon\"])\n        lat = np.arange(0, 6.1, 1.3333333333333334)\n        lon = np.arange(0, 6.1, 1.333333333333334)  # Notice one decimal less on this number\n        srccoords = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        # test straight ahead functionality\n        interp = InterpolationManager(\"none\")\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        srccoords = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        assert srccoords[cidx] == coords",
            "def test_none_interpolation(self):\n        node = podpac.data.Array(\n            source=[0, 1, 2],\n            coordinates=podpac.Coordinates([[1, 5, 9]], dims=[\"lat\"]),\n            interpolation=\"none\",\n        )\n        o = node.eval(podpac.Coordinates([podpac.crange(1, 9, 1)], dims=[\"lat\"]))\n        np.testing.assert_array_equal(o.data, node.source)",
            "def test_none_heterogeneous(self):\n        # Heterogeneous\n        node = podpac.data.Array(\n            source=[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]],\n            coordinates=podpac.Coordinates([[1, 5, 9, 13], [0, 1, 2]], dims=[\"lat\", \"lon\"]),\n            interpolation=[{\"method\": \"none\", \"dims\": [\"lat\"]}, {\"method\": \"linear\", \"dims\": [\"lon\"]}],\n        )\n        o = node.eval(podpac.Coordinates([podpac.crange(1, 9, 2), [0.5, 1.5]], dims=[\"lat\", \"lon\"]))\n        np.testing.assert_array_equal(\n            o.data,\n            [\n                [0.5, 1.5],\n                [\n                    0.5,\n                    1.5,\n                ],\n                [0.5, 1.5],\n            ],\n        )\n\n        # Heterogeneous _flipped\n        node = podpac.data.Array(\n            source=[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]],\n            coordinates=podpac.Coordinates([[1, 5, 9, 13], [0, 1, 2]], dims=[\"lat\", \"lon\"]),\n            interpolation=[{\"method\": \"linear\", \"dims\": [\"lon\"]}, {\"method\": \"none\", \"dims\": [\"lat\"]}],\n        )\n        o = node.eval(podpac.Coordinates([podpac.crange(1, 9, 2), [0.5, 1.5]], dims=[\"lat\", \"lon\"]))\n        np.testing.assert_array_equal(\n            o.data,\n            [\n                [0.5, 1.5],\n                [\n                    0.5,\n                    1.5,\n                ],\n                [0.5, 1.5],\n            ],\n        )\n\n        # Examples\n        #  source                      eval\n        #  lat_lon                     lat, lon\n        node = podpac.data.Array(\n            source=[0, 1, 2],\n            coordinates=podpac.Coordinates([[[1, 5, 9], [1, 5, 9]]], dims=[[\"lat\", \"lon\"]]),\n            interpolation=[{\"method\": \"none\", \"dims\": [\"lon\", \"lat\"]}],\n        )\n        o = node.eval(podpac.Coordinates([podpac.crange(1, 9, 1), podpac.crange(1, 9, 1)], dims=[\"lon\", \"lat\"]))\n        np.testing.assert_array_equal(o.data, node.source)\n\n        #  source                      eval\n        #  lat, lon                    lat_lon\n        node = podpac.data.Array(\n            source=[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]],\n            coordinates=podpac.Coordinates([[1, 5, 9, 13], [0, 1, 2]], dims=[\"lat\", \"lon\"]),\n            interpolation=[{\"method\": \"none\", \"dims\": [\"lat\", \"lon\"]}],\n        )\n        o = node.eval(podpac.Coordinates([[podpac.crange(1, 9, 2), podpac.crange(1, 9, 2)]], dims=[[\"lat\", \"lon\"]]))\n        np.testing.assert_array_equal(o.data, node.source[:-1, 1:])",
            "class TestNearest(object):",
            "def test_nearest_preview_select(self):\n        reqcoords = Coordinates([[-0.5, 1.5, 3.5], [0.5, 2.5, 4.5]], dims=[\"lat\", \"lon\"])\n        srccoords = Coordinates([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], dims=[\"lat\", \"lon\"])\n\n        # test straight ahead functionality\n        interp = InterpolationManager(\"nearest_preview\")\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_array_equal(coords[\"lat\"].coordinates, [0, 2, 4])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, [0, 2, 4])\n        assert srccoords[cidx] == coords\n\n        # test when selection is applied serially\n        interp = InterpolationManager(\n            [{\"method\": \"nearest_preview\", \"dims\": [\"lat\"]}, {\"method\": \"nearest_preview\", \"dims\": [\"lon\"]}]\n        )\n\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_array_equal(coords[\"lat\"].coordinates, [0, 2, 4])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, [0, 2, 4])\n        assert srccoords[cidx] == coords\n\n        # Test reverse selection\n        reqcoords = Coordinates([[-0.5, 1.5, 3.5], [0.5, 2.5, 4.5]], dims=[\"lat\", \"lon\"])\n        srccoords = Coordinates([[0, 1, 2, 3, 4, 5][::-1], [0, 1, 2, 3, 4, 5][::-1]], dims=[\"lat\", \"lon\"])\n\n        # test straight ahead functionality\n        interp = InterpolationManager(\"nearest_preview\")\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_array_equal(coords[\"lat\"].coordinates, [4, 2, 0])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, [5, 3, 1])  # Yes, this is expected behavior\n        assert srccoords[cidx] == coords\n\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_array_equal(coords[\"lat\"].coordinates, [4, 2, 0])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, [5, 3, 1])\n        assert srccoords[cidx] == coords\n\n        # Test Case where rounding issues causes problem with endpoint\n        reqcoords = Coordinates([[0, 2, 4], [0, 2, 4]], dims=[\"lat\", \"lon\"])\n        lat = np.arange(0, 6.1, 1.3333333333333334)\n        lon = np.arange(0, 6.1, 1.333333333333334)  # Notice one decimal less on this number\n        srccoords = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        # test straight ahead functionality\n        interp = InterpolationManager(\"nearest_preview\")\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_almost_equal(coords[\"lat\"].coordinates, lat[::2])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, lon[:4])\n        np.testing.assert_almost_equal(list(srccoords[cidx].bounds.values()), list(coords.bounds.values()))\n        assert srccoords[cidx].shape == coords.shape\n\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_almost_equal(coords[\"lat\"].coordinates, lat[::2])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, lon[:4])\n        np.testing.assert_almost_equal(list(srccoords[cidx].bounds.values()), list(coords.bounds.values()))\n        assert srccoords[cidx].shape == coords.shape\n\n    #",
            "def test_nearest_preview_select_stacked(self):\n    #     # TODO: how to handle stacked/unstacked coordinate asynchrony?\n    #     reqcoords = Coordinates([[-.5, 1.5, 3.5], [.5, 2.5, 4.5]], dims=['lat', 'lon'])\n    #     srccoords = Coordinates([([0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5])], dims=['lat_lon'])\n\n    #     interp = InterpolationManager('nearest_preview')\n\n    #     srccoords, srccoords_index = srccoords.intersect(reqcoords, outer=True, return_index=True)\n    #     coords, cidx = interp.select_coordinates(reqcoords, srccoords, srccoords_index)\n\n    #     assert len(coords) == len(srcoords) == len(cidx)\n    #     assert len(coords['lat']) == len(reqcoords['lat'])\n    #     assert len(coords['lon']) == len(reqcoords['lon'])\n    #     assert np.all(coords['lat'].coordinates == np.array([0, 2, 4]))",
            "def test_nearest_select_issue226(self):\n        reqcoords = Coordinates([[-0.5, 1.5, 3.5], [0.5, 2.5, 4.5]], dims=[\"lat\", \"lon\"])\n        srccoords = Coordinates([[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], dims=[\"lat\", \"lon\"])\n\n        # test straight ahead functionality\n        interp = InterpolationManager(\"nearest\")\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_array_equal(coords[\"lat\"].coordinates, [0, 2, 4])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, [0, 3, 5])\n        assert srccoords[cidx] == coords\n\n        # test when selection is applied serially\n        interp = InterpolationManager([{\"method\": \"nearest\", \"dims\": [\"lat\"]}, {\"method\": \"nearest\", \"dims\": [\"lon\"]}])\n        coords, cidx = interp.select_coordinates(srccoords, reqcoords)\n        np.testing.assert_array_equal(coords[\"lat\"].coordinates, [0, 2, 4])\n        np.testing.assert_array_equal(coords[\"lon\"].coordinates, [0, 3, 5])\n        assert srccoords[cidx] == coords",
            "def test_nearest_select_issue445(self):\n        sc = Coordinates([clinspace(-59.9, 89.9, 100, name=\"lat\"), clinspace(-179.9, 179.9, 100, name=\"lon\")])\n        node = podpac.data.Array(\n            interpolation=\"nearest_preview\", source=np.arange(sc.size).reshape(sc.shape), coordinates=sc\n        )\n        coords = Coordinates([-61, 72], dims=[\"lat\", \"lon\"])\n        out = node.eval(coords)\n        assert out.shape == (1, 1)\n        assert np.isnan(out.data[0, 0])",
            "def test_interpolation(self):\n\n        for interpolation in [\"nearest\", \"nearest_preview\"]:\n\n            # unstacked 1D\n            source = np.random.rand(5)\n            coords_src = Coordinates([np.linspace(0, 10, 5)], dims=[\"lat\"])\n            node = MockArrayDataSource(data=source, coordinates=coords_src, interpolation=interpolation)\n\n            coords_dst = Coordinates([[1, 1.2, 1.5, 5, 9]], dims=[\"lat\"])\n            output = node.eval(coords_dst)\n\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert output.values[0] == source[0] and output.values[1] == source[0] and output.values[2] == source[1]\n\n            # unstacked N-D\n            source = np.random.rand(5, 5)\n            coords_src = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n            coords_dst = Coordinates([clinspace(2, 12, 5), clinspace(2, 12, 5)], dims=[\"lat\", \"lon\"])\n\n            node = MockArrayDataSource(data=source, coordinates=coords_src, interpolation=interpolation)\n            output = node.eval(coords_dst)\n\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert output.values[0, 0] == source[1, 1]\n\n            # source = stacked, dest = stacked\n            source = np.random.rand(5)\n            coords_src = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lat_lon\"])\n            node = MockArrayDataSource(\n                data=source,\n                coordinates=coords_src,\n                interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor]},\n            )\n            coords_dst = Coordinates([(np.linspace(1, 9, 3), np.linspace(1, 9, 3))], dims=[\"lat_lon\"])\n            output = node.eval(coords_dst)\n\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert all(output.values == source[[0, 2, 4]])\n\n            # source = stacked, dest = unstacked\n            source = np.random.rand(5)\n            coords_src = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lat_lon\"])\n            node = MockArrayDataSource(\n                data=source,\n                coordinates=coords_src,\n                interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor]},\n            )\n            coords_dst = Coordinates([np.linspace(1, 9, 3), np.linspace(1, 9, 3)], dims=[\"lat\", \"lon\"])\n\n            output = node.eval(coords_dst)\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert np.all(output.values == source[np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])])\n\n            # source = unstacked, dest = stacked\n            source = np.random.rand(5, 5)\n            coords_src = Coordinates([np.linspace(0, 10, 5), np.linspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n            node = MockArrayDataSource(\n                data=source,\n                coordinates=coords_src,\n                interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor]},\n            )\n            coords_dst = Coordinates([(np.linspace(1, 9, 3), np.linspace(1, 9, 3))], dims=[\"lat_lon\"])\n\n            output = node.eval(coords_dst)\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert np.all(output.values == source[[0, 2, 4], [0, 2, 4]])\n\n            # source = unstacked and non-uniform, dest = stacked\n            source = np.random.rand(5, 5)\n            coords_src = Coordinates([[0, 1.1, 1.2, 6.1, 10], [0, 1.1, 4, 7.1, 9.9]], dims=[\"lat\", \"lon\"])\n            node = MockArrayDataSource(\n                data=source,\n                coordinates=coords_src,\n                interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor]},\n            )\n            coords_dst = Coordinates([(np.linspace(1, 9, 3), np.linspace(1, 9, 3))], dims=[\"lat_lon\"])\n\n            output = node.eval(coords_dst)\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert np.all(output.values == source[[1, 3, 4], [1, 2, 4]])\n\n            # lat_lon_time_alt --> lon, alt_time, lat\n            source = np.random.rand(5)\n            coords_src = Coordinates([[[0, 1, 2, 3, 4]] * 4], dims=[[\"lat\", \"lon\", \"time\", \"alt\"]])\n            node = MockArrayDataSource(\n                data=source,\n                coordinates=coords_src,\n                interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor]},\n            )\n            coords_dst = Coordinates(\n                [[1, 2.4, 3.9], [[1, 2.4, 3.9], [1, 2.4, 3.9]], [1, 2.4, 3.9]], dims=[\"lon\", \"alt_time\", \"lat\"]\n            )\n\n            output = node.eval(coords_dst)\n            assert isinstance(output, UnitsDataArray)\n            assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n            assert np.all(output.values[[0, 1, 2], [0, 1, 2], [0, 1, 2]] == source[[1, 2, 4]])",
            "def test_spatial_tolerance(self):\n        # unstacked 1D\n        source = np.random.rand(5)\n        coords_src = Coordinates([np.linspace(0, 10, 5)], dims=[\"lat\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"params\": {\"spatial_tolerance\": 1.1}},\n        )\n\n        coords_dst = Coordinates([[1, 1.2, 1.5, 5, 9]], dims=[\"lat\"])\n        output = node.eval(coords_dst)\n\n        print(output)\n        print(source)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert output.values[0] == source[0] and np.isnan(output.values[1]) and output.values[2] == source[1]\n\n        # stacked 1D\n        source = np.random.rand(5)\n        coords_src = Coordinates([[np.linspace(0, 10, 5), np.linspace(0, 10, 5)]], dims=[[\"lat\", \"lon\"]])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"params\": {\"spatial_tolerance\": 1.1}},\n        )\n\n        coords_dst = Coordinates([[[1, 1.2, 1.5, 5, 9], [1, 1.2, 1.5, 5, 9]]], dims=[[\"lat\", \"lon\"]])\n        output = node.eval(coords_dst)\n\n        print(output)\n        print(source)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert output.values[0] == source[0] and np.isnan(output.values[1]) and output.values[2] == source[1]",
            "def test_time_tolerance(self):\n\n        # unstacked 1D\n        source = np.random.rand(5, 5)\n        coords_src = Coordinates(\n            [np.linspace(0, 10, 5), clinspace(\"2018-01-01\", \"2018-01-09\", 5)], dims=[\"lat\", \"time\"]\n        )\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"params\": {\"spatial_tolerance\": 1.1, \"time_tolerance\": np.timedelta64(1, \"D\")},\n            },\n        )\n\n        coords_dst = Coordinates([[1, 1.2, 1.5, 5, 9], clinspace(\"2018-01-01\", \"2018-01-09\", 3)], dims=[\"lat\", \"time\"])\n        output = node.eval(coords_dst)\n\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert (\n            output.values[0, 0] == source[0, 0]\n            and output.values[0, 1] == source[0, 2]\n            and np.isnan(output.values[1, 0])\n            and np.isnan(output.values[1, 1])\n            and output.values[2, 0] == source[1, 0]\n            and output.values[2, 1] == source[1, 2]\n        )",
            "def test_stacked_source_unstacked_region_non_square(self):\n        # unstacked 1D\n        source = np.random.rand(5)\n        coords_src = Coordinates(\n            [[np.linspace(0, 10, 5), clinspace(\"2018-01-01\", \"2018-01-09\", 5)]], dims=[[\"lat\", \"time\"]]\n        )\n        node = MockArrayDataSource(\n            data=source, coordinates=coords_src, interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor]}\n        )\n\n        coords_dst = Coordinates([[1, 1.2, 1.5, 5, 9], clinspace(\"2018-01-01\", \"2018-01-09\", 3)], dims=[\"lat\", \"time\"])\n        output = node.eval(coords_dst)\n\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert np.all(output.values == source[np.array([[0, 2, 4]] * 5)])",
            "def test_time_space_scale_grid(self):\n        # Grid\n        source = np.random.rand(5, 3, 2)\n        source[2, 1, 0] = np.nan\n        coords_src = Coordinates(\n            [np.linspace(0, 10, 5), [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], [0, 10]], dims=[\"lat\", \"time\", \"alt\"]\n        )\n        coords_dst = Coordinates([5.1, \"2018-01-02T11\", 1], dims=[\"lat\", \"time\", \"alt\"])\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n                \"params\": {\n                    \"spatial_scale\": 1,\n                    \"time_scale\": \"1,D\",\n                    \"alt_scale\": 10,\n                    \"remove_nan\": True,\n                    \"use_selector\": False,\n                },\n            },\n        )\n        output = node.eval(coords_dst)\n        assert output == source[2, 2, 0]\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n                \"params\": {\n                    \"spatial_scale\": 1,\n                    \"time_scale\": \"1,s\",\n                    \"alt_scale\": 10,\n                    \"remove_nan\": True,\n                    \"use_selector\": False,\n                },\n            },\n        )\n        output = node.eval(coords_dst)\n        assert output == source[2, 1, 1]\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n                \"params\": {\n                    \"spatial_scale\": 1,\n                    \"time_scale\": \"1,s\",\n                    \"alt_scale\": 1,\n                    \"remove_nan\": True,\n                    \"use_selector\": False,\n                },\n            },\n        )\n        output = node.eval(coords_dst)\n        assert output == source[3, 1, 0]",
            "def test_remove_nan(self):\n        # Stacked\n        source = np.random.rand(5)\n        source[2] = np.nan\n        coords_src = Coordinates(\n            [[np.linspace(0, 10, 5), clinspace(\"2018-01-01\", \"2018-01-09\", 5)]], dims=[[\"lat\", \"time\"]]\n        )\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor], \"params\": {\"remove_nan\": False}},\n        )\n        coords_dst = Coordinates([[5.1]], dims=[\"lat\"])\n        output = node.eval(coords_dst)\n        assert np.isnan(output)\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n                \"params\": {\"remove_nan\": True, \"use_selector\": False},\n            },\n        )\n        output = node.eval(coords_dst)\n        assert (\n            output == source[3]\n        )  # This fails because the selector selects the nan value... can we turn off the selector?\n\n        # Grid\n        source = np.random.rand(5, 3)\n        source[2, 1] = np.nan\n        coords_src = Coordinates([np.linspace(0, 10, 5), [1, 2, 3]], dims=[\"lat\", \"time\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor], \"params\": {\"remove_nan\": False}},\n        )\n        coords_dst = Coordinates([5.1, 2.01], dims=[\"lat\", \"time\"])\n        output = node.eval(coords_dst)\n        assert np.isnan(output)\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n                \"params\": {\"remove_nan\": True, \"use_selector\": False},\n            },\n        )\n        output = node.eval(coords_dst)\n        assert output == source[2, 2]",
            "def test_respect_bounds(self):\n        source = np.random.rand(5)\n        coords_src = Coordinates([[1, 2, 3, 4, 5]], [\"alt\"])\n        coords_dst = Coordinates([[-0.5, 1.1, 2.6]], [\"alt\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n                \"params\": {\"respect_bounds\": False},\n            },\n        )\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output.data, source[[0, 0, 2]])\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [NearestNeighbor], \"params\": {\"respect_bounds\": True}},\n        )\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output.data[1:], source[[0, 2]])\n        assert np.isnan(output.data[0])",
            "def test_2Dstacked(self):\n        # With Time\n        source = np.random.rand(5, 4, 2)\n        coords_src = Coordinates(\n            [\n                [\n                    np.arange(5)[:, None] + 0.1 * np.ones((5, 4)),\n                    np.arange(4)[None, :] + 0.1 * np.ones((5, 4)),\n                ],\n                [0.4, 0.7],\n            ],\n            [\"lat_lon\", \"time\"],\n        )\n        coords_dst = Coordinates([np.arange(4) + 0.2, np.arange(1, 4) - 0.2, [0.5]], [\"lat\", \"lon\", \"time\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n            },\n        )\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output, source[:4, 1:, :1])\n\n        # Using 'xarray' coordinates type\n        node = MockArrayDataSourceXR(\n            data=source,\n            coordinates=coords_src,\n            coordinate_index_type=\"xarray\",\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n            },\n        )\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output, source[:4, 1:, :1])\n\n        # Using 'slice' coordinates type\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            coordinate_index_type=\"slice\",\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n            },\n        )\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output, source[:4, 1:, :1])\n\n        # Without Time\n        source = np.random.rand(5, 4)\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src.drop(\"time\"),\n            interpolation={\n                \"method\": \"nearest\",\n                \"interpolators\": [NearestNeighbor],\n            },\n        )\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output, source[:4, 1:])\n\n    #",
            "def test_3Dstacked(self):\n    #     # With Time\n    #     source = np.random.rand(5, 4, 2)\n    #     coords_src = Coordinates([[\n    #         np.arange(5)[:, None, None] + 0.1 * np.ones((5, 4, 2)),\n    #         np.arange(4)[None, :, None] + 0.1 * np.ones((5, 4, 2)),\n    #         np.arange(2)[None, None, :] + 0.1 * np.ones((5, 4, 2))]], [\"lat_lon_time\"])\n    #     coords_dst = Coordinates([np.arange(4)+0.2, np.arange(1, 4)-0.2, [0.5]], [\"lat\", \"lon\", \"time\"])\n    #     node = MockArrayDataSource(\n    #         data=source,\n    #         coordinates=coords_src,\n    #         interpolation={\n    #             \"method\": \"nearest\",\n    #             \"interpolators\": [NearestNeighbor],\n    #         },\n    #     )\n    #     output = node.eval(coords_dst)\n    #     np.testing.assert_array_equal(output, source[:4, 1:, :1])\n\n    #     # Using 'xarray' coordinates type\n    #     node = MockArrayDataSourceXR(\n    #         data=source,\n    #         coordinates=coords_src,\n    #         coordinate_index_type='xarray',\n    #         interpolation={\n    #             \"method\": \"nearest\",\n    #             \"interpolators\": [NearestNeighbor],\n    #         },\n    #     )\n    #     output = node.eval(coords_dst)\n    #     np.testing.assert_array_equal(output, source[:4, 1:, :1])\n\n    #     # Using 'slice' coordinates type\n    #     node = MockArrayDataSource(\n    #         data=source,\n    #         coordinates=coords_src,\n    #         coordinate_index_type='slice',\n    #         interpolation={\n    #             \"method\": \"nearest\",\n    #             \"interpolators\": [NearestNeighbor],\n    #         },\n    #     )\n    #     output = node.eval(coords_dst)\n    #     np.testing.assert_array_equal(output, source[:4, 1:, :1])\n\n    #     # Without Time\n    #     source = np.random.rand(5, 4)\n    #     node = MockArrayDataSource(\n    #         data=source,\n    #         coordinates=coords_src.drop('time'),\n    #         interpolation={\n    #             \"method\": \"nearest\",\n    #             \"interpolators\": [NearestNeighbor],\n    #         },\n    #     )\n    #     output = node.eval(coords_dst)\n    #     np.testing.assert_array_equal(output, source[:4, 1:])",
            "class TestInterpolateRasterioInterpolator(object):",
            "def test_interpolate_rasterio(self):",
            "def test_interpolate_rasterio_descending(self):",
            "class TestInterpolateScipyGrid(object):",
            "def test_interpolate_scipy_grid(self):\n\n        source = np.arange(0, 25)\n        source.resize((5, 5))\n\n        coords_src = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n        coords_dst = Coordinates([clinspace(1, 11, 5), clinspace(1, 11, 5)], dims=[\"lat\", \"lon\"])\n\n        # try one specific rasterio case to measure output\n        node = MockArrayDataSource(\n            data=source, coordinates=coords_src, interpolation={\"method\": \"nearest\", \"interpolators\": [ScipyGrid]}\n        )\n        output = node.eval(coords_dst)\n\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        print(output)\n        assert output.data[0, 0] == 0.0\n        assert output.data[0, 3] == 3.0\n        assert output.data[1, 3] == 8.0\n        assert np.isnan(output.data[0, 4])  # TODO: how to handle outside bounds\n\n        node = MockArrayDataSource(\n            data=source, coordinates=coords_src, interpolation={\"method\": \"cubic_spline\", \"interpolators\": [ScipyGrid]}\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert int(output.data[0, 0]) == 2\n        assert int(output.data[2, 4]) == 16\n\n        node = MockArrayDataSource(\n            data=source, coordinates=coords_src, interpolation={\"method\": \"bilinear\", \"interpolators\": [ScipyGrid]}\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert int(output.data[0, 0]) == 2\n        assert int(output.data[3, 3]) == 20\n        assert np.isnan(output.data[4, 4])  # TODO: how to handle outside bounds",
            "def test_interpolate_irregular_arbitrary_2dims(self):",
            "def test_interpolate_looper_helper(self):",
            "def test_interpolate_irregular_arbitrary_descending(self):",
            "def test_interpolate_irregular_arbitrary_swap(self):",
            "def test_interpolate_irregular_lat_lon(self):",
            "class TestInterpolateScipyPoint(object):",
            "def test_interpolate_scipy_point(self):",
            "class TestXarrayInterpolator(object):",
            "def test_nearest_interpolation(self):\n\n        interpolation = {\n            \"method\": \"nearest\",\n            \"interpolators\": [XarrayInterpolator],\n            \"params\": {\"fill_value\": \"extrapolate\"},\n        }\n\n        # unstacked 1D\n        source = np.random.rand(5)\n        coords_src = Coordinates([np.linspace(0, 10, 5)], dims=[\"lat\"])\n        node = MockArrayDataSource(data=source, coordinates=coords_src, interpolation=interpolation)\n\n        coords_dst = Coordinates([[1, 1.2, 1.5, 5, 9]], dims=[\"lat\"])\n        output = node.eval(coords_dst)\n\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert output.values[0] == source[0] and output.values[1] == source[0] and output.values[2] == source[1]\n\n        # unstacked N-D\n        source = np.random.rand(5, 5)\n        coords_src = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n        coords_dst = Coordinates([clinspace(2, 12, 5), clinspace(2, 12, 5)], dims=[\"lat\", \"lon\"])\n\n        node = MockArrayDataSource(data=source, coordinates=coords_src, interpolation=interpolation)\n        output = node.eval(coords_dst)\n\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert output.values[0, 0] == source[1, 1]\n\n        # stacked\n        # TODO: implement stacked handling\n        source = np.random.rand(5)\n        coords_src = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lat_lon\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [XarrayInterpolator]},\n        )\n        coords_dst = Coordinates([(np.linspace(1, 9, 3), np.linspace(1, 9, 3))], dims=[\"lat_lon\"])\n\n        with pytest.raises(InterpolationException):\n            output = node.eval(coords_dst)\n\n        # TODO: implement stacked handling\n        # source = stacked, dest = unstacked\n        source = np.random.rand(5)\n        coords_src = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lat_lon\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [XarrayInterpolator]},\n        )\n        coords_dst = Coordinates([np.linspace(1, 9, 3), np.linspace(1, 9, 3)], dims=[\"lat\", \"lon\"])\n\n        with pytest.raises(InterpolationException):\n            output = node.eval(coords_dst)\n\n        # source = unstacked, dest = stacked\n        source = np.random.rand(5, 5)\n        coords_src = Coordinates([np.linspace(0, 10, 5), np.linspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [XarrayInterpolator]},\n        )\n        coords_dst = Coordinates([(np.linspace(1, 9, 3), np.linspace(1, 9, 3))], dims=[\"lat_lon\"])\n\n        output = node.eval(coords_dst)\n        np.testing.assert_array_equal(output.data, source[[0, 2, 4], [0, 2, 4]])",
            "def test_interpolate_xarray_grid(self):\n\n        source = np.arange(0, 25)\n        source.resize((5, 5))\n\n        coords_src = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n        coords_dst = Coordinates([clinspace(1, 11, 5), clinspace(1, 11, 5)], dims=[\"lat\", \"lon\"])\n\n        # try one specific rasterio case to measure output\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"nearest\", \"interpolators\": [XarrayInterpolator]},\n        )\n        output = node.eval(coords_dst)\n\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        # print(output)\n        assert output.data[0, 0] == 0.0\n        assert output.data[0, 3] == 3.0\n        assert output.data[1, 3] == 8.0\n        assert np.isnan(output.data[0, 4])  # TODO: how to handle outside bounds\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"linear\", \"interpolators\": [XarrayInterpolator], \"params\": {\"fill_nan\": True}},\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert int(output.data[0, 0]) == 2\n        assert int(output.data[2, 3]) == 15\n\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"slinear\", \"interpolators\": [XarrayInterpolator], \"params\": {\"fill_nan\": True}},\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert int(output.data[0, 0]) == 2\n        assert int(output.data[3, 3]) == 20\n        assert np.isnan(output.data[4, 4])\n\n        # Check extrapolation\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\n                \"method\": \"linear\",\n                \"interpolators\": [XarrayInterpolator],\n                \"params\": {\"fill_nan\": True, \"fill_value\": \"extrapolate\"},\n            },\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert int(output.data[0, 0]) == 2\n        assert int(output.data[4, 4]) == 26\n        assert np.all(~np.isnan(output.data))",
            "def test_interpolate_irregular_arbitrary_2dims(self):",
            "def test_interpolate_irregular_arbitrary_descending(self):",
            "def test_interpolate_irregular_arbitrary_swap(self):",
            "def test_interpolate_irregular_lat_lon(self):",
            "def test_interpolate_fill_nan(self):\n        source = np.arange(0, 25).astype(float)\n        source.resize((5, 5))\n        source[2, 2] = np.nan\n\n        coords_src = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lat\", \"lon\"])\n        coords_dst = Coordinates([clinspace(1, 11, 5), clinspace(1, 11, 5)], dims=[\"lat\", \"lon\"])\n\n        # Ensure nan present\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"linear\", \"interpolators\": [XarrayInterpolator], \"params\": {\"fill_nan\": False}},\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        assert np.all(np.isnan(output.data[1:3, 1:3]))\n\n        # Ensure nan gone\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"linear\", \"interpolators\": [XarrayInterpolator], \"params\": {\"fill_nan\": True}},\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        np.testing.assert_array_almost_equal(output.data[1:3, 1:3].ravel(), [8.4, 9.4, 13.4, 14.4])\n\n        # Ensure nan gone, flip lat-lon on source\n        coords_src = Coordinates([clinspace(0, 10, 5), clinspace(0, 10, 5)], dims=[\"lon\", \"lat\"])\n        node = MockArrayDataSource(\n            data=source,\n            coordinates=coords_src,\n            interpolation={\"method\": \"linear\", \"interpolators\": [XarrayInterpolator], \"params\": {\"fill_nan\": True}},\n        )\n        output = node.eval(coords_dst)\n        assert isinstance(output, UnitsDataArray)\n        assert np.all(output.lat.values == coords_dst[\"lat\"].coordinates)\n        np.testing.assert_array_almost_equal(output.data[1:3, 1:3].T.ravel(), [8.4, 9.4, 13.4, 14.4])"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/test/test_interpolation.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nTest interpolation methods\n\n\n\"\"\""
        ],
        "code_snippets": [
            "class TestInterpolationMixin(object):",
            "def test_interpolation_mixin(self):",
            "class InterpArray(InterpolationMixin, ArrayRaw):\n            pass\n\n        data = np.random.rand(4, 5)\n        native_coords = Coordinates([np.linspace(0, 3, 4), np.linspace(0, 4, 5)], [\"lat\", \"lon\"])\n        coords = Coordinates([np.linspace(0, 3, 7), np.linspace(0, 4, 9)], [\"lat\", \"lon\"])\n\n        iarr_src = InterpArray(source=data, coordinates=native_coords, interpolation=\"bilinear\")\n        arr_src = Array(source=data, coordinates=native_coords, interpolation=\"bilinear\")\n        arrb_src = ArrayRaw(source=data, coordinates=native_coords)\n\n        iaso = iarr_src.eval(coords)\n        aso = arr_src.eval(coords)\n        abso = arrb_src.eval(coords)\n\n        np.testing.assert_array_equal(iaso.data, aso.data)\n        np.testing.assert_array_equal(abso.data, data)",
            "class TestInterpolation(object):\n    s1 = ArrayRaw(\n        source=np.random.rand(9, 15),\n        coordinates=Coordinates([np.linspace(0, 8, 9), np.linspace(0, 14, 15)], [\"lat\", \"lon\"]),\n    )\n    s2 = ArrayRaw(\n        source=np.random.rand(9, 15),\n        coordinates=Coordinates([np.linspace(9, 17, 9), np.linspace(0, 14, 15)], [\"lat\", \"lon\"]),\n    )\n    interp = Interpolate(source=s1, interpolation=\"nearest\")\n    coords = Coordinates([np.linspace(0, 8, 17), np.linspace(0, 14, 29)], [\"lat\", \"lon\"])\n    coords2 = Coordinates([np.linspace(0, 17, 18), np.linspace(0, 14, 15)], [\"lat\", \"lon\"])\n    coords2c = Coordinates([np.linspace(0.1, 16.8, 5), np.linspace(0.1, 13.8, 3)], [\"lat\", \"lon\"])",
            "def test_basic_interpolation(self):\n        # This JUST tests the interface, tests for the actual value of the interpolation is left\n        # to the test_interpolation_manager.py file\n\n        o = self.interp.eval(self.coords)\n\n        assert o.shape == (17, 29)",
            "def test_interpolation_definition(self):\n        node = Node.from_json(self.interp.json)\n        o1 = node.eval(self.coords)\n        o2 = self.interp.eval(self.coords)\n        np.testing.assert_array_equal(o1.data, o2.data)\n        assert node.json == self.interp.json",
            "def test_compositor_chain(self):\n        dc = TileCompositorRaw(sources=[self.s2, self.s1])\n        node = Interpolate(source=dc, interpolation=\"nearest\")\n        o = node.eval(self.coords2)\n\n        np.testing.assert_array_equal(o.data, np.concatenate([self.s1.source, self.s2.source], axis=0))",
            "def test_get_bounds(self):\n        assert self.interp.get_bounds() == self.s1.get_bounds()",
            "class TestInterpolationBehavior(object):",
            "def test_linear_1D_issue411and413(self):\n        data = [0, 1, 2]\n        raw_coords = data.copy()\n        raw_e_coords = [0, 0.5, 1, 1.5, 2]\n\n        for dim in [\"lat\", \"lon\", \"alt\", \"time\"]:\n            ec = Coordinates([raw_e_coords], [dim], crs=\"+proj=longlat +datum=WGS84 +no_defs +vunits=m\")\n\n            arrb = ArrayRaw(\n                source=data,\n                coordinates=Coordinates([raw_coords], [dim], crs=\"+proj=longlat +datum=WGS84 +no_defs +vunits=m\"),\n            )\n            node = Interpolate(source=arrb, interpolation=\"linear\")\n            o = node.eval(ec)\n\n            np.testing.assert_array_equal(o.data, raw_e_coords, err_msg=\"dim {} failed to interpolate\".format(dim))\n\n        # Do time interpolation explicitly\n        raw_coords = [\"2020-11-01\", \"2020-11-03\", \"2020-11-05\"]\n        raw_et_coords = [\"2020-11-01\", \"2020-11-02\", \"2020-11-03\", \"2020-11-04\", \"2020-11-05\"]\n        ec = Coordinates([raw_et_coords], [\"time\"])\n\n        arrb = ArrayRaw(source=data, coordinates=Coordinates([raw_coords], [\"time\"]))\n        node = Interpolate(source=arrb, interpolation=\"linear\")\n        o = node.eval(ec)\n\n        np.testing.assert_array_equal(\n            o.data, raw_e_coords, err_msg=\"dim time failed to interpolate with datetime64 coords\"\n        )",
            "def test_stacked_coords_with_partial_dims_issue123(self):\n        node = Array(\n            source=[0, 1, 2],\n            coordinates=Coordinates(\n                [[[0, 2, 1], [10, 12, 11], [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"]]], dims=[\"lat_lon_time\"]\n            ),\n            interpolation=\"nearest\",\n        )\n\n        # unstacked or and stacked requests without time\n        o1 = node.eval(Coordinates([[0.5, 1.5], [10.5, 11.5]], dims=[\"lat\", \"lon\"]))\n        o2 = node.eval(Coordinates([[[0.5, 1.5], [10.5, 11.5]]], dims=[\"lat_lon\"]))\n\n        assert_array_equal(o1.data, [[0, 2], [2, 1]])\n        assert_array_equal(o2.data, [0, 1])\n\n        # request without lat or lon\n        o3 = node.eval(Coordinates([\"2018-01-01\"], dims=[\"time\"]))\n        assert o3.data[0] == 0",
            "def test_ignored_interpolation_params_issue340(self, caplog):\n        node = Array(\n            source=[0, 1, 2],\n            coordinates=Coordinates([[0, 2, 1]], dims=[\"time\"]),\n            interpolation={\"method\": \"nearest\", \"params\": {\"fake_param\": 1.1, \"spatial_tolerance\": 1}},\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n            node.eval(Coordinates([[0.5, 1.5]], [\"time\"]))\n        assert \"interpolation parameter 'fake_param' was ignored\" in caplog.text\n        assert \"interpolation parameter 'spatial_tolerance' was ignored\" not in caplog.text",
            "def test_silent_nearest_neighbor_interp_bug_issue412(self):\n        node = podpac.data.Array(\n            source=[0, 1, 2],\n            coordinates=podpac.Coordinates([[1, 5, 9]], dims=[\"lat\"]),\n            interpolation=[{\"method\": \"bilinear\", \"dims\": [\"lat\"], \"interpolators\": [ScipyGrid]}],\n        )\n        with pytest.raises(InterpolationException, match=\"can't be handled\"):\n            o = node.eval(podpac.Coordinates([podpac.crange(1, 9, 1)], dims=[\"lat\"]))\n\n        node = podpac.data.Array(\n            source=[0, 1, 2],\n            coordinates=podpac.Coordinates([[1, 5, 9]], dims=[\"lat\"]),\n            interpolation=[{\"method\": \"bilinear\", \"dims\": [\"lat\"]}],\n        )\n        o = node.eval(podpac.Coordinates([podpac.crange(1, 9, 1)], dims=[\"lat\"]))\n        assert_array_equal(o.data, np.linspace(0, 2, 9))",
            "def test_selection_crs(self):\n        base = podpac.core.data.array_source.ArrayRaw(\n            source=[0, 1, 2],\n            coordinates=podpac.Coordinates(\n                [[1, 5, 9]], dims=[\"time\"], crs=\"+proj=longlat +datum=WGS84 +no_defs +vunits=m\"\n            ),\n        )\n        node = podpac.interpolators.Interpolate(source=base, interpolation=\"linear\")\n        tocrds = podpac.Coordinates([podpac.crange(1, 9, 1, \"time\")], crs=\"EPSG:4326\")\n        o = node.eval(tocrds)\n        assert o.crs == tocrds.crs\n        assert_array_equal(o.data, np.linspace(0, 2, 9))",
            "def test_floating_point_crs_disagreement(self):\n        tocrds = podpac.Coordinates([[39.1, 39.0, 38.9], [-77.1, -77, -77.2]], dims=[\"lat\", \"lon\"], crs=\"EPSG:4326\")\n        base = podpac.core.data.array_source.ArrayRaw(\n            source=np.random.rand(3, 3), coordinates=tocrds.transform(\"EPSG:32618\")\n        )\n        node = podpac.interpolators.Interpolate(source=base, interpolation=\"nearest\")\n        o = node.eval(tocrds)\n        assert np.all((o.lat.data - tocrds[\"lat\"].coordinates) == 0)\n\n        # now check the Mixin\n        node2 = podpac.core.data.array_source.Array(\n            source=np.random.rand(3, 3), coordinates=tocrds.transform(\"EPSG:32618\")\n        )\n        o = node2.eval(tocrds)\n        assert np.all((o.lat.data - tocrds[\"lat\"].coordinates) == 0)\n\n        # now check the reverse operation\n        tocrds = podpac.Coordinates(\n            [podpac.clinspace(4307580, 4330177, 7), podpac.clinspace(309220, 327053, 8)],\n            dims=[\"lat\", \"lon\"],\n            crs=\"EPSG:32618\",\n        )\n        srccrds = podpac.Coordinates(\n            [podpac.clinspace(39.2, 38.8, 9), podpac.clinspace(-77.3, -77.0, 9)], dims=[\"lat\", \"lon\"], crs=\"EPSG:4326\"\n        )\n        node3 = podpac.core.data.array_source.Array(source=np.random.rand(9, 9), coordinates=srccrds)\n        o = node3.eval(tocrds)\n        assert np.all((o.lat.data - tocrds[\"lat\"].coordinates) == 0)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/interpolation/test/test_selector.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestSelector(object):\n    lat_coarse = np.linspace(0, 1, 3)\n    lat_fine = np.linspace(-0.1, 1.15, 8)\n    lat_random_fine = [0.72, -0.05, 1.3, 0.35, 0.22, 0.543, 0.44, 0.971]\n    lat_random_coarse = [0.64, -0.25, 0.83]\n    lon_coarse = lat_coarse + 1\n    lon_fine = lat_fine + 1\n    time_coarse = clinspace(\"2020-01-01T12\", \"2020-01-02T12\", 3)\n    time_fine = clinspace(\"2020-01-01T09:36\", \"2020-01-02T15:35\", 8)\n    alt_coarse = lat_coarse + 3\n    alt_fine = lat_fine + 3\n\n    nn_request_fine_from_coarse = [0, 1, 2]\n    nn_request_coarse_from_fine = [1, 3, 6]\n    lin_request_fine_from_coarse = [0, 1, 2]\n    lin_request_coarse_from_fine = [0, 1, 3, 4, 6, 7]\n    # nn_request_fine_from_random_fine = [1, 1, 4, 6, 5, 0, 7, 2]\n    nn_request_fine_from_random_fine = [0, 1, 2, 4, 5, 6, 7]\n    nn_request_coarse_from_random_fine = [1, 5, 7]\n    nn_request_fine_from_random_coarse = [0, 1, 2]\n    nn_request_coarse_from_random_coarse = [0, 1, 2]\n    nn_request_coarse_from_fine_grid = [1, 2, 3, 5, 6]\n\n    coords = {}\n\n    @classmethod",
            "def setup_class(cls):\n        cls.make_coord_combos(cls)\n\n    @staticmethod",
            "def make_coord_combos(self):\n        # Make 1-D ones\n        dims = [\"lat\", \"lon\", \"time\", \"alt\"]\n        for r in [\"fine\", \"coarse\"]:\n            for i in range(4):\n                d = dims[i]\n                k = d + \"_\" + r\n                self.coords[k] = Coordinates([getattr(self, k)], [d])\n                # stack pairs 2D\n                for ii in range(i, 4):\n                    d2 = dims[ii]\n                    if d == d2:\n                        continue\n                    k2 = \"_\".join([d2, r])\n                    k2f = \"_\".join([d, d2, r])\n                    self.coords[k2f] = Coordinates([[getattr(self, k), getattr(self, k2)]], [[d, d2]])\n                    # stack pairs 3D\n                    for iii in range(ii, 4):\n                        d3 = dims[iii]\n                        if d3 == d or d3 == d2:\n                            continue\n                        k3 = \"_\".join([d3, r])\n                        k3f = \"_\".join([d, d2, d3, r])\n                        self.coords[k3f] = Coordinates(\n                            [[getattr(self, k), getattr(self, k2), getattr(self, k3)]], [[d, d2, d3]]\n                        )\n                        # stack pairs 4D\n                        for iv in range(iii, 4):\n                            d4 = dims[iv]\n                            if d4 == d or d4 == d2 or d4 == d3:\n                                continue\n                            k4 = \"_\".join([d4, r])\n                            k4f = \"_\".join([d, d2, d3, d4, r])\n                            self.coords[k4f] = Coordinates(\n                                [[getattr(self, k), getattr(self, k2), getattr(self, k3), getattr(self, k4)]],\n                                [[d, d2, d3, d4]],\n                            )",
            "def test_nn_nonmonotonic_selector(self):\n        selector = Selector(\"nearest\")\n        for request in [\"lat_coarse\", \"lat_fine\"]:\n            for source in [\"lat_random_fine\", \"lat_random_coarse\"]:\n                if \"fine\" in request and \"fine\" in source:\n                    truth = set(self.nn_request_fine_from_random_fine)\n                if \"coarse\" in request and \"coarse\" in source:\n                    truth = set(self.nn_request_coarse_from_random_coarse)\n                if \"coarse\" in request and \"fine\" in source:\n                    truth = set(self.nn_request_coarse_from_random_fine)\n                if \"fine\" in request and \"coarse\" in source:\n                    truth = set(self.nn_request_fine_from_random_coarse)\n\n                src_coords = Coordinates([getattr(self, source)], [\"lat\"])\n                req_coords = Coordinates([getattr(self, request)], [\"lat\"])\n                c, ci = selector.select(src_coords, req_coords)\n                np.testing.assert_array_equal(\n                    ci,\n                    (np.array(list(truth)),),\n                    err_msg=\"Selection using source {} and request {} failed with {} != {} (truth)\".format(\n                        source, request, ci, list(truth)\n                    ),\n                )",
            "def test_linear_selector(self):\n        selector = Selector(\"linear\")\n        for request in self.coords:\n            for source in self.coords:\n                dims = [d for d in self.coords[source].udims if d in self.coords[request].udims]\n                if len(dims) == 0:\n                    continue  # Invalid combination\n                if \"fine\" in request and \"fine\" in source:\n                    continue\n                if \"coarse\" in request and \"coarse\" in source:\n                    continue\n                if \"coarse\" in request and \"fine\" in source:\n                    truth = self.lin_request_coarse_from_fine\n                if \"fine\" in request and \"coarse\" in source:\n                    truth = self.lin_request_fine_from_coarse\n\n                c, ci = selector.select(self.coords[source], self.coords[request])\n                np.testing.assert_array_equal(\n                    ci,\n                    (np.array(truth),),\n                    err_msg=\"Selection using source {} and request {} failed with {} != {} (truth)\".format(\n                        source, request, ci, truth\n                    ),\n                )",
            "def test_bilinear_selector(self):\n        selector = Selector(\"bilinear\")\n        for request in self.coords:\n            for source in self.coords:\n                dims = [d for d in self.coords[source].udims if d in self.coords[request].udims]\n                if len(dims) == 0:\n                    continue  # Invalid combination\n                if \"fine\" in request and \"fine\" in source:\n                    continue\n                if \"coarse\" in request and \"coarse\" in source:\n                    continue\n                if \"coarse\" in request and \"fine\" in source:\n                    truth = self.lin_request_coarse_from_fine\n                if \"fine\" in request and \"coarse\" in source:\n                    truth = self.lin_request_fine_from_coarse\n\n                c, ci = selector.select(self.coords[source], self.coords[request])\n                np.testing.assert_array_equal(\n                    ci,\n                    (np.array(truth),),\n                    err_msg=\"Selection using source {} and request {} failed with {} != {} (truth)\".format(\n                        source, request, ci, truth\n                    ),\n                )",
            "def test_bilinear_selector_negative_step(self):\n        selector = Selector(\"bilinear\")\n        request1 = Coordinates([clinspace(-0.5, -1, 11)], [\"lat\"])\n        request2 = Coordinates([clinspace(-1, -0.5, 11)], [\"lat\"])\n        source1 = Coordinates([clinspace(-2, 0, 100)], [\"lat\"])\n        source2 = Coordinates([clinspace(0, -2, 100)], [\"lat\"])\n        c11, ci11 = selector.select(source1, request1)\n        assert len(c11[\"lat\"]) == 22\n        assert len(ci11[0]) == 22\n\n        c12, ci12 = selector.select(source1, request2)\n        assert len(c12[\"lat\"]) == 22\n        assert len(ci12[0]) == 22\n\n        c21, ci21 = selector.select(source2, request1)\n        assert len(c21[\"lat\"]) == 22\n        assert len(ci21[0]) == 22\n\n        c22, ci22 = selector.select(source2, request2)\n        assert len(c22[\"lat\"]) == 22\n        assert len(ci22[0]) == 22\n\n        np.testing.assert_equal(ci11[0], ci12[0])\n        np.testing.assert_equal(ci21[0], ci22[0])",
            "def test_nearest_selector_negative_step(self):\n        selector = Selector(\"nearest\")\n        request1 = Coordinates([clinspace(-0.5, -1, 11)], [\"lat\"])\n        request2 = Coordinates([clinspace(-1, -0.5, 11)], [\"lat\"])\n        source1 = Coordinates([clinspace(-2, 0, 100)], [\"lat\"])\n        source2 = Coordinates([clinspace(0, -2, 100)], [\"lat\"])\n        c11, ci11 = selector.select(source1, request1)\n        assert len(c11[\"lat\"]) == 11\n        assert len(ci11[0]) == 11\n\n        c12, ci12 = selector.select(source1, request2)\n        assert len(c12[\"lat\"]) == 11\n        assert len(ci12[0]) == 11\n\n        c21, ci21 = selector.select(source2, request1)\n        assert len(c21[\"lat\"]) == 11\n        assert len(ci21[0]) == 11\n\n        c22, ci22 = selector.select(source2, request2)\n        assert len(c22[\"lat\"]) == 11\n        assert len(ci22[0]) == 11\n\n        np.testing.assert_equal(ci11[0], ci12[0])\n        np.testing.assert_equal(ci21[0], ci22[0])",
            "def test_nearest_selector_negative_time_step(self):\n        selector = Selector(\"nearest\")\n        request1 = Coordinates([clinspace(\"2020-01-01\", \"2020-01-11\", 11)], [\"time\"])\n        request2 = Coordinates([clinspace(\"2020-01-11\", \"2020-01-01\", 11)], [\"time\"])\n        source1 = Coordinates([clinspace(\"2020-01-22T00\", \"2020-01-01T00\", 126)], [\"time\"])\n        source2 = Coordinates([clinspace(\"2020-01-01T00\", \"2020-01-22T00\", 126)], [\"time\"])\n        c11, ci11 = selector.select(source1, request1)\n        assert len(c11[\"time\"]) == 11\n        assert len(ci11[0]) == 11\n\n        c12, ci12 = selector.select(source1, request2)\n        assert len(c12[\"time\"]) == 11\n        assert len(ci12[0]) == 11\n\n        c21, ci21 = selector.select(source2, request1)\n        assert len(c21[\"time\"]) == 11\n        assert len(ci21[0]) == 11\n\n        c22, ci22 = selector.select(source2, request2)\n        assert len(c22[\"time\"]) == 11\n        assert len(ci22[0]) == 11\n\n        np.testing.assert_equal(ci11[0], ci12[0])\n        np.testing.assert_equal(ci21[0], ci22[0])",
            "def test_nn_selector(self):\n        selector = Selector(\"nearest\")\n        for request in self.coords:\n            for source in self.coords:\n                dims = [d for d in self.coords[source].udims if d in self.coords[request].udims]\n                if len(dims) == 0:\n                    continue  # Invalid combination\n                if \"fine\" in request and \"fine\" in source:\n                    continue\n                if \"coarse\" in request and \"coarse\" in source:\n                    continue\n                if \"coarse\" in request and \"fine\" in source:\n                    truth = self.nn_request_coarse_from_fine\n                if \"fine\" in request and \"coarse\" in source:\n                    truth = self.nn_request_fine_from_coarse\n\n                c, ci = selector.select(self.coords[source], self.coords[request])\n                np.testing.assert_array_equal(\n                    ci,\n                    (np.array(truth),),\n                    err_msg=\"Selection using source {} and request {} failed with {} != {} (truth)\".format(\n                        source, request, ci, truth\n                    ),\n                )",
            "def test_uniform2uniform(self):\n        fine = Coordinates([self.lat_fine, self.lon_fine], [\"lat\", \"lon\"])\n        coarse = Coordinates([self.lat_coarse, self.lon_coarse], [\"lat\", \"lon\"])\n\n        selector = Selector(\"nearest\")\n\n        c, ci = selector.select(fine, coarse)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_coarse_from_fine, self.nn_request_coarse_from_fine)):\n            np.testing.assert_array_equal(cci, trth)\n\n        c, ci = selector.select(coarse, fine)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_fine_from_coarse, self.nn_request_fine_from_coarse)):\n            np.testing.assert_array_equal(cci, trth)",
            "def test_point2uniform(self):\n        u_fine = Coordinates([self.lat_fine, self.lon_fine], [\"lat\", \"lon\"])\n        u_coarse = Coordinates([self.lat_coarse, self.lon_coarse], [\"lat\", \"lon\"])\n\n        p_fine = Coordinates([[self.lat_fine, self.lon_fine]], [[\"lat\", \"lon\"]])\n        p_coarse = Coordinates([[self.lat_coarse, self.lon_coarse]], [[\"lat\", \"lon\"]])\n\n        selector = Selector(\"nearest\")\n\n        c, ci = selector.select(u_fine, p_coarse)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_coarse_from_fine, self.nn_request_coarse_from_fine)):\n            np.testing.assert_array_equal(cci, trth)\n\n        c, ci = selector.select(u_coarse, p_fine)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_fine_from_coarse, self.nn_request_fine_from_coarse)):\n            np.testing.assert_array_equal(cci, trth)\n\n        c, ci = selector.select(p_fine, u_coarse)\n        np.testing.assert_array_equal(ci, (self.nn_request_coarse_from_fine_grid,))\n\n        c, ci = selector.select(p_coarse, u_fine)\n        np.testing.assert_array_equal(ci, (self.nn_request_fine_from_coarse,))\n\n        # Respect bounds\n        selector.respect_bounds = True\n        c, ci = selector.select(u_fine, p_coarse)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_coarse_from_fine, self.nn_request_coarse_from_fine)):\n            np.testing.assert_array_equal(cci, trth)",
            "def test_point2uniform_non_square(self):\n        u_fine = Coordinates([self.lat_fine, self.lon_fine[:-1]], [\"lat\", \"lon\"])\n        u_coarse = Coordinates([self.lat_coarse[:-1], self.lon_coarse], [\"lat\", \"lon\"])\n\n        p_fine = Coordinates([[self.lat_fine, self.lon_fine]], [[\"lat\", \"lon\"]])\n        p_coarse = Coordinates([[self.lat_coarse, self.lon_coarse]], [[\"lat\", \"lon\"]])\n\n        selector = Selector(\"nearest\")\n\n        c, ci = selector.select(u_fine, p_coarse)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_coarse_from_fine, self.nn_request_coarse_from_fine)):\n            np.testing.assert_array_equal(cci, trth)\n\n        c, ci = selector.select(u_coarse, p_fine)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_fine_from_coarse[:-1], self.nn_request_fine_from_coarse)):\n            np.testing.assert_array_equal(cci, trth)\n\n        c, ci = selector.select(p_fine, u_coarse)\n        np.testing.assert_array_equal(ci, (self.nn_request_coarse_from_fine_grid[:-1],))\n\n        c, ci = selector.select(p_coarse, u_fine)\n        np.testing.assert_array_equal(ci, (self.nn_request_fine_from_coarse,))\n\n        # Respect bounds\n        selector.respect_bounds = True\n        c, ci = selector.select(u_fine, p_coarse)\n        for cci, trth in zip(ci, np.ix_(self.nn_request_coarse_from_fine, self.nn_request_coarse_from_fine)):\n            np.testing.assert_array_equal(cci, trth)",
            "def test_point2uniform_non_square_xarray_type(self):\n        u_fine = Coordinates([self.lat_fine, self.lon_fine[:-1]], [\"lat\", \"lon\"])\n        u_coarse = Coordinates([self.lat_coarse[:-1], self.lon_coarse], [\"lat\", \"lon\"])\n\n        p_fine = Coordinates([[self.lat_fine, self.lon_fine]], [[\"lat\", \"lon\"]])\n        p_coarse = Coordinates([[self.lat_coarse, self.lon_coarse]], [[\"lat\", \"lon\"]])\n\n        selector = Selector(\"nearest\")\n        # Test xarray indices instead\n        cx, cix = selector.select(u_fine, p_coarse, index_type=\"xarray\")\n        cn, cin = selector.select(u_fine, p_coarse, index_type=\"numpy\")\n        xarr = Node().create_output_array(u_fine)\n        xarr[...] = np.random.rand(*xarr.shape)\n\n        np.testing.assert_equal(xarr[cix], xarr.data[cin])",
            "def test_slice_index(self):\n        selector = Selector(\"nearest\")\n\n        src = Coordinates([[0, 1, 2, 3, 4, 5]], dims=[\"lat\"])\n\n        # uniform\n        req = Coordinates([[2, 4]], dims=[\"lat\"])\n        c, ci = selector.select(src, req, index_type=\"slice\")\n        assert isinstance(ci[0], slice)\n        assert c == src[ci]\n\n        # non uniform\n        req = Coordinates([[1, 2, 4]], dims=[\"lat\"])\n        c, ci = selector.select(src, req, index_type=\"slice\")\n        assert isinstance(ci[0], slice)\n        assert c == src[ci]\n\n        # empty\n        req = Coordinates([[10]], dims=[\"lat\"])\n        c, ci = selector.select(src, req, index_type=\"slice\")\n        assert isinstance(ci[0], slice)\n        assert c == src[ci]\n\n        # singleton\n        req = Coordinates([[2]], dims=[\"lat\"])\n        c, ci = selector.select(src, req, index_type=\"slice\")\n        assert isinstance(ci[0], slice)\n        assert c == src[ci]"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/cfunctions.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    Create uniformly-spaced 1d coordinates with a start, stop, and step.\n\n    For numerical coordinates, the start, stop, and step are converted to ``float``. For time\n    coordinates, the start and stop are converted to numpy ``datetime64``, and the step is converted to numpy\n    ``timedelta64``. For convenience, podpac automatically converts datetime strings such as ``'2018-01-01'`` to\n    ``datetime64`` and timedelta strings such as ``'1,D'`` to ``timedelta64``.\n\n    Arguments\n    ---------\n    start : float, datetime64, datetime, str\n        Start coordinate.\n    stop : float, datetime64, datetime, str\n        Stop coordinate.\n    step : float, timedelta64, timedelta, str\n        Signed, non-zero step between coordinates.\n    name : str, optional\n        Dimension name.\n\n    Returns\n    -------\n    :class:`UniformCoordinates1d`\n        Uniformly-spaced 1d coordinates.\n    \"\"\"",
            "\"\"\"\n    Create uniformly-spaced 1d or stacked coordinates with a start, stop, and size.\n\n    For numerical coordinates, the start and stop are converted to ``float``. For time coordinates, the start and stop\n    are converted to numpy ``datetime64``. For convenience, podpac automatically converts datetime strings such as\n    ``'2018-01-01'`` to ``datetime64``.\n\n    Arguments\n    ---------\n    start : float, datetime64, datetime, str, tuple\n        Start coordinate for 1d coordinates, or tuple of start coordinates for stacked coordinates.\n    stop : float, datetime64, datetime, str, tuple\n        Stop coordinate for 1d coordinates, or tuple of stop coordinates for stacked coordinates.\n    size : int\n        Number of coordinates.\n    name : str, optional\n        Dimension name.\n\n    Returns\n    -------\n    :class:`UniformCoordinates1d`\n        Uniformly-spaced 1d coordinates.\n\n    Raises\n    ------\n    ValueError\n        If the start and stop are not the same size.\n    \"\"\""
        ],
        "code_snippets": [
            "def crange(start, stop, step, name=None):\n    \"\"\"\n    Create uniformly-spaced 1d coordinates with a start, stop, and step.\n\n    For numerical coordinates, the start, stop, and step are converted to ``float``. For time\n    coordinates, the start and stop are converted to numpy ``datetime64``, and the step is converted to numpy\n    ``timedelta64``. For convenience, podpac automatically converts datetime strings such as ``'2018-01-01'`` to\n    ``datetime64`` and timedelta strings such as ``'1,D'`` to ``timedelta64``.\n\n    Arguments\n    ---------\n    start : float, datetime64, datetime, str\n        Start coordinate.\n    stop : float, datetime64, datetime, str\n        Stop coordinate.\n    step : float, timedelta64, timedelta, str\n        Signed, non-zero step between coordinates.\n    name : str, optional\n        Dimension name.\n\n    Returns\n    -------\n    :class:`UniformCoordinates1d`\n        Uniformly-spaced 1d coordinates.\n    \"\"\"\n\n    return UniformCoordinates1d(start, stop, step=step, name=name)",
            "def clinspace(start, stop, size, name=None):\n    \"\"\"\n    Create uniformly-spaced 1d or stacked coordinates with a start, stop, and size.\n\n    For numerical coordinates, the start and stop are converted to ``float``. For time coordinates, the start and stop\n    are converted to numpy ``datetime64``. For convenience, podpac automatically converts datetime strings such as\n    ``'2018-01-01'`` to ``datetime64``.\n\n    Arguments\n    ---------\n    start : float, datetime64, datetime, str, tuple\n        Start coordinate for 1d coordinates, or tuple of start coordinates for stacked coordinates.\n    stop : float, datetime64, datetime, str, tuple\n        Stop coordinate for 1d coordinates, or tuple of stop coordinates for stacked coordinates.\n    size : int\n        Number of coordinates.\n    name : str, optional\n        Dimension name.\n\n    Returns\n    -------\n    :class:`UniformCoordinates1d`\n        Uniformly-spaced 1d coordinates.\n\n    Raises\n    ------\n    ValueError\n        If the start and stop are not the same size.\n    \"\"\"\n    if np.array(start).size != np.array(stop).size:\n        raise ValueError(\n            \"Size mismatch, 'start' and 'stop' must have the same size (%s != %s)\"\n            % (np.array(start).size, np.array(stop).size)\n        )\n\n    # As of numpy 0.16, np.array([0, (0, 10)]) no longer raises a ValueError\n    # so we have to explicitly check for sizes of start and stop (see above)\n    a = np.array([start, stop])\n    if a.ndim == 2:\n        cs = [UniformCoordinates1d(start[i], stop[i], size=size) for i in range(a[0].size)]\n        c = StackedCoordinates(cs, name=name)\n    else:\n        c = UniformCoordinates1d(start, stop, size=size, name=name)\n\n    return c"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/group_coordinates.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    List of multi-dimensional Coordinates.\n\n    GroupCoordinates contains a list of :class:`Coordinates` containing the same set of unstacked dimensions.\n\n    The GroupCoordinates object is list-like and can be indexed, appended, looped, etc like a standard ``list``. The\n    following ``Coordinates`` methods are wrapped for convenience:\n\n     * :meth:`intersect`\n\n    Parameters\n    ----------\n    udims : tuple\n        Tuple of shared dimensions.\n    \"\"\"",
            "\"\"\"\n        Create a Coordinates group.\n\n        Arguments\n        ---------\n        coords_list : list\n            list of :class:`Coordinates`\n        \"\"\"",
            "\"\"\"\n        Create a Coordinates group from a group definition.\n\n        Arguments\n        ---------\n        d : list\n            group definition\n\n        Returns\n        -------\n        :class:`CoordinatesGroup`\n            Coordinates group\n\n        See Also\n        --------\n        definition, from_json\n        \"\"\"",
            "\"\"\"\n        Create a Coordinates group from a group JSON definition.\n\n        Arguments\n        ---------\n        s : str\n            group JSON definition\n\n        Returns\n        -------\n        :class:`CoordinatesGroup`\n            Coordinates group\n\n        See Also\n        --------\n        json\n        \"\"\"",
            "\"\"\"Append :class:`Coordinates` to the group.\n\n        Arguments\n        ---------\n        c : :class:`Coordinates`\n            Coordinates to append.\n        \"\"\"",
            "\"\"\":tuple: Tuple of shared dimensions.\"\"\"",
            "\"\"\"\n        Serializable coordinates group definition.\n\n        The ``definition`` can be used to create new GroupCoordinates::\n\n            g = podpac.GroupCoordinates([...])\n            g2 = podpac.GroupCoordinates.from_definition(g.definition)\n\n        See Also\n        --------\n        from_definition, json\n        \"\"\"",
            "\"\"\"\n        Serialized coordinates group definition.\n\n        The ``definition`` can be used to create new GroupCoordinates::\n\n            g = podpac.GroupCoordinates(...)\n            g2 = podpac.GroupCoordinates.from_json(g.json)\n\n        See Also\n        --------\n        json\n        \"\"\"",
            "\"\"\"\n        GroupCoordinates hash.\n\n        *Note: To be replaced with the __hash__ method.*\n        \"\"\"",
            "\"\"\"\n        Intersect each Coordinates in the group with the given coordinates.\n\n        Parameters\n        ----------\n        other : :class:`Coordinates1d`, :class:`StackedCoordinates`, :class:`Coordinates`\n            Coordinates to intersect with.\n        outer : bool, optional\n            If True, do an *outer* intersection. Default False.\n        return_index : bool, optional\n            If True, return slice or indices for the selection in addition to coordinates. Default False.\n\n        Returns\n        -------\n        intersections : :class:`GroupCoordinates`\n            Coordinates group consisting of the intersection of each :class:`Coordinates`.\n        idx : list\n            List of lists of indices for each :class:`Coordinates` item, only if ``return_index`` is True.\n        \"\"\""
        ],
        "code_snippets": [
            "class GroupCoordinates(tl.HasTraits):\n    \"\"\"\n    List of multi-dimensional Coordinates.\n\n    GroupCoordinates contains a list of :class:`Coordinates` containing the same set of unstacked dimensions.\n\n    The GroupCoordinates object is list-like and can be indexed, appended, looped, etc like a standard ``list``. The\n    following ``Coordinates`` methods are wrapped for convenience:\n\n     * :meth:`intersect`\n\n    Parameters\n    ----------\n    udims : tuple\n        Tuple of shared dimensions.\n    \"\"\"\n\n    _items = tl.List(trait=tl.Instance(Coordinates))\n\n    @tl.validate(\"_items\")",
            "def _validate_items(self, d):\n        items = d[\"value\"]\n        if not items:\n            return items\n\n        # unstacked dims must match, but not necessarily in order\n        udims = items[0].udims\n        for c in items:\n            if set(c.udims) != set(udims):\n                raise ValueError(\"Mismatching dims: %s !~ %s\" % (udims, c.udims))\n\n        return items",
            "def __init__(self, coords_list):\n        \"\"\"\n        Create a Coordinates group.\n\n        Arguments\n        ---------\n        coords_list : list\n            list of :class:`Coordinates`\n        \"\"\"\n\n        return super(GroupCoordinates, self).__init__(_items=coords_list)",
            "def __repr__(self):\n        rep = self.__class__.__name__\n        rep += \"\\n\" + \"\\n\".join([repr(c) for c in self._items])\n        return rep\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # alternative constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @classmethod",
            "def from_definition(cls, d):\n        \"\"\"\n        Create a Coordinates group from a group definition.\n\n        Arguments\n        ---------\n        d : list\n            group definition\n\n        Returns\n        -------\n        :class:`CoordinatesGroup`\n            Coordinates group\n\n        See Also\n        --------\n        definition, from_json\n        \"\"\"\n\n        return cls([Coordinates.from_definition(elem) for elem in d])\n\n    @classmethod",
            "def from_json(cls, s):\n        \"\"\"\n        Create a Coordinates group from a group JSON definition.\n\n        Arguments\n        ---------\n        s : str\n            group JSON definition\n\n        Returns\n        -------\n        :class:`CoordinatesGroup`\n            Coordinates group\n\n        See Also\n        --------\n        json\n        \"\"\"\n\n        d = json.loads(s)\n        return cls.from_definition(d)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard list-like methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __len__(self):\n        return len(self._items)",
            "def __iter__(self):\n        return self._items.__iter__()",
            "def append(self, c):\n        \"\"\"Append :class:`Coordinates` to the group.\n\n        Arguments\n        ---------\n        c : :class:`Coordinates`\n            Coordinates to append.\n        \"\"\"\n\n        if not isinstance(c, Coordinates):\n            raise TypeError(\"Can only append Coordinates objects, not '%s'\" % type(c))\n\n        self._items = self._items + [c]",
            "def __add__(self, other):\n        if not isinstance(other, GroupCoordinates):\n            raise TypeError(\"Can only add GroupCoordinates objects, not '%s'\" % type(other))\n\n        return GroupCoordinates(self._items + other._items)",
            "def __iadd__(self, other):\n        if not isinstance(other, GroupCoordinates):\n            raise TypeError(\"Can only add GroupCoordinates objects, not '%s'\" % type(other))\n\n        self._items = self._items + other._items\n        return self\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def udims(self):",
            "def definition(self):\n        \"\"\"\n        Serializable coordinates group definition.\n\n        The ``definition`` can be used to create new GroupCoordinates::\n\n            g = podpac.GroupCoordinates([...])\n            g2 = podpac.GroupCoordinates.from_definition(g.definition)\n\n        See Also\n        --------\n        from_definition, json\n        \"\"\"\n\n        return [c.definition for c in self._items]\n\n    @property",
            "def json(self):\n        \"\"\"\n        Serialized coordinates group definition.\n\n        The ``definition`` can be used to create new GroupCoordinates::\n\n            g = podpac.GroupCoordinates(...)\n            g2 = podpac.GroupCoordinates.from_json(g.json)\n\n        See Also\n        --------\n        json\n        \"\"\"\n\n        return json.dumps(self.definition, separators=(\",\", \":\"), cls=JSONEncoder)\n\n    @property",
            "def hash(self):\n        \"\"\"\n        GroupCoordinates hash.\n\n        *Note: To be replaced with the __hash__ method.*\n        \"\"\"\n\n        return hash_alg(self.json.encode(\"utf-8\")).hexdigest()\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def intersect(self, other, outer=False, return_index=False):\n        \"\"\"\n        Intersect each Coordinates in the group with the given coordinates.\n\n        Parameters\n        ----------\n        other : :class:`Coordinates1d`, :class:`StackedCoordinates`, :class:`Coordinates`\n            Coordinates to intersect with.\n        outer : bool, optional\n            If True, do an *outer* intersection. Default False.\n        return_index : bool, optional\n            If True, return slice or indices for the selection in addition to coordinates. Default False.\n\n        Returns\n        -------\n        intersections : :class:`GroupCoordinates`\n            Coordinates group consisting of the intersection of each :class:`Coordinates`.\n        idx : list\n            List of lists of indices for each :class:`Coordinates` item, only if ``return_index`` is True.\n        \"\"\"\n\n        intersections = [c.intersect(other, outer=outer, return_index=True) for c in self._items]\n        g = [c for c, I in intersections]\n\n        if return_index:\n            return g, [I for c, I in intersections]\n        else:\n            return g"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/stacked_coordinates.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    Stacked coordinates.\n\n    StackedCoordinates contain coordinates from two or more different dimensions that are stacked together to form a\n    list of points (rather than a grid). The underlying coordinates values are :class:`Coordinates1d` objects of equal\n    size. The name for the stacked coordinates combines the underlying dimensions with underscores, e.g. ``'lat_lon'``\n    or ``'lat_lon_time'``.\n\n    When creating :class:`Coordinates`, podpac automatically detects StackedCoordinates. The following Coordinates\n    contain 3 stacked lat-lon coordinates and 2 time coordinates in a 3 x 2 grid::\n\n        >>> lat = [0, 1, 2]\n        >>> lon = [10, 20, 30]\n        >>> time = ['2018-01-01', '2018-01-02']\n        >>> podpac.Coordinates([[lat, lon], time], dims=['lat_lon', 'time'])\n        Coordinates\n            lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[0.0, 2.0], N[3]\n            lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[10.0, 30.0], N[3]\n            time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-02], N[2]\n\n    For convenience, you can also create uniformly-spaced stacked coordinates using :class:`clinspace`::\n\n        >>> lat_lon = podpac.clinspace((0, 10), (2, 30), 3)\n        >>> time = ['2018-01-01', '2018-01-02']\n        >>> podpac.Coordinates([lat_lon, time], dims=['lat_lon', 'time'])\n        Coordinates\n            lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[0.0, 2.0], N[3]\n            lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[10.0, 30.0], N[3]\n            time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-02], N[2]\n\n    Parameters\n    ----------\n    dims : tuple\n        Tuple of dimension names.\n    name : str\n        Stacked dimension name.\n    coords : dict-like\n        xarray coordinates (container of coordinate arrays)\n    coordinates : pandas.MultiIndex\n        MultiIndex of stacked coordinates values.\n\n    \"\"\"",
            "\"\"\"\n        Initialize a multidimensional coords bject.\n\n        Parameters\n        ----------\n        coords : list, :class:`StackedCoordinates`\n            Coordinate values in a list, or a StackedCoordinates object to copy.\n\n        See Also\n        --------\n        clinspace, crange\n        \"\"\"",
            "\"\"\"\n        Create 1d Coordinates from named xarray coordinates.\n\n        Arguments\n        ---------\n        x : xarray.DataArray\n            Nade DataArray of the coordinate values\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            1d coordinates\n        \"\"\"",
            "\"\"\"\n        Create StackedCoordinates from a stacked coordinates definition.\n\n        Arguments\n        ---------\n        d : list\n            stacked coordinates definition\n\n        Returns\n        -------\n        :class:`StackedCoordinates`\n            stacked coordinates object\n\n        See Also\n        --------\n        definition\n        \"\"\"",
            "\"\"\":tuple: Tuple of dimension names.\"\"\"",
            "\"\"\":int: coordinates array ndim.\"\"\"",
            "\"\"\":str: Stacked dimension name. Stacked dimension names are the individual `dims` joined by an underscore.\"\"\"",
            "\"\"\":int: Number of stacked coordinates.\"\"\"",
            "\"\"\":tuple: Shape of the stacked coordinates.\"\"\"",
            "\"\"\":dict: Dictionary of (low, high) coordinates bounds in each dimension\"\"\"",
            "\"\"\":dict-like: xarray coordinates (container of coordinate arrays)\"\"\"",
            "\"\"\":list: Serializable stacked coordinates definition.\"\"\"",
            "\"\"\":list: Serializable stacked coordinates definition, containing all properties. For internal use.\"\"\"",
            "\"\"\"\n        Make a copy of the stacked coordinates.\n\n        Returns\n        -------\n        :class:`StackedCoordinates`\n            Copy of the stacked coordinates.\n        \"\"\"",
            "\"\"\"\n        Remove duplicate stacked coordinate values.\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n\n        Returns\n        -------\n        unique : :class:`StackedCoordinates`\n            New StackedCoordinates object with unique, sorted, flattened coordinate values.\n        unique_index : list of indices\n            index\n        \"\"\"",
            "\"\"\"Get coordinate area bounds, including boundary information, for each unstacked dimension.\n\n        Arguments\n        ---------\n        boundary : dict\n            dictionary of boundary offsets for each unstacked dimension. Point dimensions can be omitted.\n\n        Returns\n        -------\n        area_bounds : dict\n            Dictionary of (low, high) coordinates area_bounds in each unstacked dimension\n        \"\"\"",
            "\"\"\"\n        Get the coordinate values that are within the given bounds in all dimensions.\n\n        *Note: you should not generally need to call this method directly.*\n\n        Parameters\n        ----------\n        bounds : dict\n            dictionary of dim -> (low, high) selection bounds\n        outer : bool, optional\n            If True, do *outer* selections. Default False.\n        return_index : bool, optional\n            If True, return index for the selections in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`StackedCoordinates`\n            StackedCoordinates object consisting of the selection in all dimensions.\n        selection_index : slice, boolean array\n            Slice or index for the selected coordinates, only if ``return_index`` is True.\n        \"\"\"",
            "\"\"\"\n        Transpose (re-order) the dimensions of the StackedCoordinates.\n\n        Parameters\n        ----------\n        dim_1, dim_2, ... : str, optional\n            Reorder dims to this order. By default, reverse the dims.\n        in_place : boolean, optional\n            If True, transpose the dimensions in-place.\n            Otherwise (default), return a new, transposed Coordinates object.\n\n        Returns\n        -------\n        transposed : :class:`StackedCoordinates`\n            The transposed StackedCoordinates object.\n        \"\"\"",
            "\"\"\"Report whether other coordinates contains these coordinates.\n\n        Arguments\n        ---------\n        other : Coordinates, StackedCoordinates\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates.\n        \"\"\"",
            "\"\"\"Return the horizontal resolution of a Uniform 1D Coordinate\n\n        Parameters\n        ----------\n        ellipsoid_tuple: tuple\n            a tuple containing ellipsoid information from the the original coordinates to pass to geopy\n        coordinate_name: str\n            \"cartesian\" or \"ellipsoidal\", to tell calculate_distance what kind of calculation to do\n        restype: str\n            The kind of horizontal resolution that should be returned. Supported values are:\n            - \"nominal\" <-- Gives average nearest distance of all points, with some error\n            - \"summary\" <-- Gives the mean and standard deviation of nearest distance betweem points, with some error\n            - \"full\" <-- Gives exact distance matrix\n        units: str\n            desired unit to return\n\n        Returns\n        -------\n        float * (podpac.unit)\n            If restype == \"nominal\", return the average nearest distance with some error\n        tuple * (podpac.unit)\n            If restype == \"summary\", return average and std.dev of nearest distances, with some error\n        np.ndarray * (podpac.unit)\n            if restype == \"full\", return exact distance matrix\n        ValueError\n            if unknown restype\n\n        \"\"\"",
            "\"\"\"Use a KDTree to return approximate stacked resolution with some loss of accuracy.\n\n            Returns\n            -------\n            The average min distance of every point\n\n            \"\"\"",
            "\"\"\"Return the approximate mean resolution and std.deviation using a KDTree\n\n            Returns\n            -------\n            tuple\n                Average min distance of every point and standard deviation of those min distances\n            \"\"\"",
            "\"\"\"Returns the exact distance between every point using brute force\n\n            Returns\n            -------\n            distance matrix of size (NxN), where N is the number of points in the dimension\n            \"\"\""
        ],
        "code_snippets": [
            "class StackedCoordinates(BaseCoordinates):\n    \"\"\"\n    Stacked coordinates.\n\n    StackedCoordinates contain coordinates from two or more different dimensions that are stacked together to form a\n    list of points (rather than a grid). The underlying coordinates values are :class:`Coordinates1d` objects of equal\n    size. The name for the stacked coordinates combines the underlying dimensions with underscores, e.g. ``'lat_lon'``\n    or ``'lat_lon_time'``.\n\n    When creating :class:`Coordinates`, podpac automatically detects StackedCoordinates. The following Coordinates\n    contain 3 stacked lat-lon coordinates and 2 time coordinates in a 3 x 2 grid::\n\n        >>> lat = [0, 1, 2]\n        >>> lon = [10, 20, 30]\n        >>> time = ['2018-01-01', '2018-01-02']\n        >>> podpac.Coordinates([[lat, lon], time], dims=['lat_lon', 'time'])\n        Coordinates\n            lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[0.0, 2.0], N[3]\n            lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[10.0, 30.0], N[3]\n            time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-02], N[2]\n\n    For convenience, you can also create uniformly-spaced stacked coordinates using :class:`clinspace`::\n\n        >>> lat_lon = podpac.clinspace((0, 10), (2, 30), 3)\n        >>> time = ['2018-01-01', '2018-01-02']\n        >>> podpac.Coordinates([lat_lon, time], dims=['lat_lon', 'time'])\n        Coordinates\n            lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[0.0, 2.0], N[3]\n            lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[10.0, 30.0], N[3]\n            time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-02], N[2]\n\n    Parameters\n    ----------\n    dims : tuple\n        Tuple of dimension names.\n    name : str\n        Stacked dimension name.\n    coords : dict-like\n        xarray coordinates (container of coordinate arrays)\n    coordinates : pandas.MultiIndex\n        MultiIndex of stacked coordinates values.\n\n    \"\"\"\n\n    _coords = tl.List(trait=tl.Instance(Coordinates1d), read_only=True)",
            "def __init__(self, coords, name=None, dims=None):\n        \"\"\"\n        Initialize a multidimensional coords bject.\n\n        Parameters\n        ----------\n        coords : list, :class:`StackedCoordinates`\n            Coordinate values in a list, or a StackedCoordinates object to copy.\n\n        See Also\n        --------\n        clinspace, crange\n        \"\"\"\n\n        if not isinstance(coords, (list, tuple)):\n            raise TypeError(\"Unrecognized coords type '%s'\" % type(coords))\n\n        if len(coords) < 2:\n            raise ValueError(\"Stacked coords must have at least 2 coords, got %d\" % len(coords))\n\n        # coerce\n        coords = tuple(c if isinstance(c, Coordinates1d) else ArrayCoordinates1d(c) for c in coords)\n\n        # set coords\n        self.set_trait(\"_coords\", coords)\n\n        # propagate properties\n        if dims is not None and name is not None:\n            raise TypeError(\"StackedCoordinates expected 'dims' or 'name', not both\")\n        if dims is not None:\n            self._set_dims(dims)\n        if name is not None:\n            self._set_name(name)\n\n        # finalize\n        super(StackedCoordinates, self).__init__()\n\n    @tl.validate(\"_coords\")",
            "def _validate_coords(self, d):\n        val = d[\"value\"]\n\n        # check sizes\n        shape = val[0].shape\n        for c in val[1:]:\n            if c.shape != shape:\n                raise ValueError(\"Shape mismatch in stacked coords %s != %s\" % (c.shape, shape))\n\n        # check dims\n        dims = [c.name for c in val]\n        for i, dim in enumerate(dims):\n            if dim is not None and dim in dims[:i]:\n                raise ValueError(\"Duplicate dimension '%s' in stacked coords\" % dim)\n\n        return val",
            "def _set_name(self, value):\n        dims = value.split(\"_\")\n\n        # check length\n        if len(dims) != len(self._coords):\n            raise ValueError(\"Invalid name '%s' for StackedCoordinates with length %d\" % (value, len(self._coords)))\n\n        self._set_dims(dims)",
            "def _set_dims(self, dims):\n        # check size\n        if len(dims) != len(self._coords):\n            raise ValueError(\"Invalid dims '%s' for StackedCoordinates with length %d\" % (dims, len(self._coords)))\n\n        for i, dim in enumerate(dims):\n            if dim is not None and dim in dims[:i]:\n                raise ValueError(\"Duplicate dimension '%s' in dims\" % dim)\n\n        # set names, checking for duplicates\n        for i, (c, dim) in enumerate(zip(self._coords, dims)):\n            if dim is None:\n                continue\n            c._set_name(dim)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Alternate constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @classmethod",
            "def from_xarray(cls, x, **kwargs):\n        \"\"\"\n        Create 1d Coordinates from named xarray coordinates.\n\n        Arguments\n        ---------\n        x : xarray.DataArray\n            Nade DataArray of the coordinate values\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            1d coordinates\n        \"\"\"\n\n        dims = x.dims[0].split(\"_\")\n        cs = [x[dim].data for dim in dims]\n        return cls(cs, dims=dims, **kwargs)\n\n    @classmethod",
            "def from_definition(cls, d):\n        \"\"\"\n        Create StackedCoordinates from a stacked coordinates definition.\n\n        Arguments\n        ---------\n        d : list\n            stacked coordinates definition\n\n        Returns\n        -------\n        :class:`StackedCoordinates`\n            stacked coordinates object\n\n        See Also\n        --------\n        definition\n        \"\"\"\n\n        coords = []\n        for elem in d:\n            if \"start\" in elem and \"stop\" in elem and (\"step\" in elem or \"size\" in elem):\n                c = UniformCoordinates1d.from_definition(elem)\n            elif \"values\" in elem:\n                c = ArrayCoordinates1d.from_definition(elem)\n            else:\n                raise ValueError(\"Could not parse coordinates definition with keys %s\" % elem.keys())\n\n            coords.append(c)\n\n        return cls(coords)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard methods, list-like\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __repr__(self):\n        rep = str(self.__class__.__name__)\n        for c in self._coords:\n            rep += \"\\n\\t%s[%s]: %s\" % (self.name, c.name or \"?\", c)\n        return rep",
            "def __iter__(self):\n        return iter(self._coords)",
            "def __len__(self):\n        return len(self._coords)",
            "def __getitem__(self, index):\n        if isinstance(index, string_types):\n            if index not in self.dims:\n                raise KeyError(\"Dimension '%s' not found in dims %s\" % (index, self.dims))\n\n            return self._coords[self.dims.index(index)]\n\n        else:\n            return self._getsubset(index)",
            "def _getsubset(self, index):\n        return StackedCoordinates([c[index] for c in self._coords])",
            "def __setitem__(self, dim, c):\n        if not dim in self.dims:\n            raise KeyError(\"Cannot set dimension '%s' in StackedCoordinates %s\" % (dim, self.dims))\n\n        # try to cast to ArrayCoordinates1d\n        if not isinstance(c, Coordinates1d):\n            c = ArrayCoordinates1d(c)\n\n        if c.name is None:\n            c.name = dim\n\n        # replace the element of the coords list\n        idx = self.dims.index(dim)\n        coords = list(self._coords)\n        coords[idx] = c\n\n        # set (and check) new coords list\n        self.set_trait(\"_coords\", coords)",
            "def __contains__(self, item):\n        try:\n            item = np.array([make_coord_value(value) for value in item])\n        except:\n            return False\n\n        if len(item) != len(self._coords):\n            return False\n\n        if any(val not in c for val, c in zip(item, self._coords)):\n            return False\n\n        return (self.flatten().coordinates == item).all(axis=1).any()",
            "def _eq_base(self, other):\n        if not isinstance(other, StackedCoordinates):\n            return False\n\n        # shortcuts\n        if self.dims != other.dims:\n            return False\n\n        if self.shape != other.shape:\n            return False\n\n        return True",
            "def __eq__(self, other):\n        if not self._eq_base(other):\n            return False\n\n        # full check of underlying coordinates\n        if self._coords != other._coords:\n            return False\n\n        return True\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def dims(self):",
            "def ndim(self):",
            "def name(self):",
            "def size(self):",
            "def shape(self):",
            "def bounds(self):",
            "def coordinates(self):\n        dtypes = [c.dtype for c in self._coords]\n        if len(set(dtypes)) == 1:\n            dtype = dtypes[0]\n        else:\n            dtype = object\n        return np.dstack([c.coordinates.astype(dtype) for c in self._coords]).squeeze()\n\n    @property",
            "def xcoords(self):",
            "def definition(self):",
            "def full_definition(self):",
            "def is_stacked(self):\n        return True\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # Methods\n    # -----------------------------------------------------------------------------------------------------------------",
            "def copy(self):\n        \"\"\"\n        Make a copy of the stacked coordinates.\n\n        Returns\n        -------\n        :class:`StackedCoordinates`\n            Copy of the stacked coordinates.\n        \"\"\"\n\n        return StackedCoordinates(self._coords)",
            "def unique(self, return_index=False):\n        \"\"\"\n        Remove duplicate stacked coordinate values.\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n\n        Returns\n        -------\n        unique : :class:`StackedCoordinates`\n            New StackedCoordinates object with unique, sorted, flattened coordinate values.\n        unique_index : list of indices\n            index\n        \"\"\"\n\n        flat = self.flatten()\n        a, I = np.unique(flat.coordinates, axis=0, return_index=True)\n        if return_index:\n            return flat[I], I\n        else:\n            return flat[I]",
            "def get_area_bounds(self, boundary):\n        \"\"\"Get coordinate area bounds, including boundary information, for each unstacked dimension.\n\n        Arguments\n        ---------\n        boundary : dict\n            dictionary of boundary offsets for each unstacked dimension. Point dimensions can be omitted.\n\n        Returns\n        -------\n        area_bounds : dict\n            Dictionary of (low, high) coordinates area_bounds in each unstacked dimension\n        \"\"\"\n\n        if None in self.dims:\n            raise ValueError(\"Cannot get area_bounds for StackedCoordinates with un-named dimensions\")\n        return {dim: self[dim].get_area_bounds(boundary.get(dim)) for dim in self.dims}",
            "def select(self, bounds, outer=False, return_index=False):\n        \"\"\"\n        Get the coordinate values that are within the given bounds in all dimensions.\n\n        *Note: you should not generally need to call this method directly.*\n\n        Parameters\n        ----------\n        bounds : dict\n            dictionary of dim -> (low, high) selection bounds\n        outer : bool, optional\n            If True, do *outer* selections. Default False.\n        return_index : bool, optional\n            If True, return index for the selections in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`StackedCoordinates`\n            StackedCoordinates object consisting of the selection in all dimensions.\n        selection_index : slice, boolean array\n            Slice or index for the selected coordinates, only if ``return_index`` is True.\n        \"\"\"\n\n        # logical AND of the selection in each dimension\n        indices = [c.select(bounds, outer=outer, return_index=True)[1] for c in self._coords]\n        index = self._and_indices(indices)\n\n        if return_index:\n            return self[index], index\n        else:\n            return self[index]",
            "def _and_indices(self, indices):",
            "def _index_len(index):\n            if isinstance(index, slice):\n                if index.stop is None:\n                    stop = self.size\n                elif index.stop < 0:\n                    stop = self.size - index.stop\n                else:\n                    stop = index.stop\n                if index.start is None:\n                    start = 0\n                elif index.start < 0:\n                    start = self.size - index.start\n                else:\n                    start = index.start\n                return stop - start\n            return len(index)\n\n        if all(isinstance(index, slice) for index in indices):\n            index = slice(max(index.start or 0 for index in indices), min(index.stop or self.size for index in indices))\n            # for consistency\n            if index.start == 0 and index.stop == self.size:\n                if self.ndim > 1:\n                    index = [slice(None, None) for dim in self.dims]\n                else:\n                    index = slice(None, None)\n        elif any(_index_len(index) == 0 for index in indices):\n            index = slice(0, 0)\n        else:\n            # convert any slices to boolean array\n            for i, index in enumerate(indices):\n                if isinstance(index, slice):\n                    indices[i] = np.zeros(self.shape, dtype=bool)\n                    indices[i][index] = True\n\n            # logical and\n            index = np.logical_and.reduce(indices)\n\n            # for consistency\n            if np.all(index):\n                if self.ndim > 1:\n                    index = [slice(None, None) for dim in self.dims]\n                else:\n                    index = slice(None, None)\n\n        return index",
            "def _transform(self, transformer):\n        if self.size == 0:\n            return self.copy()\n\n        coords = [c.copy() for c in self._coords]\n\n        if \"lat\" in self.dims and \"lon\" in self.dims and \"alt\" in self.dims:\n            ilat = self.dims.index(\"lat\")\n            ilon = self.dims.index(\"lon\")\n            ialt = self.dims.index(\"alt\")\n\n            lat = coords[ilat]\n            lon = coords[ilon]\n            alt = coords[ialt]\n            tlon, tlat, talt = transformer.transform(lon.coordinates, lat.coordinates, alt.coordinates)\n\n            coords[ilat] = ArrayCoordinates1d(tlat, \"lat\").simplify()\n            coords[ilon] = ArrayCoordinates1d(tlon, \"lon\").simplify()\n            coords[ialt] = ArrayCoordinates1d(talt, \"alt\").simplify()\n\n        elif \"lat\" in self.dims and \"lon\" in self.dims:\n            ilat = self.dims.index(\"lat\")\n            ilon = self.dims.index(\"lon\")\n\n            lat = coords[ilat]\n            lon = coords[ilon]\n            tlon, tlat = transformer.transform(lon.coordinates, lat.coordinates)\n\n            if (\n                self.ndim == 2\n                and all(np.allclose(a, tlat[:, 0]) for a in tlat.T)\n                and all(np.allclose(a, tlon[0]) for a in tlon)\n            ):\n                coords[ilat] = ArrayCoordinates1d(tlat[:, 0], name=\"lat\").simplify()\n                coords[ilon] = ArrayCoordinates1d(tlon[0], name=\"lon\").simplify()\n                return coords\n\n            coords[ilat] = ArrayCoordinates1d(tlat, \"lat\").simplify()\n            coords[ilon] = ArrayCoordinates1d(tlon, \"lon\").simplify()\n\n        elif \"alt\" in self.dims:\n            ialt = self.dims.index(\"alt\")\n\n            alt = coords[ialt]\n            _, _, talt = transformer.transform(np.zeros(self.size), np.zeros(self.size), alt.coordinates)\n\n            coords[ialt] = ArrayCoordinates1d(talt, \"alt\").simplify()\n\n        return StackedCoordinates(coords).simplify()",
            "def transpose(self, *dims, **kwargs):\n        \"\"\"\n        Transpose (re-order) the dimensions of the StackedCoordinates.\n\n        Parameters\n        ----------\n        dim_1, dim_2, ... : str, optional\n            Reorder dims to this order. By default, reverse the dims.\n        in_place : boolean, optional\n            If True, transpose the dimensions in-place.\n            Otherwise (default), return a new, transposed Coordinates object.\n\n        Returns\n        -------\n        transposed : :class:`StackedCoordinates`\n            The transposed StackedCoordinates object.\n        \"\"\"\n\n        in_place = kwargs.get(\"in_place\", False)\n\n        if len(dims) == 0:\n            dims = list(self.dims[::-1])\n\n        if set(dims) != set(self.dims):\n            raise ValueError(\"Invalid transpose dimensions, input %s does match any dims in %s\" % (dims, self.dims))\n\n        coordinates = [self._coords[self.dims.index(dim)] for dim in dims]\n\n        if in_place:\n            self.set_trait(\"_coords\", coordinates)\n            return self\n        else:\n            return StackedCoordinates(coordinates)",
            "def flatten(self):\n        return StackedCoordinates([c.flatten() for c in self._coords])",
            "def reshape(self, newshape):\n        return StackedCoordinates([c.reshape(newshape) for c in self._coords])",
            "def issubset(self, other):\n        \"\"\"Report whether other coordinates contains these coordinates.\n\n        Arguments\n        ---------\n        other : Coordinates, StackedCoordinates\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates.\n        \"\"\"\n\n        from podpac.core.coordinates import Coordinates\n\n        if not isinstance(other, (Coordinates, StackedCoordinates)):\n            raise TypeError(\n                \"StackedCoordinates issubset expected Coordinates or StackedCoordinates, not '%s'\" % type(other)\n            )\n\n        if isinstance(other, StackedCoordinates):\n            if set(self.dims) != set(other.dims):\n                return False\n\n            mine = self.flatten().coordinates\n            other = other.flatten().transpose(*self.dims).coordinates\n            if len(mine.shape) > len(other.shape):\n                other = other.reshape(-1, 1)\n            return set(map(tuple, mine)).issubset(map(tuple, other))\n\n        elif isinstance(other, Coordinates):\n            if not all(dim in other.udims for dim in self.dims):\n                return False\n\n            acs = []\n            ocs = []\n            for coords in other.values():\n                dims = [dim for dim in coords.dims if dim in self.dims]\n\n                if len(dims) == 0:\n                    continue\n\n                elif len(dims) == 1:\n                    acs.append(self[dims[0]])\n                    if isinstance(coords, Coordinates1d):\n                        ocs.append(coords)\n                    elif isinstance(coords, StackedCoordinates):\n                        ocs.append(coords[dims[0]])\n\n                elif len(dims) > 1:\n                    acs.append(StackedCoordinates([self[dim] for dim in dims]))\n                    if isinstance(coords, StackedCoordinates):\n                        ocs.append(StackedCoordinates([coords[dim] for dim in dims]))\n\n            return all(a.issubset(o) for a, o in zip(acs, ocs))",
            "def simplify(self):\n        if self.is_affine:\n            from podpac.core.coordinates.affine_coordinates import AffineCoordinates\n\n            # build the geotransform directly\n            lat = self[\"lat\"].coordinates\n            lon = self[\"lon\"].coordinates\n\n            # We don't have to check every point in lat/lon for the same step\n            # since the self.is_affine call did that already\n            dlati = (lat[-1, 0] - lat[0, 0]) / (lat.shape[0] - 1)\n            dlatj = (lat[0, -1] - lat[0, 0]) / (lat.shape[1] - 1)\n            dloni = (lon[-1, 0] - lon[0, 0]) / (lon.shape[0] - 1)\n            dlonj = (lon[0, -1] - lon[0, 0]) / (lon.shape[1] - 1)\n\n            # origin point\n            p0 = [lat[0, 0], lon[0, 0]] - np.array([[dlati, dlatj], [dloni, dlonj]]) @ np.ones(2) / 2\n\n            # This is defined as x ulc, x width, x height, y ulc, y width, y height\n            # x and y are defined by the CRS. Here we are assuming that it's always\n            # lon and lat == x and y\n            geotransform = [p0[1], dlonj, dloni, p0[0], dlatj, dlati]\n\n            a = AffineCoordinates(geotransform=geotransform, shape=self.shape)\n\n            # simplify in order to convert to UniformCoordinates if appropriate\n            return a.simplify()\n\n        return StackedCoordinates([c.simplify() for c in self._coords])\n\n    @property",
            "def is_affine(self):\n        if set(self.dims) != {\"lat\", \"lon\"}:\n            return False\n\n        if not (self.ndim == 2 and self.shape[0] > 1 and self.shape[1] > 1):\n            return False\n\n        lat = self[\"lat\"].coordinates\n        lon = self[\"lon\"].coordinates\n\n        d = lat[1:] - lat[:-1]\n        if not np.allclose(d, d[0, 0]):\n            return False\n\n        d = lat[:, 1:] - lat[:, :-1]\n        if not np.allclose(d, d[0, 0]):\n            return False\n\n        d = lon[1:] - lon[:-1]\n        if not np.allclose(d, d[0, 0]):\n            return False\n\n        d = lon[:, 1:] - lon[:, :-1]\n        if not np.allclose(d, d[0, 0]):\n            return False\n\n        return True",
            "def horizontal_resolution(self, latitude, ellipsoid_tuple, coordinate_name, restype=\"nominal\", units=\"meter\"):\n        \"\"\"Return the horizontal resolution of a Uniform 1D Coordinate\n\n        Parameters\n        ----------\n        ellipsoid_tuple: tuple\n            a tuple containing ellipsoid information from the the original coordinates to pass to geopy\n        coordinate_name: str\n            \"cartesian\" or \"ellipsoidal\", to tell calculate_distance what kind of calculation to do\n        restype: str\n            The kind of horizontal resolution that should be returned. Supported values are:\n            - \"nominal\" <-- Gives average nearest distance of all points, with some error\n            - \"summary\" <-- Gives the mean and standard deviation of nearest distance betweem points, with some error\n            - \"full\" <-- Gives exact distance matrix\n        units: str\n            desired unit to return\n\n        Returns\n        -------\n        float * (podpac.unit)\n            If restype == \"nominal\", return the average nearest distance with some error\n        tuple * (podpac.unit)\n            If restype == \"summary\", return average and std.dev of nearest distances, with some error\n        np.ndarray * (podpac.unit)\n            if restype == \"full\", return exact distance matrix\n        ValueError\n            if unknown restype\n\n        \"\"\"\n        order = tuple([self.dims.index(d) for d in [\"lat\", \"lon\"]])",
            "def nominal_stacked_resolution():\n            \"\"\"Use a KDTree to return approximate stacked resolution with some loss of accuracy.\n\n            Returns\n            -------\n            The average min distance of every point\n\n            \"\"\"\n            tree = spatial.KDTree(self.coordinates[:, order] + [0, 180.0], boxsize=[0.0, 360.0000000000001])\n            return np.average(\n                calculate_distance(\n                    tree.data - [0, 180.0],\n                    tree.data[tree.query(tree.data, k=2)[1][:, 1]] - [0, 180.0],\n                    ellipsoid_tuple,\n                    coordinate_name,\n                    units,\n                )\n            )",
            "def summary_stacked_resolution():\n            \"\"\"Return the approximate mean resolution and std.deviation using a KDTree\n\n            Returns\n            -------\n            tuple\n                Average min distance of every point and standard deviation of those min distances\n            \"\"\"\n            tree = spatial.KDTree(self.coordinates[:, order] + [0, 180.0], boxsize=[0.0, 360.0000000000001])\n            distances = calculate_distance(\n                tree.data - [0, 180.0],\n                tree.data[tree.query(tree.data, k=2)[1][:, 1]] - [0, 180.0],\n                ellipsoid_tuple,\n                coordinate_name,\n                units,\n            )\n            return (np.average(distances), np.std(distances))",
            "def full_stacked_resolution():\n            \"\"\"Returns the exact distance between every point using brute force\n\n            Returns\n            -------\n            distance matrix of size (NxN), where N is the number of points in the dimension\n            \"\"\"\n            distance_matrix = np.zeros((len(self.coordinates), len(self.coordinates)))\n            for i in range(len(self.coordinates)):\n                distance_matrix[i, :] = calculate_distance(\n                    self.coordinates[i, order], self.coordinates[:, order], ellipsoid_tuple, coordinate_name, units\n                ).magnitude\n            return distance_matrix * podpac.units(units)\n\n        if restype == \"nominal\":\n            return nominal_stacked_resolution()\n        elif restype == \"summary\":\n            return summary_stacked_resolution()\n        elif restype == \"full\":\n            return full_stacked_resolution()\n        else:\n            raise ValueError(\"Invalid value for type: {}\".format(restype))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/affine_coordinates.py",
        "comments": [
            "//gdal.org/tutorials/geotransforms_tut.html"
        ],
        "docstrings": [
            "\"\"\"\n    A grid of latitude and longitude coordinates, defined by an affine transformation.\n\n    Parameters\n    ----------\n    geotransform : tuple\n        GDAL geotransform\n    shape : tuple\n        shape (m, n) of the grid.\n    dims : tuple\n        Tuple of dimension names.\n    coords : dict-like\n        xarray coordinates (container of coordinate arrays)\n    coordinates : tuple\n        Tuple of 2d coordinate values in each dimension.\n\n    Notes\n    -----\n\n    https://gdal.org/tutorials/geotransforms_tut.html\n\n    GT(0) x-coordinate of the upper-left corner of the upper-left pixel.\n    GT(1) w-e pixel resolution / pixel width.\n    GT(2) row rotation (typically zero).\n    GT(3) y-coordinate of the upper-left corner of the upper-left pixel.\n    GT(4) column rotation (typically zero).\n    GT(5) n-s pixel resolution / pixel height (negative value for a north-up image).\n\n    \"\"\"",
            "\"\"\"\n        Create a grid of coordinates from a `geotransform` and `shape`.\n\n        Parameters\n        ----------\n        geotransform : tuple\n            GDAL geotransform\n        shape : tuple\n            shape (m, n) of the grid.\n        \"\"\"",
            "\"\"\"\n        Create AffineCoordinates from an affine coordinates definition.\n\n        Arguments\n        ---------\n        d : dict\n            affine coordinates definition\n\n        Returns\n        -------\n        :class:`AffineCoordinates`\n            affine coordinates object\n\n        See Also\n        --------\n        definition\n        \"\"\"",
            "\"\"\":affine.Affine: affine transformation for computing the coordinates from indexing values.\"\"\"",
            "\"\"\":tuple: computed coordinate values for each dimension.\"\"\"",
            "\"\"\"\n        Make a copy of the affine coordinates.\n\n        Returns\n        -------\n        :class:`AffineCoordinates`\n            Copy of the affine coordinates.\n        \"\"\"",
            "\"\"\"Get coordinate area bounds, including boundary information, for each unstacked dimension.\n\n        Arguments\n        ---------\n        boundary : dict\n            dictionary of boundary offsets for each unstacked dimension. Point dimensions can be omitted.\n\n        Returns\n        -------\n        area_bounds : dict\n            Dictionary of (low, high) coordinates area_bounds in each unstacked dimension\n        \"\"\"",
            "\"\"\"\n        Get the coordinate values that are within the given bounds in all dimensions.\n\n        *Note: you should not generally need to call this method directly.*\n\n        Parameters\n        ----------\n        bounds : dict\n            dictionary of dim -> (low, high) selection bounds\n        outer : bool, optional\n            If True, do *outer* selections. Default False.\n        return_index : bool, optional\n            If True, return index for the selections in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`StackedCoordinates`, :class:`AffineCoordinates`\n            coordinates consisting of the selection in all dimensions.\n        selection_index : list\n            index for the selected coordinates, only if ``return_index`` is True.\n        \"\"\""
        ],
        "code_snippets": [
            "class AffineCoordinates(StackedCoordinates):\n    \"\"\"\n    A grid of latitude and longitude coordinates, defined by an affine transformation.\n\n    Parameters\n    ----------\n    geotransform : tuple\n        GDAL geotransform\n    shape : tuple\n        shape (m, n) of the grid.\n    dims : tuple\n        Tuple of dimension names.\n    coords : dict-like\n        xarray coordinates (container of coordinate arrays)\n    coordinates : tuple\n        Tuple of 2d coordinate values in each dimension.\n\n    Notes\n    -----\n\n    https:",
            "def __init__(self, geotransform=None, shape=None):\n        \"\"\"\n        Create a grid of coordinates from a `geotransform` and `shape`.\n\n        Parameters\n        ----------\n        geotransform : tuple\n            GDAL geotransform\n        shape : tuple\n            shape (m, n) of the grid.\n        \"\"\"\n        if isinstance(geotransform, np.ndarray):\n            geotransform = tuple(geotransform.tolist())\n        self.set_trait(\"geotransform\", geotransform)\n        self.set_trait(\"shape\", shape)\n\n        # private traits\n        self._affine = affine.Affine.from_gdal(*self.geotransform)\n\n    @tl.validate(\"shape\")",
            "def _validate_shape(self, d):\n        val = d[\"value\"]\n        if val[0] <= 0 or val[1] <= 0:\n            raise ValueError(\"Invalid shape %s, shape must be positive\" % (val,))\n        return val\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Alternate Constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @classmethod",
            "def from_definition(cls, d):\n        \"\"\"\n        Create AffineCoordinates from an affine coordinates definition.\n\n        Arguments\n        ---------\n        d : dict\n            affine coordinates definition\n\n        Returns\n        -------\n        :class:`AffineCoordinates`\n            affine coordinates object\n\n        See Also\n        --------\n        definition\n        \"\"\"\n\n        if \"geotransform\" not in d:\n            raise ValueError('AffineCoordinates definition requires \"geotransform\" property')\n        if \"shape\" not in d:\n            raise ValueError('AffineCoordinates definition requires \"shape\" property')\n        return AffineCoordinates(geotransform=d[\"geotransform\"], shape=d[\"shape\"])\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __repr__(self):\n        return \"%s(%s): Bounds(lat, lon)([%g, %g], [%g, %g]), Shape%s\" % (\n            self.__class__.__name__,\n            self.dims,\n            self.bounds[\"lat\"][0],\n            self.bounds[\"lat\"][1],\n            self.bounds[\"lon\"][0],\n            self.bounds[\"lon\"][1],\n            self.shape,\n        )",
            "def __eq__(self, other):\n        if not self._eq_base(other):\n            return False\n\n        if not other.is_affine:\n            return False\n\n        if not np.allclose(self.geotransform, other.geotransform):\n            return False\n\n        return True",
            "def _getsubset(self, index):\n        if isinstance(index, tuple) and isinstance(index[0], slice) and isinstance(index[1], slice):\n            lat = self[\"lat\"].coordinates[index]\n            lon = self[\"lon\"].coordinates[index]\n\n            # We don't have to check every point in lat/lon for the same step\n            # since the self.is_affine call did that already\n            dlati = (lat[-1, 0] - lat[0, 0]) / (lat.shape[0] - 1)\n            dlatj = (lat[0, -1] - lat[0, 0]) / (lat.shape[1] - 1)\n            dloni = (lon[-1, 0] - lon[0, 0]) / (lon.shape[0] - 1)\n            dlonj = (lon[0, -1] - lon[0, 0]) / (lon.shape[1] - 1)\n\n            # origin point\n            p0 = np.array([lat[0, 0], lon[0, 0]]) - np.array([[dlati, dlatj], [dloni, dlonj]]) @ np.ones(2) / 2\n\n            # This is defined as x ulc, x width, x height, y ulc, y width, y height\n            # x and y are defined by the CRS. Here we are assuming that it's always\n            # lon and lat == x and y\n            geotransform = [p0[1], dlonj, dloni, p0[0], dlatj, dlati]\n\n            # get shape from indexed coordinates\n            shape = lat.shape\n\n            return AffineCoordinates(geotransform=geotransform, shape=lat.shape)\n\n        else:\n            return super(AffineCoordinates, self)._getsubset(index).simplify()\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def _coords(self):\n        if not hasattr(self, \"_coords_\"):\n            self._coords_ = [\n                ArrayCoordinates1d(c, name=dim) for c, dim in zip(self.coordinates.transpose(2, 0, 1), self.dims)\n            ]\n        return self._coords_\n\n    @property",
            "def ndim(self):\n        return 2\n\n    @property",
            "def affine(self):",
            "def dims(self):\n        return (\"lat\", \"lon\")\n\n    @property",
            "def is_affine(self):\n        return True\n\n    @property",
            "def origin(self):\n        origin = self.affine * [0, 0]\n        if self.dims == (\"lat\", \"lon\"):\n            origin = origin[::-1]\n        return origin\n\n    @property",
            "def coordinates(self):",
            "def definition(self):\n        d = OrderedDict()\n        d[\"geotransform\"] = self.geotransform\n        d[\"shape\"] = self.shape\n        return d\n\n    @property",
            "def full_definition(self):\n        return self.definition\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def copy(self):\n        \"\"\"\n        Make a copy of the affine coordinates.\n\n        Returns\n        -------\n        :class:`AffineCoordinates`\n            Copy of the affine coordinates.\n        \"\"\"\n        return AffineCoordinates(self.geotransform, self.shape)",
            "def get_area_bounds(self, boundary):\n        \"\"\"Get coordinate area bounds, including boundary information, for each unstacked dimension.\n\n        Arguments\n        ---------\n        boundary : dict\n            dictionary of boundary offsets for each unstacked dimension. Point dimensions can be omitted.\n\n        Returns\n        -------\n        area_bounds : dict\n            Dictionary of (low, high) coordinates area_bounds in each unstacked dimension\n        \"\"\"\n\n        # TODO the boundary offsets need to be transformed\n        warnings.warn(\"AffineCoordinates area_bounds are not yet correctly implemented.\")\n        return super(AffineCoordinates, self).get_area_bounds(boundary)",
            "def select(self, bounds, outer=False, return_index=False):\n        \"\"\"\n        Get the coordinate values that are within the given bounds in all dimensions.\n\n        *Note: you should not generally need to call this method directly.*\n\n        Parameters\n        ----------\n        bounds : dict\n            dictionary of dim -> (low, high) selection bounds\n        outer : bool, optional\n            If True, do *outer* selections. Default False.\n        return_index : bool, optional\n            If True, return index for the selections in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`StackedCoordinates`, :class:`AffineCoordinates`\n            coordinates consisting of the selection in all dimensions.\n        selection_index : list\n            index for the selected coordinates, only if ``return_index`` is True.\n        \"\"\"\n\n        if not outer:\n            # if the geotransform is rotated, the inner selection is not a grid\n            # returning the general stacked coordinates is a general solution\n            return super(AffineCoordinates, self).select(bounds, outer=outer, return_index=return_index)\n\n        # same rotation and step, new origin and shape\n        lat = self.coordinates[:, :, 0]\n        lon = self.coordinates[:, :, 1]\n        b = (\n            (lat >= bounds[\"lat\"][0])\n            & (lat <= bounds[\"lat\"][1])\n            & (lon >= bounds[\"lon\"][0])\n            & (lon <= bounds[\"lon\"][1])\n        )\n\n        I, J = np.where(b)\n        imin = max(0, np.min(I) - 1)\n        jmin = max(0, np.min(J) - 1)\n        imax = min(self.shape[0] - 1, np.max(I) + 1)\n        jmax = min(self.shape[1] - 1, np.max(J) + 1)\n\n        origin = np.array([lat[imin, jmin], lon[imin, jmin]])\n        origin -= np.array([lat[0, 0], lon[0, 0]]) - self.origin\n\n        shape = int(imax - imin + 1), int(jmax - jmin + 1)\n\n        geotransform = (\n            origin[1],\n            self.geotransform[1],\n            self.geotransform[2],\n            origin[0],\n            self.geotransform[4],\n            self.geotransform[5],\n        )\n\n        selected = AffineCoordinates(geotransform=geotransform, shape=shape)\n\n        if return_index:\n            return selected, (slice(imin, imax + 1), slice(jmin, jmax + 1))\n        else:\n            return selected",
            "def simplify(self):\n        # NOTE: podpac prefers unstacked UniformCoordinates to AffineCoordinates\n        #       if that changes, just return self.copy()\n        if self.affine.is_rectilinear:\n            tol = 1e-15  # tolerance for deciding when a number is zero\n            a = self.affine\n            shape = self.shape\n\n            if np.abs(a.e) <= tol and np.abs(a.a) <= tol:\n                order = -1\n                step = np.array([a.d, a.b])\n            else:\n                order = 1\n                step = np.array([a.e, a.a])\n\n            origin = a.f + step[0] / 2, a.c + step[1] / 2\n            end = origin[0] + step[0] * (shape[::order][0] - 1), origin[1] + step[1] * (shape[::order][1] - 1)\n            # when the shape == 1, UniformCoordinates1d cannot infer the step from the size\n            # we have have to create the UniformCoordinates1d manually\n            if shape[::order][0] == 1:\n                lat = UniformCoordinates1d(origin[0], end[0], step=step[0], name=\"lat\")\n            else:\n                lat = clinspace(origin[0], end[0], shape[::order][0], \"lat\")\n            if shape[::order][1] == 1:\n                lon = UniformCoordinates1d(origin[1], end[1], step=step[1], name=\"lon\")\n            else:\n                lon = clinspace(origin[1], end[1], shape[::order][1], \"lon\")\n            return [lat, lon][::order]\n\n        return self.copy()\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Debug\n    # ------------------------------------------------------------------------------------------------------------------",
            "def plot(self, marker=\"b.\", origin_marker=\"bo\", corner_marker=\"bx\"):\n        from matplotlib import pyplot\n\n        x = self.coordinates[:, :, 0]\n        y = self.coordinates[:, :, 1]\n        pyplot.plot(x.flatten(), y.flatten(), marker)\n        ox, oy = self.origin\n        pyplot.plot(ox, oy, origin_marker)\n        pyplot.gca().set_aspect(\"equal\")"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/coordinates1d.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nOne-Dimensional Coordinates\n\"\"\"",
            "\"\"\"\n    Base class for 1-dimensional coordinates.\n\n    Coordinates1d objects contain values and metadata for a single dimension of coordinates. :class:`podpac.Coordinates` and\n    :class:`StackedCoordinates` use Coordinate1d objects.\n\n    Parameters\n    ----------\n    name : str\n        Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n    coordinates : array, read-only\n        Full array of coordinate values.\n\n    See Also\n    --------\n    :class:`ArrayCoordinates1d`, :class:`UniformCoordinates1d`\n    \"\"\"",
            "\"\"\"used by child __eq__ methods for common checks\"\"\"",
            "\"\"\":dict: xarray coords\"\"\"",
            "\"\"\":type: Coordinates dtype.\n\n        ``float`` for numerical coordinates and numpy ``datetime64`` for datetime coordinates.\n        \"\"\"",
            "\"\"\"Low and high coordinate bounds.\"\"\"",
            "\"\"\":dict: Dictionary of the coordinate properties.\"\"\"",
            "\"\"\":dict: Serializable 1d coordinates definition.\"\"\"",
            "\"\"\":dict: Serializable 1d coordinates definition, containing all properties. For internal use.\"\"\"",
            "\"\"\"\n        Make a deep copy of the 1d Coordinates.\n\n        Returns\n        -------\n        :class:`Coordinates1d`\n            Copy of the coordinates.\n        \"\"\"",
            "\"\"\"Get the simplified/optimized representation of these coordinates.\n\n        Returns\n        -------\n        simplified : Coordinates1d\n            simplified version of the coordinates\n        \"\"\"",
            "\"\"\"\n        Get low and high coordinate area bounds.\n\n        Arguments\n        ---------\n        boundary : float, timedelta, array, None\n            Boundary offsets in this dimension.\n\n            * For a centered uniform boundary (same for every coordinate), use a single positive float or timedelta\n                offset. This represents the \"total segment length\" / 2.\n            * For a uniform boundary (segment or polygon same for every coordinate), use an array of float or\n                timedelta offsets\n            * For a fully specified boundary, use an array of boundary arrays (2-D array, N_coords x boundary spec),\n                 one per coordinate. The boundary_spec can be a single number, two numbers, or an array of numbers.\n            * For point coordinates, use None.\n\n        Returns\n        -------\n        low: float, np.datetime64\n            low area bound\n        high: float, np.datetime64\n            high area bound\n        \"\"\"",
            "\"\"\"\n        Get the coordinate values that are within the given bounds.\n\n        The default selection returns coordinates that are within the bounds::\n\n            In [1]: c = ArrayCoordinates1d([0, 1, 2, 3], name='lat')\n\n            In [2]: c.select([1.5, 2.5]).coordinates\n            Out[2]: array([2.])\n\n        The *outer* selection returns the minimal set of coordinates that contain the bounds::\n\n            In [3]: c.select([1.5, 2.5], outer=True).coordinates\n            Out[3]: array([1., 2., 3.])\n\n        The *outer* selection also returns a boundary coordinate if a bound is outside this coordinates bounds but\n        *inside* its area bounds::\n\n            In [4]: c.select([3.25, 3.35], outer=True).coordinates\n            Out[4]: array([3.0], dtype=float64)\n\n            In [5]: c.select([10.0, 11.0], outer=True).coordinates\n            Out[5]: array([], dtype=float64)\n\n        Parameters\n        ----------\n        bounds : (low, high) or dict\n            Selection bounds. If a dictionary of dim -> (low, high) bounds is supplied, the bounds matching these\n            coordinates will be selected if available, otherwise the full coordinates will be returned.\n        outer : bool, optional\n            If True, do an *outer* selection. Default False.\n        return_index : bool, optional\n            If True, return index for the selection in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`Coordinates1d`\n            Coordinates1d object with coordinates within the bounds.\n        I : slice, boolean array\n            index or slice for the selected coordinates (only if return_index=True)\n        \"\"\"",
            "\"\"\"Report whether other coordinates contains these coordinates.\n\n        Arguments\n        ---------\n        other : Coordinates, Coordinates1d\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates.\n        \"\"\"",
            "\"\"\"Return the horizontal resolution of a Uniform 1D Coordinate\n\n        Parameters\n        ----------\n        latitude: Coordinates1D\n            The latitude coordiantes of the current coordinate system, required for both lat and lon resolution.\n        ellipsoid_tuple: tuple\n            a tuple containing ellipsoid information from the the original coordinates to pass to geopy\n        coordinate_name: str\n            \"cartesian\" or \"ellipsoidal\", to tell calculate_distance what kind of calculation to do\n        restype: str\n            The kind of horizontal resolution that should be returned. Supported values are:\n            - \"nominal\" <-- returns average resolution based upon bounds and number of grid squares\n            - \"summary\" <-- Gives the exact mean and standard deviation of grid square resolutions\n            - \"full\" <-- Gives exact grid differences.\n\n        Returns\n        -------\n        float * (podpac.unit)\n            if restype == \"nominal\", return average distance in desired units\n        tuple * (podpac.unit)\n            if restype == \"summary\", return average distance and standard deviation in desired units\n        np.ndarray * (podpac.unit)\n            if restype == \"full\", give full resolution. 1D array for latitude, MxN matrix for longitude.\n        \"\"\"",
            "\"\"\"Return resolution for unstacked coordiantes using the bounds\n\n            Returns\n            --------\n            The average distance between each grid square for this dimension\n            \"\"\"",
            "\"\"\"Return summary resolution for the dimension.\n\n            Returns\n            -------\n            tuple\n                the average distance between grid squares\n                the standard deviation of those distances\n            \"\"\"",
            "\"\"\"Calculate full resolution of unstacked dimension\n\n            Returns\n            -------\n            A matrix of distances\n            \"\"\""
        ],
        "code_snippets": [
            "class Coordinates1d(BaseCoordinates):\n    \"\"\"\n    Base class for 1-dimensional coordinates.\n\n    Coordinates1d objects contain values and metadata for a single dimension of coordinates. :class:`podpac.Coordinates` and\n    :class:`StackedCoordinates` use Coordinate1d objects.\n\n    Parameters\n    ----------\n    name : str\n        Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n    coordinates : array, read-only\n        Full array of coordinate values.\n\n    See Also\n    --------\n    :class:`ArrayCoordinates1d`, :class:`UniformCoordinates1d`\n    \"\"\"\n\n    name = Dimension(allow_none=True)\n    _properties = tl.Set()\n\n    @tl.observe(\"name\")",
            "def _set_property(self, d):\n        if d[\"new\"] is not None:\n            self._properties.add(d[\"name\"])",
            "def _set_name(self, value):\n        # set name if it is not set already, otherwise check that it matches\n        if \"name\" not in self._properties and value is not None:\n            self.name = value\n        elif self.name != value:\n            raise ValueError(\"Dimension mismatch, %s != %s\" % (value, self.name))\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __repr__(self):\n        if self.name is None:\n            name = \"%s\" % (self.__class__.__name__,)\n        else:\n            name = \"%s(%s)\" % (self.__class__.__name__, self.name)\n\n        if self.ndim == 1:\n            desc = \"Bounds[%s, %s], N[%d]\" % (self.bounds[0], self.bounds[1], self.size)\n        else:\n            desc = \"Bounds[%s, %s], N[%s], Shape%s\" % (self.bounds[0], self.bounds[1], self.size, self.shape)\n\n        return \"%s: %s\" % (name, desc)",
            "def _eq_base(self, other):",
            "def __len__(self):\n        return self.shape[0]",
            "def __contains__(self, item):\n        try:\n            item = make_coord_value(item)\n        except:\n            return False\n\n        if type(item) != self.dtype:\n            return False\n\n        return item in self.coordinates\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def dims(self):\n        if self.name is None:\n            raise TypeError(\"cannot access dims property of unnamed Coordinates1d\")\n        return (self.name,)\n\n    @property",
            "def xcoords(self):",
            "def dtype(self):\n        \"\"\":type: Coordinates dtype.\n\n        ``float`` for numerical coordinates and numpy ``datetime64`` for datetime coordinates.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @property",
            "def deltatype(self):\n        if self.dtype is np.datetime64:\n            return np.timedelta64\n        else:\n            return self.dtype\n\n    @property",
            "def is_monotonic(self):\n        raise NotImplementedError\n\n    @property",
            "def is_descending(self):\n        raise NotImplementedError\n\n    @property",
            "def is_uniform(self):\n        raise NotImplementedError\n\n    @property",
            "def start(self):\n        raise NotImplementedError\n\n    @property",
            "def stop(self):\n        raise NotImplementedError\n\n    @property",
            "def step(self):\n        raise NotImplementedError\n\n    @property",
            "def bounds(self):",
            "def properties(self):",
            "def definition(self):",
            "def full_definition(self):",
            "def _get_definition(self, full=True):\n        raise NotImplementedError\n\n    @property",
            "def _full_properties(self):\n        return {\"name\": self.name}\n\n    @property",
            "def is_stacked(self):\n        return False\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def copy(self):\n        \"\"\"\n        Make a deep copy of the 1d Coordinates.\n\n        Returns\n        -------\n        :class:`Coordinates1d`\n            Copy of the coordinates.\n        \"\"\"\n\n        raise NotImplementedError",
            "def simplify(self):\n        \"\"\"Get the simplified/optimized representation of these coordinates.\n\n        Returns\n        -------\n        simplified : Coordinates1d\n            simplified version of the coordinates\n        \"\"\"\n\n        raise NotImplementedError",
            "def get_area_bounds(self, boundary):\n        \"\"\"\n        Get low and high coordinate area bounds.\n\n        Arguments\n        ---------\n        boundary : float, timedelta, array, None\n            Boundary offsets in this dimension.\n\n            * For a centered uniform boundary (same for every coordinate), use a single positive float or timedelta\n                offset. This represents the \"total segment length\" / 2.\n            * For a uniform boundary (segment or polygon same for every coordinate), use an array of float or\n                timedelta offsets\n            * For a fully specified boundary, use an array of boundary arrays (2-D array, N_coords x boundary spec),\n                 one per coordinate. The boundary_spec can be a single number, two numbers, or an array of numbers.\n            * For point coordinates, use None.\n\n        Returns\n        -------\n        low: float, np.datetime64\n            low area bound\n        high: float, np.datetime64\n            high area bound\n        \"\"\"\n\n        # point coordinates\n        if boundary is None:\n            return self.bounds\n\n        # empty coordinates\n        if self.size == 0:\n            return self.bounds\n\n        if np.array(boundary).ndim == 0:\n            # shortcut for uniform centered boundary\n            boundary = make_coord_delta(boundary)\n            lo_offset = -boundary\n            hi_offset = boundary\n        elif np.array(boundary).ndim == 1:\n            # uniform boundary polygon\n            boundary = make_coord_delta_array(boundary)\n            lo_offset = min(boundary)\n            hi_offset = max(boundary)\n        else:\n            L, H = self.argbounds\n            lo_offset = min(make_coord_delta_array(boundary[L]))\n            hi_offset = max(make_coord_delta_array(boundary[H]))\n\n        lo, hi = self.bounds\n        lo = add_coord(lo, lo_offset)\n        hi = add_coord(hi, hi_offset)\n\n        return lo, hi",
            "def _select_empty(self, return_index):\n        I = []\n        if return_index:\n            return self[I], I\n        else:\n            return self[I]",
            "def _select_full(self, return_index):\n        I = slice(None)\n        if return_index:\n            return self[I], I\n        else:\n            return self[I]",
            "def select(self, bounds, return_index=False, outer=False):\n        \"\"\"\n        Get the coordinate values that are within the given bounds.\n\n        The default selection returns coordinates that are within the bounds::\n\n            In [1]: c = ArrayCoordinates1d([0, 1, 2, 3], name='lat')\n\n            In [2]: c.select([1.5, 2.5]).coordinates\n            Out[2]: array([2.])\n\n        The *outer* selection returns the minimal set of coordinates that contain the bounds::\n\n            In [3]: c.select([1.5, 2.5], outer=True).coordinates\n            Out[3]: array([1., 2., 3.])\n\n        The *outer* selection also returns a boundary coordinate if a bound is outside this coordinates bounds but\n        *inside* its area bounds::\n\n            In [4]: c.select([3.25, 3.35], outer=True).coordinates\n            Out[4]: array([3.0], dtype=float64)\n\n            In [5]: c.select([10.0, 11.0], outer=True).coordinates\n            Out[5]: array([], dtype=float64)\n\n        Parameters\n        ----------\n        bounds : (low, high) or dict\n            Selection bounds. If a dictionary of dim -> (low, high) bounds is supplied, the bounds matching these\n            coordinates will be selected if available, otherwise the full coordinates will be returned.\n        outer : bool, optional\n            If True, do an *outer* selection. Default False.\n        return_index : bool, optional\n            If True, return index for the selection in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`Coordinates1d`\n            Coordinates1d object with coordinates within the bounds.\n        I : slice, boolean array\n            index or slice for the selected coordinates (only if return_index=True)\n        \"\"\"\n\n        # empty case\n        if self.dtype is None:\n            return self._select_empty(return_index)\n\n        if isinstance(bounds, dict):\n            bounds = bounds.get(self.name)\n            if bounds is None:\n                return self._select_full(return_index)\n\n        bounds = make_coord_value(bounds[0]), make_coord_value(bounds[1])\n\n        # check type\n        if not isinstance(bounds[0], self.dtype):\n            raise TypeError(\n                \"Input bounds do match the coordinates dtype (%s != %s)\" % (type(self.bounds[0]), self.dtype)\n            )\n        if not isinstance(bounds[1], self.dtype):\n            raise TypeError(\n                \"Input bounds do match the coordinates dtype (%s != %s)\" % (type(self.bounds[1]), self.dtype)\n            )\n\n        my_bounds = self.bounds\n\n        # If the bounds are of instance datetime64, then the comparison should happen at the lowest precision\n        if self.dtype == np.datetime64:\n            my_bounds, bounds = lower_precision_time_bounds(my_bounds, bounds, outer)\n\n        # full\n        if my_bounds[0] >= bounds[0] and my_bounds[1] <= bounds[1]:\n            return self._select_full(return_index)\n\n        # none\n        if my_bounds[0] > bounds[1] or my_bounds[1] < bounds[0]:\n            return self._select_empty(return_index)\n\n        # partial, implemented in child classes\n        return self._select(bounds, return_index, outer)",
            "def _select(self, bounds, return_index, outer):\n        raise NotImplementedError",
            "def _transform(self, transformer):\n        if self.name != \"alt\":\n            # this assumes that the transformer does not have a spatial transform\n            return self.copy()\n\n        # transform \"alt\" coordinates\n        from podpac.core.coordinates.array_coordinates1d import ArrayCoordinates1d\n\n        _, _, tcoordinates = transformer.transform(np.zeros(self.shape), np.zeros(self.shape), self.coordinates)\n        return ArrayCoordinates1d(tcoordinates, **self.properties)",
            "def issubset(self, other):\n        \"\"\"Report whether other coordinates contains these coordinates.\n\n        Arguments\n        ---------\n        other : Coordinates, Coordinates1d\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates.\n        \"\"\"\n\n        from podpac.core.coordinates import Coordinates\n\n        if isinstance(other, Coordinates):\n            if self.name not in other.dims:\n                return False\n            other = other[self.name]\n\n        # short-cuts that don't require checking coordinates\n        if self.size == 0:\n            return True\n\n        if other.size == 0:\n            return False\n\n        if self.dtype != other.dtype:\n            return False\n\n        if self.bounds[0] < other.bounds[0] or self.bounds[1] > other.bounds[1]:\n            return False\n\n        # check actual coordinates using built-in set method issubset\n        # for datetimes, convert to the higher resolution\n        my_coordinates = self.coordinates.ravel()\n        other_coordinates = other.coordinates.ravel()\n\n        if self.dtype == np.datetime64:\n            if my_coordinates[0].dtype < other_coordinates[0].dtype:\n                my_coordinates = my_coordinates.astype(other_coordinates.dtype)\n            elif other_coordinates[0].dtype < my_coordinates[0].dtype:\n                other_coordinates = other_coordinates.astype(my_coordinates.dtype)\n\n        return set(my_coordinates).issubset(other_coordinates)",
            "def horizontal_resolution(self, latitude, ellipsoid_tuple, coordinate_name, restype=\"nominal\", units=\"meter\"):\n        \"\"\"Return the horizontal resolution of a Uniform 1D Coordinate\n\n        Parameters\n        ----------\n        latitude: Coordinates1D\n            The latitude coordiantes of the current coordinate system, required for both lat and lon resolution.\n        ellipsoid_tuple: tuple\n            a tuple containing ellipsoid information from the the original coordinates to pass to geopy\n        coordinate_name: str\n            \"cartesian\" or \"ellipsoidal\", to tell calculate_distance what kind of calculation to do\n        restype: str\n            The kind of horizontal resolution that should be returned. Supported values are:\n            - \"nominal\" <-- returns average resolution based upon bounds and number of grid squares\n            - \"summary\" <-- Gives the exact mean and standard deviation of grid square resolutions\n            - \"full\" <-- Gives exact grid differences.\n\n        Returns\n        -------\n        float * (podpac.unit)\n            if restype == \"nominal\", return average distance in desired units\n        tuple * (podpac.unit)\n            if restype == \"summary\", return average distance and standard deviation in desired units\n        np.ndarray * (podpac.unit)\n            if restype == \"full\", give full resolution. 1D array for latitude, MxN matrix for longitude.\n        \"\"\"",
            "def nominal_unstacked_resolution():\n            \"\"\"Return resolution for unstacked coordiantes using the bounds\n\n            Returns\n            --------\n            The average distance between each grid square for this dimension\n            \"\"\"\n            if self.name == \"lat\":\n                return (\n                    calculate_distance(\n                        (self.bounds[0], 0), (self.bounds[1], 0), ellipsoid_tuple, coordinate_name, units\n                    ).magnitude\n                    / (self.size - 1)\n                ) * podpac.units(units)\n            elif self.name == \"lon\":\n                median_lat = ((latitude.bounds[1] - latitude.bounds[0]) / 2) + latitude.bounds[0]\n                return (\n                    calculate_distance(\n                        (median_lat, self.bounds[0]),\n                        (median_lat, self.bounds[1]),\n                        ellipsoid_tuple,\n                        coordinate_name,\n                        units,\n                    ).magnitude\n                    / (self.size - 1)\n                ) * podpac.units(units)\n            else:\n                return ValueError(\"Unknown dim: {}\".format(self.name))",
            "def summary_unstacked_resolution():\n            \"\"\"Return summary resolution for the dimension.\n\n            Returns\n            -------\n            tuple\n                the average distance between grid squares\n                the standard deviation of those distances\n            \"\"\"\n            if self.name == \"lat\" or self.name == \"lon\":\n                full_res = full_unstacked_resolution().magnitude\n                return (np.average(full_res) * podpac.units(units), np.std(full_res) * podpac.units(units))\n            else:\n                return ValueError(\"Unknown dim: {}\".format(self.name))",
            "def full_unstacked_resolution():\n            \"\"\"Calculate full resolution of unstacked dimension\n\n            Returns\n            -------\n            A matrix of distances\n            \"\"\"\n            if self.name == \"lat\":\n                top_bounds = np.stack(\n                    [latitude.coordinates[1:], np.full((latitude.coordinates[1:]).shape[0], 0)], axis=1\n                )  # use exact lat values\n                bottom_bounds = np.stack(\n                    [latitude.coordinates[:-1], np.full((latitude.coordinates[:-1]).shape[0], 0)], axis=1\n                )  # use exact lat values\n                return calculate_distance(top_bounds, bottom_bounds, ellipsoid_tuple, coordinate_name, units)\n            elif self.name == \"lon\":\n                M = latitude.coordinates.size\n                N = self.size\n                diff = np.zeros((M, N - 1))\n                for i in range(M):\n                    lat_value = latitude.coordinates[i]\n                    lon_values = self.coordinates\n                    top_bounds = np.stack(\n                        [np.full((lon_values[1:]).shape[0], lat_value), lon_values[1:]], axis=1\n                    )  # use exact lat values\n                    bottom_bounds = np.stack(\n                        [np.full((lon_values[:-1]).shape[0], lat_value), lon_values[:-1]], axis=1\n                    )  # use exact lat values\n                    diff[i] = calculate_distance(\n                        top_bounds, bottom_bounds, ellipsoid_tuple, coordinate_name, units\n                    ).magnitude\n                return diff * podpac.units(units)\n            else:\n                raise ValueError(\"Unknown dim: {}\".format(self.name))\n\n        if restype == \"nominal\":\n            return nominal_unstacked_resolution()\n        elif restype == \"summary\":\n            return summary_unstacked_resolution()\n        elif restype == \"full\":\n            return full_unstacked_resolution()\n        else:\n            raise ValueError(\"Invalid value for type: {}\".format(restype))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/utils.py",
        "comments": [
            "// 12"
        ],
        "docstrings": [
            "\"\"\"\nUtilities functions for handling podpac coordinates.\n\n.. testsetup:: podpac.core.coordinates.utils\n    \n    import numpy as np\n    from podpac.core.coordinates.utils import *\n\"\"\"",
            "\"\"\"\n    Make a numpy timedelta from a podpac timedelta string.\n\n    The time delta string must be in the form '<n>,<unit>', where <n> is an\n    integer and <unit> is the character for the timedelta unit.\n\n    Parameters\n    ----------\n    s : str\n        podpac timedelta string, in the form '<n>,<unit>'\n\n    Returns\n    -------\n    np.timedelta64\n        numpy timedelta\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> get_timedelta('2,D')\n        numpy.timedelta64(2,'D')\n\n        >>> get_timedelta('-3,h')\n        numpy.timedelta64(-3,'h')\n\n    \"\"\"",
            "\"\"\"\n    Get the unit character from a numpy timedelta.\n\n    Parameters\n    ----------\n    delta : np.timedelta64\n        numpy timedelta\n\n    Returns\n    -------\n    str\n        the character for the timedelta unit\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> get_timedelta_unit(np.timedelta64(1, 'D'))\n        'D'\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    \"\"\"",
            "\"\"\"\n    Make a podpac timedelta string from a numpy timedelta.\n\n    Parameters\n    ----------\n    delta : np.timedelta64\n        numpy timedelta\n\n    Returns\n    -------\n    str\n        podpac timedelta string, in the form '<n>,<units>'\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> make_timedelta_string(np.timedelta64(2, 'D'))\n        '2,D'\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    \"\"\"",
            "\"\"\"\n    Make a podpac coordinate value by casting to the correct type.\n\n    Parameters\n    ----------\n    val : str, number, datetime.date, np.ndarray\n        Input coordinate value.\n\n    Returns\n    -------\n    val : float, np.datetime64\n        Cast coordinate value.\n\n    Notes\n    -----\n     * the value is extracted from singleton and 0-dimensional arrays\n     * strings interpreted as inputs to numpy.datetime64\n     * datetime datetimes are converted to numpy datetimes\n     * numbers are converted to floats\n\n    Raises\n    ------\n    Value\n        val is an unsupported type\n\n    \"\"\"",
            "\"\"\"\n    Make a podpac coordinate delta by casting to the correct type.\n\n    Parameters\n    ----------\n    val : str, number, datetime.timedelta, np.ndarray\n        Input coordinate delta.\n\n    Returns\n    -------\n    val : float, np.timedelta64\n        Cast coordinate delta.\n\n    Notes\n    -----\n     * the value is extracted from singleton and 0-dimensional arrays\n     * strings are interpreted as inputs to get_timedelta\n     * datetime timedeltas are converted to numpy timedeltas\n     * numbers are converted to floats\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    \"\"\"",
            "\"\"\"\n    Make an array of podpac coordinate values by casting to the correct type.\n\n    Parameters\n    ----------\n    values : array-like\n        Input coordinates.\n\n    Returns\n    -------\n    a : np.ndarray\n        Cast coordinate values.\n\n    Notes\n    -----\n     * all of the values must be of the same type\n     * strings and datetimes are converted to numpy datetime64\n     * numbers are converted to floats\n    \"\"\"",
            "\"\"\"\n    Make an array of podpac coordinate deltas by casting to the correct type.\n\n    Parameters\n    ----------\n    values : array-like\n        Input coordinate deltas.\n\n    Returns\n    -------\n    a : np.ndarray\n        Cast coordinate deltas.\n\n    Notes\n    -----\n     * all of the deltas must be of the same type\n     * strings and timedeltas are converted to numpy timdelta64\n     * numbers are converted to floats\n    \"\"\"",
            "\"\"\"\n    Add a coordinate delta to a coordinate value.\n\n    Parameters\n    ----------\n    base : float, np.datetime64\n        The base coordinate value.\n    delta : float, np.timedelta64\n        The coordinate delta. This can also be a numpy array.\n\n    Returns\n    -------\n    result : float, np.datetime64\n        The sum, with month and year timedeltas handled. If delta is an array,\n        the result will be an array.\n\n    Notes\n    -----\n    Month and year deltas are added nominally, which differs from how numpy\n    adds timedeltas to datetimes. When adding months or years, if the new date\n    exceeds the number of days in the month, it is set to the last day of the\n    month.\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> add_coord(1.5, 1.0)\n        2.5\n\n        >>> add_coord(1.5, np.array([1.0, 2.0]))\n        array([2.5, 3.5])\n\n        >>> add_coord(np.datetime64('2018-01-01'), np.timedelta64(1, 'D'))\n        numpy.datetime64('2018-01-02')\n\n        >>> np.datetime64('2018-01-01') + np.timedelta64(1, 'M')\n        Traceback (most recent call last):\n          File \"<stdin>\", line 1, in <module>\n        TypeError: Cannot get a common metadata divisor for NumPy datetime metadata [D] and [M] because they have incompatible nonlinear base time units\n\n        >>> add_coord(np.datetime64('2018-01-01'), np.timedelta64(1, 'M'))\n        numpy.datetime64('2018-02-01')\n    \"\"\"",
            "\"\"\"\n    Divide a coordinate delta by a numerical divisor.\n\n    Parameters\n    ----------\n    delta : float, np.timedelta64\n        The base delta.\n    divisor : number\n        The divisor\n\n    Returns\n    -------\n    result : float, np.timedelta64\n        The result, with timedeltas converted to higher resolution if necessary.\n\n    \"\"\"",
            "\"\"\"\n    Divide a timedelta by a numerical divisor. This is a helper function for divide_delta.\n\n    Parameters\n    ----------\n    delta : np.timedelta64\n        The base delta.\n    divisor : number\n        The divisor\n\n    Returns\n    -------\n    result : np.timedelta64\n        The result, converted to higher resolution if necessary.\n\n    \"\"\"",
            "\"\"\"Check if a numpy timedelta64 is evenly divisible by another.\n\n    Arguments\n    ---------\n    numerator : numpy.timedelta64\n        numerator\n    divisor : numpy.timedelta64\n        divisor\n\n    Returns\n    -------\n    divisible : bool\n        if the numerator is evenly divisible by the divisor\n    \"\"\"",
            "\"\"\"\n    When given two bounds of np.datetime64, this function will convert both bounds to the lower-precision (in terms of\n    time unit) numpy datetime4 object if outer==True, otherwise only my_bounds will be converted.\n\n    Parameters\n    -----------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection\n    outer : bool\n        When the other_bounds are higher precision than the input_bounds, only convert these IF outer=True\n\n    Returns\n    --------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset at the new precision\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection at the new precision, if outer == True, otherwise return original coordinates\n    \"\"\"",
            "\"\"\"\n    When given two bounds of np.datetime64, this function will convert both bounds to the higher-precision (in terms of\n    time unit) numpy datetime4 object if outer==True, otherwise only my_bounds will be converted.\n\n    Parameters\n    -----------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection\n    outer : bool\n        When the other_bounds are higher precision than the input_bounds, only convert these IF outer=True\n\n    Returns\n    --------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset at the new precision\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection at the new precision, if outer == True, otherwise return original coordinates\n\n    Notes\n    ------\n    When converting the upper bound with outer=True, the whole lower-precision time unit is valid. E.g. when converting\n    YYYY-MM-DD to YYYY-MM-DD HH, the largest value for HH is used, since the whole day is valid.\n    \"\"\"",
            "\"\"\"\n    Check if the CRS has vertical units.\n\n    Arguments\n    ---------\n    crs : pyproj.CRS\n        CRS to check\n\n    Returns\n    -------\n    has_alt_units : bool\n        True if the CRS has vunits or other altitude units.\n    \"\"\"",
            "\"\"\"Return distance of 2 points in desired unit measurement\n\n    Parameters\n    ----------\n    point1 : tuple\n    point2 : tuple\n\n    Returns\n    -------\n    float\n        The distance between point1 and point2, according to the current coordinate system's distance metric, using the desired units\n    \"\"\""
        ],
        "code_snippets": [
            "def get_timedelta(s):\n    \"\"\"\n    Make a numpy timedelta from a podpac timedelta string.\n\n    The time delta string must be in the form '<n>,<unit>', where <n> is an\n    integer and <unit> is the character for the timedelta unit.\n\n    Parameters\n    ----------\n    s : str\n        podpac timedelta string, in the form '<n>,<unit>'\n\n    Returns\n    -------\n    np.timedelta64\n        numpy timedelta\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> get_timedelta('2,D')\n        numpy.timedelta64(2,'D')\n\n        >>> get_timedelta('-3,h')\n        numpy.timedelta64(-3,'h')\n\n    \"\"\"\n\n    a, b = s.split(\",\")\n    return np.timedelta64(int(a), b)",
            "def get_timedelta_unit(delta):\n    \"\"\"\n    Get the unit character from a numpy timedelta.\n\n    Parameters\n    ----------\n    delta : np.timedelta64\n        numpy timedelta\n\n    Returns\n    -------\n    str\n        the character for the timedelta unit\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> get_timedelta_unit(np.timedelta64(1, 'D'))\n        'D'\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    \"\"\"\n\n    try:\n        dname = delta.dtype.name\n    except AttributeError:\n        raise TypeError(\"Cannot get timedelta unit from type '%s'\" % type(delta))\n    if not dname.startswith(\"timedelta\"):\n        raise TypeError(\"Cannot get timedelta unit from dtype '%s'\" % dname)\n    return dname[12:-1]",
            "def make_timedelta_string(delta):\n    \"\"\"\n    Make a podpac timedelta string from a numpy timedelta.\n\n    Parameters\n    ----------\n    delta : np.timedelta64\n        numpy timedelta\n\n    Returns\n    -------\n    str\n        podpac timedelta string, in the form '<n>,<units>'\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> make_timedelta_string(np.timedelta64(2, 'D'))\n        '2,D'\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    \"\"\"\n\n    if not isinstance(delta, np.timedelta64):\n        raise TypeError(\"Cannot make timedelta string from type '%s'\" % type(delta))\n\n    mag = delta.astype(int)\n    unit = get_timedelta_unit(delta)\n    return \"%d,%s\" % (mag, unit)",
            "def make_coord_value(val):\n    \"\"\"\n    Make a podpac coordinate value by casting to the correct type.\n\n    Parameters\n    ----------\n    val : str, number, datetime.date, np.ndarray\n        Input coordinate value.\n\n    Returns\n    -------\n    val : float, np.datetime64\n        Cast coordinate value.\n\n    Notes\n    -----\n     * the value is extracted from singleton and 0-dimensional arrays\n     * strings interpreted as inputs to numpy.datetime64\n     * datetime datetimes are converted to numpy datetimes\n     * numbers are converted to floats\n\n    Raises\n    ------\n    Value\n        val is an unsupported type\n\n    \"\"\"\n\n    # extract value from singleton and 0-dimensional arrays\n    if isinstance(val, np.ndarray):\n        try:\n            val = val.item()\n        except ValueError:\n            raise TypeError(\"Invalid coordinate value, unsupported type '%s'\" % type(val))\n\n    # type checking and conversion\n    if isinstance(val, (string_types, datetime.date)):\n        val = np.datetime64(val)\n    elif isinstance(val, np.datetime64):\n        pass\n    elif isinstance(val, numbers.Number):\n        val = float(val)\n    else:\n        raise TypeError(\"Invalid coordinate value, unsupported type '%s'\" % type(val))\n\n    return val",
            "def make_coord_delta(val):\n    \"\"\"\n    Make a podpac coordinate delta by casting to the correct type.\n\n    Parameters\n    ----------\n    val : str, number, datetime.timedelta, np.ndarray\n        Input coordinate delta.\n\n    Returns\n    -------\n    val : float, np.timedelta64\n        Cast coordinate delta.\n\n    Notes\n    -----\n     * the value is extracted from singleton and 0-dimensional arrays\n     * strings are interpreted as inputs to get_timedelta\n     * datetime timedeltas are converted to numpy timedeltas\n     * numbers are converted to floats\n\n    Raises\n    ------\n    TypeError\n        Description\n\n    \"\"\"\n\n    # extract value from singleton and 0-dimensional arrays\n    if isinstance(val, np.ndarray):\n        try:\n            val = val.item()\n        except ValueError:\n            raise TypeError(\"Invalid coordinate delta, unsupported type '%s'\" % type(val))\n\n    # type checking and conversion\n    if isinstance(val, string_types):\n        val = get_timedelta(val)\n    elif isinstance(val, datetime.timedelta):\n        val = np.timedelta64(val)\n    elif isinstance(val, np.timedelta64):\n        pass\n    elif isinstance(val, numbers.Number):\n        val = float(val)\n    else:\n        raise TypeError(\"Invalid coordinate delta, unsupported type '%s'\" % type(val))\n\n    return val",
            "def make_coord_array(values):\n    \"\"\"\n    Make an array of podpac coordinate values by casting to the correct type.\n\n    Parameters\n    ----------\n    values : array-like\n        Input coordinates.\n\n    Returns\n    -------\n    a : np.ndarray\n        Cast coordinate values.\n\n    Notes\n    -----\n     * all of the values must be of the same type\n     * strings and datetimes are converted to numpy datetime64\n     * numbers are converted to floats\n    \"\"\"\n\n    a = np.atleast_1d(values)\n\n    if a.dtype == float or np.issubdtype(a.dtype, np.datetime64):\n        pass\n\n    elif np.issubdtype(a.dtype, np.number):\n        a = a.astype(float)\n\n    else:\n        a = np.array([make_coord_value(e) for e in np.atleast_1d(np.array(values, dtype=object)).flatten()]).reshape(\n            a.shape\n        )\n\n        if not np.issubdtype(a.dtype, np.datetime64):\n            raise ValueError(\"Invalid coordinate values (must be all numbers or all datetimes)\")\n\n    return a",
            "def make_coord_delta_array(values):\n    \"\"\"\n    Make an array of podpac coordinate deltas by casting to the correct type.\n\n    Parameters\n    ----------\n    values : array-like\n        Input coordinate deltas.\n\n    Returns\n    -------\n    a : np.ndarray\n        Cast coordinate deltas.\n\n    Notes\n    -----\n     * all of the deltas must be of the same type\n     * strings and timedeltas are converted to numpy timdelta64\n     * numbers are converted to floats\n    \"\"\"\n\n    a = np.atleast_1d(values)\n\n    if a.ndim != 1:\n        raise ValueError(\"Invalid coordinate deltas (ndim=%d, must be ndim=1)\" % a.ndim)\n\n    if a.dtype == float or np.issubdtype(a.dtype, np.timedelta64):\n        pass\n\n    elif np.issubdtype(a.dtype, np.number):\n        a = a.astype(float)\n\n    else:\n        a = np.array([make_coord_delta(e) for e in np.atleast_1d(np.array(values, dtype=object))])\n\n        if not np.issubdtype(a.dtype, np.timedelta64):\n            raise ValueError(\"Invalid coordinate deltas (must be all numbers or all compatible timedeltas)\")\n\n    return a",
            "def add_coord(base, delta):\n    \"\"\"\n    Add a coordinate delta to a coordinate value.\n\n    Parameters\n    ----------\n    base : float, np.datetime64\n        The base coordinate value.\n    delta : float, np.timedelta64\n        The coordinate delta. This can also be a numpy array.\n\n    Returns\n    -------\n    result : float, np.datetime64\n        The sum, with month and year timedeltas handled. If delta is an array,\n        the result will be an array.\n\n    Notes\n    -----\n    Month and year deltas are added nominally, which differs from how numpy\n    adds timedeltas to datetimes. When adding months or years, if the new date\n    exceeds the number of days in the month, it is set to the last day of the\n    month.\n\n    Examples\n    --------\n\n    .. doctest:: podpac.core.coordinates.utils\n\n        >>> add_coord(1.5, 1.0)\n        2.5\n\n        >>> add_coord(1.5, np.array([1.0, 2.0]))\n        array([2.5, 3.5])\n\n        >>> add_coord(np.datetime64('2018-01-01'), np.timedelta64(1, 'D'))\n        numpy.datetime64('2018-01-02')\n\n        >>> np.datetime64('2018-01-01') + np.timedelta64(1, 'M')\n        Traceback (most recent call last):\n          File \"<stdin>\", line 1, in <module>\n        TypeError: Cannot get a common metadata divisor for NumPy datetime metadata [D] and [M] because they have incompatible nonlinear base time units\n\n        >>> add_coord(np.datetime64('2018-01-01'), np.timedelta64(1, 'M'))\n        numpy.datetime64('2018-02-01')\n    \"\"\"\n\n    try:\n        return base + delta\n    except TypeError as e:\n        if isinstance(base, np.datetime64) and np.issubdtype(delta.dtype, np.timedelta64):\n            return _add_nominal_timedelta(base, delta)\n        else:\n            raise e",
            "def _add_nominal_timedelta(base, delta):\n    dunit = get_timedelta_unit(delta)\n    if dunit not in [\"Y\", \"M\"]:\n        return base + delta\n\n    shape = delta.shape\n    # The following is needed when the time resolution is smaller than a ms -- cannot create datetime in those cases\n\n    if not isinstance(base.item(), datetime.datetime):\n        base = base.astype(\"datetime64[ms]\")\n    base = base.item()\n    tds = np.array(delta).astype(int).flatten()\n\n    dates = []\n    for td in tds:\n        if dunit == \"Y\":\n            date = _replace_safe(base, year=base.year + td)\n        elif dunit == \"M\":\n            date = _replace_safe(base, month=base.month + td)\n        dates.append(date)\n\n    dates = np.array([np.datetime64(date) for date in dates]).reshape(shape)\n    if shape == ():\n        dates = dates[()]\n    return dates",
            "def _replace_safe(dt, year=None, month=None):\n    if year is None:\n        year = dt.year\n    if month is None:\n        month = dt.month\n\n    year = year + (month - 1)",
            "def divide_delta(delta, divisor):\n    \"\"\"\n    Divide a coordinate delta by a numerical divisor.\n\n    Parameters\n    ----------\n    delta : float, np.timedelta64\n        The base delta.\n    divisor : number\n        The divisor\n\n    Returns\n    -------\n    result : float, np.timedelta64\n        The result, with timedeltas converted to higher resolution if necessary.\n\n    \"\"\"\n\n    if isinstance(delta, np.timedelta64):\n        try:\n            return divide_timedelta(delta, divisor)\n        except ValueError:\n            raise ValueError(\"Cannot divide timedelta '%s' evenly by %d\" % (make_timedelta_string(delta), divisor))\n    else:\n        return delta / divisor",
            "def divide_timedelta(delta, divisor):\n    \"\"\"\n    Divide a timedelta by a numerical divisor. This is a helper function for divide_delta.\n\n    Parameters\n    ----------\n    delta : np.timedelta64\n        The base delta.\n    divisor : number\n        The divisor\n\n    Returns\n    -------\n    result : np.timedelta64\n        The result, converted to higher resolution if necessary.\n\n    \"\"\"\n\n    result = delta / divisor\n    if divisor * result.astype(int) == delta.astype(int):\n        return result\n\n    if delta.dtype.str in _TIMEDELTA_ZOOM:\n        return divide_timedelta(delta.astype(_TIMEDELTA_ZOOM[delta.dtype.str]), divisor)\n\n    # months, for example\n    raise ValueError(\"Cannot divide timedelta '%s' evenly by %d\" % (make_timedelta_string(delta), divisor))",
            "def timedelta_divisible(numerator, divisor):\n    \"\"\"Check if a numpy timedelta64 is evenly divisible by another.\n\n    Arguments\n    ---------\n    numerator : numpy.timedelta64\n        numerator\n    divisor : numpy.timedelta64\n        divisor\n\n    Returns\n    -------\n    divisible : bool\n        if the numerator is evenly divisible by the divisor\n    \"\"\"\n\n    try:\n        # NOTE: numerator % divisor works in some versions of numpy, but not all\n        r = numerator / divisor\n        return float(r) == int(r)\n    except TypeError:\n        # e.g. months and days are not comparible\n        return False\n\n\n_TIMEDELTA_ZOOM = {\n    \"<m8[Y]\": \"<m8[D]\",\n    \"<m8[D]\": \"<m8[h]\",\n    \"<m8[h]\": \"<m8[m]\",\n    \"<m8[m]\": \"<m8[s]\",\n    \"<m8[s]\": \"<m8[ms]\",  # already probably farther then necessary...\n    \"<m8[ms]\": \"<m8[us]\",\n    \"<m8[us]\": \"<m8[ns]\",\n}\n\nVALID_DIMENSION_NAMES = [\"lat\", \"lon\", \"alt\", \"time\"]",
            "class Dimension(tl.Enum):",
            "def __init__(self, *args, **kwargs):\n        super(Dimension, self).__init__(VALID_DIMENSION_NAMES, *args, **kwargs)",
            "def lower_precision_time_bounds(my_bounds, other_bounds, outer):\n    \"\"\"\n    When given two bounds of np.datetime64, this function will convert both bounds to the lower-precision (in terms of\n    time unit) numpy datetime4 object if outer==True, otherwise only my_bounds will be converted.\n\n    Parameters\n    -----------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection\n    outer : bool\n        When the other_bounds are higher precision than the input_bounds, only convert these IF outer=True\n\n    Returns\n    --------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset at the new precision\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection at the new precision, if outer == True, otherwise return original coordinates\n    \"\"\"\n    if not isinstance(other_bounds[0], np.datetime64) or not isinstance(other_bounds[1], np.datetime64):\n        raise TypeError(\"Input bounds should be of type np.datetime64 when selecting data from:\", str(my_bounds))\n\n    if not isinstance(my_bounds[0], np.datetime64) or not isinstance(my_bounds[1], np.datetime64):\n        raise TypeError(\"Native bounds should be of type np.datetime64 when selecting data using:\", str(other_bounds))\n\n    if my_bounds[0].dtype < other_bounds[0].dtype and outer:\n        other_bounds = [b.astype(my_bounds[0].dtype) for b in other_bounds]\n    else:\n        my_bounds = [b.astype(other_bounds[0].dtype) for b in my_bounds]\n\n    return my_bounds, other_bounds",
            "def higher_precision_time_bounds(my_bounds, other_bounds, outer):\n    \"\"\"\n    When given two bounds of np.datetime64, this function will convert both bounds to the higher-precision (in terms of\n    time unit) numpy datetime4 object if outer==True, otherwise only my_bounds will be converted.\n\n    Parameters\n    -----------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection\n    outer : bool\n        When the other_bounds are higher precision than the input_bounds, only convert these IF outer=True\n\n    Returns\n    --------\n    my_bounds : List(np.datetime64)\n        The bounds of the coordinates of the dataset at the new precision\n    other_bounds : List(np.datetime64)\n        The bounds used for the selection at the new precision, if outer == True, otherwise return original coordinates\n\n    Notes\n    ------\n    When converting the upper bound with outer=True, the whole lower-precision time unit is valid. E.g. when converting\n    YYYY-MM-DD to YYYY-MM-DD HH, the largest value for HH is used, since the whole day is valid.\n    \"\"\"\n    if not isinstance(other_bounds[0], np.datetime64) or not isinstance(other_bounds[1], np.datetime64):\n        raise TypeError(\"Input bounds should be of type np.datetime64 when selecting data from:\", str(my_bounds))\n\n    if not isinstance(my_bounds[0], np.datetime64) or not isinstance(my_bounds[1], np.datetime64):\n        raise TypeError(\"Native bounds should be of type np.datetime64 when selecting data using:\", str(other_bounds))\n\n    if my_bounds[0].dtype > other_bounds[0].dtype and outer:\n        # for the upper bound, the whole lower-precision time unit is valid (see note)\n        # select the largest value for the higher-precision time unit by adding one lower-precision time unit and\n        # subtracting one higher-precision time unit.\n        other_bounds = [\n            other_bounds[0].astype(my_bounds[0].dtype),\n            (other_bounds[1] + 1).astype(my_bounds[0].dtype) - 1,\n        ]\n    else:\n        my_bounds = [b.astype(other_bounds[0].dtype) for b in my_bounds]\n\n    return my_bounds, other_bounds",
            "def has_alt_units(crs):\n    \"\"\"\n    Check if the CRS has vertical units.\n\n    Arguments\n    ---------\n    crs : pyproj.CRS\n        CRS to check\n\n    Returns\n    -------\n    has_alt_units : bool\n        True if the CRS has vunits or other altitude units.\n    \"\"\"\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return crs.is_vertical or \"vunits\" in crs.to_dict() or any(axis.direction == \"up\" for axis in crs.axis_info)",
            "def calculate_distance(point1, point2, ellipsoid_tuple, coordinate_name, units=\"meter\"):\n    \"\"\"Return distance of 2 points in desired unit measurement\n\n    Parameters\n    ----------\n    point1 : tuple\n    point2 : tuple\n\n    Returns\n    -------\n    float\n        The distance between point1 and point2, according to the current coordinate system's distance metric, using the desired units\n    \"\"\"\n    if coordinate_name == \"cartesian\":\n        return np.linalg.norm(point1 - point2, axis=-1, units=\"meter\") * podpac.units(units)\n    else:\n        if not isinstance(point1, tuple) and point1.size > 2:\n            distances = np.empty(len(point1))\n            for i in range(len(point1)):\n                distances[i] = geodesic(point1[i], point2[i], ellipsoid=ellipsoid_tuple).m\n            return distances * podpac.units(\"metre\").to(podpac.units(units))\n        if not isinstance(point2, tuple) and point2.size > 2:\n            distances = np.empty(len(point2))\n            for i in range(len(point2)):\n                distances[i] = geodesic(point1, point2[i], ellipsoid=ellipsoid_tuple).m\n            return distances * podpac.units(\"metre\").to(podpac.units(units))\n        else:\n            return (geodesic(point1, point2, ellipsoid=ellipsoid_tuple).m) * podpac.units(\"metre\").to(\n                podpac.units(units)\n            )"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/coordinates.py",
        "comments": [
            "//xarray.pydata.org/en/stable/data-structures.html>`_:",
            "//proj.org).",
            "//proj.org/).",
            "//proj.org/).",
            "//docs.geoserver.org/stable/en/user/services/wms/reference.html",
            "//docs.geoserver.org/stable/en/user/services/wms/basics.html",
            "//proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems",
            "//proj.org/usage/projections.html#cartographic-projection>`_ for",
            "//proj4.org/operations/conversions/unitconvert.html#distance-units>`_ for unit string"
        ],
        "docstrings": [
            "\"\"\"\nMultidimensional Coordinates\n\"\"\"",
            "\"\"\"\n    Multidimensional Coordinates.\n\n    Coordinates are used to evaluate Nodes and to define the coordinates of a DataSource nodes. The API is modeled after\n    coords in `xarray <http://xarray.pydata.org/en/stable/data-structures.html>`_:\n\n     * Coordinates are created from a list of coordinate values and dimension names.\n     * Coordinate values are always either ``float`` or ``np.datetime64``. For convenience, podpac\n       automatically converts datetime strings such as ``'2018-01-01'`` to ``np.datetime64``.\n     * The allowed dimensions are ``'lat'``, ``'lon'``, ``'time'``, and ``'alt'``.\n     * Coordinates from multiple dimensions can be stacked together to represent a *list* of coordinates instead of a\n       *grid* of coordinates. The name of the stacked coordinates uses an underscore to combine the underlying\n       dimensions, e.g. ``'lat_lon'``.\n\n    Coordinates are dict-like, for example:\n\n     * get coordinates by dimension name: ``coords['lat']``\n     * get iterable dimension keys and coordinates values: ``coords.keys()``, ``coords.values()``\n     * loop through dimensions: ``for dim in coords: ...``\n\n    Parameters\n    ----------\n    dims\n        Tuple of dimension names, potentially stacked.\n    udims\n        Tuple of individual dimension names, always unstacked.\n    \"\"\"",
            "\"\"\"\n        Create multidimensional coordinates.\n\n        Arguments\n        ---------\n        coords : list\n            List of coordinate values for each dimension. Valid coordinate values:\n\n             * single coordinate value (number, datetime64, or str)\n             * array of coordinate values\n             * list of stacked coordinate values\n             * :class:`Coordinates1d` or :class:`StackedCoordinates` object\n        dims : list of str, optional\n            List of dimension names. Optional if all items in ``coords`` are named. Valid names are\n\n             * 'lat', 'lon', 'alt', or 'time' for unstacked coordinates\n             * dimension names joined by an underscore for stacked coordinates\n        crs : str, optional\n            Coordinate reference system. Supports PROJ4 and WKT.\n        validate_crs : bool, optional\n            Use False to skip crs validation. Default True.\n        \"\"\"",
            "\"\"\"\n        Create a grid of coordinates.\n\n        Valid coordinate values:\n\n         * single coordinate value (number, datetime64, or str)\n         * array of coordinate values\n         * ``(start, stop, step)`` tuple for uniformly-spaced coordinates\n         * Coordinates1d object\n\n        This is equivalent to creating unstacked coordinates with a list of coordinate values::\n\n            podpac.Coordinates.grid(lat=[0, 1, 2], lon=[10, 20], dims=['lat', 'lon'])\n            podpac.Coordinates([[0, 1, 2], [10, 20]], dims=['lan', 'lon'])\n\n        Arguments\n        ---------\n        lat : optional\n            coordinates for the latitude dimension\n        lon : optional\n            coordinates for the longitude dimension\n        alt : optional\n            coordinates for the altitude dimension\n        time : optional\n            coordinates for the time dimension\n        dims : list of str, optional in Python>=3.6\n            List of dimension names, must match the provided keyword arguments. In Python 3.6 and above, the ``dims``\n            argument is optional, and the dims will match the order of the provided keyword arguments.\n        crs : str, optional\n            Coordinate reference system. Supports any PROJ4 or PROJ6 compliant string (https://proj.org).\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n\n        See Also\n        --------\n        points\n        \"\"\"",
            "\"\"\"\n        Create a list of multidimensional coordinates.\n\n        Valid coordinate values:\n\n         * single coordinate value (number, datetime64, or str)\n         * array of coordinate values\n         * ``(start, stop, step)`` tuple for uniformly-spaced coordinates\n         * Coordinates1d object\n\n        Note that the coordinates for each dimension must be the same size.\n\n        This is equivalent to creating stacked coordinates with a list of coordinate values and a stacked dimension\n        name::\n\n            podpac.Coordinates.points(lat=[0, 1, 2], lon=[10, 20, 30], dims=['lat', 'lon'])\n            podpac.Coordinates([[[0, 1, 2], [10, 20, 30]]], dims=['lan_lon'])\n\n        Arguments\n        ---------\n        lat : optional\n            coordinates for the latitude dimension\n        lon : optional\n            coordinates for the longitude dimension\n        alt : optional\n            coordinates for the altitude dimension\n        time : optional\n            coordinates for the time dimension\n        dims : list of str, optional in Python>=3.6\n            List of dimension names, must match the provided keyword arguments. In Python 3.6 and above, the ``dims``\n            argument is optional, and the dims will match the order of the provided keyword arguments.\n        crs : str, optional\n            Coordinate reference system. Supports any PROJ4 or PROJ6 compliant string (https://proj.org/).\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n\n        See Also\n        --------\n        grid\n        \"\"\"",
            "\"\"\"\n        Create podpac Coordinates from xarray coords.\n\n        Arguments\n        ---------\n        x : DataArray, Dataset, DataArrayCoordinates, DatasetCoordinates\n            DataArray, Dataset, or xarray coordinates\n        crs : str, optional\n            Coordinate reference system. Supports any PROJ4 or PROJ6 compliant string (https://proj.org/).\n            If not provided, the crs will be loaded from ``x.attrs`` if possible.\n        validate_crs: bool, optional\n            Default is False. If True, the crs will be validated.\n\n        Returns\n        -------\n        coords : :class:`Coordinates`\n            podpac Coordinates\n        \"\"\"",
            "\"\"\"\n        Create podpac Coordinates from a coordinates JSON definition.\n\n        Example JSON definition::\n\n            [\n                {\n                    \"name\": \"lat\",\n                    \"start\": 1,\n                    \"stop\": 10,\n                    \"step\": 0.5,\n                },\n                {\n                    \"name\": \"lon\",\n                    \"start\": 1,\n                    \"stop\": 2,\n                    \"size\": 100\n                },\n                {\n                    \"name\": \"time\",\n                    \"values\": [\n                        \"2018-01-01\",\n                        \"2018-01-03\",\n                        \"2018-01-10\"\n                    ]\n                }\n            ]\n\n        Arguments\n        ---------\n        s : str\n            coordinates JSON definition\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n\n        See Also\n        --------\n        json\n        \"\"\"",
            "\"\"\"\n        Create podpac Coordinates from a WMS/WCS request.\n\n        Arguments\n        ---------\n        url : str, dict\n            The raw WMS/WCS request url, or a dictionary of query parameters\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n        \"\"\"",
            "\"\"\"Creates Coordinates from GDAL Geotransform.\"\"\"",
            "\"\"\"\n        Create podpac Coordinates from a coordinates definition.\n\n        Arguments\n        ---------\n        d : list\n            coordinates definition\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n\n        See Also\n        --------\n        from_json, definition\n        \"\"\"",
            "\"\"\"dict-like keys: dims\"\"\"",
            "\"\"\"dict-like values: coordinates for each key/dimension\"\"\"",
            "\"\"\"dict-like items: (dim, coordinates) pairs\"\"\"",
            "\"\"\"dict-like get: get coordinates by dimension name with an optional\"\"\"",
            "\"\"\"dict-like update: add/replace coordinates using another Coordinates object\"\"\"",
            "\"\"\":tuple: Tuple of dimension names, potentially stacked.\n\n        See Also\n        --------\n        udims\n        \"\"\"",
            "\"\"\":tuple: Tuple of indexing dimension names used to make xarray DataArray.\n\n        Unless there are shaped (ndim>1) coordinates, this will match the ``dims``.\n        \"\"\"",
            "\"\"\":tuple: Tuple of unstacked dimension names.\n\n        If there are no stacked dimensions, then ``dims`` and ``udims`` will be the same::\n\n            In [1]: lat = [0, 1]\n\n            In [2]: lon = [10, 20]\n\n            In [3]: time = '2018-01-01'\n\n            In [4]: c = podpac.Coordinates([lat, lon, time], dims=['lat', 'lon', 'time'])\n\n            In [5]: c.dims\n            Out[5]: ('lat', 'lon', 'time')\n\n            In [6]: c.udims\n            Out[6]: ('lat', 'lon', 'time')\n\n\n        If there are stacked dimensions, then ``udims`` contains the individual dimension names::\n\n            In [7]: c = podpac.Coordinates([[lat, lon], time], dims=['lat_lon', 'time'])\n\n            In [8]: c.dims\n            Out[8]: ('lat_lon', 'time')\n\n            In [9]: c.udims\n            Out[9]: ('lat', 'lon', 'time')\n\n        See Also\n        --------\n        dims\n        \"\"\"",
            "\"\"\":tuple: Tuple of the number of coordinates in each dimension.\"\"\"",
            "\"\"\":int: Number of dimensions.\"\"\"",
            "\"\"\":int: Total number of coordinates.\"\"\"",
            "\"\"\":dict: Dictionary of (low, high) coordinates bounds in each unstacked dimension\"\"\"",
            "\"\"\"\n        :dict: xarray coords\n        \"\"\"",
            "\"\"\":dict: Dictionary of the coordinate properties.\"\"\"",
            "\"\"\":list: Serializable coordinates definition.\"\"\"",
            "\"\"\":list: Serializable coordinates definition, containing all properties. For internal use.\"\"\"",
            "\"\"\":str: JSON-serialized coordinates definition.\n\n        The ``json`` can be used to create new Coordinates::\n\n            c = podapc.Coordinates(...)\n            c2 = podpac.Coordinates.from_json(c.definition)\n\n        The serialized definition is used to define coordinates in node definitions and to transport coordinates, e.g.\n        over HTTP and in AWS lambda functions. It also provides a consistent hashable value.\n\n        See Also\n        --------\n        from_json\n        \"\"\"",
            "\"\"\":str: Coordinates hash value.\"\"\"",
            "\"\"\":tuple: GDAL geotransform.\"\"\"",
            "\"\"\"Get coordinate area bounds, including segment information, for each unstacked dimension.\n\n        Arguments\n        ---------\n        boundary : dict\n            dictionary of boundary offsets for each unstacked dimension. Non-segment dimensions can be omitted.\n\n        Returns\n        -------\n        area_bounds : dict\n            Dictionary of (low, high) coordinates area_bounds in each unstacked dimension\n        \"\"\"",
            "\"\"\"\n        Remove the given dimensions from the Coordinates `dims`.\n\n        Parameters\n        ----------\n        dims : str, list\n            Dimension(s) to drop.\n        ignore_missing : bool, optional\n            If True, do not raise an exception if a given dimension is not in ``dims``. Default ``False``.\n\n        Returns\n        -------\n        :class:`Coordinates`\n            Coordinates object with the given dimensions removed\n\n        Raises\n        ------\n        KeyError\n            If a given dimension is missing in the Coordinates (and ignore_missing is ``False``).\n\n        See Also\n        --------\n        udrop\n        \"\"\"",
            "\"\"\"\n        Remove the given individual dimensions from the Coordinates `udims`.\n\n        Unlike `drop`, ``udrop`` will remove parts of stacked coordinates::\n\n            In [1]: c = podpac.Coordinates([[[0, 1], [10, 20]], '2018-01-01'], dims=['lat_lon', 'time'])\n\n            In [2]: c\n            Out[2]:\n            Coordinates\n                lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[0.0, 1.0], N[2]\n                lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[10.0, 20.0], N[2]\n                time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-01], N[1]\n\n            In [3]: c.udrop('lat')\n            Out[3]:\n            Coordinates\n                lon: ArrayCoordinates1d(lon): Bounds[10.0, 20.0], N[2]\n                time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-01], N[1]\n\n        Parameters\n        ----------\n        dims : str, list\n            Individual dimension(s) to drop.\n        ignore_missing : bool, optional\n            If True, do not raise an exception if a given dimension is not in ``udims``. Default ``False``.\n\n        Returns\n        -------\n        :class:`Coordinates`\n            Coordinates object with the given dimensions removed.\n\n        Raises\n        ------\n        KeyError\n            If a given dimension is missing in the Coordinates (and ignore_missing is ``False``).\n\n        See Also\n        --------\n        drop\n        \"\"\"",
            "\"\"\"\n        Get the coordinate values that are within the bounds of a given coordinates object.\n\n        The intersection is calculated in each dimension separately.\n\n        The default intersection selects coordinates that are within the other coordinates bounds::\n\n            In [1]: coords = Coordinates([[0, 1, 2, 3]], dims=['lat'])\n\n            In [2]: other = Coordinates([[1.5, 2.5]], dims=['lat'])\n\n            In [3]: coords.intersect(other).coords\n            Out[3]:\n            Coordinates:\n              * lat      (lat) float64 2.0\n\n        The *outer* intersection selects the minimal set of coordinates that contain the other coordinates::\n\n            In [4]: coords.intersect(other, outer=True).coords\n            Out[4]:\n            Coordinates:\n              * lat      (lat) float64 1.0 2.0 3.0\n\n        The *outer* intersection also selects a boundary coordinate if the other coordinates are outside this\n        coordinates bounds but *inside* its area bounds::\n\n            In [5]: other_near = Coordinates([[3.25]], dims=['lat'])\n\n            In [6]: other_far = Coordinates([[10.0]], dims=['lat'])\n\n            In [7]: coords.intersect(other_near, outer=True).coords\n            Coordinates:\n              * lat      (lat) float64 3.0\n\n            In [8]: coords.intersect(other_far, outer=True).coords\n            Coordinates:\n              * lat      (lat) float64\n\n        Parameters\n        ----------\n        other : :class:`Coordinates1d`, :class:`StackedCoordinates`, :class:`Coordinates`\n            Coordinates to intersect with.\n        dims : list, optional\n            Restrict intersection to the given dimensions. Default is all available dimensions.\n        outer : bool, optional\n            If True, do an *outer* intersection. Default False.\n        return_index : bool, optional\n            If True, return index for the selection in addition to coordinates. Default False.\n\n        Returns\n        -------\n        intersection : :class:`Coordinates`\n            Coordinates object consisting of the intersection in each dimension.\n        selection_index : list\n            List of indices for each dimension that produces the intersection, only if ``return_index`` is True.\n        \"\"\"",
            "\"\"\"\n        Get the coordinate values that are within the given bounds for each dimension.\n\n        The default selection returns coordinates that are within the bounds::\n\n            In [1]: c = Coordinates([[0, 1, 2, 3], [10, 20, 30, 40]], dims=['lat', 'lon'])\n\n            In [2]: c.select({'lat': [1.5, 3.5]})\n            Out[2]:\n            Coordinates\n                    lat: ArrayCoordinates1d(lat): Bounds[2.0, 3.0], N[2]\n                    lon: ArrayCoordinates1d(lon): Bounds[10.0, 40.0], N[4]\n\n            In [3]: c.select({'lat': [1.5, 3.5], 'lon': [25, 45]})\n            Out[3]:\n            Coordinates\n                    lat: ArrayCoordinates1d(lat): Bounds[2.0, 3.0], N[2]\n                    lon: ArrayCoordinates1d(lon): Bounds[30.0, 40.0], N[2]\n\n        The *outer* selection returns the minimal set of coordinates that contain the bounds::\n\n            In [4]: c.select({'lat':[1.5, 3.5]}, outer=True)\n            Out[4]:\n            Coordinates\n                    lat: ArrayCoordinates1d(lat): Bounds[1.0, 3.0], N[3]\n                    lon: ArrayCoordinates1d(lon): Bounds[10.0, 40.0], N[4]\n\n        Parameters\n        ----------\n        bounds : dict\n            Selection bounds for the desired coordinates.\n        outer : bool, optional\n            If True, do *outer* selections. Default False.\n        return_index : bool, optional\n            If True, return index for the selections in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`Coordinates`\n            Coordinates object with coordinates within the given bounds.\n        selection_index : list\n            index for the selected coordinates in each dimension (only if return_index=True)\n        \"\"\"",
            "\"\"\"\n        Remove duplicate coordinate values from each dimension.\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n        Returns\n        -------\n        unique : :class:`podpac.Coordinates`\n            New Coordinates object with unique, sorted coordinate values in each dimension.\n        unique_index : list of indices\n            index for the unique coordinates in each dimension (only if return_index=True)\n        \"\"\"",
            "\"\"\"\n        Unstack the coordinates of all of the dimensions.\n\n        Returns\n        -------\n        unstacked : :class:`podpac.Coordinates`\n            A new Coordinates object with unstacked coordinates.\n\n        See Also\n        --------\n        xr.DataArray.unstack\n        \"\"\"",
            "\"\"\"\n        Get a generator that yields Coordinates no larger than the given shape until the entire Coordinates is covered.\n\n        Parameters\n        ----------\n        shape : tuple\n            The maximum shape of the chunk, with sizes corresponding to the `dims`.\n        return_slice : boolean, optional\n            Return slice in addition to Coordinates chunk.\n\n        Yields\n        ------\n        coords : :class:`Coordinates`\n            A Coordinates object with one chunk of the coordinates.\n        slices : list\n            slices for this Coordinates chunk, only if ``return_slices`` is True\n        \"\"\"",
            "\"\"\"\n        Transpose (re-order) the dimensions of the Coordinates.\n\n        Parameters\n        ----------\n        dim_1, dim_2, ... : str, optional\n            Reorder dims to this order. By default, reverse the dims.\n        in_place : boolean, optional\n            If True, transpose the dimensions in-place.\n            Otherwise (default), return a new, transposed Coordinates object.\n\n        Returns\n        -------\n        transposed : :class:`Coordinates`\n            The transposed Coordinates object.\n\n        See Also\n        --------\n        xarray.DataArray.transpose : return a transposed DataArray\n\n        \"\"\"",
            "\"\"\"\n        Transform coordinate dimensions (`lat`, `lon`, `alt`) into a different coordinate reference system.\n        Uses PROJ syntax for coordinate reference systems and units.\n\n        See `PROJ Documentation <https://proj.org/usage/projections.html#cartographic-projection>`_ for\n        more information about creating PROJ4 strings. See `PROJ4 Distance Units\n        <https://proj4.org/operations/conversions/unitconvert.html#distance-units>`_ for unit string\n        references.\n\n        Examples\n        --------\n        Transform gridded coordinates::\n\n            c = Coordinates([np.linspace(-10, 10, 21), np.linspace(-30, -10, 21)], dims=['lat', 'lon'])\n            c.crs\n\n            >> 'EPSG:4326'\n\n            c.transform('EPSG:2193')\n\n            >> Coordinates\n                lat: ArrayCoordinates1d(lat): Bounds[-9881992.849134896, 29995929.885877542], N[21]\n                lon: ArrayCoordinates1d(lon): Bounds[1928928.7360588573, 4187156.434405213], N[21]\n\n        Transform stacked coordinates::\n\n            c = Coordinates([(np.linspace(-10, 10, 21), np.linspace(-30, -10, 21))], dims=['lat_lon'])\n            c.transform('EPSG:2193')\n\n            >> Coordinates\n                lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[-9881992.849134896, 29995929.885877542], N[21]\n                lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[1928928.7360588573, 4187156.434405213], N[21]\n\n        Transform coordinates using a PROJ4 string::\n\n            c = Coordinates([np.linspace(-10, 10, 21), np.linspace(-30, -10, 21)], dims=['lat', 'lon'])\n            c.transform('+proj=merc +lat_ts=56.5 +ellps=GRS80')\n\n            >> Coordinates\n                lat: ArrayCoordinates1d(lat): Bounds[-1847545.541169525, -615848.513723175], N[21]\n                lon: ArrayCoordinates1d(lon): Bounds[-614897.0725896168, 614897.0725896184], N[21]\n\n        Parameters\n        ----------\n        crs : str\n            PROJ4 compatible coordinate reference system string.\n\n        Returns\n        -------\n        :class:`podpac.Coordinates`\n            Transformed Coordinates\n\n        Raises\n        ------\n        ValueError\n            Coordinates must have both lat and lon dimensions if either is defined\n        \"\"\"",
            "\"\"\"Simplify coordinates in each dimension.\n\n        Returns\n        -------\n        simplified : Coordinates\n            Simplified coordinates.\n        \"\"\"",
            "\"\"\"Report whether other Coordinates contains these coordinates.\n\n        Note that the dimension order and stacking is ignored.\n\n        Arguments\n        ---------\n        other : Coordinates\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates in every dimension.\n        \"\"\"",
            "\"\"\"\n        Returns horizontal resolution of coordinate system.\n\n        Parameters\n        ----------\n        units : str\n            The desired unit the returned resolution should be in. Supports any unit supported by podpac.units (i.e. pint). Default is 'meter'.\n        restype : str\n            The kind of horizontal resolution that should be returned. Supported values are:\n            - \"nominal\" <-- Returns a number. Gives a 'nominal' resolution over the entire domain. This is wrong but fast.\n            - \"summary\" <-- Returns a tuple (mean, standard deviation). Gives the exact mean and standard deviation for unstacked coordinates, some error for stacked coordinates\n            - \"full\" <-- Returns a 1 or 2-D array. Gives exact grid differences if unstacked coordinates or distance matrix if stacked coordinates\n\n        Returns\n        -------\n        OrderedDict\n            A dictionary with:\n            keys : str\n                dimension names\n            values\n                resolution (format determined by 'type' parameter)\n\n        Raises\n        ------\n        ValueError\n            If the 'restype' is not one of the supported resolution types\n\n\n        \"\"\"",
            "\"\"\"\n    Merge the coordinates.\n\n    Arguments\n    ---------\n    coords_list : list\n        List of :class:`Coordinates` with unique dimensions.\n\n    validate_crs : bool, optional\n        Default is True. If False, the coordinates will not be checked for a common crs,\n        and the crs of the first item in the list will be used.\n\n    Returns\n    -------\n    coords : :class:`Coordinates`\n        Coordinates with merged dimensions.\n\n    Raises\n    ------\n    ValueError\n        If dimensions are duplicated.\n    \"\"\"",
            "\"\"\"\n    Combine the given coordinates by concatenating coordinate values in each dimension.\n\n    Arguments\n    ---------\n    coords_list : list\n        List of :class:`Coordinates`.\n\n    Returns\n    -------\n    coords : :class:`Coordinates`\n        Coordinates with concatenated coordinate values in each dimension.\n\n    See Also\n    --------\n    :class:`union`\n    \"\"\"",
            "\"\"\"\n    Combine the given coordinates by collecting the unique, sorted coordinate values in each dimension.\n\n    Arguments\n    ---------\n    coords_list : list\n        List of :class:`Coordinates`.\n\n    Returns\n    -------\n    coords : :class:`Coordinates`\n        Coordinates with unique, sorted coordinate values in each dimension.\n\n    See Also\n    --------\n    :class:`concat`\n    \"\"\""
        ],
        "code_snippets": [
            "class Coordinates(tl.HasTraits):\n    \"\"\"\n    Multidimensional Coordinates.\n\n    Coordinates are used to evaluate Nodes and to define the coordinates of a DataSource nodes. The API is modeled after\n    coords in `xarray <http:",
            "def __init__(self, coords, dims=None, crs=None, validate_crs=True):\n        \"\"\"\n        Create multidimensional coordinates.\n\n        Arguments\n        ---------\n        coords : list\n            List of coordinate values for each dimension. Valid coordinate values:\n\n             * single coordinate value (number, datetime64, or str)\n             * array of coordinate values\n             * list of stacked coordinate values\n             * :class:`Coordinates1d` or :class:`StackedCoordinates` object\n        dims : list of str, optional\n            List of dimension names. Optional if all items in ``coords`` are named. Valid names are\n\n             * 'lat', 'lon', 'alt', or 'time' for unstacked coordinates\n             * dimension names joined by an underscore for stacked coordinates\n        crs : str, optional\n            Coordinate reference system. Supports PROJ4 and WKT.\n        validate_crs : bool, optional\n            Use False to skip crs validation. Default True.\n        \"\"\"\n\n        if not isinstance(coords, (list, tuple, np.ndarray, xr.DataArray)):\n            raise TypeError(\"Invalid coords, expected list or array, not '%s'\" % type(coords))\n\n        if dims is not None and not isinstance(dims, (tuple, list)):\n            raise TypeError(\"Invalid dims type '%s'\" % type(dims))\n\n        if dims is None:\n            for i, c in enumerate(coords):\n                if not isinstance(c, (BaseCoordinates, xr.DataArray)):\n                    raise TypeError(\n                        \"Cannot get dim for coordinates at position %d with type '%s'\"\n                        \"(expected 'Coordinates1d' or 'DataArray')\" % (i, type(c))\n                    )\n\n            dims = [c.name for c in coords]\n\n        if len(dims) != len(coords):\n            raise ValueError(\"coords and dims size mismatch, %d != %d\" % (len(dims), len(coords)))\n\n        # get/create coordinates\n        dcoords = OrderedDict()\n        for i, dim in enumerate(dims):\n            if isinstance(dim, (tuple, list)):\n                dim = \"_\".join(dim)\n\n            if dim in dcoords:\n                raise ValueError(\"Duplicate dimension '%s' at position %d\" % (dim, i))\n\n            # coerce\n            if isinstance(coords[i], BaseCoordinates):\n                c = coords[i]\n            elif \"_\" in dim:\n                c = StackedCoordinates(coords[i])\n            else:\n                c = ArrayCoordinates1d(coords[i])\n\n            # propagate name\n            c._set_name(dim)\n\n            # set coords\n            dcoords[dim] = c\n\n        self.set_trait(\"_coords\", dcoords)\n\n        if crs is not None:\n            # validate\n            if validate_crs:\n                # raises pyproj.CRSError if invalid\n                CRS = pyproj.CRS(crs)\n\n                # make sure CRS defines vertical units\n                if \"alt\" in self.udims and not has_alt_units(CRS):\n                    raise ValueError(\"Altitude dimension is defined, but CRS does not contain vertical unit\")\n\n            crs = self.set_trait(\"crs\", crs)\n\n        super(Coordinates, self).__init__()\n\n    @tl.validate(\"_coords\")",
            "def _validate_coords(self, d):\n        val = d[\"value\"]\n\n        if len(val) == 0:\n            return val\n\n        for dim, c in val.items():\n            if dim != c.name:\n                raise ValueError(\"Dimension mismatch, '%s' != '%s'\" % (dim, c.name))\n\n        dims = [dim for c in val.values() for dim in c.dims]\n        for dim in dims:\n            if dims.count(dim) != 1:\n                raise ValueError(\"Duplicate dimension '%s' in dims %s\" % (dim, tuple(val.keys())))\n\n        return val\n\n    @tl.default(\"crs\")",
            "def _default_crs(self):\n        return settings[\"DEFAULT_CRS\"]\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Alternate constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @staticmethod",
            "def _coords_from_dict(d, order=None):\n        if sys.version < \"3.6\":\n            if order is None and len(d) > 1:\n                raise TypeError(\"order required\")\n\n        if order is not None:\n            if set(order) != set(d):\n                raise ValueError(\"order %s does not match dims %s\" % (order, d))\n        else:\n            order = d.keys()\n\n        coords = []\n        for dim in order:\n            if isinstance(d[dim], Coordinates1d):\n                c = d[dim].copy(name=dim)\n            elif isinstance(d[dim], tuple):\n                c = UniformCoordinates1d.from_tuple(d[dim], name=dim)\n            else:\n                c = ArrayCoordinates1d(d[dim], name=dim)\n            coords.append(c)\n\n        return coords\n\n    @classmethod",
            "def grid(cls, dims=None, crs=None, **kwargs):\n        \"\"\"\n        Create a grid of coordinates.\n\n        Valid coordinate values:\n\n         * single coordinate value (number, datetime64, or str)\n         * array of coordinate values\n         * ``(start, stop, step)`` tuple for uniformly-spaced coordinates\n         * Coordinates1d object\n\n        This is equivalent to creating unstacked coordinates with a list of coordinate values::\n\n            podpac.Coordinates.grid(lat=[0, 1, 2], lon=[10, 20], dims=['lat', 'lon'])\n            podpac.Coordinates([[0, 1, 2], [10, 20]], dims=['lan', 'lon'])\n\n        Arguments\n        ---------\n        lat : optional\n            coordinates for the latitude dimension\n        lon : optional\n            coordinates for the longitude dimension\n        alt : optional\n            coordinates for the altitude dimension\n        time : optional\n            coordinates for the time dimension\n        dims : list of str, optional in Python>=3.6\n            List of dimension names, must match the provided keyword arguments. In Python 3.6 and above, the ``dims``\n            argument is optional, and the dims will match the order of the provided keyword arguments.\n        crs : str, optional\n            Coordinate reference system. Supports any PROJ4 or PROJ6 compliant string (https:",
            "def points(cls, crs=None, dims=None, **kwargs):\n        \"\"\"\n        Create a list of multidimensional coordinates.\n\n        Valid coordinate values:\n\n         * single coordinate value (number, datetime64, or str)\n         * array of coordinate values\n         * ``(start, stop, step)`` tuple for uniformly-spaced coordinates\n         * Coordinates1d object\n\n        Note that the coordinates for each dimension must be the same size.\n\n        This is equivalent to creating stacked coordinates with a list of coordinate values and a stacked dimension\n        name::\n\n            podpac.Coordinates.points(lat=[0, 1, 2], lon=[10, 20, 30], dims=['lat', 'lon'])\n            podpac.Coordinates([[[0, 1, 2], [10, 20, 30]]], dims=['lan_lon'])\n\n        Arguments\n        ---------\n        lat : optional\n            coordinates for the latitude dimension\n        lon : optional\n            coordinates for the longitude dimension\n        alt : optional\n            coordinates for the altitude dimension\n        time : optional\n            coordinates for the time dimension\n        dims : list of str, optional in Python>=3.6\n            List of dimension names, must match the provided keyword arguments. In Python 3.6 and above, the ``dims``\n            argument is optional, and the dims will match the order of the provided keyword arguments.\n        crs : str, optional\n            Coordinate reference system. Supports any PROJ4 or PROJ6 compliant string (https:",
            "def from_xarray(cls, x, crs=None, validate_crs=False):\n        \"\"\"\n        Create podpac Coordinates from xarray coords.\n\n        Arguments\n        ---------\n        x : DataArray, Dataset, DataArrayCoordinates, DatasetCoordinates\n            DataArray, Dataset, or xarray coordinates\n        crs : str, optional\n            Coordinate reference system. Supports any PROJ4 or PROJ6 compliant string (https:",
            "def from_json(cls, s):\n        \"\"\"\n        Create podpac Coordinates from a coordinates JSON definition.\n\n        Example JSON definition::\n\n            [\n                {\n                    \"name\": \"lat\",\n                    \"start\": 1,\n                    \"stop\": 10,\n                    \"step\": 0.5,\n                },\n                {\n                    \"name\": \"lon\",\n                    \"start\": 1,\n                    \"stop\": 2,\n                    \"size\": 100\n                },\n                {\n                    \"name\": \"time\",\n                    \"values\": [\n                        \"2018-01-01\",\n                        \"2018-01-03\",\n                        \"2018-01-10\"\n                    ]\n                }\n            ]\n\n        Arguments\n        ---------\n        s : str\n            coordinates JSON definition\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n\n        See Also\n        --------\n        json\n        \"\"\"\n\n        d = json.loads(s)\n        return cls.from_definition(d)\n\n    @classmethod",
            "def from_url(cls, url):\n        \"\"\"\n        Create podpac Coordinates from a WMS/WCS request.\n\n        Arguments\n        ---------\n        url : str, dict\n            The raw WMS/WCS request url, or a dictionary of query parameters\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n        \"\"\"\n        params = _get_query_params_from_url(url)\n        coords = OrderedDict()\n\n        # The ordering here is lat/lon or y/x for WMS 1.3.0\n        # The ordering here is lon/lat or x/y for WMS 1.1\n        # See https:",
            "def from_geotransform(cls, geotransform, shape, crs=None, validate_crs=True):",
            "def from_definition(cls, d):\n        \"\"\"\n        Create podpac Coordinates from a coordinates definition.\n\n        Arguments\n        ---------\n        d : list\n            coordinates definition\n\n        Returns\n        -------\n        :class:`Coordinates`\n            podpac Coordinates\n\n        See Also\n        --------\n        from_json, definition\n        \"\"\"\n\n        if not isinstance(d, dict):\n            raise TypeError(\"Could not parse coordinates definition, expected type 'dict', got '%s'\" % type(d))\n\n        if \"coords\" not in d:\n            raise ValueError(\"Could not parse coordinates definition, 'coords' required\")\n\n        if not isinstance(d[\"coords\"], list):\n            raise TypeError(\n                \"Could not parse coordinates definition, expected 'coords' of type 'list', got '%s'\"\n                % (type(d[\"coords\"]))\n            )\n\n        coords = []\n        for e in d[\"coords\"]:\n            if isinstance(e, list):\n                c = StackedCoordinates.from_definition(e)\n            elif \"start\" in e and \"stop\" in e and (\"step\" in e or \"size\" in e):\n                c = UniformCoordinates1d.from_definition(e)\n            elif \"name\" in e and \"values\" in e:\n                c = ArrayCoordinates1d.from_definition(e)\n            elif \"geotransform\" in e and \"shape\" in e:\n                c = AffineCoordinates.from_definition(e)\n            else:\n                raise ValueError(\"Could not parse coordinates definition item with keys %s\" % e.keys())\n\n            coords.append(c)\n\n        kwargs = {k: v for k, v in d.items() if k != \"coords\"}\n        return cls(coords, **kwargs)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard dict-like methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def keys(self):",
            "def values(self):",
            "def items(self):",
            "def __iter__(self):\n        return iter(self._coords)",
            "def get(self, dim, default=None):",
            "def __getitem__(self, index):\n        if isinstance(index, string_types):\n            dim = index\n            if dim in self._coords:\n                return self._coords[dim]\n\n            # extracts individual coords from stacked and dependent coordinates\n            for c in self._coords.values():\n                if isinstance(c, StackedCoordinates) and dim in c.dims:\n                    return c[dim]\n\n            raise KeyError(\"Dimension '%s' not found in Coordinates %s\" % (dim, self.dims))\n\n        else:\n            # extend index to a tuple of the correct length\n            if not isinstance(index, tuple):\n                index = (index,)\n            index = index + tuple(slice(None) for i in range(self.ndim - len(index)))\n\n            # bundle shaped coordinates indices\n            indices = []\n            i = 0\n            for c in self._coords.values():\n                if c.ndim == 1:\n                    indices.append(index[i])\n                else:\n                    indices.append(tuple(index[i : i + c.ndim]))\n                i += c.ndim\n\n            cs = [c[I] for c, I in zip(self._coords.values(), indices)]\n            return Coordinates(cs, validate_crs=False, **self.properties)",
            "def __setitem__(self, dim, c):\n\n        # coerce\n        if isinstance(c, BaseCoordinates):\n            pass\n        elif isinstance(c, Coordinates):\n            c = c[dim]\n        elif \"_\" in dim:\n            c = StackedCoordinates(c)\n        else:\n            c = ArrayCoordinates1d(c)\n\n        c._set_name(dim)\n\n        if dim in self.dims:\n            d = self._coords.copy()\n            d[dim] = c\n            self._coords = d\n\n        elif dim in self.udims:\n            stacked_dim = [sd for sd in self.dims if dim in sd][0]\n            self._coords[stacked_dim][dim] = c\n        else:\n            raise KeyError(\"Cannot set dimension '%s' in Coordinates %s\" % (dim, self.dims))",
            "def __delitem__(self, dim):\n        if not dim in self.dims:\n            raise KeyError(\"Cannot delete dimension '%s' in Coordinates %s\" % (dim, self.dims))\n\n        del self._coords[dim]",
            "def __len__(self):\n        return len(self._coords)",
            "def update(self, other):",
            "def __eq__(self, other):\n        if not isinstance(other, Coordinates):\n            return False\n\n        # shortcuts\n        if self.dims != other.dims:\n            return False\n\n        if self.shape != other.shape:\n            return False\n\n        # properties\n        # TODO check transform instead\n        if self.CRS != other.CRS:\n            return False\n\n        # full check of underlying coordinates\n        if self._coords != other._coords:\n            return False\n\n        return True\n\n    if sys.version < \"3\":",
            "def __ne__(self, other):\n            return not self.__eq__(other)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def dims(self):\n        \"\"\":tuple: Tuple of dimension names, potentially stacked.\n\n        See Also\n        --------\n        udims\n        \"\"\"\n\n        return tuple(c.name for c in self._coords.values())\n\n    @property",
            "def xdims(self):\n        \"\"\":tuple: Tuple of indexing dimension names used to make xarray DataArray.\n\n        Unless there are shaped (ndim>1) coordinates, this will match the ``dims``.\n        \"\"\"\n\n        return tuple(dim for c in self._coords.values() for dim in c.xdims)\n\n    @property",
            "def udims(self):\n        \"\"\":tuple: Tuple of unstacked dimension names.\n\n        If there are no stacked dimensions, then ``dims`` and ``udims`` will be the same::\n\n            In [1]: lat = [0, 1]\n\n            In [2]: lon = [10, 20]\n\n            In [3]: time = '2018-01-01'\n\n            In [4]: c = podpac.Coordinates([lat, lon, time], dims=['lat', 'lon', 'time'])\n\n            In [5]: c.dims\n            Out[5]: ('lat', 'lon', 'time')\n\n            In [6]: c.udims\n            Out[6]: ('lat', 'lon', 'time')\n\n\n        If there are stacked dimensions, then ``udims`` contains the individual dimension names::\n\n            In [7]: c = podpac.Coordinates([[lat, lon], time], dims=['lat_lon', 'time'])\n\n            In [8]: c.dims\n            Out[8]: ('lat_lon', 'time')\n\n            In [9]: c.udims\n            Out[9]: ('lat', 'lon', 'time')\n\n        See Also\n        --------\n        dims\n        \"\"\"\n\n        return tuple(dim for c in self._coords.values() for dim in c.udims)\n\n    @property",
            "def shape(self):",
            "def ushape(self):\n        return tuple(self[dim].size for dim in self.udims)\n\n    @property",
            "def ndim(self):",
            "def size(self):",
            "def bounds(self):",
            "def xcoords(self):\n        \"\"\"\n        :dict: xarray coords\n        \"\"\"\n\n        xcoords = OrderedDict()\n        for c in self._coords.values():\n            xcoords.update(c.xcoords)\n        return xcoords\n\n    @property",
            "def CRS(self):\n        return pyproj.CRS(self.crs)\n\n    @property",
            "def alt_units(self):\n        CRS = self.CRS\n\n        if not has_alt_units(CRS):\n            return None\n\n        # try to get vunits\n        d = CRS.to_dict()\n        if \"vunits\" in d:\n            return d[\"vunits\"]\n\n        # get from axis info (is this is ever useful)\n        for axis in self.CRS.axis_info:\n            if axis.direction == \"up\":\n                return axis.unit_name  # may need to be converted, e.g. \"centimetre\" > \"cm\"\n\n        raise RuntimeError(\"Could not get alt_units from crs '%s'\" % self.crs)\n\n    @property",
            "def properties(self):\n        \"\"\":dict: Dictionary of the coordinate properties.\"\"\"\n\n        d = OrderedDict()\n        d[\"crs\"] = self.crs\n        return d\n\n    @property",
            "def definition(self):\n        \"\"\":list: Serializable coordinates definition.\"\"\"\n\n        d = OrderedDict()\n        d[\"coords\"] = [c.definition for c in self._coords.values()]\n        d.update(self.properties)\n        return d\n\n    @property",
            "def full_definition(self):\n        \"\"\":list: Serializable coordinates definition, containing all properties. For internal use.\"\"\"\n\n        d = OrderedDict()\n        d[\"coords\"] = [c.full_definition for c in self._coords.values()]\n        # \"wkt\" is suggested as best format: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n        d[\"crs\"] = self.CRS.to_wkt()\n        return d\n\n    @property",
            "def json(self):\n        \"\"\":str: JSON-serialized coordinates definition.\n\n        The ``json`` can be used to create new Coordinates::\n\n            c = podapc.Coordinates(...)\n            c2 = podpac.Coordinates.from_json(c.definition)\n\n        The serialized definition is used to define coordinates in node definitions and to transport coordinates, e.g.\n        over HTTP and in AWS lambda functions. It also provides a consistent hashable value.\n\n        See Also\n        --------\n        from_json\n        \"\"\"\n\n        return json.dumps(self.definition, separators=(\",\", \":\"), cls=podpac.core.utils.JSONEncoder)\n\n    @cached_property",
            "def hash(self):\n        \"\"\":str: Coordinates hash value.\"\"\"\n        # We can't use self.json for the hash because the CRS is not standardized.\n        # As such, we json.dumps the full definition.\n        json_d = json.dumps(self.full_definition, separators=(\",\", \":\"), cls=podpac.core.utils.JSONEncoder)\n        return hash_alg(json_d.encode(\"utf-8\")).hexdigest()\n\n    @property",
            "def geotransform(self):\n        \"\"\":tuple: GDAL geotransform.\"\"\"\n        # Make sure we only have 1 time and alt dimension\n        if \"time\" in self.udims and self[\"time\"].size > 1:\n            raise TypeError(\n                'Only 2-D coordinates have a GDAL transform. This array has a \"time\" dimension of {} > 1'.format(\n                    self[\"time\"].size\n                )\n            )\n        if \"alt\" in self.udims and self[\"alt\"].size > 1:\n            raise TypeError(\n                'Only 2-D coordinates have a GDAL transform. This array has a \"alt\" dimension of {} > 1'.format(\n                    self[\"alt\"].size\n                )\n            )\n\n        # Do the uniform coordinates case\n        if (\n            \"lat\" in self.dims\n            and \"lon\" in self.dims\n            and self._coords[\"lat\"].is_uniform\n            and self._coords[\"lon\"].is_uniform\n        ):\n            if self.dims.index(\"lon\") < self.dims.index(\"lat\"):\n                first, second = \"lat\", \"lon\"\n            else:\n                first, second = \"lon\", \"lat\"  # This case will have the exact correct geotransform\n            transform = rasterio.transform.Affine.translation(\n                self[first].start - self[first].step / 2, self[second].start - self[second].step / 2\n            ) * rasterio.transform.Affine.scale(self[first].step, self[second].step)\n            transform = transform.to_gdal()\n        elif \"lat_lon\" in self.dims and isinstance(self._coords[\"lat_lon\"], AffineCoordinates):\n            transform = self._coords[\"lat_lon\"].geotransform\n        elif \"lon_lat\" in self.dims and isinstance(self._coords[\"lon_lat\"], AffineCoordinates):\n            transform = self._coords[\"lon_lat\"].geotransform\n        else:\n            raise TypeError(\n                \"Only 2-D coordinates that are uniform or rotated have a GDAL transform. These coordinates \"\n                \"{} do not.\".format(self)\n            )\n        if self.udims.index(\"lon\") < self.udims.index(\"lat\"):\n            # transform = (transform[3], transform[5], transform[4], transform[0], transform[2], transform[1])\n            transform = transform[3:] + transform[:3]\n\n        return transform\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def get_area_bounds(self, boundary):\n        \"\"\"Get coordinate area bounds, including segment information, for each unstacked dimension.\n\n        Arguments\n        ---------\n        boundary : dict\n            dictionary of boundary offsets for each unstacked dimension. Non-segment dimensions can be omitted.\n\n        Returns\n        -------\n        area_bounds : dict\n            Dictionary of (low, high) coordinates area_bounds in each unstacked dimension\n        \"\"\"\n\n        area_bounds = {}\n        for dim, c in self._coords.items():\n            if isinstance(c, StackedCoordinates):\n                area_bounds.update(c.get_area_bounds(boundary))\n            else:\n                area_bounds[dim] = c.get_area_bounds(boundary.get(dim))\n        return area_bounds",
            "def drop(self, dims, ignore_missing=False):\n        \"\"\"\n        Remove the given dimensions from the Coordinates `dims`.\n\n        Parameters\n        ----------\n        dims : str, list\n            Dimension(s) to drop.\n        ignore_missing : bool, optional\n            If True, do not raise an exception if a given dimension is not in ``dims``. Default ``False``.\n\n        Returns\n        -------\n        :class:`Coordinates`\n            Coordinates object with the given dimensions removed\n\n        Raises\n        ------\n        KeyError\n            If a given dimension is missing in the Coordinates (and ignore_missing is ``False``).\n\n        See Also\n        --------\n        udrop\n        \"\"\"\n\n        if not isinstance(dims, (tuple, list)):\n            dims = (dims,)\n\n        for dim in dims:\n            if not isinstance(dim, string_types):\n                raise TypeError(\"Invalid drop dimension type '%s'\" % type(dim))\n            if dim not in self.dims and not ignore_missing:\n                raise KeyError(\"Dimension '%s' not found in Coordinates with dims %s\" % (dim, self.dims))\n\n        return Coordinates(\n            [c for c in self._coords.values() if c.name not in dims], validate_crs=False, **self.properties\n        )",
            "def udrop(self, dims, ignore_missing=False):\n        \"\"\"\n        Remove the given individual dimensions from the Coordinates `udims`.\n\n        Unlike `drop`, ``udrop`` will remove parts of stacked coordinates::\n\n            In [1]: c = podpac.Coordinates([[[0, 1], [10, 20]], '2018-01-01'], dims=['lat_lon', 'time'])\n\n            In [2]: c\n            Out[2]:\n            Coordinates\n                lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[0.0, 1.0], N[2]\n                lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[10.0, 20.0], N[2]\n                time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-01], N[1]\n\n            In [3]: c.udrop('lat')\n            Out[3]:\n            Coordinates\n                lon: ArrayCoordinates1d(lon): Bounds[10.0, 20.0], N[2]\n                time: ArrayCoordinates1d(time): Bounds[2018-01-01, 2018-01-01], N[1]\n\n        Parameters\n        ----------\n        dims : str, list\n            Individual dimension(s) to drop.\n        ignore_missing : bool, optional\n            If True, do not raise an exception if a given dimension is not in ``udims``. Default ``False``.\n\n        Returns\n        -------\n        :class:`Coordinates`\n            Coordinates object with the given dimensions removed.\n\n        Raises\n        ------\n        KeyError\n            If a given dimension is missing in the Coordinates (and ignore_missing is ``False``).\n\n        See Also\n        --------\n        drop\n        \"\"\"\n\n        if not isinstance(dims, (tuple, list)):\n            dims = (dims,)\n\n        for dim in dims:\n            if not isinstance(dim, string_types):\n                raise TypeError(\"Invalid drop dimension type '%s'\" % type(dim))\n            if dim not in self.udims and not ignore_missing:\n                raise KeyError(\"Dimension '%s' not found in Coordinates with udims %s\" % (dim, self.udims))\n\n        cs = []\n        for c in self._coords.values():\n            if isinstance(c, Coordinates1d):\n                if c.name not in dims:\n                    cs.append(c)\n            elif isinstance(c, StackedCoordinates):\n                stacked = [s for s in c if s.name not in dims]\n                if len(stacked) == len(c):\n                    # preserves parameterized stacked coordinates such as AffineCoordinates\n                    cs.append(c)\n                elif len(stacked) > 1:\n                    cs.append(StackedCoordinates(stacked))\n                elif len(stacked) == 1:\n                    cs.append(stacked[0])\n\n        return Coordinates(cs, validate_crs=False, **self.properties)",
            "def intersect(self, other, dims=None, outer=False, return_index=False):\n        \"\"\"\n        Get the coordinate values that are within the bounds of a given coordinates object.\n\n        The intersection is calculated in each dimension separately.\n\n        The default intersection selects coordinates that are within the other coordinates bounds::\n\n            In [1]: coords = Coordinates([[0, 1, 2, 3]], dims=['lat'])\n\n            In [2]: other = Coordinates([[1.5, 2.5]], dims=['lat'])\n\n            In [3]: coords.intersect(other).coords\n            Out[3]:\n            Coordinates:\n              * lat      (lat) float64 2.0\n\n        The *outer* intersection selects the minimal set of coordinates that contain the other coordinates::\n\n            In [4]: coords.intersect(other, outer=True).coords\n            Out[4]:\n            Coordinates:\n              * lat      (lat) float64 1.0 2.0 3.0\n\n        The *outer* intersection also selects a boundary coordinate if the other coordinates are outside this\n        coordinates bounds but *inside* its area bounds::\n\n            In [5]: other_near = Coordinates([[3.25]], dims=['lat'])\n\n            In [6]: other_far = Coordinates([[10.0]], dims=['lat'])\n\n            In [7]: coords.intersect(other_near, outer=True).coords\n            Coordinates:\n              * lat      (lat) float64 3.0\n\n            In [8]: coords.intersect(other_far, outer=True).coords\n            Coordinates:\n              * lat      (lat) float64\n\n        Parameters\n        ----------\n        other : :class:`Coordinates1d`, :class:`StackedCoordinates`, :class:`Coordinates`\n            Coordinates to intersect with.\n        dims : list, optional\n            Restrict intersection to the given dimensions. Default is all available dimensions.\n        outer : bool, optional\n            If True, do an *outer* intersection. Default False.\n        return_index : bool, optional\n            If True, return index for the selection in addition to coordinates. Default False.\n\n        Returns\n        -------\n        intersection : :class:`Coordinates`\n            Coordinates object consisting of the intersection in each dimension.\n        selection_index : list\n            List of indices for each dimension that produces the intersection, only if ``return_index`` is True.\n        \"\"\"\n\n        if not isinstance(other, Coordinates):\n            raise TypeError(\"Coordinates cannot be intersected with type '%s'\" % type(other))\n\n        if other.crs.lower() != self.crs.lower():\n            other = other.transform(self.crs)\n\n        bounds = other.bounds\n        if dims is not None:\n            bounds = {dim: bounds[dim] for dim in dims}  # if dim in bounds}\n\n        return self.select(bounds, outer=outer, return_index=return_index)",
            "def select(self, bounds, return_index=False, outer=False):\n        \"\"\"\n        Get the coordinate values that are within the given bounds for each dimension.\n\n        The default selection returns coordinates that are within the bounds::\n\n            In [1]: c = Coordinates([[0, 1, 2, 3], [10, 20, 30, 40]], dims=['lat', 'lon'])\n\n            In [2]: c.select({'lat': [1.5, 3.5]})\n            Out[2]:\n            Coordinates\n                    lat: ArrayCoordinates1d(lat): Bounds[2.0, 3.0], N[2]\n                    lon: ArrayCoordinates1d(lon): Bounds[10.0, 40.0], N[4]\n\n            In [3]: c.select({'lat': [1.5, 3.5], 'lon': [25, 45]})\n            Out[3]:\n            Coordinates\n                    lat: ArrayCoordinates1d(lat): Bounds[2.0, 3.0], N[2]\n                    lon: ArrayCoordinates1d(lon): Bounds[30.0, 40.0], N[2]\n\n        The *outer* selection returns the minimal set of coordinates that contain the bounds::\n\n            In [4]: c.select({'lat':[1.5, 3.5]}, outer=True)\n            Out[4]:\n            Coordinates\n                    lat: ArrayCoordinates1d(lat): Bounds[1.0, 3.0], N[3]\n                    lon: ArrayCoordinates1d(lon): Bounds[10.0, 40.0], N[4]\n\n        Parameters\n        ----------\n        bounds : dict\n            Selection bounds for the desired coordinates.\n        outer : bool, optional\n            If True, do *outer* selections. Default False.\n        return_index : bool, optional\n            If True, return index for the selections in addition to coordinates. Default False.\n\n        Returns\n        -------\n        selection : :class:`Coordinates`\n            Coordinates object with coordinates within the given bounds.\n        selection_index : list\n            index for the selected coordinates in each dimension (only if return_index=True)\n        \"\"\"\n\n        selections = [c.select(bounds, outer=outer, return_index=return_index) for c in self._coords.values()]\n        return self._make_selected_coordinates(selections, return_index)",
            "def _make_selected_coordinates(self, selections, return_index):\n        if return_index:\n            coords = Coordinates([c for c, I in selections], validate_crs=False, **self.properties)\n            # unbundle shaped indices\n            I = [I if c.ndim > 1 else [I] for c, I in selections]\n            I = [e for l in I for e in l]\n            return coords, tuple(I)\n        else:\n            return Coordinates(selections, validate_crs=False, **self.properties)",
            "def unique(self, return_index=False):\n        \"\"\"\n        Remove duplicate coordinate values from each dimension.\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n        Returns\n        -------\n        unique : :class:`podpac.Coordinates`\n            New Coordinates object with unique, sorted coordinate values in each dimension.\n        unique_index : list of indices\n            index for the unique coordinates in each dimension (only if return_index=True)\n        \"\"\"\n\n        if self.ndim == 0:\n            if return_index:\n                return self[:], tuple()\n            else:\n                return self[:]\n\n        cs, I = zip(*[c.unique(return_index=True) for c in self.values()])\n        unique = Coordinates(cs, validate_crs=False, **self.properties)\n\n        if return_index:\n            return unique, I\n        else:\n            return unique",
            "def unstack(self):\n        \"\"\"\n        Unstack the coordinates of all of the dimensions.\n\n        Returns\n        -------\n        unstacked : :class:`podpac.Coordinates`\n            A new Coordinates object with unstacked coordinates.\n\n        See Also\n        --------\n        xr.DataArray.unstack\n        \"\"\"\n\n        return Coordinates([self[dim] for dim in self.udims], validate_crs=False, **self.properties)",
            "def iterchunks(self, shape, return_slices=False):\n        \"\"\"\n        Get a generator that yields Coordinates no larger than the given shape until the entire Coordinates is covered.\n\n        Parameters\n        ----------\n        shape : tuple\n            The maximum shape of the chunk, with sizes corresponding to the `dims`.\n        return_slice : boolean, optional\n            Return slice in addition to Coordinates chunk.\n\n        Yields\n        ------\n        coords : :class:`Coordinates`\n            A Coordinates object with one chunk of the coordinates.\n        slices : list\n            slices for this Coordinates chunk, only if ``return_slices`` is True\n        \"\"\"\n\n        l = [[slice(i, i + n) for i in range(0, m, n)] for m, n in zip(self.shape, shape)]\n        for slices in itertools.product(*l):\n            coords = Coordinates(\n                [self._coords[dim][slc] for dim, slc in zip(self.dims, slices)], validate_crs=False, **self.properties\n            )\n            if return_slices:\n                yield coords, slices\n            else:\n                yield coords",
            "def transpose(self, *dims, **kwargs):\n        \"\"\"\n        Transpose (re-order) the dimensions of the Coordinates.\n\n        Parameters\n        ----------\n        dim_1, dim_2, ... : str, optional\n            Reorder dims to this order. By default, reverse the dims.\n        in_place : boolean, optional\n            If True, transpose the dimensions in-place.\n            Otherwise (default), return a new, transposed Coordinates object.\n\n        Returns\n        -------\n        transposed : :class:`Coordinates`\n            The transposed Coordinates object.\n\n        See Also\n        --------\n        xarray.DataArray.transpose : return a transposed DataArray\n\n        \"\"\"\n\n        in_place = kwargs.get(\"in_place\", False)\n\n        if len(dims) == 0:\n            dims = list(self._coords.keys())[::-1]\n\n        if len(dims) != len(self.dims):\n            raise ValueError(\"Invalid transpose dimensions, input %s does not match dims %s\" % (dims, self.dims))\n\n        coords = []\n        for dim in dims:\n            if dim in self._coords:\n                coords.append(self._coords[dim])\n            elif \"_\" in dim and dim.split(\"_\")[0] in self.udims:\n                target_dims = dim.split(\"_\")\n                source_dim = [_dim for _dim in self.dims if target_dims[0] in _dim][0]\n                coords.append(self._coords[source_dim].transpose(*target_dims, in_place=in_place))\n            else:\n                raise ValueError(\"Invalid transpose dimensions, input %s does match any dims in %s\" % (dim, self.dims))\n\n        if in_place:\n            self._coords = OrderedDict(zip(dims, coords))\n            return self\n        else:\n            return Coordinates(coords, validate_crs=False, **self.properties)",
            "def transform_time(self, units):\n        if \"time\" not in self.dims:\n            raise ValueError(\"Time dimension is required to do a time transformation.\")\n\n        time_coords = self[\"time\"].coordinates\n        xr_time = xr.Dataset({\"time\": time_coords})\n        new_time = getattr(xr_time.time.dt, units).data\n\n        new_time_coord = ArrayCoordinates1d(coordinates=new_time, name=\"time\").simplify()\n        coords = (self).drop(\"time\")\n        # transpose will make a copy\n        coords = merge_dims([coords, Coordinates([new_time_coord], crs=self.crs)]).transpose(*self.dims, in_place=False)\n        return coords",
            "def transform(self, crs):\n        \"\"\"\n        Transform coordinate dimensions (`lat`, `lon`, `alt`) into a different coordinate reference system.\n        Uses PROJ syntax for coordinate reference systems and units.\n\n        See `PROJ Documentation <https://proj.org/usage/projections.html#cartographic-projection>`_ for\n        more information about creating PROJ4 strings. See `PROJ4 Distance Units\n        <https://proj4.org/operations/conversions/unitconvert.html#distance-units>`_ for unit string\n        references.\n\n        Examples\n        --------\n        Transform gridded coordinates::\n\n            c = Coordinates([np.linspace(-10, 10, 21), np.linspace(-30, -10, 21)], dims=['lat', 'lon'])\n            c.crs\n\n            >> 'EPSG:4326'\n\n            c.transform('EPSG:2193')\n\n            >> Coordinates\n                lat: ArrayCoordinates1d(lat): Bounds[-9881992.849134896, 29995929.885877542], N[21]\n                lon: ArrayCoordinates1d(lon): Bounds[1928928.7360588573, 4187156.434405213], N[21]\n\n        Transform stacked coordinates::\n\n            c = Coordinates([(np.linspace(-10, 10, 21), np.linspace(-30, -10, 21))], dims=['lat_lon'])\n            c.transform('EPSG:2193')\n\n            >> Coordinates\n                lat_lon[lat]: ArrayCoordinates1d(lat): Bounds[-9881992.849134896, 29995929.885877542], N[21]\n                lat_lon[lon]: ArrayCoordinates1d(lon): Bounds[1928928.7360588573, 4187156.434405213], N[21]\n\n        Transform coordinates using a PROJ4 string::\n\n            c = Coordinates([np.linspace(-10, 10, 21), np.linspace(-30, -10, 21)], dims=['lat', 'lon'])\n            c.transform('+proj=merc +lat_ts=56.5 +ellps=GRS80')\n\n            >> Coordinates\n                lat: ArrayCoordinates1d(lat): Bounds[-1847545.541169525, -615848.513723175], N[21]\n                lon: ArrayCoordinates1d(lon): Bounds[-614897.0725896168, 614897.0725896184], N[21]\n\n        Parameters\n        ----------\n        crs : str\n            PROJ4 compatible coordinate reference system string.\n\n        Returns\n        -------\n        :class:`podpac.Coordinates`\n            Transformed Coordinates\n\n        Raises\n        ------\n        ValueError\n            Coordinates must have both lat and lon dimensions if either is defined\n        \"\"\"\n        from_crs = self.CRS\n        to_crs = pyproj.CRS(crs)\n\n        # no transform needed\n        if from_crs == to_crs:\n            return deepcopy(self)\n\n        # make sure the CRS defines vertical units\n        if \"alt\" in self.udims and not has_alt_units(to_crs):\n            raise ValueError(\"Altitude dimension is defined, but CRS to transform does not contain vertical unit\")\n\n        if \"lat\" in self.udims and \"lon\" not in self.udims:\n            raise ValueError(\"Cannot transform lat coordinates without lon coordinates\")\n\n        if \"lon\" in self.udims and \"lat\" not in self.udims:\n            raise ValueError(\"Cannot transform lon coordinates without lat coordinates\")\n\n        if \"lat\" in self.dims and \"lon\" in self.dims and abs(self.dims.index(\"lat\") - self.dims.index(\"lon\")) != 1:\n            raise ValueError(\"Cannot transform coordinates with nonadjacent lat and lon, transpose first\")\n\n        transformer = pyproj.Transformer.from_proj(from_crs, to_crs, always_xy=True)\n\n        # Collect the individual coordinates\n        cs = [c for c in self.values()]\n\n        if \"lat\" in self.dims and \"lon\" in self.dims:\n            st = self._simplified_transform(transformer, cs)\n\n            if st:  # We could do the shortcut, and we have the result already\n                cs = st\n            # otherwise, replace lat and lon coordinates with a single stacked lat_lon:\n            else:  # Have to convert every coordinate\n                ilat = self.dims.index(\"lat\")\n                ilon = self.dims.index(\"lon\")\n                if ilat == ilon - 1:\n                    c1, c2 = self[\"lat\"], self[\"lon\"]\n                elif ilon == ilat - 1:\n                    c1, c2 = self[\"lon\"], self[\"lat\"]\n                else:\n                    raise RuntimeError(\"lat and lon dimensions should be adjacent\")\n\n                c = StackedCoordinates(\n                    np.meshgrid(c1.coordinates, c2.coordinates, indexing=\"ij\"), dims=[c1.name, c2.name]\n                )\n\n                # replace 'lat' and 'lon' entries with single 'lat_lon' entry\n                i = min(ilat, ilon)\n                cs.pop(i)\n                cs.pop(i)\n                cs.insert(i, c)\n\n        # transform remaining altitude or stacked spatial dimensions if needed\n        ts = []\n        for c in cs:\n            tc = c._transform(transformer)\n            if isinstance(tc, list):\n                ts.extend(tc)\n            else:\n                ts.append(tc)\n\n        return Coordinates(ts, crs=crs, validate_crs=False)",
            "def _simplified_transform(self, transformer, cs):\n        lat_sample = np.linspace(self[\"lat\"].bounds[0], self[\"lat\"].bounds[1], 5)\n        lon_sample = np.linspace(self[\"lon\"].bounds[0], self[\"lon\"].bounds[1], 5)\n        sample = StackedCoordinates(np.meshgrid(lat_sample, lon_sample, indexing=\"ij\"), dims=[\"lat\", \"lon\"])\n        # The sample tests if the crs transform is linear, or non-linear. The results are as follows:\n        #\n        # Start from \"uniform stacked\"\n        # 1. Returns \"uniform unstacked\"  <-- simple scaling between crs's\n        # 2. Returns \"array unstacked\" <-- Orthogonal coordinates still, but non-linear in this dim\n        # 3. Returns \"Stacked\" <-- not orthogonal from one crs to the other\n        #\n        t = sample._transform(transformer)\n\n        if isinstance(t, StackedCoordinates):  # Need to transform ALL the coordinates\n            return\n        # Then we can do a faster transform, either already done or just the diagonal\n        for i, j in zip([0, 1], [1, 0]):\n            if isinstance(t[i], UniformCoordinates1d) and isinstance(cs[i], UniformCoordinates1d):  # already done\n                start = t[i].start\n                stop = t[i].stop\n                if self[t[i].name].is_descending:\n                    start, stop = stop, start\n                cs[self.dims.index(t[i].name)] = clinspace(start, stop, self[t[i].name].size, name=t[i].name)\n                continue\n            # Transform all of the points for this dimension (either lat or lon) and record result\n            this = self[t[i].name]\n            that = self[t[j].name]\n            if this.size > 1 and that.size > 1:\n                other = clinspace(that.bounds[0], that.bounds[1], this.size)\n            elif this.size == that.size:\n                other = that.coordinates\n            else:\n                other = np.zeros(this.size) + that.coordinates.mean()\n            diagonal = StackedCoordinates([this.coordinates, other], dims=[this.name, that.name])\n            t_diagonal = diagonal._transform(transformer)\n            cs[self.dims.index(this.name)] = t_diagonal[this.name]\n        return cs",
            "def simplify(self):\n        \"\"\"Simplify coordinates in each dimension.\n\n        Returns\n        -------\n        simplified : Coordinates\n            Simplified coordinates.\n        \"\"\"\n\n        cs = []\n        for c in self._coords.values():\n            c2 = c.simplify()\n            if isinstance(c2, list):\n                cs += c2\n            else:\n                cs.append(c2)\n        return Coordinates(cs, **self.properties)",
            "def issubset(self, other):\n        \"\"\"Report whether other Coordinates contains these coordinates.\n\n        Note that the dimension order and stacking is ignored.\n\n        Arguments\n        ---------\n        other : Coordinates\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates in every dimension.\n        \"\"\"\n\n        if set(self.udims) != set(other.udims):\n            return False\n\n        return all(c.issubset(other) for c in self.values())",
            "def is_stacked(self, dim):  # re-wrote to be able to iterate through c.dims\n        value = (dim in self.dims) + (dim in self.udims)\n        if value == 0:\n            raise ValueError(\"Dimension {} is not in self.dims={}\".format(dim, self.dims))\n        elif value == 1:  # one true, one false\n            return True\n        elif value == 2:  # both true\n            return False",
            "def horizontal_resolution(self, units=\"meter\", restype=\"nominal\"):\n        \"\"\"\n        Returns horizontal resolution of coordinate system.\n\n        Parameters\n        ----------\n        units : str\n            The desired unit the returned resolution should be in. Supports any unit supported by podpac.units (i.e. pint). Default is 'meter'.\n        restype : str\n            The kind of horizontal resolution that should be returned. Supported values are:\n            - \"nominal\" <-- Returns a number. Gives a 'nominal' resolution over the entire domain. This is wrong but fast.\n            - \"summary\" <-- Returns a tuple (mean, standard deviation). Gives the exact mean and standard deviation for unstacked coordinates, some error for stacked coordinates\n            - \"full\" <-- Returns a 1 or 2-D array. Gives exact grid differences if unstacked coordinates or distance matrix if stacked coordinates\n\n        Returns\n        -------\n        OrderedDict\n            A dictionary with:\n            keys : str\n                dimension names\n            values\n                resolution (format determined by 'type' parameter)\n\n        Raises\n        ------\n        ValueError\n            If the 'restype' is not one of the supported resolution types\n\n\n        \"\"\"\n        # This function handles mainly edge case sanitation.\n        # It calls StackedCoordinates and Coordinates1d 'horizontal_resolution' methods to get the actual values.\n\n        if \"lat\" not in self.udims:  # require latitude\n            raise ValueError(\"Latitude required for horizontal resolution.\")\n\n        # ellipsoid tuple to pass to geodesic\n        ellipsoid_tuple = (\n            self.CRS.ellipsoid.semi_major_metre / 1000,\n            self.CRS.ellipsoid.semi_minor_metre / 1000,\n            1 / self.CRS.ellipsoid.inverse_flattening,\n        )\n\n        # main execution loop\n        resolutions = OrderedDict()  # To return\n        for name, dim in self.items():\n            if dim.is_stacked:\n                if \"lat\" in dim.dims and \"lon\" in dim.dims:\n                    resolutions[name] = dim.horizontal_resolution(\n                        None, ellipsoid_tuple, self.CRS.coordinate_system.name, restype, units\n                    )\n                elif \"lat\" in dim.dims:\n                    # Calling self['lat'] forces UniformCoordinates1d, even if stacked\n                    resolutions[\"lat\"] = self[\"lat\"].horizontal_resolution(\n                        self[\"lat\"], ellipsoid_tuple, self.CRS.coordinate_system.name, restype, units\n                    )\n                elif \"lon\" in dim.dims:\n                    # Calling self['lon'] forces UniformCoordinates1d, even if stacked\n                    resolutions[\"lon\"] = self[\"lon\"].dim.horizontal_resolution(\n                        self[\"lat\"], ellipsoid_tuple, self.CRS.coordinate_system.name, restype, units\n                    )\n            elif (\n                name == \"lat\" or name == \"lon\"\n            ):  # need to do this inside of loop in case of stacked [[alt,time]] but unstacked [lat, lon]\n                resolutions[name] = dim.horizontal_resolution(\n                    self[\"lat\"], ellipsoid_tuple, self.CRS.coordinate_system.name, restype, units\n                )\n\n        return resolutions\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Operators/Magic Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __repr__(self):\n        rep = str(self.__class__.__name__)\n        if self.crs:\n            rep += \" ({})\".format(self.crs)\n        for c in self._coords.values():\n            if isinstance(c, Coordinates1d):\n                rep += \"\\n\\t%s: %s\" % (c.name, c)\n            elif isinstance(c, AffineCoordinates):\n                rep += \"\\n\\t%s: %s\" % (c.name, c)\n            elif isinstance(c, StackedCoordinates):\n                for dim in c.dims:\n                    rep += \"\\n\\t%s[%s]: %s\" % (c.name, dim, c[dim])\n        return rep",
            "def merge_dims(coords_list, validate_crs=True):\n    \"\"\"\n    Merge the coordinates.\n\n    Arguments\n    ---------\n    coords_list : list\n        List of :class:`Coordinates` with unique dimensions.\n\n    validate_crs : bool, optional\n        Default is True. If False, the coordinates will not be checked for a common crs,\n        and the crs of the first item in the list will be used.\n\n    Returns\n    -------\n    coords : :class:`Coordinates`\n        Coordinates with merged dimensions.\n\n    Raises\n    ------\n    ValueError\n        If dimensions are duplicated.\n    \"\"\"\n\n    coords_list = list(coords_list)\n    for coords in coords_list:\n        if not isinstance(coords, Coordinates):\n            raise TypeError(\"Cannot merge '%s' with Coordinates\" % type(coords))\n\n    if len(coords_list) == 0:\n        return Coordinates([], crs=None)\n\n    # check crs\n    crs = coords_list[0].crs\n    if validate_crs and not all(coords.crs == crs for coords in coords_list):\n        raise ValueError(\"Cannot merge Coordinates, crs mismatch\")\n\n    # merge\n    coords = sum([list(coords.values()) for coords in coords_list], [])\n    return Coordinates(coords, crs=crs, validate_crs=False)",
            "def concat(coords_list):\n    \"\"\"\n    Combine the given coordinates by concatenating coordinate values in each dimension.\n\n    Arguments\n    ---------\n    coords_list : list\n        List of :class:`Coordinates`.\n\n    Returns\n    -------\n    coords : :class:`Coordinates`\n        Coordinates with concatenated coordinate values in each dimension.\n\n    See Also\n    --------\n    :class:`union`\n    \"\"\"\n\n    coords_list = list(coords_list)\n    for coords in coords_list:\n        if not isinstance(coords, Coordinates):\n            raise TypeError(\"Cannot concat '%s' with Coordinates\" % type(coords))\n\n    if not coords_list:\n        return Coordinates([], crs=None)\n\n    # check crs\n    crs = coords_list[0].crs\n    if not all(coords.crs == crs for coords in coords_list):\n        raise ValueError(\"Cannot concat Coordinates, crs mismatch\")\n\n    # concatenate\n    d = OrderedDict()\n    for coords in coords_list:\n        for dim, c in coords.items():\n            if isinstance(c, Coordinates1d):\n                if dim not in d:\n                    d[dim] = c.coordinates\n                else:\n                    d[dim] = np.concatenate([d[dim], c.coordinates])\n            elif isinstance(c, StackedCoordinates):\n                if dim not in d:\n                    d[dim] = [s.coordinates for s in c]\n                else:\n                    d[dim] = [np.concatenate([d[dim][i], s.coordinates]) for i, s in enumerate(c)]\n\n    return Coordinates(list(d.values()), dims=list(d.keys()), crs=crs, validate_crs=False)",
            "def union(coords_list):\n    \"\"\"\n    Combine the given coordinates by collecting the unique, sorted coordinate values in each dimension.\n\n    Arguments\n    ---------\n    coords_list : list\n        List of :class:`Coordinates`.\n\n    Returns\n    -------\n    coords : :class:`Coordinates`\n        Coordinates with unique, sorted coordinate values in each dimension.\n\n    See Also\n    --------\n    :class:`concat`\n    \"\"\"\n\n    return concat(coords_list).unique()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/polar_coordinates.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    Parameterized spatial coordinates defined by a center, radius coordinates, and theta coordinates.\n\n    Attributes\n    ----------\n    center\n    radius\n    theta\n    \"\"\""
        ],
        "code_snippets": [
            "class PolarCoordinates(StackedCoordinates):\n    \"\"\"\n    Parameterized spatial coordinates defined by a center, radius coordinates, and theta coordinates.\n\n    Attributes\n    ----------\n    center\n    radius\n    theta\n    \"\"\"\n\n    center = ArrayTrait(shape=(2,), dtype=float, read_only=True)\n    radius = tl.Instance(Coordinates1d, read_only=True)\n    theta = tl.Instance(Coordinates1d, read_only=True)\n    dims = tl.Tuple(tl.Unicode(), tl.Unicode(), read_only=True)",
            "def __init__(self, center, radius, theta=None, theta_size=None, dims=None):\n\n        # radius\n        if not isinstance(radius, Coordinates1d):\n            radius = ArrayCoordinates1d(radius)\n\n        # theta\n        if theta is not None and theta_size is not None:\n            raise TypeError(\"PolarCoordinates expected theta or theta_size, not both.\")\n        if theta is None and theta_size is None:\n            raise TypeError(\"PolarCoordinates requires theta or theta_size.\")\n\n        if theta_size is not None:\n            theta = UniformCoordinates1d(start=0, stop=2 * np.pi, size=theta_size + 1)[:-1]\n        elif not isinstance(theta, Coordinates1d):\n            theta = ArrayCoordinates1d(theta)\n\n        self.set_trait(\"center\", center)\n        self.set_trait(\"radius\", radius)\n        self.set_trait(\"theta\", theta)\n        if dims is not None:\n            self.set_trait(\"dims\", dims)\n\n    @tl.validate(\"dims\")",
            "def _validate_dims(self, d):\n        val = d[\"value\"]\n        for dim in val:\n            if dim not in [\"lat\", \"lon\"]:\n                raise ValueError(\"PolarCoordinates dims must be 'lat' or 'lon', not '%s'\" % dim)\n        if val[0] == val[1]:\n            raise ValueError(\"Duplicate dimension '%s'\" % val[0])\n        return val\n\n    @tl.validate(\"radius\")",
            "def _validate_radius(self, d):\n        val = d[\"value\"]\n        if np.any(val.coordinates <= 0):\n            raise ValueError(\"PolarCoordinates radius must all be positive\")\n        return val",
            "def _set_name(self, value):\n        self._set_dims(value.split(\"_\"))",
            "def _set_dims(self, dims):\n        self.set_trait(\"dims\", dims)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Alternate Constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @classmethod",
            "def from_definition(cls, d):\n        if \"center\" not in d:\n            raise ValueError('PolarCoordinates definition requires \"center\" property')\n        if \"radius\" not in d:\n            raise ValueError('PolarCoordinates definition requires \"radius\" property')\n        if \"theta\" not in d and \"theta_size\" not in d:\n            raise ValueError('PolarCoordinates definition requires \"theta\" or \"theta_size\" property')\n        if \"dims\" not in d:\n            raise ValueError('PolarCoordinates definition requires \"dims\" property')\n\n        # center\n        center = d[\"center\"]\n\n        # radius\n        if isinstance(d[\"radius\"], list):\n            radius = ArrayCoordinates1d(d[\"radius\"])\n        elif \"values\" in d[\"radius\"]:\n            radius = ArrayCoordinates1d.from_definition(d[\"radius\"])\n        elif \"start\" in d[\"radius\"] and \"stop\" in d[\"radius\"] and (\"step\" in d[\"radius\"] or \"size\" in d[\"radius\"]):\n            radius = UniformCoordinates1d.from_definition(d[\"radius\"])\n        else:\n            raise ValueError(\"Could not parse radius coordinates definition with keys %s\" % d.keys())\n\n        # theta\n        if \"theta\" not in d:\n            theta = None\n        elif isinstance(d[\"theta\"], list):\n            theta = ArrayCoordinates1d(d[\"theta\"])\n        elif \"values\" in d[\"theta\"]:\n            theta = ArrayCoordinates1d.from_definition(d[\"theta\"])\n        elif \"start\" in d[\"theta\"] and \"stop\" in d[\"theta\"] and (\"step\" in d[\"theta\"] or \"size\" in d[\"theta\"]):\n            theta = UniformCoordinates1d.from_definition(d[\"theta\"])\n        else:\n            raise ValueError(\"Could not parse theta coordinates definition with keys %s\" % d.keys())\n\n        kwargs = {k: v for k, v in d.items() if k not in [\"center\", \"radius\", \"theta\"]}\n        return PolarCoordinates(center, radius, theta, **kwargs)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __repr__(self):\n        return \"%s(%s): center%s, shape%s\" % (self.__class__.__name__, self.dims, self.center, self.shape)",
            "def __eq__(self, other):\n        if not isinstance(other, PolarCoordinates):\n            return False\n\n        if not np.allclose(self.center, other.center):\n            return False\n\n        if self.radius != other.radius:\n            return False\n\n        if self.theta != other.theta:\n            return False\n\n        return True",
            "def __getitem__(self, index):\n        if isinstance(index, slice):\n            index = index, slice(None)\n\n        if isinstance(index, tuple) and isinstance(index[0], slice) and isinstance(index[1], slice):\n            return PolarCoordinates(self.center, self.radius[index[0]], self.theta[index[1]], dims=self.dims)\n        else:\n            # convert to raw StackedCoordinates (which creates the _coords attribute that the indexing requires)\n            return StackedCoordinates(self.coordinates, dims=self.dims).__getitem__(index)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def _coords(self):\n        raise RuntimeError(\"PolarCoordinates do not have a _coords attribute.\")\n\n    @property",
            "def ndim(self):\n        return 2\n\n    @property",
            "def shape(self):\n        return self.radius.size, self.theta.size\n\n    @property",
            "def xdims(self):\n        return (\"r\", \"t\")\n\n    @property",
            "def coordinates(self):\n        r, theta = np.meshgrid(self.radius.coordinates, self.theta.coordinates)\n        lat = r * np.sin(theta) + self.center[0]\n        lon = r * np.cos(theta) + self.center[1]\n        return lat.T, lon.T\n\n    @property",
            "def definition(self):\n        d = OrderedDict()\n        d[\"dims\"] = self.dims\n        d[\"center\"] = self.center\n        d[\"radius\"] = self.radius.definition\n        d[\"theta\"] = self.theta.definition\n        return d\n\n    @property",
            "def full_definition(self):\n        return self.definition\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def copy(self):\n        return PolarCoordinates(self.center, self.radius, self.theta, dims=self.dims)\n\n    # TODO return PolarCoordinates when possible\n    #",
            "def select(self, other, outer=False):\n    #     raise NotImplementedError(\"TODO\")\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Debug\n    # ------------------------------------------------------------------------------------------------------------------\n\n    #",
            "def plot(self, marker='b.', center_marker='bx'):\n    #     from matplotlib import pyplot\n    #     super(PolarCoordinates, self).plot(marker=marker)\n    #     cx, cy = self.center\n    #     pyplot.plot(cx, cy, center_marker)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/base_coordinates.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Base class for single or stacked one-dimensional coordinates.\"\"\"",
            "\"\"\":str: Dimension name.\"\"\"",
            "\"\"\":tuple: Dimensions.\"\"\"",
            "\"\"\":tuple: Tuple of unstacked dimension names, for compatibility. This is the same as the dims.\"\"\"",
            "\"\"\":tuple: Tuple of indexing dimensions used to create xarray DataArray.\"\"\"",
            "\"\"\"coordinates array ndim.\"\"\"",
            "\"\"\"coordinates array size.\"\"\"",
            "\"\"\"coordinates array shape.\"\"\"",
            "\"\"\"Coordinate values.\"\"\"",
            "\"\"\"xarray coords\"\"\"",
            "\"\"\"Coordinates definition.\"\"\"",
            "\"\"\"Coordinates definition, containing all properties. For internal use.\"\"\"",
            "\"\"\"stacked or unstacked property\"\"\"",
            "\"\"\"Get Coordinates from a coordinates definition.\"\"\"",
            "\"\"\"Deep copy of the coordinates and their properties.\"\"\"",
            "\"\"\"Remove duplicate coordinate values.\"\"\"",
            "\"\"\"Get coordinate area bounds, including boundary information, for each unstacked dimension.\"\"\"",
            "\"\"\"Get coordinate values that are with the given bounds.\"\"\"",
            "\"\"\"Get the simplified/optimized representation of these coordinates.\"\"\"",
            "\"\"\"Get a copy of the coordinates with a flattened array.\"\"\"",
            "\"\"\"Get a copy of the coordinates with a reshaped array (wraps numpy.reshape).\"\"\"",
            "\"\"\"Report if these coordinates are a subset of other coordinates.\"\"\"",
            "\"\"\"Get horizontal resolution of coordiantes.\"\"\""
        ],
        "code_snippets": [
            "class BaseCoordinates(tl.HasTraits):",
            "def _set_name(self, value):\n        raise NotImplementedError\n\n    @property",
            "def name(self):",
            "def dims(self):",
            "def udims(self):",
            "def xdims(self):",
            "def ndim(self):",
            "def size(self):",
            "def shape(self):",
            "def coordinates(self):",
            "def xcoords(self):",
            "def definition(self):",
            "def full_definition(self):",
            "def is_stacked(self):",
            "def from_definition(cls, d):",
            "def copy(self):",
            "def unique(self, return_index=False):",
            "def get_area_bounds(self, boundary):\n        \"\"\"Get coordinate area bounds, including boundary information, for each unstacked dimension.\"\"\"\n        raise NotImplementedError",
            "def select(self, bounds, outer=False, return_index=False):\n        \"\"\"Get coordinate values that are with the given bounds.\"\"\"\n        raise NotImplementedError",
            "def simplify(self):\n        \"\"\"Get the simplified/optimized representation of these coordinates.\"\"\"\n        raise NotImplementedError",
            "def flatten(self):\n        \"\"\"Get a copy of the coordinates with a flattened array.\"\"\"\n        raise NotImplementedError",
            "def reshape(self, newshape):\n        \"\"\"Get a copy of the coordinates with a reshaped array (wraps numpy.reshape).\"\"\"\n        raise NotImplementedError",
            "def issubset(self, other):\n        \"\"\"Report if these coordinates are a subset of other coordinates.\"\"\"\n        raise NotImplementedError",
            "def horizontal_resolution(self, latitude, ellipsoid_tuple, coordinate_name, restype=\"nominal\", units=\"meter\"):\n        \"\"\"Get horizontal resolution of coordiantes.\"\"\"\n        raise NotImplementedError",
            "def __getitem__(self, index):\n        raise NotImplementedError",
            "def __repr__(self):\n        raise NotImplementedError",
            "def __eq__(self, other):\n        raise NotImplementedError\n\n    # python 2 compatibility\n    if sys.version < \"3\":",
            "def __ne__(self, other):\n            return not self.__eq__(other)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/uniform_coordinates1d.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    1-dimensional array of uniformly-spaced coordinates defined by a start, stop, and step.\n\n    UniformCoordinates1d efficiently stores a uniformly-spaced coordinate array; the full coordinates array is only\n    calculated when needed. For numerical coordinates, the start, stop, and step are converted to ``float``. For time\n    coordinates, the start and stop are converted to numpy ``datetime64``, and the step is converted to numpy\n    ``timedelta64``. For convenience, podpac automatically converts datetime strings such as ``'2018-01-01'`` to\n    ``datetime64`` and timedelta strings such as ``'1,D'`` to ``timedelta64``.\n\n    UniformCoordinates1d can also be created by specifying the size instead of the step.\n\n    Parameters\n    ----------\n    start : float or datetime64\n        Start coordinate.\n    stop : float or datetime64\n        Stop coordinate. Unless fix_stop_val == True at creation, this may not always be\n        exactly equal to what the user specified. Internally we ensure that stop = start + step * (size - 1)\n    step : float or timedelta64\n        Signed, non-zero step between coordinates. Note, the specified step my be changed internally to satisfy floating point consistency.\n        That is, the consistent step will ensure that step = (stop - start)  / (size - 1)\n    name : str\n        Dimension name, one of 'lat', 'lon', 'time', 'alt'.\n    coordinates : array, read-only\n        Full array of coordinate values.\n\n    See Also\n    --------\n    :class:`Coordinates1d`, :class:`ArrayCoordinates1d`, :class:`crange`, :class:`clinspace`\n    \"\"\"",
            "\"\"\"\n        Create uniformly-spaced 1d coordinates from a `start`, `stop`, and `step` or `size`.\n\n        Parameters\n        ----------\n        start : float or datetime64\n            Start coordinate.\n        stop : float or datetime64\n            Stop coordinate.\n        step : float or timedelta64\n            Signed, nonzero step between coordinates (either step or size required).\n        size : int\n            Number of coordinates (either step or size required).\n        name : str, optional\n            Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n        fix_stop_val : bool, optional\n            Default is False. If True, the constructor will modify the step to be consistent\n            instead of the stop value. Otherwise, the stop value *may* be modified to ensure that\n            stop = start + step * size\n\n        Notes\n        ------\n        When the user specifies fix_stop_val, then `stop` will always be exact as specified by the user.\n\n        For floating point coordinates, the specified `step` my be changed internally to satisfy floating point consistency.\n        That is, for consistency `step = (stop - start)  / (size - 1)`\n        \"\"\"",
            "\"\"\"\n        Create uniformly-spaced 1d Coordinates from a coordinates definition.\n\n        The definition must contain the coordinate start, stop, and step or size::\n\n            c = UniformCoordinates1d.from_definition({\n                \"start\": 1,\n                \"stop\": 10,\n                \"step\": 0.5\n            })\n\n            c = UniformCoordinates1d.from_definition({\n                \"start\": 1,\n                \"stop\": 10,\n                \"size\": 21\n            })\n\n        The definition may also contain any of the 1d Coordinates properties::\n\n            c = UniformCoordinates1d.from_definition({\n                \"start\": 1,\n                \"stop\": 10,\n                \"step\": 0.5,\n                \"name\": \"lat\"\n            })\n\n        Arguments\n        ---------\n        d : dict\n            uniform 1d coordinates definition\n\n        Returns\n        -------\n        :class:`UniformCoordinates1d`\n            uniformly-spaced 1d Coordinates\n\n        See Also\n        --------\n        definition\n        \"\"\"",
            "\"\"\":array, read-only: Coordinate values.\"\"\"",
            "\"\"\"Number of coordinates.\"\"\"",
            "\"\"\":type: Coordinates dtype.\n\n        ``float`` for numerical coordinates and numpy ``datetime64`` for datetime coordinates.\n        \"\"\"",
            "\"\"\"Low and high coordinate bounds.\"\"\"",
            "\"\"\"\n        Make a deep copy of the uniform 1d Coordinates.\n\n        Returns\n        -------\n        :class:`UniformCoordinates1d`\n            Copy of the coordinates.\n        \"\"\"",
            "\"\"\"\n        Return the coordinates (uniform coordinates are already unique).\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n\n        Returns\n        -------\n        unique : :class:`ArrayCoordinates1d`\n            New ArrayCoordinates1d object with unique, sorted coordinate values.\n        unique_index : list of indices\n            index\n        \"\"\"",
            "\"\"\"Get the simplified/optimized representation of these coordinates.\n\n        Returns\n        -------\n        simplified : UniformCoordinates1d\n            These coordinates (the coordinates are already simplified).\n        \"\"\"",
            "\"\"\"\n        Return a copy of the uniform coordinates, for consistency.\n\n        Returns\n        -------\n        :class:`UniformCoordinates1d`\n            Flattened coordinates.\n        \"\"\"",
            "\"\"\"Report whether other coordinates contains these coordinates.\n\n        Arguments\n        ---------\n        other : Coordinates, Coordinates1d\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates.\n\n        Notes\n        -----\n        This overrides the Coordinates1d.issubset method with optimizations for uniform coordinates.\n        \"\"\""
        ],
        "code_snippets": [
            "class UniformCoordinates1d(Coordinates1d):\n    \"\"\"\n    1-dimensional array of uniformly-spaced coordinates defined by a start, stop, and step.\n\n    UniformCoordinates1d efficiently stores a uniformly-spaced coordinate array; the full coordinates array is only\n    calculated when needed. For numerical coordinates, the start, stop, and step are converted to ``float``. For time\n    coordinates, the start and stop are converted to numpy ``datetime64``, and the step is converted to numpy\n    ``timedelta64``. For convenience, podpac automatically converts datetime strings such as ``'2018-01-01'`` to\n    ``datetime64`` and timedelta strings such as ``'1,D'`` to ``timedelta64``.\n\n    UniformCoordinates1d can also be created by specifying the size instead of the step.\n\n    Parameters\n    ----------\n    start : float or datetime64\n        Start coordinate.\n    stop : float or datetime64\n        Stop coordinate. Unless fix_stop_val == True at creation, this may not always be\n        exactly equal to what the user specified. Internally we ensure that stop = start + step * (size - 1)\n    step : float or timedelta64\n        Signed, non-zero step between coordinates. Note, the specified step my be changed internally to satisfy floating point consistency.\n        That is, the consistent step will ensure that step = (stop - start)  / (size - 1)\n    name : str\n        Dimension name, one of 'lat', 'lon', 'time', 'alt'.\n    coordinates : array, read-only\n        Full array of coordinate values.\n\n    See Also\n    --------\n    :class:`Coordinates1d`, :class:`ArrayCoordinates1d`, :class:`crange`, :class:`clinspace`\n    \"\"\"\n\n    start = tl.Union([tl.Float(), tl.Instance(np.datetime64)], read_only=True)\n    start.__doc__ = \":float, datetime64: Start coordinate.\"\n\n    stop = tl.Union([tl.Float(), tl.Instance(np.datetime64)], read_only=True)\n    stop.__doc__ = \":float, datetime64: Stop coordinate.\"\n\n    step = tl.Union([tl.Float(), tl.Instance(np.timedelta64)], read_only=True)\n    step.__doc__ = \":float, timedelta64: Signed, non-zero step between coordinates.\"",
            "def __init__(self, start, stop, step=None, size=None, name=None, fix_stop_val=False):\n        \"\"\"\n        Create uniformly-spaced 1d coordinates from a `start`, `stop`, and `step` or `size`.\n\n        Parameters\n        ----------\n        start : float or datetime64\n            Start coordinate.\n        stop : float or datetime64\n            Stop coordinate.\n        step : float or timedelta64\n            Signed, nonzero step between coordinates (either step or size required).\n        size : int\n            Number of coordinates (either step or size required).\n        name : str, optional\n            Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n        fix_stop_val : bool, optional\n            Default is False. If True, the constructor will modify the step to be consistent\n            instead of the stop value. Otherwise, the stop value *may* be modified to ensure that\n            stop = start + step * size\n\n        Notes\n        ------\n        When the user specifies fix_stop_val, then `stop` will always be exact as specified by the user.\n\n        For floating point coordinates, the specified `step` my be changed internally to satisfy floating point consistency.\n        That is, for consistency `step = (stop - start)  / (size - 1)`\n        \"\"\"\n\n        if step is not None and size is not None:\n            raise TypeError(\"only one of 'step' and 'size' is allowed\")\n        elif step is None and size is None:\n            raise TypeError(\"'step' or 'size' is required\")\n\n        # validate and set start, stop, and step\n        start = make_coord_value(start)\n        stop = make_coord_value(stop)\n\n        if step is not None:\n            step = make_coord_delta(step)\n        elif isinstance(size, (int, np.compat.long, np.integer)) and not isinstance(size, np.timedelta64):\n            step = divide_delta(stop - start, size - 1)\n        else:\n            raise TypeError(\"size must be an integer, not '%s'\" % type(size))\n\n        if isinstance(start, float) and isinstance(stop, float) and isinstance(step, float):\n            fstep = step\n        elif isinstance(start, np.datetime64) and isinstance(stop, np.datetime64) and isinstance(step, np.timedelta64):\n            fstep = step.astype(float)\n        else:\n            raise TypeError(\n                \"UniformCoordinates1d mismatching types (start '%s', stop '%s', step '%s').\"\n                % (type(start), type(stop), type(step))\n            )\n\n        if fstep == 0:\n            raise ValueError(\"Uniformcoordinates1d step cannot be zero\")\n\n        if fstep <= 0 and start < stop:\n            raise ValueError(\"UniformCoordinates1d step must be greater than zero if start < stop.\")\n\n        if fstep >= 0 and start > stop:\n            raise ValueError(\"UniformCoordinates1d step must be less than zero if start > stop.\")\n\n        self.set_trait(\"start\", start)\n        self.set_trait(\"stop\", stop)\n        self.set_trait(\"step\", step)\n\n        if not fix_stop_val:  # Need to make sure that 'stop' is consistent with self.coordinates[-1]\n            self.set_trait(\"stop\", add_coord(self.start, (self.size - 1) * self.step))\n\n        # Make sure step is floating-point error consistent in all cases\n        # This is only needed when the type is float\n        if fstep == step and self.size > 1:\n            step = divide_delta(self.stop - self.start, self.size - 1)\n            self.set_trait(\"step\", step)\n\n        # set common properties\n        super(UniformCoordinates1d, self).__init__(name=name)",
            "def __eq__(self, other):\n        if not self._eq_base(other):\n            return False\n\n        if isinstance(other, UniformCoordinates1d):\n            if self.dtype == float:\n                if not np.allclose([self.start, self.stop, self.step], [other.start, other.stop, other.step]):\n                    return False\n            elif self.start != other.start or self.stop != other.stop or self.step != other.step:\n                return False\n\n        if isinstance(other, ArrayCoordinates1d):\n            if self.dtype == float:\n                if not np.allclose(self.coordinates, other.coordinates):\n                    return False\n            else:\n                if not np.array_equal(self.coordinates, other.coordinates):\n                    return False\n\n        return True\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Alternate Constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @classmethod",
            "def from_tuple(cls, items, **kwargs):\n        if not isinstance(items, tuple) or len(items) != 3:\n            raise ValueError(\n                \"UniformCoordinates1d.from_tuple expects a tuple of (start, stop, step/size), got %s\" % (items,)\n            )\n        elif isinstance(items[2], int):\n            return cls(items[0], items[1], size=items[2], **kwargs)\n        else:\n            step = make_coord_delta(items[2])\n            return cls(items[0], items[1], step, **kwargs)\n\n    @classmethod",
            "def from_definition(cls, d):\n        \"\"\"\n        Create uniformly-spaced 1d Coordinates from a coordinates definition.\n\n        The definition must contain the coordinate start, stop, and step or size::\n\n            c = UniformCoordinates1d.from_definition({\n                \"start\": 1,\n                \"stop\": 10,\n                \"step\": 0.5\n            })\n\n            c = UniformCoordinates1d.from_definition({\n                \"start\": 1,\n                \"stop\": 10,\n                \"size\": 21\n            })\n\n        The definition may also contain any of the 1d Coordinates properties::\n\n            c = UniformCoordinates1d.from_definition({\n                \"start\": 1,\n                \"stop\": 10,\n                \"step\": 0.5,\n                \"name\": \"lat\"\n            })\n\n        Arguments\n        ---------\n        d : dict\n            uniform 1d coordinates definition\n\n        Returns\n        -------\n        :class:`UniformCoordinates1d`\n            uniformly-spaced 1d Coordinates\n\n        See Also\n        --------\n        definition\n        \"\"\"\n\n        if \"start\" not in d:\n            raise ValueError('UniformCoordinates1d definition requires \"start\" property')\n        if \"stop\" not in d:\n            raise ValueError('UniformCoordinates1d definition requires \"stop\" property')\n\n        start = d[\"start\"]\n        stop = d[\"stop\"]\n        kwargs = {k: v for k, v in d.items() if k not in [\"start\", \"stop\"]}\n        return cls(start, stop, **kwargs)\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # Standard methods, array-like\n    # -----------------------------------------------------------------------------------------------------------------",
            "def __getitem__(self, index):\n        # fallback for non-slices\n        if not isinstance(index, slice):\n            # The following 3 lines is copied from ArrayCoordinates1d.__getitem__\n            if self.ndim == 1 and np.ndim(index) > 1 and np.array(index).dtype == int:\n                index = np.array(index).flatten().tolist()\n            return ArrayCoordinates1d(self.coordinates[index], **self.properties)\n\n        # start, stop, step\n        if index.start is None:\n            start = self.start\n        elif index.start >= 0:\n            start = add_coord(self.start, self.step * min(index.start, self.size - 1))\n        else:\n            start = add_coord(self.start, self.step * max(0, self.size + index.start))\n\n        if index.stop is None:\n            stop = self.stop\n        elif index.stop >= 0:\n            stop = add_coord(self.start, self.step * (min(index.stop, self.size) - 1))\n        else:\n            stop = add_coord(self.start, self.step * max(0, self.size + index.stop - 1))\n\n        if index.step is None:\n            step = self.step\n        else:\n            step = index.step * self.step\n            if index.step < 0:\n                start, stop = stop, start\n\n        # empty slice\n        if ((start > stop) and np.array(step).astype(float) > 0) or (\n            (start < stop) and np.array(step).astype(float) < 0\n        ):\n            return ArrayCoordinates1d([], **self.properties)\n        return UniformCoordinates1d(start, stop, step, **self.properties)",
            "def __contains__(self, item):\n        # overrides the Coordinates1d.__contains__ method with optimizations for uniform coordinates.\n\n        try:\n            item = make_coord_value(item)\n        except:\n            return False\n\n        if type(item) != self.dtype:\n            return False\n\n        if item < self.bounds[0] or item > self.bounds[1]:\n            return False\n\n        if self.dtype == np.datetime64:\n            return timedelta_divisible(item - self.start, self.step)\n        else:\n            return (item - self.start) % self.step == 0\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @cached_property",
            "def coordinates(self):",
            "def ndim(self):\n        return 1\n\n    @property",
            "def shape(self):\n        return (self.size,)\n\n    @property",
            "def size(self):",
            "def dtype(self):\n        \"\"\":type: Coordinates dtype.\n\n        ``float`` for numerical coordinates and numpy ``datetime64`` for datetime coordinates.\n        \"\"\"\n\n        return type(self.start)\n\n    @property",
            "def is_monotonic(self):\n        return True\n\n    @property",
            "def is_descending(self):\n        if self.start == self.stop:\n            return None\n\n        return self.stop < self.start\n\n    @property",
            "def is_uniform(self):\n        return True\n\n    @property",
            "def bounds(self):",
            "def argbounds(self):\n        if self.is_descending:\n            return -1, 0\n        else:\n            return 0, -1",
            "def _get_definition(self, full=True):\n        d = OrderedDict()\n        d[\"start\"] = self.start\n        d[\"stop\"] = self.stop\n        d[\"step\"] = self.step\n        d.update(self._full_properties if full else self.properties)\n        return d\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def copy(self):\n        \"\"\"\n        Make a deep copy of the uniform 1d Coordinates.\n\n        Returns\n        -------\n        :class:`UniformCoordinates1d`\n            Copy of the coordinates.\n        \"\"\"\n\n        kwargs = self.properties\n        return UniformCoordinates1d(self.start, self.stop, self.step, **kwargs)",
            "def unique(self, return_index=False):\n        \"\"\"\n        Return the coordinates (uniform coordinates are already unique).\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n\n        Returns\n        -------\n        unique : :class:`ArrayCoordinates1d`\n            New ArrayCoordinates1d object with unique, sorted coordinate values.\n        unique_index : list of indices\n            index\n        \"\"\"\n\n        if return_index:\n            return self.copy(), np.arange(self.size).tolist()\n        else:\n            return self.copy()",
            "def simplify(self):\n        \"\"\"Get the simplified/optimized representation of these coordinates.\n\n        Returns\n        -------\n        simplified : UniformCoordinates1d\n            These coordinates (the coordinates are already simplified).\n        \"\"\"\n\n        return self.copy()",
            "def flatten(self):\n        \"\"\"\n        Return a copy of the uniform coordinates, for consistency.\n\n        Returns\n        -------\n        :class:`UniformCoordinates1d`\n            Flattened coordinates.\n        \"\"\"\n\n        return self.copy()",
            "def reshape(self, newshape):\n        return ArrayCoordinates1d(self.coordinates, **self.properties).reshape(newshape)",
            "def issubset(self, other):\n        \"\"\"Report whether other coordinates contains these coordinates.\n\n        Arguments\n        ---------\n        other : Coordinates, Coordinates1d\n            Other coordinates to check\n\n        Returns\n        -------\n        issubset : bool\n            True if these coordinates are a subset of the other coordinates.\n\n        Notes\n        -----\n        This overrides the Coordinates1d.issubset method with optimizations for uniform coordinates.\n        \"\"\"\n\n        from podpac.core.coordinates import Coordinates\n\n        if isinstance(other, Coordinates):\n            if self.name not in other.dims:\n                return False\n            other = other[self.name]\n\n        # use Coordinates1d implementation when the other coordinates are not uniform\n        if not other.is_uniform:\n            return super(UniformCoordinates1d, self).issubset(other)\n\n        # use Coordinates1d implementation when the steps cannot be compared (e.g. months and days)\n        try:\n            self.step / other.step\n        except TypeError:\n            return super(UniformCoordinates1d, self).issubset(other)\n\n        # short-cuts that don't require checking coordinates\n        if self.dtype != other.dtype:\n            return False\n\n        if self.bounds[0] < other.bounds[0] or self.bounds[1] > other.bounds[1]:\n            return False\n\n        # check start and step\n        if self.start not in other:\n            return False\n\n        if self.size == 1:\n            return True\n\n        if self.dtype == np.datetime64:\n            return timedelta_divisible(self.step, other.step)\n        else:\n            return self.step % other.step == 0",
            "def _select(self, bounds, return_index, outer):\n        # TODO is there an easier way to do this with the new outer flag?\n        my_bounds = self.bounds\n\n        # If the bounds are of instance datetime64, then the comparison should happen at the lowest precision\n        if self.dtype == np.datetime64:\n            my_bounds, bounds = lower_precision_time_bounds(my_bounds, bounds, outer)\n\n        lo = max(bounds[0], my_bounds[0])\n        hi = min(bounds[1], my_bounds[1])\n\n        fmin = (lo - my_bounds[0]) / np.abs(self.step)\n        fmax = (hi - my_bounds[0]) / np.abs(self.step)\n        imin = int(np.ceil(fmin))\n        imax = int(np.floor(fmax))\n\n        if outer:\n            if imin != fmin:\n                imin -= 1\n            if imax != fmax:\n                imax += 1\n\n        imax = np.clip(imax + 1, 0, self.size)\n        imin = np.clip(imin, 0, self.size)\n\n        # empty case\n        if imin >= imax:\n            return self._select_empty(return_index)\n\n        if self.is_descending:\n            imax, imin = self.size - imin, self.size - imax\n\n        I = slice(imin, imax)\n        if return_index:\n            return self[I], I\n        else:\n            return self[I]"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/array_coordinates1d.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nSingle-Dimensional Coordinates: Array\n\"\"\"",
            "\"\"\"\n    1-dimensional array of coordinates.\n\n    ArrayCoordinates1d is a basic array of 1d coordinates created from an array of coordinate values. Numerical\n    coordinates values are converted to ``float``, and time coordinate values are converted to numpy ``datetime64``.\n    For convenience, podpac automatically converts datetime strings such as ``'2018-01-01'`` to ``datetime64``. The\n    coordinate values must all be of the same type.\n\n    Parameters\n    ----------\n    name : str\n        Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n    coordinates : array, read-only\n        Full array of coordinate values.\n\n    See Also\n    --------\n    :class:`Coordinates1d`, :class:`UniformCoordinates1d`\n    \"\"\"",
            "\"\"\"\n        Create 1d coordinates from an array.\n\n        Arguments\n        ---------\n        coordinates : array-like\n            coordinate values.\n        name : str, optional\n            Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n        \"\"\"",
            "\"\"\"\n        Create 1d Coordinates from named xarray coordinates.\n\n        Arguments\n        ---------\n        x : xarray.DataArray\n            Nade DataArray of the coordinate values\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            1d coordinates\n        \"\"\"",
            "\"\"\"\n        Create 1d coordinates from a coordinates definition.\n\n        The definition must contain the coordinate values::\n\n            c = ArrayCoordinates1d.from_definition({\n                \"values\": [0, 1, 2, 3]\n            })\n\n        The definition may also contain any of the 1d Coordinates properties::\n\n            c = ArrayCoordinates1d.from_definition({\n                \"values\": [0, 1, 2, 3],\n                \"name\": \"lat\"\n            })\n\n        Arguments\n        ---------\n        d : dict\n            1d coordinates array definition\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            1d Coordinates\n\n        See Also\n        --------\n        definition\n        \"\"\"",
            "\"\"\"\n        Make a deep copy of the 1d Coordinates array.\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            Copy of the coordinates.\n        \"\"\"",
            "\"\"\"\n        Remove duplicate coordinate values from each dimension.\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n\n        Returns\n        -------\n        unique : :class:`ArrayCoordinates1d`\n            New ArrayCoordinates1d object with unique, sorted coordinate values.\n        unique_index : list of indices\n            index\n        \"\"\"",
            "\"\"\"Get the simplified/optimized representation of these coordinates.\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`, :class:`UniformCoordinates1d`\n            UniformCoordinates1d if the coordinates are uniform, otherwise ArrayCoordinates1d\n        \"\"\"",
            "\"\"\"\n        Get a copy of the coordinates with a flattened array (wraps numpy.flatten).\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            Flattened coordinates.\n        \"\"\"",
            "\"\"\"\n        Get a copy of the coordinates with a reshaped array (wraps numpy.reshape).\n\n        Arguments\n        ---------\n        newshape: int, tuple\n            The new shape.\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            Reshaped coordinates.\n        \"\"\"",
            "\"\"\"Number of coordinates.\"\"\"",
            "\"\"\":type: Coordinates dtype.\n\n        ``float`` for numerical coordinates and numpy ``datetime64`` for datetime coordinates.\n        \"\"\"",
            "\"\"\"Low and high coordinate bounds.\"\"\""
        ],
        "code_snippets": [
            "class ArrayCoordinates1d(Coordinates1d):\n    \"\"\"\n    1-dimensional array of coordinates.\n\n    ArrayCoordinates1d is a basic array of 1d coordinates created from an array of coordinate values. Numerical\n    coordinates values are converted to ``float``, and time coordinate values are converted to numpy ``datetime64``.\n    For convenience, podpac automatically converts datetime strings such as ``'2018-01-01'`` to ``datetime64``. The\n    coordinate values must all be of the same type.\n\n    Parameters\n    ----------\n    name : str\n        Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n    coordinates : array, read-only\n        Full array of coordinate values.\n\n    See Also\n    --------\n    :class:`Coordinates1d`, :class:`UniformCoordinates1d`\n    \"\"\"\n\n    coordinates = ArrayTrait(read_only=True)\n\n    _is_monotonic = None\n    _is_descending = None\n    _is_uniform = None\n    _step = None\n    _start = None\n    _stop = None",
            "def __init__(self, coordinates, name=None, **kwargs):\n        \"\"\"\n        Create 1d coordinates from an array.\n\n        Arguments\n        ---------\n        coordinates : array-like\n            coordinate values.\n        name : str, optional\n            Dimension name, one of 'lat', 'lon', 'time', or 'alt'.\n        \"\"\"\n\n        # validate and set coordinates\n        coordinates = make_coord_array(coordinates)\n        self.set_trait(\"coordinates\", coordinates)\n        self.not_a_trait = coordinates\n\n        # precalculate once\n        if self.coordinates.size == 0:\n            pass\n\n        elif self.coordinates.size == 1:\n            self._is_monotonic = True\n\n        elif self.coordinates.ndim > 1:\n            self._is_monotonic = None\n            self._is_descending = None\n            self._is_uniform = None\n\n        else:\n            deltas = self.deltas\n            if np.any(deltas <= 0):\n                self._is_monotonic = False\n                self._is_descending = False\n                self._is_uniform = False\n            else:\n                self._is_monotonic = True\n                self._is_descending = self.coordinates[1] < self.coordinates[0]\n                self._is_uniform = np.allclose(deltas, deltas[0])\n                if self._is_uniform:\n                    self._start = self.coordinates[0]\n                    self._stop = self.coordinates[-1]\n                    self._step = (self._stop - self._start) / (self.coordinates.size - 1)\n\n        # set common properties\n        super(ArrayCoordinates1d, self).__init__(name=name, **kwargs)",
            "def __eq__(self, other):\n        if not self._eq_base(other):\n            return False\n\n        if not np.array_equal(self.coordinates, other.coordinates):\n            return False\n\n        return True\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Alternate Constructors\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @classmethod",
            "def from_xarray(cls, x, **kwargs):\n        \"\"\"\n        Create 1d Coordinates from named xarray coordinates.\n\n        Arguments\n        ---------\n        x : xarray.DataArray\n            Nade DataArray of the coordinate values\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            1d coordinates\n        \"\"\"\n\n        return cls(x.data, name=x.name, **kwargs).simplify()\n\n    @classmethod",
            "def from_definition(cls, d):\n        \"\"\"\n        Create 1d coordinates from a coordinates definition.\n\n        The definition must contain the coordinate values::\n\n            c = ArrayCoordinates1d.from_definition({\n                \"values\": [0, 1, 2, 3]\n            })\n\n        The definition may also contain any of the 1d Coordinates properties::\n\n            c = ArrayCoordinates1d.from_definition({\n                \"values\": [0, 1, 2, 3],\n                \"name\": \"lat\"\n            })\n\n        Arguments\n        ---------\n        d : dict\n            1d coordinates array definition\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            1d Coordinates\n\n        See Also\n        --------\n        definition\n        \"\"\"\n\n        if \"values\" not in d:\n            raise ValueError('ArrayCoordinates1d definition requires \"values\" property')\n\n        coordinates = d[\"values\"]\n        kwargs = {k: v for k, v in d.items() if k != \"values\"}\n        return cls(coordinates, **kwargs)",
            "def copy(self):\n        \"\"\"\n        Make a deep copy of the 1d Coordinates array.\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            Copy of the coordinates.\n        \"\"\"\n\n        return ArrayCoordinates1d(self.coordinates, **self.properties)",
            "def unique(self, return_index=False):\n        \"\"\"\n        Remove duplicate coordinate values from each dimension.\n\n        Arguments\n        ---------\n        return_index : bool, optional\n            If True, return index for the unique coordinates in addition to the coordinates. Default False.\n\n        Returns\n        -------\n        unique : :class:`ArrayCoordinates1d`\n            New ArrayCoordinates1d object with unique, sorted coordinate values.\n        unique_index : list of indices\n            index\n        \"\"\"\n\n        # shortcut, monotonic coordinates are already unique\n        if self.is_monotonic:\n            if return_index:\n                return self.flatten(), np.arange(self.size).tolist()\n            else:\n                return self.flatten()\n\n        a, I = np.unique(self.coordinates, return_index=True)\n        if return_index:\n            return self.flatten()[I], I\n        else:\n            return self.flatten()[I]",
            "def simplify(self):\n        \"\"\"Get the simplified/optimized representation of these coordinates.\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`, :class:`UniformCoordinates1d`\n            UniformCoordinates1d if the coordinates are uniform, otherwise ArrayCoordinates1d\n        \"\"\"\n\n        from podpac.core.coordinates.uniform_coordinates1d import UniformCoordinates1d\n\n        if self.is_uniform:\n            return UniformCoordinates1d(self.start, self.stop, self.step, **self.properties)\n\n        return self",
            "def flatten(self):\n        \"\"\"\n        Get a copy of the coordinates with a flattened array (wraps numpy.flatten).\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            Flattened coordinates.\n        \"\"\"\n\n        if self.ndim == 1:\n            return self.copy()\n\n        return ArrayCoordinates1d(self.coordinates.flatten(), **self.properties)",
            "def reshape(self, newshape):\n        \"\"\"\n        Get a copy of the coordinates with a reshaped array (wraps numpy.reshape).\n\n        Arguments\n        ---------\n        newshape: int, tuple\n            The new shape.\n\n        Returns\n        -------\n        :class:`ArrayCoordinates1d`\n            Reshaped coordinates.\n        \"\"\"\n\n        return ArrayCoordinates1d(self.coordinates.reshape(newshape), **self.properties)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # standard methods, array-like\n    # ------------------------------------------------------------------------------------------------------------------",
            "def __getitem__(self, index):\n        # The following 3 lines are copied by UniformCoordinates1d.__getitem__\n        if self.ndim == 1 and np.ndim(index) > 1 and np.array(index).dtype == int:\n            index = np.array(index).flatten().tolist()\n        try:\n            return ArrayCoordinates1d(self.coordinates[index], **self.properties)\n        except IndexError as e:  # This happens when index is a list, but should be a tuple\n            if isinstance(index, list):\n                return ArrayCoordinates1d(self.coordinates[tuple(index)], **self.properties)\n            raise (e)\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Properties\n    # ------------------------------------------------------------------------------------------------------------------\n\n    @property",
            "def deltas(self):\n        return (self.coordinates[1:] - self.coordinates[:-1]).astype(float) * np.sign(\n            self.coordinates[1] - self.coordinates[0]\n        ).astype(float)\n\n    @property",
            "def ndim(self):\n        return self.coordinates.ndim\n\n    @property",
            "def size(self):",
            "def shape(self):\n        return self.coordinates.shape\n\n    @property",
            "def dtype(self):\n        \"\"\":type: Coordinates dtype.\n\n        ``float`` for numerical coordinates and numpy ``datetime64`` for datetime coordinates.\n        \"\"\"\n\n        if self.size == 0:\n            return None\n        elif self.coordinates.dtype == float:\n            return float\n        elif np.issubdtype(self.coordinates.dtype, np.datetime64):\n            return np.datetime64\n\n    @property",
            "def is_monotonic(self):\n        return self._is_monotonic\n\n    @property",
            "def is_descending(self):\n        return self._is_descending\n\n    @property",
            "def is_uniform(self):\n        return self._is_uniform\n\n    @property",
            "def start(self):\n        return self._start\n\n    @property",
            "def stop(self):\n        return self._stop\n\n    @property",
            "def step(self):\n        return self._step\n\n    @property",
            "def bounds(self):",
            "def argbounds(self):\n        if self.size == 0:\n            raise RuntimeError(\"Cannot get argbounds for empty coordinates\")\n\n        if not self.is_monotonic:\n            argbounds = np.argmin(self.coordinates), np.argmax(self.coordinates)\n            return np.unravel_index(argbounds[0], self.shape), np.unravel_index(argbounds[1], self.shape)\n        elif not self.is_descending:\n            return 0, -1\n        else:\n            return -1, 0",
            "def _get_definition(self, full=True):\n        d = OrderedDict()\n        d[\"values\"] = self.coordinates\n        d.update(self._full_properties if full else self.properties)\n        return d\n\n    # ------------------------------------------------------------------------------------------------------------------\n    # Methods\n    # ------------------------------------------------------------------------------------------------------------------",
            "def _select(self, bounds, return_index, outer):\n        if self.dtype == np.datetime64:\n            _, bounds = higher_precision_time_bounds(self.bounds, bounds, outer)\n\n        if not outer:\n            gt = self.coordinates >= bounds[0]\n            lt = self.coordinates <= bounds[1]\n            b = gt & lt\n\n        elif self.is_monotonic:\n            gt = np.where(self.coordinates >= bounds[0])[0]\n            lt = np.where(self.coordinates <= bounds[1])[0]\n            lo, hi = bounds[0], bounds[1]\n            if self.is_descending:\n                lt, gt = gt, lt\n                lo, hi = hi, lo\n            if self.coordinates[gt[0]] != lo:\n                gt[0] -= 1\n            if self.coordinates[lt[-1]] != hi:\n                lt[-1] += 1\n            start = max(0, gt[0])\n            stop = min(self.size - 1, lt[-1])\n            b = slice(start, stop + 1)\n\n        else:\n            try:\n                gt = self.coordinates >= max(self.coordinates[self.coordinates <= bounds[0]])\n            except ValueError as e:\n                if self.dtype == np.datetime64:\n                    gt = ~np.isnat(self.coordinates)\n                else:\n                    gt = self.coordinates >= -np.inf\n            try:\n                lt = self.coordinates <= min(self.coordinates[self.coordinates >= bounds[1]])\n            except ValueError as e:\n                if self.dtype == np.datetime64:\n                    lt = ~np.isnat(self.coordinates)\n                else:\n                    lt = self.coordinates <= np.inf\n\n            b = gt & lt\n\n        if return_index:\n            return self[b], b\n        else:\n            return self[b]"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_stacked_coordinates.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Test Horizontal Resolution of Stacked Coordinates. Edge cases are handled in Coordinates.py\"\"\""
        ],
        "code_snippets": [
            "class TestStackedCoordinatesCreation(object):",
            "def test_init_explicit(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert c.udims == (\"lat\", \"lon\", \"time\")\n        assert c.name == \"lat_lon_time\"\n        repr(c)\n\n        # un-named\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"])\n        c = StackedCoordinates([lat, lon, time])\n        assert c.dims == (None, None, None)\n        assert c.udims == (None, None, None)\n        assert c.name is None\n\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        c = StackedCoordinates([lat, lon, time])\n        assert c.dims == (\"lat\", None, None)\n        assert c.udims == (\"lat\", None, None)\n        assert c.name == \"lat_?_?\"\n\n        repr(c)",
            "def test_init_explicit_shaped(self):\n        lat = ArrayCoordinates1d([[0, 1, 2], [10, 11, 12]], name=\"lat\")\n        lon = ArrayCoordinates1d([[10, 20, 30], [11, 21, 31]], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.name == \"lat_lon\"\n        repr(c)",
            "def test_coercion_with_dims(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        assert c.dims == (\"lat\", \"lon\")\n        assert_equal(c[\"lat\"].coordinates, lat)\n        assert_equal(c[\"lon\"].coordinates, lon)",
            "def test_coercion_with_name(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        c = StackedCoordinates([lat, lon], name=\"lat_lon\")\n        assert c.dims == (\"lat\", \"lon\")\n        assert_equal(c[\"lat\"].coordinates, lat)\n        assert_equal(c[\"lon\"].coordinates, lon)",
            "def test_coercion_shaped_with_dims(self):\n        lat = [[0, 1, 2], [10, 11, 12]]\n        lon = [[10, 20, 30], [11, 21, 31]]\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        assert c.dims == (\"lat\", \"lon\")\n        assert_equal(c[\"lat\"].coordinates, lat)\n        assert_equal(c[\"lon\"].coordinates, lon)",
            "def test_coercion_shaped_with_name(self):\n        lat = [[0, 1, 2], [10, 11, 12]]\n        lon = [[10, 20, 30], [11, 21, 31]]\n        c = StackedCoordinates([lat, lon], name=\"lat_lon\")\n        assert c.dims == (\"lat\", \"lon\")\n        assert_equal(c[\"lat\"].coordinates, lat)\n        assert_equal(c[\"lon\"].coordinates, lon)",
            "def test_invalid_coords_type(self):\n        with pytest.raises(TypeError, match=\"Unrecognized coords type\"):\n            StackedCoordinates({})",
            "def test_invalid_init_dims_and_name(self):\n        with pytest.raises(TypeError):\n            StackedCoordinates([[0, 1, 2], [10, 20, 30]], dims=[\"lat\", \"lon\"], name=\"lat_lon\")",
            "def test_duplicate_dims(self):\n        with pytest.raises(ValueError, match=\"Duplicate dimension\"):\n            StackedCoordinates([[0, 1, 2], [10, 20, 30]], dims=[\"lat\", \"lat\"])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension\"):\n            StackedCoordinates([[0, 1, 2], [10, 20, 30]], name=\"lat_lat\")",
            "def test_invalid_coords(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([0, 1, 2, 3], name=\"lon\")\n        c = ArrayCoordinates1d([0, 1, 2])\n\n        with pytest.raises(ValueError, match=\"Stacked coords must have at least 2 coords\"):\n            StackedCoordinates([lat])\n\n        with pytest.raises(ValueError, match=\"Shape mismatch in stacked coords\"):\n            StackedCoordinates([lat, lon])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension\"):\n            StackedCoordinates([lat, lat])\n\n        # (but duplicate None name is okay)\n        StackedCoordinates([c, c])",
            "def test_invalid_coords_shaped(self):\n        # same size, different shape\n        lat = ArrayCoordinates1d(np.arange(12).reshape((3, 4)), name=\"lat\")\n        lon = ArrayCoordinates1d(np.arange(12).reshape((4, 3)), name=\"lon\")\n\n        with pytest.raises(ValueError, match=\"Shape mismatch in stacked coords\"):\n            StackedCoordinates([lat, lon])",
            "def test_from_xarray(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n        x = xr.DataArray(np.empty(c.shape), coords=c.xcoords, dims=c.xdims)\n\n        c2 = StackedCoordinates.from_xarray(x.coords)\n        assert c2.dims == (\"lat\", \"lon\", \"time\")\n        assert_equal(c2[\"lat\"].coordinates, lat.coordinates)\n        assert_equal(c2[\"lon\"].coordinates, lon.coordinates)\n        assert_equal(c2[\"time\"].coordinates, time.coordinates)",
            "def test_copy(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        c2 = c.copy()\n        assert c2 is not c\n        assert c2 == c",
            "class TestStackedCoordinatesEq(object):",
            "def test_eq_type(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n        assert c != [[0, 1, 2], [10, 20, 30]]",
            "def test_eq_size_shortcut(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        c1 = StackedCoordinates([lat, lon])\n        c2 = StackedCoordinates([lat[:2], lon[:2]])\n        assert c1 != c2",
            "def test_eq_dims_shortcut(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        c1 = StackedCoordinates([lat, lon])\n        c2 = StackedCoordinates([lon, lat])\n        assert c1 != c2",
            "def test_eq_coordinates(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        c1 = StackedCoordinates([lat, lon])\n        c2 = StackedCoordinates([lat, lon])\n        c3 = StackedCoordinates([lat[::-1], lon])\n        c4 = StackedCoordinates([lat, lon[::-1]])\n\n        assert c1 == c2\n        assert c1 != c3\n        assert c1 != c4",
            "def test_eq_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c1 = StackedCoordinates([lat, lon])\n        c2 = StackedCoordinates([lat, lon])\n        c3 = StackedCoordinates([lat[::-1], lon])\n        c4 = StackedCoordinates([lat, lon[::-1]])\n\n        assert c1 == c2\n        assert c1 != c3\n        assert c1 != c4",
            "class TestStackedCoordinatesSerialization(object):",
            "def test_definition(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = UniformCoordinates1d(\"2018-01-01\", \"2018-01-03\", \"1,D\", name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n        d = c.definition\n\n        assert isinstance(d, list)\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = StackedCoordinates.from_definition(d)\n        assert c2 == c",
            "def test_invalid_definition(self):\n        with pytest.raises(ValueError, match=\"Could not parse coordinates definition with keys\"):\n            StackedCoordinates.from_definition([{\"apple\": 10}, {}])",
            "def test_definition_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        d = c.definition\n        assert isinstance(d, list)\n        assert len(d) == 2\n\n        # serializable\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)\n\n        # from definition\n        c2 = StackedCoordinates.from_definition(d)\n        assert c2 == c",
            "def test_full_definition_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon])\n        d = c.full_definition\n        assert isinstance(d, list)\n        assert len(d) == 2\n\n        # serializable\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)",
            "class TestStackedCoordinatesProperties(object):",
            "def test_set_dims(self):\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"])\n        c = StackedCoordinates([lat, lon, time])\n        c._set_dims([\"lat\", \"lon\", \"time\"])\n\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert lat.name == \"lat\"\n        assert lon.name == \"lon\"\n        assert time.name == \"time\"\n\n        # some can already be set\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n        c._set_dims([\"lat\", \"lon\", \"time\"])\n\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert lat.name == \"lat\"\n        assert lon.name == \"lon\"\n        assert time.name == \"time\"\n\n        # but they have to match\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"])\n        c = StackedCoordinates([lat, lon, time])\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            c._set_dims([\"lon\", \"lat\", \"time\"])\n\n        # invalid dims\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"])\n        c = StackedCoordinates([lat, lon, time])\n        with pytest.raises(ValueError, match=\"Invalid dims\"):\n            c._set_dims([\"lat\", \"lon\"])",
            "def test_set_name(self):\n        # note: mostly tested by test_set_dims\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"])\n        c = StackedCoordinates([lat, lon, time])\n\n        c._set_name(\"lat_lon_time\")\n        assert c.name == \"lat_lon_time\"\n\n        # invalid\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"])\n        c = StackedCoordinates([lat, lon, time])\n        with pytest.raises(ValueError, match=\"Invalid name\"):\n            c._set_name(\"lat_lon\")",
            "def test_size(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3])\n        lon = ArrayCoordinates1d([10, 20, 30, 40])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"])\n        c = StackedCoordinates([lat, lon, time])\n\n        assert c.size == 4",
            "def test_shape(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3])\n        lon = ArrayCoordinates1d([10, 20, 30, 40])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"])\n        c = StackedCoordinates([lat, lon, time])\n\n        assert c.shape == (4,)",
            "def test_size_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon])\n        assert c.size == 12",
            "def test_shape_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon])\n        assert c.shape == (3, 4)",
            "def test_coordinates(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        assert_equal(c.coordinates, np.array([lat.coordinates, lon.coordinates, time.coordinates]).T)\n        assert c.coordinates.dtype == object\n\n        # single dtype\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n\n        assert_equal(c.coordinates, np.array([lat.coordinates, lon.coordinates]).T)\n        assert c.coordinates.dtype == float",
            "def test_coordinates_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon])\n        assert_equal(c.coordinates, np.array([lat.T, lon.T]).T)",
            "def test_xdims(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n        assert c.xdims == (\"lat_lon_time\",)",
            "def test_xdims_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        assert len(set(c.xdims)) == 2",
            "def test_xcoords(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        assert isinstance(c.xcoords, dict)\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n        assert x.dims == (\"lat_lon_time\",)\n        assert_equal(x.coords[\"lat\"], c[\"lat\"].coordinates)\n        assert_equal(x.coords[\"lon\"], c[\"lon\"].coordinates)\n        assert_equal(x.coords[\"time\"], c[\"time\"].coordinates)\n\n        # unnamed\n        lat = ArrayCoordinates1d([0, 1, 2, 3])\n        lon = ArrayCoordinates1d([10, 20, 30, 40])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"])\n        c = StackedCoordinates([lat, lon, time])\n        with pytest.raises(ValueError, match=\"Cannot get xcoords\"):\n            c.xcoords",
            "def test_xcoords_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        assert isinstance(c.xcoords, dict)\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n        assert_equal(x.coords[\"lat\"], c[\"lat\"].coordinates)\n        assert_equal(x.coords[\"lon\"], c[\"lon\"].coordinates)\n\n        c = StackedCoordinates([lat, lon])\n        with pytest.raises(ValueError, match=\"Cannot get xcoords\"):\n            c.xcoords",
            "def test_bounds(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        bounds = c.bounds\n        assert isinstance(bounds, dict)\n        assert set(bounds.keys()) == set(c.udims)\n        assert_equal(bounds[\"lat\"], c[\"lat\"].bounds)\n        assert_equal(bounds[\"lon\"], c[\"lon\"].bounds)\n\n        c = StackedCoordinates([lat, lon])\n        with pytest.raises(ValueError, match=\"Cannot get bounds\"):\n            c.bounds",
            "def test_bounds_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        bounds = c.bounds\n        assert isinstance(bounds, dict)\n        assert set(bounds.keys()) == set(c.udims)\n        assert_equal(bounds[\"lat\"], c[\"lat\"].bounds)\n        assert_equal(bounds[\"lon\"], c[\"lon\"].bounds)\n\n        c = StackedCoordinates([lat, lon])\n        with pytest.raises(ValueError, match=\"Cannot get bounds\"):\n            c.bounds",
            "class TestStackedCoordinatesIndexing(object):",
            "def test_get_dim(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        assert c[\"lat\"] is lat\n        assert c[\"lon\"] is lon\n        assert c[\"time\"] is time\n        with pytest.raises(KeyError, match=\"Dimension 'other' not found in dims\"):\n            c[\"other\"]",
            "def test_get_index(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        # integer index\n        I = 0\n        cI = c[I]\n        assert isinstance(cI, StackedCoordinates)\n        assert cI.size == 1\n        assert cI.dims == c.dims\n        assert_equal(cI[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n\n        # index array\n        I = [1, 2]\n        cI = c[I]\n        assert isinstance(cI, StackedCoordinates)\n        assert cI.size == 2\n        assert cI.dims == c.dims\n        assert_equal(cI[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n\n        # boolean array\n        I = [False, True, True, False]\n        cI = c[I]\n        assert isinstance(cI, StackedCoordinates)\n        assert cI.size == 2\n        assert cI.dims == c.dims\n        assert_equal(cI[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n\n        # slice\n        cI = c[1:3]\n        assert isinstance(cI, StackedCoordinates)\n        assert cI.size == 2\n        assert cI.dims == c.dims\n        assert_equal(cI[\"lat\"].coordinates, c[\"lat\"].coordinates[1:3])",
            "def test_get_index_shaped(self):\n        lat = np.linspace(0, 1, 60).reshape((5, 4, 3))\n        lon = np.linspace(1, 2, 60).reshape((5, 4, 3))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        I = [3, 1, 2]\n        J = slice(1, 3)\n        K = 1\n        B = lat > 0.5\n\n        # full\n        c2 = c[I, J, K]\n        assert isinstance(c2, StackedCoordinates)\n        assert c2.shape == (3, 2)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][I, J, K]\n        assert c2[\"lon\"] == c[\"lon\"][I, J, K]\n        assert_equal(c2[\"lat\"].coordinates, lat[I, J, K])\n        assert_equal(c2[\"lon\"].coordinates, lon[I, J, K])\n\n        # partial/implicit\n        c2 = c[I, J]\n        assert isinstance(c2, StackedCoordinates)\n        assert c2.shape == (3, 2, 3)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][I, J]\n        assert c2[\"lon\"] == c[\"lon\"][I, J]\n        assert_equal(c2[\"lat\"].coordinates, lat[I, J])\n        assert_equal(c2[\"lon\"].coordinates, lon[I, J])\n\n        # boolean\n        c2 = c[B]\n        assert isinstance(c2, StackedCoordinates)\n        assert c2.shape == (30,)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][B]\n        assert c2[\"lon\"] == c[\"lon\"][B]\n        assert_equal(c2[\"lat\"].coordinates, lat[B])\n        assert_equal(c2[\"lon\"].coordinates, lon[B])",
            "def test_iter(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3])\n        lon = ArrayCoordinates1d([10, 20, 30, 40])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"])\n        c = StackedCoordinates([lat, lon, time])\n\n        for item in c:\n            assert isinstance(item, Coordinates1d)",
            "def test_len(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3])\n        lon = ArrayCoordinates1d([10, 20, 30, 40])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"])\n        c = StackedCoordinates([lat, lon, time])\n\n        assert len(c) == 3",
            "def test_in(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3])\n        lon = ArrayCoordinates1d([10, 20, 30, 40])\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"])\n        c = StackedCoordinates([lat, lon, time])\n\n        assert (0, 10, \"2018-01-01\") in c\n        assert (1, 10, \"2018-01-01\") not in c\n        assert (\"2018-01-01\", 10, 0) not in c\n        assert (0,) not in c\n        assert \"test\" not in c",
            "def test_in_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        assert (lat[0, 0], lon[0, 0]) in c\n        assert (lat[0, 0], lon[0, 1]) not in c\n        assert (lon[0, 0], lat[0, 0]) not in c\n        assert lat[0, 0] not in c",
            "class TestStackedCoordinatesSelection(object):",
            "def test_select_single(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        # single dimension\n        s = c.select({\"lat\": [0.5, 2.5]})\n        assert s == c[1:3]\n\n        s, I = c.select({\"lat\": [0.5, 2.5]}, return_index=True)\n        assert s == c[I]\n        assert s == c[1:3]\n\n        # a different single dimension\n        s = c.select({\"lon\": [5, 25]})\n        assert s == c[0:2]\n\n        s, I = c.select({\"lon\": [5, 25]}, return_index=True)\n        assert s == c[I]\n        assert s == c[0:2]\n\n        # outer\n        s = c.select({\"lat\": [0.5, 2.5]}, outer=True)\n        assert s == c[0:4]\n\n        s, I = c.select({\"lat\": [0.5, 2.5]}, outer=True, return_index=True)\n        assert s == c[I]\n        assert s == c[0:4]\n\n        # no matching dimension\n        s = c.select({\"alt\": [0, 10]})\n        assert s == c\n\n        s, I = c.select({\"alt\": [0, 10]}, return_index=True)\n        assert s == c[I]\n        assert s == c",
            "def test_select_multiple(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 4, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40, 50, 60], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n\n        # this should be the AND of both intersections\n        slat = c.select({\"lat\": [0.5, 3.5]})\n        slon = c.select({\"lon\": [25, 55]})\n        s = c.select({\"lat\": [0.5, 3.5], \"lon\": [25, 55]})\n        assert slat == c[1:4]\n        assert slon == c[2:5]\n        assert s == c[2:4]\n\n        s, I = c.select({\"lat\": [0.5, 3.5], \"lon\": [25, 55]}, return_index=True)\n        assert s == c[2:4]\n        assert s == c[I]",
            "def test_select_single_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        # single dimension\n        bounds = {\"lat\": [0.25, 0.55]}\n        E0, E1 = [0, 1, 1, 1], [3, 0, 1, 2]  # expected\n\n        s = c.select(bounds)\n        assert isinstance(s, StackedCoordinates)\n        assert s == c[E0, E1]\n\n        s, I = c.select(bounds, return_index=True)\n        assert isinstance(s, StackedCoordinates)\n        assert s == c[I]\n        assert s == c[E0, E1]\n\n        # a different single dimension\n        bounds = {\"lon\": [12.5, 17.5]}\n        E0, E1 = [0, 1, 1, 1, 1, 2], [3, 0, 1, 2, 3, 0]\n\n        s = c.select(bounds)\n        assert isinstance(s, StackedCoordinates)\n        assert s == c[E0, E1]\n\n        s, I = c.select(bounds, return_index=True)\n        assert isinstance(s, StackedCoordinates)\n        assert s == c[I]\n        assert s == c[E0, E1]\n\n        # outer\n        bounds = {\"lat\": [0.25, 0.75]}\n        E0, E1 = [0, 0, 1, 1, 1, 1, 2, 2], [2, 3, 0, 1, 2, 3, 0, 1]\n\n        s = c.select(bounds, outer=True)\n        assert isinstance(s, StackedCoordinates)\n        assert s == c[E0, E1]\n\n        s, I = c.select(bounds, outer=True, return_index=True)\n        assert isinstance(s, StackedCoordinates)\n        assert s == c[I]\n        assert s == c[E0, E1]\n\n        # no matching dimension\n        bounds = {\"alt\": [0, 10]}\n        s = c.select(bounds)\n        assert s == c\n\n        s, I = c.select(bounds, return_index=True)\n        assert s == c[I]\n        assert s == c",
            "def test_select_multiple_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n\n        # this should be the AND of both intersections\n        bounds = {\"lat\": [0.25, 0.95], \"lon\": [10.5, 17.5]}\n        E0, E1 = [0, 1, 1, 1, 1, 2], [3, 0, 1, 2, 3, 0]\n        s = c.select(bounds)\n        assert s == c[E0, E1]\n\n        s, I = c.select(bounds, return_index=True)\n        assert s == c[I]\n        assert s == c[E0, E1]",
            "class TestStackedCoordinatesMethods(object):",
            "def test_transpose(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        t = c.transpose(\"lon\", \"lat\", \"time\")\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert t.dims == (\"lon\", \"lat\", \"time\")\n        assert t[\"lat\"] == lat\n        assert t[\"lon\"] == lon\n        assert t[\"time\"] == time\n\n        # default transpose\n        t = c.transpose()\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert t.dims == (\"time\", \"lon\", \"lat\")",
            "def test_transpose_invalid(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        with pytest.raises(ValueError, match=\"Invalid transpose dimensions\"):\n            c.transpose(\"lon\", \"lat\")",
            "def test_transpose_in_place(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = StackedCoordinates([lat, lon, time])\n\n        t = c.transpose(\"lon\", \"lat\", \"time\", in_place=False)\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert t.dims == (\"lon\", \"lat\", \"time\")\n\n        c.transpose(\"lon\", \"lat\", \"time\", in_place=True)\n        assert c.dims == (\"lon\", \"lat\", \"time\")\n        assert t[\"lat\"] == lat\n        assert t[\"lon\"] == lon\n        assert t[\"time\"] == time",
            "def test_unique(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 1, 0, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 20, 20, 10, 60], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n\n        c2 = c.unique()\n        assert_equal(c2[\"lat\"].coordinates, [0, 1, 2, 5])\n        assert_equal(c2[\"lon\"].coordinates, [10, 20, 20, 60])\n\n        c2, I = c.unique(return_index=True)\n        assert_equal(c2[\"lat\"].coordinates, [0, 1, 2, 5])\n        assert_equal(c2[\"lon\"].coordinates, [10, 20, 20, 60])\n        assert c[I] == c2",
            "def test_unque_shaped(self):\n        lat = ArrayCoordinates1d([[0, 1, 2], [1, 0, 5]], name=\"lat\")\n        lon = ArrayCoordinates1d([[10, 20, 20], [20, 10, 60]], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n\n        # flattens\n        c2 = c.unique()\n        assert_equal(c2[\"lat\"].coordinates, [0, 1, 2, 5])\n        assert_equal(c2[\"lon\"].coordinates, [10, 20, 20, 60])\n\n        c2, I = c.unique(return_index=True)\n        assert_equal(c2[\"lat\"].coordinates, [0, 1, 2, 5])\n        assert_equal(c2[\"lon\"].coordinates, [10, 20, 20, 60])\n        assert c.flatten()[I] == c2",
            "def test_get_area_bounds(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        c = StackedCoordinates([lat, lon])\n        d = c.get_area_bounds({\"lat\": 0.5, \"lon\": 1})\n        # this is just a pass through\n        assert d[\"lat\"] == lat.get_area_bounds(0.5)\n        assert d[\"lon\"] == lon.get_area_bounds(1)\n\n        # has to be named\n        lat = ArrayCoordinates1d([0, 1, 2])\n        lon = ArrayCoordinates1d([10, 20, 30])\n        c = StackedCoordinates([lat, lon])\n        with pytest.raises(ValueError, match=\"Cannot get area_bounds\"):\n            c.get_area_bounds({\"lat\": 0.5, \"lon\": 1})",
            "def test_issubset(self):\n        lat = np.arange(4)\n        lon = 10 * np.arange(4)\n        time = 100 * np.arange(4)\n\n        sc = StackedCoordinates([lat, lon], name=\"lat_lon\")\n        sc_2 = StackedCoordinates([lat + 100, lon], name=\"lat_lon\")  # different coordinates\n        sc_3 = StackedCoordinates([lat[::-1], lon], name=\"lat_lon\")  # same coordinates, paired differently\n        sc_t = sc.transpose(\"lon\", \"lat\")\n        sc_time = StackedCoordinates([lat, lon, time], name=\"lat_lon_time\")\n\n        assert sc.issubset(sc)\n        assert sc[:2].issubset(sc)\n        assert not sc.issubset(sc[:2])\n        assert not sc_2.issubset(sc)\n        assert not sc_3.issubset(sc)\n\n        assert sc_t.issubset(sc)\n\n        # extra/missing dimension\n        assert not sc.issubset(sc_time)\n        assert not sc_time.issubset(sc)",
            "def test_issubset_coordinates(self):\n        lat = np.arange(4)\n        lon = 10 * np.arange(4)\n        time = 100 * np.arange(4)\n\n        sc = StackedCoordinates([lat, lon], name=\"lat_lon\")\n        sc_2 = StackedCoordinates([lat + 100, lon], name=\"lat_lon\")\n        sc_3 = StackedCoordinates([lat[::-1], lon], name=\"lat_lon\")\n        sc_t = sc.transpose(\"lon\", \"lat\")\n        sc_time = StackedCoordinates([lat, lon, time], name=\"lat_lon_time\")\n\n        # coordinates with stacked lat_lon\n        cs = podpac.Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        assert sc.issubset(cs)\n        assert sc[:2].issubset(cs)\n        assert sc[::-1].issubset(cs)\n        assert not sc_2.issubset(cs)\n        assert not sc_3.issubset(cs)\n        assert sc_t.issubset(cs)\n        assert not sc_time.issubset(cs)\n\n        # coordinates with shaped stacked lat_lon\n        cd = podpac.Coordinates([[lat.reshape((2, 2)), lon.reshape((2, 2))]], dims=[\"lat_lon\"])\n        assert sc.issubset(cd)\n        assert sc[:2].issubset(cd)\n        assert sc[::-1].issubset(cd)\n        assert not sc_2.issubset(cd)\n        assert not sc_3.issubset(cd)\n        assert sc_t.issubset(cd)\n        assert not sc_time.issubset(cd)\n\n        # coordinates with unstacked lat, lon\n        cu = podpac.Coordinates([lat, lon[::-1]], dims=[\"lat\", \"lon\"])\n        assert sc.issubset(cu)\n        assert sc[:2].issubset(cu)\n        assert sc[::-1].issubset(cu)\n        assert not sc_2.issubset(cu)\n        assert sc_3.issubset(cu)  # this is an important case!\n        assert sc_t.issubset(cu)\n        assert not sc_time.issubset(cu)\n\n        # coordinates with unstacked lat, lon, time\n        cu_time = podpac.Coordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n        assert sc.issubset(cu_time)\n        assert sc[:2].issubset(cu_time)\n        assert sc[::-1].issubset(cu_time)\n        assert not sc_2.issubset(cu_time)\n        assert sc_3.issubset(cu_time)\n        assert sc_t.issubset(cu_time)\n        assert sc_time.issubset(cu_time)\n\n        assert not sc.issubset(cu_time[:2, :, :])\n\n        # mixed coordinates\n        cmixed = podpac.Coordinates([[lat, lon], time], dims=[\"lat_lon\", \"time\"])\n        assert sc.issubset(cmixed)\n        assert sc[:2].issubset(cmixed)\n        assert sc[::-1].issubset(cmixed)\n        assert not sc_2.issubset(cmixed)\n        assert not sc_3.issubset(cmixed)\n        assert sc_t.issubset(cmixed)\n        assert sc_time.issubset(cmixed)  # this is the most general case\n\n        assert not sc.issubset(cmixed[:2, :])\n        assert not sc_time.issubset(cmixed[:, :1])",
            "def test_issubset_other(self):\n        sc = StackedCoordinates([[1, 2, 3], [10, 20, 30]], name=\"lat_lon\")\n\n        with pytest.raises(TypeError, match=\"StackedCoordinates issubset expected Coordinates or StackedCoordinates\"):\n            sc.issubset([])",
            "def test_issubset_shaped(self):\n        lat = np.arange(12).reshape(3, 4)\n        lon = 10 * np.arange(12).reshape(3, 4)\n        time = 100 * np.arange(12).reshape(3, 4)\n\n        dc = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        dc_2 = StackedCoordinates([lat + 100, lon], dims=[\"lat\", \"lon\"])  # different coordinates\n        dc_3 = StackedCoordinates([lat[::-1], lon], dims=[\"lat\", \"lon\"])  # same coordinates, but paired differently\n        dc_t = dc.transpose(\"lon\", \"lat\")\n        dc_shape = StackedCoordinates([lat.reshape(6, 2), lon.reshape(6, 2)], dims=[\"lat\", \"lon\"])\n        dc_time = StackedCoordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n\n        assert dc.issubset(dc)\n        assert dc[:2, :2].issubset(dc)\n        assert not dc.issubset(dc[:2, :2])\n        assert not dc_2.issubset(dc)\n        assert not dc_3.issubset(dc)\n\n        assert dc_t.issubset(dc)\n        assert dc_shape.issubset(dc)\n\n        # extra/missing dimension\n        assert not dc_time.issubset(dc)\n        assert not dc.issubset(dc_time)",
            "def test_issubset_coordinates_shaped(self):\n        ulat = np.arange(12)\n        ulon = 10 * np.arange(12)\n        utime = 100 * np.arange(12)\n\n        lat = ulat.reshape(3, 4)\n        lon = ulon.reshape(3, 4)\n        time = utime.reshape(3, 4)\n\n        dc = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        dc_2 = StackedCoordinates([lat + 100, lon], dims=[\"lat\", \"lon\"])  # different coordinates\n        dc_3 = StackedCoordinates([lat[::-1], lon], dims=[\"lat\", \"lon\"])  # same coordinates, but paired differently\n        dc_t = dc.transpose(\"lon\", \"lat\")\n        dc_shape = StackedCoordinates([lat.reshape(6, 2), lon.reshape(6, 2)], dims=[\"lat\", \"lon\"])\n        dc_time = StackedCoordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n\n        # coordinates with stacked lat_lon\n        cs = podpac.Coordinates([[ulat, ulon]], dims=[\"lat_lon\"])\n        assert dc.issubset(cs)\n        assert dc[:2, :3].issubset(cs)\n        assert dc[::-1].issubset(cs)\n        assert not dc_2.issubset(cs)\n        assert not dc_3.issubset(cs)\n        assert dc_t.issubset(cs)\n        assert dc_shape.issubset(cs)\n        assert not dc_time.issubset(cs)\n\n        # coordinates with dependent lat,lon\n        cd = podpac.Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        assert dc.issubset(cd)\n        assert dc[:2, :3].issubset(cd)\n        assert dc[::-1].issubset(cd)\n        assert not dc_2.issubset(cd)\n        assert not dc_3.issubset(cd)\n        assert dc_t.issubset(cd)\n        assert dc_shape.issubset(cd)\n        assert not dc_time.issubset(cd)\n\n        # coordinates with unstacked lat, lon\n        cu = podpac.Coordinates([ulat, ulon[::-1]], dims=[\"lat\", \"lon\"])\n        assert dc.issubset(cu)\n        assert dc[:2, :3].issubset(cu)\n        assert dc[::-1].issubset(cu)\n        assert not dc_2.issubset(cu)\n        assert dc_3.issubset(cu)  # this is an important case!\n        assert dc_t.issubset(cu)\n        assert dc_shape.issubset(cu)\n        assert not dc_time.issubset(cu)\n\n        # coordinates with unstacked lat, lon, time\n        cu_time = podpac.Coordinates([ulat, ulon, utime], dims=[\"lat\", \"lon\", \"time\"])\n        assert dc.issubset(cu_time)\n        assert dc[:2, :3].issubset(cu_time)\n        assert dc[::-1].issubset(cu_time)\n        assert not dc_2.issubset(cu_time)\n        assert dc_3.issubset(cu_time)\n        assert dc_t.issubset(cu_time)\n        assert dc_shape.issubset(cu_time)\n        assert dc_time.issubset(cu_time)\n\n        assert not dc.issubset(cu_time[:2, :, :])\n\n        # mixed coordinates\n        cmixed = podpac.Coordinates([[ulat, ulon], utime], dims=[\"lat_lon\", \"time\"])\n        assert dc.issubset(cmixed)\n        assert dc[:2, :3].issubset(cmixed)\n        assert dc[::-1].issubset(cmixed)\n        assert not dc_2.issubset(cmixed)\n        assert not dc_3.issubset(cmixed)\n        assert dc_t.issubset(cmixed)\n        assert dc_shape.issubset(cmixed)\n        assert dc_time.issubset(cmixed)  # this is the most general case\n\n        assert not dc.issubset(cmixed[:2, :])\n        assert not dc_time.issubset(cmixed[:, :1])",
            "def test_flatten(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon])\n\n        assert c.flatten() == StackedCoordinates([lat.flatten(), lon.flatten()])",
            "def test_reshape(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = StackedCoordinates([lat, lon])\n\n        assert c.reshape((4, 3)) == StackedCoordinates([lat.reshape((4, 3)), lon.reshape((4, 3))])\n        assert c.flatten().reshape((3, 4)) == c",
            "def test_horizontal_resolution(self):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_cfunctions.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "def test_crange():\n    c = crange(0, 1, 0.2)\n    assert isinstance(c, UniformCoordinates1d)\n    assert c.start == 0.0\n    assert c.stop == 1.0\n    assert c.step == 0.2\n\n    c = crange(\"2018-01-01\", \"2018-01-05\", \"1,D\")\n    assert isinstance(c, UniformCoordinates1d)\n    assert c.start == np.datetime64(\"2018-01-01\")\n    assert c.stop == np.datetime64(\"2018-01-05\")\n    assert c.step == np.timedelta64(1, \"D\")",
            "def test_clinspace():\n    # numerical\n    c = clinspace(0, 1, 6)\n    assert isinstance(c, UniformCoordinates1d)\n    assert c.start == 0.0\n    assert c.stop == 1.0\n    assert c.size == 6\n\n    # datetime\n    c = clinspace(\"2018-01-01\", \"2018-01-05\", 5)\n    assert isinstance(c, UniformCoordinates1d)\n    assert c.start == np.datetime64(\"2018-01-01\")\n    assert c.stop == np.datetime64(\"2018-01-05\")\n    assert c.size == 5\n\n    # named\n    c = clinspace(0, 1, 6, name=\"lat\")\n    assert c.name == \"lat\"",
            "def test_clinspace_stacked():\n    c = clinspace((0, 10, \"2018-01-01\"), (1, 20, \"2018-01-06\"), 6)\n    assert isinstance(c, StackedCoordinates)\n\n    c1, c2, c3 = c\n    assert isinstance(c1, UniformCoordinates1d)\n    assert c1.start == 0.0\n    assert c1.stop == 1.0\n    assert c1.size == 6\n    assert isinstance(c2, UniformCoordinates1d)\n    assert c2.start == 10.0\n    assert c2.stop == 20.0\n    assert c2.size == 6\n    assert isinstance(c3, UniformCoordinates1d)\n    assert c3.start == np.datetime64(\"2018-01-01\")\n    assert c3.stop == np.datetime64(\"2018-01-06\")\n    assert c3.size == 6\n\n    # named\n    c = clinspace((0, 10, \"2018-01-01\"), (1, 20, \"2018-01-06\"), 6, name=\"lat_lon_time\")\n    assert c.name == \"lat_lon_time\"\n\n    # size must be an integer\n    with pytest.raises(TypeError):\n        clinspace((0, 10), (1, 20), (6, 6))\n\n    with pytest.raises(TypeError):\n        clinspace((0, 10), (1, 20), 0.2)\n\n    with pytest.raises(TypeError):\n        clinspace((0, 10), (1, 20), (0.2, 1.0))",
            "def test_clinspace_shape_mismatch():\n    with pytest.raises(ValueError, match=\"Size mismatch, 'start' and 'stop' must have the same size\"):\n        clinspace(0, (0, 10), 6)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_coordinates1d.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\n    See test_array_coordinates1d.py for additional Coordinates1d coverage\n    \"\"\"",
            "\"\"\"Test horizontal resolution implentation for Coordinates1d. Edge cases are handled in Coordinates.py\"\"\""
        ],
        "code_snippets": [
            "class TestCoordinates1d(object):\n    \"\"\"\n    See test_array_coordinates1d.py for additional Coordinates1d coverage\n    \"\"\"",
            "def test_common_api(self):\n        c = Coordinates1d(name=\"lat\")\n\n        attrs = [\n            \"name\",\n            \"is_monotonic\",\n            \"is_descending\",\n            \"is_uniform\",\n            \"start\",\n            \"stop\",\n            \"step\",\n            \"dims\",\n            \"xdims\",\n            \"udims\",\n            \"shape\",\n            \"size\",\n            \"dtype\",\n            \"deltatype\",\n            \"bounds\",\n            \"xcoords\",\n            \"definition\",\n            \"full_definition\",\n        ]\n\n        for attr in attrs:\n            try:\n                getattr(c, attr)\n            except NotImplementedError:\n                pass\n\n        try:\n            c.from_definition({})\n        except NotImplementedError:\n            pass\n\n        try:\n            c.copy()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.select([0, 1])\n        except NotImplementedError:\n            pass\n\n        try:\n            c.select([0, 1], outer=True, return_index=True)\n        except NotImplementedError:\n            pass\n\n        try:\n            c._select([0, 1], False, False)\n        except NotImplementedError:\n            pass\n\n        try:\n            c.simplify()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.flatten()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.reshape((10, 10))\n        except NotImplementedError:\n            pass\n\n        try:\n            c.issubset(c)\n        except NotImplementedError:\n            pass",
            "def test_horizontal_resolution(self):"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_array_coordinates1d.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestArrayCoordinatesInit(object):",
            "def test_empty(self):\n        c = ArrayCoordinates1d([])\n        a = np.array([], dtype=float)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [np.nan, np.nan])\n        with pytest.raises(RuntimeError):\n            c.argbounds\n        assert c.size == 0\n        assert c.shape == (0,)\n        assert c.dtype is None\n        assert c.deltatype is None\n        assert c.is_monotonic is None\n        assert c.is_descending is None\n        assert c.is_uniform is None\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)",
            "def test_numerical_singleton(self):\n        a = np.array([10], dtype=float)\n        c = ArrayCoordinates1d(10)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [10.0, 10.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 1\n        assert c.shape == (1,)\n        assert c.dtype == float\n        assert c.deltatype == float\n        assert c.is_monotonic == True\n        assert c.is_descending is None\n        assert c.is_uniform is None\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)",
            "def test_numerical_array(self):\n        # unsorted\n        values = [1, 6, 0, 4.0]\n        a = np.array(values, dtype=float)\n        c = ArrayCoordinates1d(a)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0.0, 6.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == float\n        assert c.deltatype == float\n        assert c.is_monotonic == False\n        assert c.is_descending is False\n        assert c.is_uniform == False\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)\n\n        # sorted ascending\n        values = [0, 1, 4, 6]\n        a = np.array(values, dtype=float)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0.0, 6.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == float\n        assert c.deltatype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == False\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)\n\n        # sorted descending\n        values = [6, 4, 1, 0]\n        a = np.array(values, dtype=float)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0.0, 6.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == float\n        assert c.deltatype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == False\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)\n\n        # uniform ascending\n        values = [0, 2, 4, 6]\n        a = np.array(values, dtype=float)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0.0, 6.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == float\n        assert c.deltatype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n        assert c.start == 0.0\n        assert c.stop == 6.0\n        assert c.step == 2\n        repr(c)\n\n        # uniform descending\n        values = [6, 4, 2, 0]\n        a = np.array(values, dtype=float)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0.0, 6.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == float\n        assert c.deltatype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True\n        assert c.start == 6.0\n        assert c.stop == 0.0\n        assert c.step == -2\n        repr(c)",
            "def test_datetime_singleton(self):\n        a = np.array(\"2018-01-01\").astype(np.datetime64)\n        c = ArrayCoordinates1d(\"2018-01-01\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, np.array([\"2018-01-01\", \"2018-01-01\"]).astype(np.datetime64))\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 1\n        assert c.shape == (1,)\n        assert c.dtype == np.datetime64\n        assert c.deltatype == np.timedelta64\n        assert c.is_monotonic == True\n        assert c.is_descending is None\n        assert c.is_uniform is None\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)",
            "def test_datetime_array(self):\n        # unsorted\n        values = [\"2018-01-01\", \"2019-01-01\", \"2017-01-01\", \"2018-01-02\"]\n        a = np.array(values).astype(np.datetime64)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, np.array([\"2017-01-01\", \"2019-01-01\"]).astype(np.datetime64))\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == np.datetime64\n        assert c.deltatype == np.timedelta64\n        assert c.is_monotonic == False\n        assert c.is_descending == False\n        assert c.is_uniform == False\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)\n\n        # sorted ascending\n        values = [\"2017-01-01\", \"2018-01-01\", \"2018-01-02\", \"2019-01-01\"]\n        a = np.array(values).astype(np.datetime64)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, np.array([\"2017-01-01\", \"2019-01-01\"]).astype(np.datetime64))\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == np.datetime64\n        assert c.deltatype == np.timedelta64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == False\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)\n\n        # sorted descending\n        values = [\"2019-01-01\", \"2018-01-02\", \"2018-01-01\", \"2017-01-01\"]\n        a = np.array(values).astype(np.datetime64)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, np.array([\"2017-01-01\", \"2019-01-01\"]).astype(np.datetime64))\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (4,)\n        assert c.dtype == np.datetime64\n        assert c.deltatype == np.timedelta64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == False\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)\n\n        # uniform ascending\n        values = [\"2017-01-01\", \"2018-01-01\", \"2019-01-01\"]\n        a = np.array(values).astype(np.datetime64)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, np.array([\"2017-01-01\", \"2019-01-01\"]).astype(np.datetime64))\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 3\n        assert c.shape == (3,)\n        assert c.dtype == np.datetime64\n        assert c.deltatype == np.timedelta64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n        assert c.start == np.datetime64(\"2017-01-01\")\n        assert c.stop == np.datetime64(\"2019-01-01\")\n        assert c.step == np.timedelta64(365, \"D\")\n        repr(c)\n\n        # uniform descending\n        values = [\"2019-01-01\", \"2018-01-01\", \"2017-01-01\"]\n        a = np.array(values).astype(np.datetime64)\n        c = ArrayCoordinates1d(values)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, np.array([\"2017-01-01\", \"2019-01-01\"]).astype(np.datetime64))\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 3\n        assert c.shape == (3,)\n        assert c.dtype == np.datetime64\n        assert c.deltatype == np.timedelta64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True\n        assert c.start == np.datetime64(\"2019-01-01\")\n        assert c.stop == np.datetime64(\"2017-01-01\")\n        assert c.step == np.timedelta64(-365, \"D\")\n        repr(c)",
            "def test_numerical_shaped(self):\n        values = [[1.0, 2.0, 3.0], [11.0, 12.0, 13.0]]\n        c = ArrayCoordinates1d(values)\n        a = np.array(values, dtype=float)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [1.0, 13.0])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 6\n        assert c.shape == (2, 3)\n        assert c.dtype is float\n        assert c.deltatype is float\n        assert c.is_monotonic is None\n        assert c.is_descending is None\n        assert c.is_uniform is None\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)",
            "def test_datetime_shaped(self):\n        values = [[\"2017-01-01\", \"2018-01-01\"], [\"2019-01-01\", \"2020-01-01\"]]\n        c = ArrayCoordinates1d(values)\n        a = np.array(values, dtype=np.datetime64)\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [np.datetime64(\"2017-01-01\"), np.datetime64(\"2020-01-01\")])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 4\n        assert c.shape == (2, 2)\n        assert c.dtype is np.datetime64\n        assert c.deltatype is np.timedelta64\n        assert c.is_monotonic is None\n        assert c.is_descending is None\n        assert c.is_uniform is None\n        assert c.start is None\n        assert c.stop is None\n        assert c.step is None\n        repr(c)",
            "def test_invalid_coords(self):\n        with pytest.raises(ValueError, match=\"Invalid coordinate values\"):\n            ArrayCoordinates1d([1, 2, \"2018-01\"])",
            "def test_from_xarray(self):\n        # numerical\n        x = xr.DataArray([0, 1, 2], name=\"lat\")\n        c = ArrayCoordinates1d.from_xarray(x)\n        assert c.name == \"lat\"\n        assert_equal(c.coordinates, x.data)\n\n        # datetime\n        x = xr.DataArray([np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-02\")], name=\"time\")\n        c = ArrayCoordinates1d.from_xarray(x)\n        assert c.name == \"time\"\n        assert_equal(c.coordinates, x.data)\n\n        # unnamed\n        x = xr.DataArray([0, 1, 2])\n        c = ArrayCoordinates1d.from_xarray(x)\n        assert c.name is None",
            "def test_copy(self):\n        c = ArrayCoordinates1d([1, 2, 3], name=\"lat\")\n        c2 = c.copy()\n        assert c is not c2\n        assert c == c2",
            "def test_name(self):\n        ArrayCoordinates1d([])\n        ArrayCoordinates1d([], name=\"lat\")\n        ArrayCoordinates1d([], name=\"lon\")\n        ArrayCoordinates1d([], name=\"alt\")\n        ArrayCoordinates1d([], name=\"time\")\n\n        with pytest.raises(tl.TraitError):\n            ArrayCoordinates1d([], name=\"depth\")\n\n        repr(ArrayCoordinates1d([], name=\"lat\"))",
            "def test_set_name(self):\n        # set if not already set\n        c = ArrayCoordinates1d([])\n        c._set_name(\"lat\")\n        assert c.name == \"lat\"\n\n        # check if set already\n        c = ArrayCoordinates1d([], name=\"lat\")\n        c._set_name(\"lat\")\n        assert c.name == \"lat\"\n\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            c._set_name(\"lon\")\n\n        # invalid name\n        c = ArrayCoordinates1d([])\n        with pytest.raises(tl.TraitError):\n            c._set_name(\"depth\")",
            "class TestArrayCoordinatesEq(object):",
            "def test_eq_type(self):\n        c1 = ArrayCoordinates1d([0, 1, 3])\n        assert c1 != [0, 1, 3]",
            "def test_eq_coordinates(self):\n        c1 = ArrayCoordinates1d([0, 1, 3])\n        c2 = ArrayCoordinates1d([0, 1, 3])\n        c3 = ArrayCoordinates1d([0, 1, 3, 4])\n        c4 = ArrayCoordinates1d([0, 1, 4])\n        c5 = ArrayCoordinates1d([0, 3, 1])\n\n        assert c1 == c2\n        assert not c1 == c3\n        assert not c1 == c4\n        assert not c1 == c5\n\n        c1 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-04\"])\n        c2 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-04\"])\n        c3 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-04\", \"2018-01-05\"])\n        c4 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-04\", \"2018-01-02\"])\n\n        assert c1 == c2\n        assert not c1 == c3\n        assert not c1 == c4",
            "def test_eq_coordinates_shaped(self):\n        c1 = ArrayCoordinates1d([0, 1, 3, 4])\n        c2 = ArrayCoordinates1d([0, 1, 3, 4])\n        c3 = ArrayCoordinates1d([[0, 1], [3, 4]])\n        c4 = ArrayCoordinates1d([[0, 1], [3, 4]])\n        c5 = ArrayCoordinates1d([[1, 0], [3, 4]])\n\n        assert c1 == c2\n        assert not c1 == c3\n        assert not c1 == c4\n        assert not c1 == c5\n\n        assert c3 == c4\n        assert not c3 == c5",
            "def test_ne(self):\n        # this matters in python 2\n        c1 = ArrayCoordinates1d([0, 1, 3])\n        c2 = ArrayCoordinates1d([0, 1, 3])\n        c3 = ArrayCoordinates1d([0, 1, 3, 4])\n        c4 = ArrayCoordinates1d([0, 1, 4])\n        c5 = ArrayCoordinates1d([0, 3, 1])\n\n        assert not c1 != c2\n        assert c1 != c3\n        assert c1 != c4\n        assert c1 != c5\n\n        c1 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-04\"])\n        c2 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-04\"])\n        c3 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-04\", \"2018-01-05\"])\n        c4 = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-04\", \"2018-01-02\"])\n\n        assert not c1 != c2\n        assert c1 != c3\n        assert c1 != c4",
            "def test_eq_name(self):\n        c1 = ArrayCoordinates1d([0, 1, 3], name=\"lat\")\n        c2 = ArrayCoordinates1d([0, 1, 3], name=\"lat\")\n        c3 = ArrayCoordinates1d([0, 1, 3], name=\"lon\")\n        c4 = ArrayCoordinates1d([0, 1, 3])\n\n        assert c1 == c2\n        assert c1 != c3\n        assert c1 != c4\n\n        c4.name = \"lat\"\n        assert c1 == c4",
            "class TestArrayCoordinatesSerialization(object):",
            "def test_definition(self):\n        # numerical\n        c = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        d = c.definition\n        assert isinstance(d, dict)\n        assert set(d.keys()) == {\"values\", \"name\"}\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = ArrayCoordinates1d.from_definition(d)  # test from_definition\n        assert c2 == c\n\n        # datetimes\n        c = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\"])\n        d = c.definition\n        assert isinstance(d, dict)\n        assert set(d.keys()) == {\"values\"}\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = ArrayCoordinates1d.from_definition(d)  # test from_definition\n        assert c2 == c",
            "def test_definition_shaped(self):\n        # numerical\n        c = ArrayCoordinates1d([[0, 1, 2], [3, 4, 5]], name=\"lat\")\n        d = c.definition\n        assert isinstance(d, dict)\n        assert set(d.keys()) == {\"values\", \"name\"}\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = ArrayCoordinates1d.from_definition(d)  # test from_definition\n        assert c2 == c\n\n        # datetimes\n        c = ArrayCoordinates1d([[\"2018-01-01\", \"2018-01-02\"], [\"2018-01-03\", \"2018-01-04\"]])\n        d = c.definition\n        assert isinstance(d, dict)\n        assert set(d.keys()) == {\"values\"}\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = ArrayCoordinates1d.from_definition(d)  # test from_definition\n        assert c2 == c",
            "class TestArrayCoordinatesProperties(object):",
            "def test_dims(self):\n        c = ArrayCoordinates1d([], name=\"lat\")\n        assert c.dims == (\"lat\",)\n        assert c.udims == (\"lat\",)\n\n        c = ArrayCoordinates1d([])\n        with pytest.raises(TypeError, match=\"cannot access dims property of unnamed Coordinates1d\"):\n            c.dims\n        with pytest.raises(TypeError, match=\"cannot access dims property of unnamed Coordinates1d\"):\n            c.udims",
            "def test_xdims(self):\n        c = ArrayCoordinates1d([], name=\"lat\")\n        assert isinstance(c.xdims, tuple)\n        assert c.xdims == (\"lat\",)\n\n        c = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        assert isinstance(c.xdims, tuple)\n        assert c.xdims == (\"lat\",)",
            "def test_xdims_shaped(self):\n        c = ArrayCoordinates1d([[0, 1, 2], [10, 11, 12]], name=\"lat\")\n        assert isinstance(c.xdims, tuple)\n        assert len(set(c.xdims)) == 2",
            "def test_properties(self):\n        c = ArrayCoordinates1d([])\n        assert isinstance(c.properties, dict)\n        assert set(c.properties) == set()\n\n        c = ArrayCoordinates1d([], name=\"lat\")\n        assert isinstance(c.properties, dict)\n        assert set(c.properties) == {\"name\"}",
            "def test_xcoords(self):\n        c = ArrayCoordinates1d([1, 2], name=\"lat\")\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n        np.testing.assert_array_equal(x[\"lat\"].data, c.coordinates)",
            "def test_xcoords_shaped(self):\n        c = ArrayCoordinates1d([[0, 1, 2], [10, 11, 12]], name=\"lat\")\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n        np.testing.assert_array_equal(x[\"lat\"].data, c.coordinates)",
            "def test_xcoords_unnamed(self):\n        c = ArrayCoordinates1d([1, 2])\n        with pytest.raises(ValueError, match=\"Cannot get xcoords\"):\n            c.xcoords",
            "class TestArrayCoordinatesIndexing(object):",
            "def test_len(self):\n        c = ArrayCoordinates1d([])\n        assert len(c) == 0\n\n        c = ArrayCoordinates1d([0, 1, 2])\n        assert len(c) == 3",
            "def test_len_shaped(self):\n        c = ArrayCoordinates1d([[0, 1, 2], [3, 4, 5]])\n        assert len(c) == 2",
            "def test_index(self):\n        c = ArrayCoordinates1d([20, 50, 60, 90, 40, 10], name=\"lat\")\n\n        # int\n        c2 = c[2]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [60])\n\n        c2 = c[-2]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [40])\n\n        # slice\n        c2 = c[:2]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [20, 50])\n\n        c2 = c[::2]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [20, 60, 40])\n\n        c2 = c[1:-1]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [50, 60, 90, 40])\n\n        c2 = c[::-1]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [10, 40, 90, 60, 50, 20])\n\n        # array\n        c2 = c[[0, 3, 1]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [20, 90, 50])\n\n        # boolean array\n        c2 = c[[True, True, True, False, True, False]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [20, 50, 60, 40])\n\n        # invalid\n        with pytest.raises(IndexError):\n            c[0.3]\n\n        with pytest.raises(IndexError):\n            c[10]",
            "def test_index_shaped(self):\n        c = ArrayCoordinates1d([[20, 50, 60], [90, 40, 10]], name=\"lat\")\n\n        # multi-index\n        c2 = c[0, 2]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.ndim == 1\n        assert c2.shape == (1,)\n        assert_equal(c2.coordinates, [60])\n\n        # single-index\n        c2 = c[0]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.ndim == 1\n        assert c2.shape == (3,)\n        assert_equal(c2.coordinates, [20, 50, 60])\n\n        # boolean array\n        c2 = c[np.array([[True, True, True], [False, True, False]])]  # has to be a numpy array\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.ndim == 1\n        assert c2.shape == (4,)\n        assert_equal(c2.coordinates, [20, 50, 60, 40])",
            "def test_in(self):\n        c = ArrayCoordinates1d([20, 50, 60, 90, 40, 10], name=\"lat\")\n        assert 20.0 in c\n        assert 50.0 in c\n        assert 20 in c\n        assert 5.0 not in c\n        assert np.datetime64(\"2018\") not in c\n        assert \"a\" not in c\n\n        c = ArrayCoordinates1d([\"2020-01-01\", \"2020-01-05\", \"2020-01-04\"], name=\"time\")\n        assert np.datetime64(\"2020-01-01\") in c\n        assert np.datetime64(\"2020-01-05\") in c\n        assert \"2020-01-01\" in c\n        assert np.datetime64(\"2020-01-02\") not in c\n        assert 10 not in c\n        assert \"a\" not in c",
            "def test_in_shaped(self):\n        c = ArrayCoordinates1d([[20, 50, 60], [90, 40, 10]], name=\"lat\")\n        assert 20.0 in c\n        assert 50.0 in c\n        assert 20 in c\n        assert 5.0 not in c\n        assert np.datetime64(\"2018\") not in c\n        assert \"a\" not in c\n\n        c = ArrayCoordinates1d([[\"2020-01-01\", \"2020-01-05\"], [\"2020-01-04\", \"2020-01-03\"]], name=\"time\")\n        assert np.datetime64(\"2020-01-01\") in c\n        assert np.datetime64(\"2020-01-05\") in c\n        assert \"2020-01-01\" in c\n        assert np.datetime64(\"2020-01-02\") not in c\n        assert 10 not in c\n        assert \"a\" not in c",
            "class TestArrayCoordinatesAreaBounds(object):",
            "def test_get_area_bounds_numerical(self):\n        values = np.array([0.0, 1.0, 4.0, 6.0])\n        c = ArrayCoordinates1d(values)\n\n        # point\n        area_bounds = c.get_area_bounds(None)\n        assert_equal(area_bounds, [0.0, 6.0])\n\n        # uniform\n        area_bounds = c.get_area_bounds(0.5)\n        assert_equal(area_bounds, [-0.5, 6.5])\n\n        # segment\n        area_bounds = c.get_area_bounds([-0.2, 0.7])\n        assert_equal(area_bounds, [-0.2, 6.7])\n\n        # polygon (i.e. there would be corresponding offets for another dimension)\n        area_bounds = c.get_area_bounds([-0.2, -0.5, 0.7, 0.5])\n        assert_equal(area_bounds, [-0.5, 6.7])\n\n        # boundaries\n        area_bounds = c.get_area_bounds([[-0.4, 0.1], [-0.3, 0.2], [-0.2, 0.3], [-0.1, 0.4]])\n        assert_equal(area_bounds, [-0.4, 6.4])",
            "def test_get_area_bounds_datetime(self):\n        values = make_coord_array([\"2017-01-02\", \"2017-01-01\", \"2019-01-01\", \"2018-01-01\"])\n        c = ArrayCoordinates1d(values)\n\n        # point\n        area_bounds = c.get_area_bounds(None)\n        assert_equal(area_bounds, make_coord_array([\"2017-01-01\", \"2019-01-01\"]))\n\n        # uniform\n        area_bounds = c.get_area_bounds(\"1,D\")\n        assert_equal(area_bounds, make_coord_array([\"2016-12-31\", \"2019-01-02\"]))\n\n        area_bounds = c.get_area_bounds(\"1,M\")\n        assert_equal(area_bounds, make_coord_array([\"2016-12-01\", \"2019-02-01\"]))\n\n        area_bounds = c.get_area_bounds(\"1,Y\")\n        assert_equal(area_bounds, make_coord_array([\"2016-01-01\", \"2020-01-01\"]))\n\n        # segment\n        area_bounds = c.get_area_bounds([\"0,h\", \"12,h\"])\n        assert_equal(area_bounds, make_coord_array([\"2017-01-01 00:00\", \"2019-01-01 12:00\"]))",
            "def test_get_area_bounds_empty(self):\n        c = ArrayCoordinates1d([])\n        area_bounds = c.get_area_bounds(1.0)\n        assert np.all(np.isnan(area_bounds))\n\n    @pytest.mark.xfail(reason=\"spec uncertain\")",
            "def test_get_area_bounds_overlapping(self):\n        values = np.array([0.0, 1.0, 4.0, 6.0])\n        c = ArrayCoordinates1d(values)\n\n        area_bounds = c.get_area_bounds([[-0.1, 0.1], [-10.0, 10.0], [-0.1, 0.1], [-0.1, 0.1]])\n        assert_equal(area_bounds, [-11.0, 11.0])",
            "class TestArrayCoordinatesSelection(object):",
            "def test_select_empty_shortcut(self):\n        c = ArrayCoordinates1d([])\n        bounds = [0, 1]\n\n        s = c.select(bounds)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select(bounds, return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_all_shortcut(self):\n        c = ArrayCoordinates1d([20.0, 50.0, 60.0, 90.0, 40.0, 10.0])\n        bounds = [0, 100]\n\n        s = c.select(bounds)\n        assert_equal(s.coordinates, c.coordinates)\n\n        s, I = c.select(bounds, return_index=True)\n        assert_equal(s.coordinates, c.coordinates)\n        assert_equal(c.coordinates[I], c.coordinates)",
            "def test_select_none_shortcut(self):\n        c = ArrayCoordinates1d([20.0, 50.0, 60.0, 90.0, 40.0, 10.0])\n\n        # above\n        s = c.select([100, 200])\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([100, 200], return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])\n\n        # below\n        s = c.select([0, 5])\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([0, 5], return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select(self):\n        c = ArrayCoordinates1d([20.0, 50.0, 60.0, 90.0, 40.0, 10.0])\n\n        # inner\n        s = c.select([30.0, 55.0])\n        assert_equal(s.coordinates, [50.0, 40.0])\n\n        s, I = c.select([30.0, 55.0], return_index=True)\n        assert_equal(s.coordinates, [50.0, 40.0])\n        assert_equal(c.coordinates[I], [50.0, 40.0])\n\n        # inner with aligned bounds\n        s = c.select([40.0, 60.0])\n        assert_equal(s.coordinates, [50.0, 60.0, 40.0])\n\n        s, I = c.select([40.0, 60.0], return_index=True)\n        assert_equal(s.coordinates, [50.0, 60.0, 40.0])\n        assert_equal(c.coordinates[I], [50.0, 60.0, 40.0])\n\n        # above\n        s = c.select([50, 100])\n        assert_equal(s.coordinates, [50.0, 60.0, 90.0])\n\n        s, I = c.select([50, 100], return_index=True)\n        assert_equal(s.coordinates, [50.0, 60.0, 90.0])\n        assert_equal(c.coordinates[I], [50.0, 60.0, 90.0])\n\n        # below\n        s = c.select([0, 50])\n        assert_equal(s.coordinates, [20.0, 50.0, 40.0, 10.0])\n\n        s, I = c.select([0, 50], return_index=True)\n        assert_equal(s.coordinates, [20.0, 50.0, 40.0, 10.0])\n        assert_equal(c.coordinates[I], [20.0, 50.0, 40.0, 10.0])\n\n        # between coordinates\n        s = c.select([52, 55])\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([52, 55], return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])\n\n        # backwards bounds\n        s = c.select([70, 30])\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_outer_ascending(self):\n        c = ArrayCoordinates1d([10.0, 20.0, 40.0, 50.0, 60.0, 90.0])\n\n        # inner\n        s = c.select([30.0, 55.0], outer=True)\n        assert_equal(s.coordinates, [20, 40.0, 50.0, 60.0])\n\n        s, I = c.select([30.0, 55.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [20, 40.0, 50.0, 60.0])\n        assert_equal(c.coordinates[I], [20, 40.0, 50.0, 60.0])\n\n        # inner with aligned bounds\n        s = c.select([40.0, 60.0], outer=True)\n        assert_equal(s.coordinates, [40.0, 50.0, 60.0])\n\n        s, I = c.select([40.0, 60.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [40.0, 50.0, 60.0])\n        assert_equal(c.coordinates[I], [40.0, 50.0, 60.0])\n\n        # above\n        s = c.select([50, 100], outer=True)\n        assert_equal(s.coordinates, [50.0, 60.0, 90.0])\n\n        s, I = c.select([50, 100], outer=True, return_index=True)\n        assert_equal(s.coordinates, [50.0, 60.0, 90.0])\n        assert_equal(c.coordinates[I], [50.0, 60.0, 90.0])\n\n        # below\n        s = c.select([0, 50], outer=True)\n        assert_equal(s.coordinates, [10.0, 20.0, 40.0, 50.0])\n\n        s, I = c.select([0, 50], outer=True, return_index=True)\n        assert_equal(s.coordinates, [10.0, 20.0, 40.0, 50.0])\n        assert_equal(c.coordinates[I], [10.0, 20.0, 40.0, 50.0])\n\n        # between coordinates\n        s = c.select([52, 55], outer=True)\n        assert_equal(s.coordinates, [50, 60])\n\n        s, I = c.select([52, 55], outer=True, return_index=True)\n        assert_equal(s.coordinates, [50, 60])\n        assert_equal(c.coordinates[I], [50, 60])\n\n        # backwards bounds\n        s = c.select([70, 30], outer=True)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], outer=True, return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_outer_descending(self):\n        c = ArrayCoordinates1d([90.0, 60.0, 50.0, 40.0, 20.0, 10.0])\n\n        # inner\n        s = c.select([30.0, 55.0], outer=True)\n        assert_equal(s.coordinates, [60.0, 50.0, 40.0, 20.0])\n\n        s, I = c.select([30.0, 55.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60.0, 50.0, 40.0, 20.0])\n        assert_equal(c.coordinates[I], [60.0, 50.0, 40.0, 20.0])\n\n        # inner with aligned bounds\n        s = c.select([40.0, 60.0], outer=True)\n        assert_equal(s.coordinates, [60.0, 50.0, 40.0])\n\n        s, I = c.select([40.0, 60.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60.0, 50.0, 40.0])\n        assert_equal(c.coordinates[I], [60.0, 50.0, 40.0])\n\n        # above\n        s = c.select([50, 100], outer=True)\n        assert_equal(s.coordinates, [90.0, 60.0, 50.0])\n\n        s, I = c.select([50, 100], outer=True, return_index=True)\n        assert_equal(s.coordinates, [90.0, 60.0, 50.0])\n        assert_equal(c.coordinates[I], [90.0, 60.0, 50.0])\n\n        # below\n        s = c.select([0, 50], outer=True)\n        assert_equal(s.coordinates, [50.0, 40.0, 20.0, 10.0])\n\n        s, I = c.select([0, 50], outer=True, return_index=True)\n        assert_equal(s.coordinates, [50.0, 40.0, 20.0, 10.0])\n        assert_equal(c.coordinates[I], [50.0, 40.0, 20.0, 10.0])\n\n        # between coordinates\n        s = c.select([52, 55], outer=True)\n        assert_equal(s.coordinates, [60, 50])\n\n        s, I = c.select([52, 55], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60, 50])\n        assert_equal(c.coordinates[I], [60, 50])\n\n        # backwards bounds\n        s = c.select([70, 30], outer=True)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], outer=True, return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_outer_nonmonotonic(self):\n        c = ArrayCoordinates1d([20.0, 40.0, 60.0, 10.0, 90.0, 50.0])\n\n        # inner\n        s = c.select([30.0, 55.0], outer=True)\n        assert_equal(s.coordinates, [20, 40.0, 60.0, 50.0])\n\n        s, I = c.select([30.0, 55.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [20, 40.0, 60.0, 50.0])\n        assert_equal(c.coordinates[I], [20, 40.0, 60.0, 50.0])\n\n        # inner with aligned bounds\n        s = c.select([40.0, 60.0], outer=True)\n        assert_equal(s.coordinates, [40.0, 60.0, 50.0])\n\n        s, I = c.select([40.0, 60.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [40.0, 60.0, 50.0])\n        assert_equal(c.coordinates[I], [40.0, 60.0, 50.0])\n\n        # above\n        s = c.select([50, 100], outer=True)\n        assert_equal(s.coordinates, [60.0, 90.0, 50.0])\n\n        s, I = c.select([50, 100], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60.0, 90.0, 50.0])\n        assert_equal(c.coordinates[I], [60.0, 90.0, 50.0])\n\n        # below\n        s = c.select([0, 50], outer=True)\n        assert_equal(s.coordinates, [20.0, 40.0, 10.0, 50.0])\n\n        s, I = c.select([0, 50], outer=True, return_index=True)\n        assert_equal(s.coordinates, [20.0, 40.0, 10.0, 50.0])\n        assert_equal(c.coordinates[I], [20.0, 40.0, 10.0, 50.0])\n\n        # between coordinates\n        s = c.select([52, 55], outer=True)\n        assert_equal(s.coordinates, [60, 50])\n\n        s, I = c.select([52, 55], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60, 50])\n        assert_equal(c.coordinates[I], [60, 50])\n\n        # backwards bounds\n        s = c.select([70, 30], outer=True)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], outer=True, return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_dict(self):\n        c = ArrayCoordinates1d([20.0, 40.0, 60.0, 10.0, 90.0, 50.0], name=\"lat\")\n\n        s = c.select({\"lat\": [30.0, 55.0]})\n        assert_equal(s.coordinates, [40.0, 50.0])\n\n        s = c.select({\"lon\": [30.0, 55]})\n        assert s == c",
            "def test_select_time(self):\n        c = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        s = c.select({\"time\": [np.datetime64(\"2018-01-03\"), \"2018-02-06\"]})\n        assert_equal(s.coordinates, np.array([\"2018-01-03\", \"2018-01-04\"]).astype(np.datetime64))",
            "def test_select_time_variable_precision(self):\n        c = ArrayCoordinates1d([\"2012-05-19\"], name=\"time\")\n        c2 = ArrayCoordinates1d([\"2012-05-19T12:00:00\"], name=\"time\")\n        s = c.select(c2.bounds, outer=True)\n        s1 = c.select(c2.bounds, outer=False)\n        s2 = c2.select(c.bounds)\n        assert s.size == 1\n        assert s1.size == 0\n        assert s2.size == 1",
            "def test_select_dtype(self):\n        c = ArrayCoordinates1d([20.0, 40.0, 60.0, 10.0, 90.0, 50.0], name=\"lat\")\n        with pytest.raises(TypeError):\n            c.select({\"lat\": [np.datetime64(\"2018-01-01\"), \"2018-02-01\"]})\n\n        c = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        with pytest.raises(TypeError):\n            c.select({\"time\": [1, 10]})",
            "def test_select_shaped(self):\n        c = ArrayCoordinates1d([[20.0, 50.0, 60.0], [90.0, 40.0, 10.0]])\n\n        # inner\n        s = c.select([30.0, 55.0])\n        assert_equal(s.coordinates, [50.0, 40.0])\n\n        s, I = c.select([30.0, 55.0], return_index=True)\n        assert_equal(s.coordinates, [50.0, 40.0])\n        assert_equal(c.coordinates[I], [50.0, 40.0])\n\n        # inner with aligned bounds\n        s = c.select([40.0, 60.0])\n        assert_equal(s.coordinates, [50.0, 60.0, 40.0])\n\n        s, I = c.select([40.0, 60.0], return_index=True)\n        assert_equal(s.coordinates, [50.0, 60.0, 40.0])\n        assert_equal(c.coordinates[I], [50.0, 60.0, 40.0])\n\n        # above\n        s = c.select([50, 100])\n        assert_equal(s.coordinates, [50.0, 60.0, 90.0])\n\n        s, I = c.select([50, 100], return_index=True)\n        assert_equal(s.coordinates, [50.0, 60.0, 90.0])\n        assert_equal(c.coordinates[I], [50.0, 60.0, 90.0])\n\n        # below\n        s = c.select([0, 50])\n        assert_equal(s.coordinates, [20.0, 50.0, 40.0, 10.0])\n\n        s, I = c.select([0, 50], return_index=True)\n        assert_equal(s.coordinates, [20.0, 50.0, 40.0, 10.0])\n        assert_equal(c.coordinates[I], [20.0, 50.0, 40.0, 10.0])\n\n        # between coordinates\n        s = c.select([52, 55])\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([52, 55], return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])\n\n        # backwards bounds\n        s = c.select([70, 30])\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_shaped_outer_nonmonotonic(self):\n        c = ArrayCoordinates1d([[20.0, 40.0, 60.0], [10.0, 90.0, 50.0]])\n\n        # inner\n        s = c.select([30.0, 55.0], outer=True)\n        assert_equal(s.coordinates, [20, 40.0, 60.0, 50.0])\n\n        s, I = c.select([30.0, 55.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [20, 40.0, 60.0, 50.0])\n        assert_equal(c.coordinates[I], [20, 40.0, 60.0, 50.0])\n\n        # inner with aligned bounds\n        s = c.select([40.0, 60.0], outer=True)\n        assert_equal(s.coordinates, [40.0, 60.0, 50.0])\n\n        s, I = c.select([40.0, 60.0], outer=True, return_index=True)\n        assert_equal(s.coordinates, [40.0, 60.0, 50.0])\n        assert_equal(c.coordinates[I], [40.0, 60.0, 50.0])\n\n        # above\n        s = c.select([50, 100], outer=True)\n        assert_equal(s.coordinates, [60.0, 90.0, 50.0])\n\n        s, I = c.select([50, 100], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60.0, 90.0, 50.0])\n        assert_equal(c.coordinates[I], [60.0, 90.0, 50.0])\n\n        # below\n        s = c.select([0, 50], outer=True)\n        assert_equal(s.coordinates, [20.0, 40.0, 10.0, 50.0])\n\n        s, I = c.select([0, 50], outer=True, return_index=True)\n        assert_equal(s.coordinates, [20.0, 40.0, 10.0, 50.0])\n        assert_equal(c.coordinates[I], [20.0, 40.0, 10.0, 50.0])\n\n        # between coordinates\n        s = c.select([52, 55], outer=True)\n        assert_equal(s.coordinates, [60, 50])\n\n        s, I = c.select([52, 55], outer=True, return_index=True)\n        assert_equal(s.coordinates, [60, 50])\n        assert_equal(c.coordinates[I], [60, 50])\n\n        # backwards bounds\n        s = c.select([70, 30], outer=True)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], outer=True, return_index=True)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "class TestArrayCoordinatesMethods(object):",
            "def test_unique(self):\n        c = ArrayCoordinates1d([1, 2, 3, 2])\n\n        u = c.unique()\n        assert u.shape == (3,)\n        assert_equal(u.coordinates, [1, 2, 3])\n\n        u, I = c.unique(return_index=True)\n        assert u == c[I]\n        assert_equal(u.coordinates, [1, 2, 3])",
            "def test_unique_monotonic(self):\n        c = ArrayCoordinates1d([1, 2, 3, 5])\n\n        u = c.unique()\n        assert u == c\n\n        u, I = c.unique(return_index=True)\n        assert u == c\n        assert u == c[I]",
            "def test_unique_shaped(self):\n        c = ArrayCoordinates1d([[1, 2], [3, 2]])\n\n        # also flattens\n        u = c.unique()\n        assert u.shape == (3,)\n        assert_equal(u.coordinates, [1, 2, 3])\n\n        u, I = c.unique(return_index=True)\n        assert u == c.flatten()[I]\n        assert_equal(u.coordinates, [1, 2, 3])",
            "def test_simplify(self):\n        # convert to UniformCoordinates\n        c = ArrayCoordinates1d([1, 2, 3, 4])\n        c2 = c.simplify()\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2 == c\n\n        # reversed, step -2\n        c = ArrayCoordinates1d([4, 2, 0])\n        c2 = c.simplify()\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2 == c\n\n        # don't simplify\n        c = ArrayCoordinates1d([1, 2, 4])\n        c2 = c.simplify()\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2 == c\n\n        # time, convert to UniformCoordinates\n        c = ArrayCoordinates1d([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"])\n        c2 = c.simplify()\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2 == c\n\n        # time, reverse -2,H\n        c = ArrayCoordinates1d([\"2020-01-01T12:00\", \"2020-01-01T10:00\", \"2020-01-01T08:00\"])\n        c2 = c.simplify()\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2 == c\n\n        # time, don't simplify\n        c = ArrayCoordinates1d([\"2020-01-01\", \"2020-01-02\", \"2020-01-04\"])\n        c2 = c.simplify()\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2 == c\n\n        # empty\n        c = ArrayCoordinates1d([])\n        c2 = c.simplify()\n        assert c2 == c",
            "def test_simplify_shaped(self):\n        # don't simplify\n        c = ArrayCoordinates1d([[1, 2], [3, 4]])\n        c2 = c.simplify()\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2 == c",
            "def test_issubset(self):\n        c1 = ArrayCoordinates1d([2, 1])\n        c2 = ArrayCoordinates1d([1, 2, 3])\n        c3 = ArrayCoordinates1d([1, 2, 4])\n        e = ArrayCoordinates1d([])\n\n        # self\n        assert c1.issubset(c1)\n        assert e.issubset(e)\n\n        # subsets\n        assert c1.issubset(c2)\n        assert c1.issubset(c3)\n        assert e.issubset(c1)\n\n        # not subsets\n        assert not c2.issubset(c1)\n        assert not c3.issubset(c1)\n        assert not c2.issubset(c3)\n        assert not c3.issubset(c2)\n        assert not c1.issubset(e)",
            "def test_issubset_datetime(self):\n        c1 = ArrayCoordinates1d([\"2020-01-01\", \"2020-01-02\"])\n        c2 = ArrayCoordinates1d([\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"])\n        c3 = ArrayCoordinates1d([\"2020-01-01T00:00\", \"2020-01-02T00:00\"])\n        c4 = ArrayCoordinates1d([\"2020-01-01T12:00\", \"2020-01-02T12:00\"])\n\n        # same resolution\n        assert c1.issubset(c1)\n        assert c1.issubset(c2)\n        assert not c2.issubset(c1)\n\n        # different resolution\n        assert c3.issubset(c2)\n        assert c1.issubset(c3)\n        assert not c1.issubset(c4)\n        assert not c4.issubset(c1)",
            "def test_issubset_dtype(self):\n        c1 = ArrayCoordinates1d([0, 1])\n        c2 = ArrayCoordinates1d([\"2018\", \"2019\"])\n        assert not c1.issubset(c2)\n        assert not c2.issubset(c1)",
            "def test_issubset_uniform_coordinates(self):\n        a = ArrayCoordinates1d([2, 1])\n        u1 = UniformCoordinates1d(start=1, stop=3, step=1)\n        u2 = UniformCoordinates1d(start=1, stop=3, step=0.5)\n        u3 = UniformCoordinates1d(start=1, stop=4, step=2)\n\n        # self\n        assert a.issubset(u1)\n        assert a.issubset(u2)\n        assert not a.issubset(u3)",
            "def test_issubset_coordinates(self):\n        a = ArrayCoordinates1d([3, 1], name=\"lat\")\n        c1 = Coordinates([[1, 2, 3], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[1, 2, 4], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([[10, 20, 30]], dims=[\"alt\"])\n\n        assert a.issubset(c1)\n        assert not a.issubset(c2)\n        assert not a.issubset(c3)",
            "def test_issubset_shaped(self):\n        c1 = ArrayCoordinates1d([2, 1])\n        c2 = ArrayCoordinates1d([[1], [2]])\n        c3 = ArrayCoordinates1d([[1, 2], [3, 4]])\n\n        # self\n        assert c1.issubset(c1)\n        assert c2.issubset(c2)\n        assert c3.issubset(c3)\n\n        # subsets\n        assert c1.issubset(c2)\n        assert c1.issubset(c3)\n        assert c2.issubset(c1)\n        assert c2.issubset(c3)\n\n        # not subsets\n        assert not c3.issubset(c1)\n        assert not c3.issubset(c2)",
            "def test_flatten(self):\n        c = ArrayCoordinates1d([1, 2, 3, 2])\n        c2 = ArrayCoordinates1d([[1, 2], [3, 2]])\n        assert c != c2\n        assert c2.flatten() == c\n        assert c.flatten() == c",
            "def test_reshape(self):\n        c = ArrayCoordinates1d([1, 2, 3, 2])\n        c2 = ArrayCoordinates1d([[1, 2], [3, 2]])\n        assert c.reshape((2, 2)) == c2\n        assert c2.reshape((4,)) == c\n        assert c.reshape((4, 1)) == c2.reshape((4, 1))\n\n        with pytest.raises(ValueError, match=\"cannot reshape array\"):\n            c.reshape((5, 4))\n\n        with pytest.raises(ValueError, match=\"cannot reshape array\"):\n            c2.reshape((2, 1))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_coordinates.py",
        "comments": [
            "//testwms/?map=map&&service=WMS&request=GetMap&layers=layer&styles=&format=image%2Fpng\"",
            "//testwms/?map=map&&service=WCS&request=GetMap&layers=layer&styles=&format=image%2Fpng\""
        ],
        "docstrings": [
            "\"\"\"Test edge cases of resolutions, and that Resolutions are returned correctly in an OrderedDict.\n        StackedCoordinates and Coordiantes1d handle the resolution calculations, so correctness of the resolutions are tested there.\n        \"\"\""
        ],
        "code_snippets": [
            "class TestCoordinateCreation(object):",
            "def test_empty(self):\n        c = Coordinates([])\n        assert c.dims == tuple()\n        assert c.udims == tuple()\n        assert c.xdims == tuple()\n        assert c.shape == tuple()\n        assert c.ndim == 0\n        assert c.size == 0",
            "def test_single_dim(self):\n        # single value\n        date = \"2018-01-01\"\n\n        c = Coordinates([date], dims=[\"time\"])\n        assert c.dims == (\"time\",)\n        assert c.udims == (\"time\",)\n        assert c.xdims == (\"time\",)\n        assert c.shape == (1,)\n        assert c.ndim == 1\n        assert c.size == 1\n\n        # array\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates([dates], dims=[\"time\"])\n        assert c.dims == (\"time\",)\n        assert c.udims == (\"time\",)\n        assert c.xdims == (\"time\",)\n        assert c.shape == (2,)\n        assert c.ndim == 1\n        assert c.size == 2\n\n        c = Coordinates([np.array(dates).astype(np.datetime64)], dims=[\"time\"])\n        assert c.dims == (\"time\",)\n        assert c.udims == (\"time\",)\n        assert c.xdims == (\"time\",)\n        assert c.shape == (2,)\n        assert c.ndim == 1\n\n        c = Coordinates([xr.DataArray(dates).astype(np.datetime64)], dims=[\"time\"])\n        assert c.dims == (\"time\",)\n        assert c.udims == (\"time\",)\n        assert c.xdims == (\"time\",)\n        assert c.shape == (2,)\n        assert c.ndim == 1\n        assert c.size == 2\n\n        # use DataArray name, but dims overrides the DataArray name\n        c = Coordinates([xr.DataArray(dates, name=\"time\").astype(np.datetime64)])\n        assert c.dims == (\"time\",)\n        assert c.udims == (\"time\",)\n        assert c.xdims == (\"time\",)\n        assert c.shape == (2,)\n        assert c.ndim == 1\n        assert c.size == 2\n\n        c = Coordinates([xr.DataArray(dates, name=\"a\").astype(np.datetime64)], dims=[\"time\"])\n        assert c.dims == (\"time\",)\n        assert c.udims == (\"time\",)\n        assert c.xdims == (\"time\",)\n        assert c.shape == (2,)\n        assert c.ndim == 1\n        assert c.size == 2",
            "def test_unstacked(self):\n        # single value\n        c = Coordinates([0, 10], dims=[\"lat\", \"lon\"])\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat\", \"lon\")\n        assert c.shape == (1, 1)\n        assert c.ndim == 2\n        assert c.size == 1\n\n        # arrays\n        lat = [0, 1, 2]\n        lon = [10, 20, 30, 40]\n\n        c = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat\", \"lon\")\n        assert c.shape == (3, 4)\n        assert c.ndim == 2\n        assert c.size == 12\n\n        # use DataArray names\n        c = Coordinates([xr.DataArray(lat, name=\"lat\"), xr.DataArray(lon, name=\"lon\")])\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat\", \"lon\")\n        assert c.shape == (3, 4)\n        assert c.ndim == 2\n        assert c.size == 12\n\n        # dims overrides the DataArray names\n        c = Coordinates([xr.DataArray(lat, name=\"a\"), xr.DataArray(lon, name=\"b\")], dims=[\"lat\", \"lon\"])\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat\", \"lon\")\n        assert c.shape == (3, 4)\n        assert c.ndim == 2\n        assert c.size == 12",
            "def test_stacked(self):\n        # single value\n        c = Coordinates([[0, 10]], dims=[\"lat_lon\"])\n        assert c.dims == (\"lat_lon\",)\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat_lon\",)\n        assert c.shape == (1,)\n        assert c.ndim == 1\n        assert c.size == 1\n\n        # arrays\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        c = Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        assert c.dims == (\"lat_lon\",)\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat_lon\",)\n        assert c.shape == (3,)\n        assert c.ndim == 1\n        assert c.size == 3\n\n        # nested dims version\n        c = Coordinates([[lat, lon]], dims=[[\"lat\", \"lon\"]])\n        assert c.dims == (\"lat_lon\",)\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"lat_lon\",)\n        assert c.shape == (3,)\n        assert c.ndim == 1\n        assert c.size == 3",
            "def test_stacked_shaped(self):\n        # explicit\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        latlon = StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        c = Coordinates([latlon])\n        assert c.dims == (\"lat_lon\",)\n        assert c.udims == (\"lat\", \"lon\")\n        assert len(set(c.xdims)) == 2  # doesn't really matter what they are called\n        assert c.shape == (3, 4)\n        assert c.ndim == 2\n        assert c.size == 12\n\n        # implicit\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        assert c.dims == (\"lat_lon\",)\n        assert c.udims == (\"lat\", \"lon\")\n        assert len(set(c.xdims)) == 2  # doesn't really matter what they are called\n        assert c.shape == (3, 4)\n        assert c.ndim == 2\n        assert c.size == 12",
            "def test_rotated(self):\n        latlon = AffineCoordinates(geotransform=(10.0, 2.0, 0.0, 20.0, 0.0, -3.0), shape=(3, 4))\n        c = Coordinates([latlon])\n        assert c.dims == (\"lat_lon\",)\n        assert c.udims == (\"lat\", \"lon\")\n        assert len(set(c.xdims)) == 2  # doesn't really matter what they are called\n        assert c.shape == (3, 4)\n        assert c.ndim == 2\n        assert c.size == 12",
            "def test_mixed(self):\n        # stacked\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates([[lat, lon], dates], dims=[\"lat_lon\", \"time\"])\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert c.udims == (\"lat\", \"lon\", \"time\")\n        assert c.xdims == (\"lat_lon\", \"time\")\n        assert c.shape == (3, 2)\n        assert c.ndim == 2\n        assert c.size == 6\n        repr(c)\n\n        # stacked, nested dims version\n        c = Coordinates([[lat, lon], dates], dims=[[\"lat\", \"lon\"], \"time\"])\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert c.udims == (\"lat\", \"lon\", \"time\")\n        assert c.xdims == (\"lat_lon\", \"time\")\n        assert c.shape == (3, 2)\n        assert c.ndim == 2\n        assert c.size == 6\n        repr(c)",
            "def test_mixed_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        dates = [[\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\"]]\n        c = Coordinates([[lat, lon], dates], dims=[\"lat_lon\", \"time\"])\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert c.udims == (\"lat\", \"lon\", \"time\")\n        assert len(set(c.xdims)) == 4  # doesn't really matter what they are called\n        assert c.shape == (3, 4, 2, 3)\n        assert c.ndim == 4\n        assert c.size == 72\n        repr(c)",
            "def test_mixed_affine(sesf):\n        latlon = AffineCoordinates(geotransform=(10.0, 2.0, 0.0, 20.0, 0.0, -3.0), shape=(3, 4))\n        dates = [[\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\"]]\n        c = Coordinates([latlon, dates], dims=[\"lat_lon\", \"time\"])\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert c.udims == (\"lat\", \"lon\", \"time\")\n        assert len(set(c.xdims)) == 4  # doesn't really matter what they are called\n        assert c.shape == (3, 4, 2, 3)\n        assert c.ndim == 4\n        assert c.size == 72\n        repr(c)",
            "def test_invalid_dims(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        with pytest.raises(TypeError, match=\"Invalid dims type\"):\n            Coordinates([dates], dims=\"time\")\n\n        with pytest.raises(ValueError, match=\"coords and dims size mismatch\"):\n            Coordinates(dates, dims=[\"time\"])\n\n        with pytest.raises(ValueError, match=\"coords and dims size mismatch\"):\n            Coordinates([lat, lon, dates], dims=[\"lat_lon\", \"time\"])\n\n        with pytest.raises(ValueError, match=\"coords and dims size mismatch\"):\n            Coordinates([[lat, lon], dates], dims=[\"lat\", \"lon\", \"dates\"])\n\n        with pytest.raises(ValueError, match=\"coords and dims size mismatch\"):\n            Coordinates([lat, lon], dims=[\"lat_lon\"])\n\n        with pytest.raises(ValueError, match=\"coords and dims size mismatch\"):\n            Coordinates([[lat, lon]], dims=[\"lat\", \"lon\"])\n\n        with pytest.raises(ValueError, match=\"coords and dims size mismatch\"):\n            Coordinates([lat, lon], dims=[\"lat_lon\"])\n\n        with pytest.raises(TypeError, match=\"Cannot get dim for coordinates at position\"):\n            # this doesn't work because lat and lon are not named BaseCoordinates/xarray objects\n            Coordinates([lat, lon])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension\"):\n            Coordinates([lat, lon], dims=[\"lat\", \"lat\"])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension\"):\n            Coordinates([[lat, lon], lon], dims=[\"lat_lon\", \"lat\"])",
            "def test_dims_mismatch(self):\n        c1d = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            Coordinates([c1d], dims=[\"lon\"])",
            "def test_invalid_coords(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        with pytest.raises(TypeError, match=\"Invalid coords\"):\n            Coordinates({\"lat\": lat, \"lon\": lon})",
            "def test_base_coordinates(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates(\n            [\n                StackedCoordinates([ArrayCoordinates1d(lat, name=\"lat\"), ArrayCoordinates1d(lon, name=\"lon\")]),\n                ArrayCoordinates1d(dates, name=\"time\"),\n            ]\n        )\n\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert c.shape == (3, 2)\n\n        # TODO default and overridden properties",
            "def test_grid(self):\n        # array\n        lat = [0, 1, 2]\n        lon = [10, 20, 30, 40]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates.grid(lat=lat, lon=lon, time=dates, dims=[\"time\", \"lat\", \"lon\"])\n        assert c.dims == (\"time\", \"lat\", \"lon\")\n        assert c.udims == (\"time\", \"lat\", \"lon\")\n        assert c.shape == (2, 3, 4)\n        assert c.ndim == 3\n        assert c.size == 24\n\n        # size\n        lat = (0, 1, 3)\n        lon = (10, 40, 4)\n        dates = (\"2018-01-01\", \"2018-01-05\", 5)\n\n        c = Coordinates.grid(lat=lat, lon=lon, time=dates, dims=[\"time\", \"lat\", \"lon\"])\n        assert c.dims == (\"time\", \"lat\", \"lon\")\n        assert c.udims == (\"time\", \"lat\", \"lon\")\n        assert c.shape == (5, 3, 4)\n        assert c.ndim == 3\n        assert c.size == 60\n\n        # step\n        lat = (0, 1, 0.5)\n        lon = (10, 40, 10.0)\n        dates = (\"2018-01-01\", \"2018-01-05\", \"1,D\")\n\n        c = Coordinates.grid(lat=lat, lon=lon, time=dates, dims=[\"time\", \"lat\", \"lon\"])\n        assert c.dims == (\"time\", \"lat\", \"lon\")\n        assert c.udims == (\"time\", \"lat\", \"lon\")\n        assert c.shape == (5, 3, 4)\n        assert c.ndim == 3\n        assert c.size == 60",
            "def test_points(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"]\n\n        c = Coordinates.points(lat=lat, lon=lon, time=dates, dims=[\"time\", \"lat\", \"lon\"])\n        assert c.dims == (\"time_lat_lon\",)\n        assert c.udims == (\"time\", \"lat\", \"lon\")\n        assert c.shape == (3,)\n        assert c.ndim == 1\n        assert c.size == 3",
            "def test_grid_points_order(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30, 40]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        with pytest.raises(ValueError):\n            Coordinates.grid(lat=lat, lon=lon, time=dates, dims=[\"lat\", \"lon\"])\n\n        with pytest.raises(ValueError):\n            Coordinates.grid(lat=lat, lon=lon, dims=[\"lat\", \"lon\", \"time\"])\n\n        if sys.version < \"3.6\":\n            with pytest.raises(TypeError):\n                Coordinates.grid(lat=lat, lon=lon, time=dates)\n        else:\n            Coordinates.grid(lat=lat, lon=lon, time=dates)",
            "def test_from_url(self):\n        crds = Coordinates([[41, 40], [-71, -70], \"2018-05-19\"], dims=[\"lat\", \"lon\", \"time\"])\n        crds2 = crds.transform(\"EPSG:3857\")\n\n        url = (\n            r\"http:",
            "def test_from_xarray(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates(\n            [\n                StackedCoordinates([ArrayCoordinates1d(lat, name=\"lat\"), ArrayCoordinates1d(lon, name=\"lon\")]),\n                ArrayCoordinates1d(dates, name=\"time\"),\n            ],\n            crs=\"EPSG:2193\",\n        )\n\n        # from DataArray\n        x = xr.DataArray(np.empty(c.shape), coords=c.xcoords, dims=c.xdims, attrs={\"crs\": c.crs})\n        c2 = Coordinates.from_xarray(x)\n        assert c2 == c\n        assert c2.crs == \"EPSG:2193\"\n\n        # prefer crs argument over attrs.crs\n        x = xr.DataArray(np.empty(c.shape), coords=c.xcoords, dims=c.xdims, attrs={\"crs\": c.crs})\n        c3 = Coordinates.from_xarray(x, crs=\"EPSG:4326\")\n        assert c3.crs == \"EPSG:4326\"\n\n        # from DataArrayCoords\n        c4 = Coordinates.from_xarray(x.coords, crs=\"EPSG:2193\")\n        assert c4 == c\n        assert c4.crs == \"EPSG:2193\"\n\n        # crs warning\n        with pytest.warns(UserWarning, match=\"using default crs\"):\n            c2 = Coordinates.from_xarray(x.coords)\n\n        # invalid\n        with pytest.raises(TypeError, match=\"Coordinates.from_xarray expects an xarray\"):\n            Coordinates.from_xarray([0, 10])",
            "def test_from_xarray_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        dates = [[\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\"]]\n        c = Coordinates([[lat, lon], dates], dims=[\"lat_lon\", \"time\"], crs=\"EPSG:2193\")\n\n        # from xarray\n        x = xr.DataArray(np.empty(c.shape), coords=c.xcoords, dims=c.xdims, attrs={\"crs\": c.crs})\n        c2 = Coordinates.from_xarray(x)\n        assert c2 == c",
            "def test_from_xarray_with_outputs(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n\n        c = Coordinates([lat, lon], dims=[\"lat\", \"lon\"], crs=\"EPSG:2193\")\n\n        # from xarray\n        dims = c.xdims + (\"output\",)\n        coords = {\"output\": [\"a\", \"b\"], **c.xcoords}\n        shape = c.shape + (2,)\n\n        x = xr.DataArray(np.empty(c.shape + (2,)), coords=coords, dims=dims, attrs={\"crs\": c.crs})\n        c2 = Coordinates.from_xarray(x)\n        assert c2 == c",
            "def test_crs(self):\n        lat = ArrayCoordinates1d([0, 1, 2], \"lat\")\n        lon = ArrayCoordinates1d([0, 1, 2], \"lon\")\n\n        # default\n        c = Coordinates([lat, lon])\n        assert c.crs == podpac.settings[\"DEFAULT_CRS\"]\n        assert set(c.properties.keys()) == {\"crs\"}\n\n        # crs\n        c = Coordinates([lat, lon], crs=\"EPSG:2193\")\n        assert c.crs == \"EPSG:2193\"\n        assert set(c.properties.keys()) == {\"crs\"}\n\n        # proj4\n        c = Coordinates([lat, lon], crs=\"EPSG:2193\")\n        assert c.crs == \"EPSG:2193\"\n        assert set(c.properties.keys()) == {\"crs\"}\n\n        c = Coordinates([lat, lon], crs=\"+proj=merc +lat_ts=56.5 +ellps=GRS80\")\n        assert c.crs == \"+proj=merc +lat_ts=56.5 +ellps=GRS80\"\n        assert set(c.properties.keys()) == {\"crs\"}\n\n        # with vunits\n        c = Coordinates([lat, lon], crs=\"+proj=merc +lat_ts=56.5 +ellps=GRS80 +vunits=ft\")\n        assert c.crs == \"+proj=merc +lat_ts=56.5 +ellps=GRS80 +vunits=ft\"\n        assert set(c.properties.keys()) == {\"crs\"}\n\n        # invalid\n        with pytest.raises(pyproj.crs.CRSError):\n            Coordinates([lat, lon], crs=\"abcd\")",
            "def test_crs_with_vertical_units(self):\n\n        alt = ArrayCoordinates1d([0, 1, 2], name=\"alt\")\n\n        c = Coordinates([alt], crs=\"+proj=merc +vunits=us-ft\")\n        assert set(c.properties.keys()) == {\"crs\"}\n\n        # with crs\n        ct = c.transform(\"+proj=merc +vunits=m\")\n        np.testing.assert_array_almost_equal(ct[\"alt\"].coordinates, 0.30480061 * c[\"alt\"].coordinates)\n\n        # invalid\n        with pytest.raises(ValueError):\n            Coordinates([alt], crs=\"EPSG:2193\")",
            "def test_CRS(self):\n        lat = ArrayCoordinates1d([0, 1, 2], \"lat\")\n        lon = ArrayCoordinates1d([0, 1, 2], \"lon\")\n        c = Coordinates([lat, lon])\n        assert isinstance(c.CRS, pyproj.CRS)",
            "def test_alt_units(self):\n        lat = ArrayCoordinates1d([0, 1, 2], \"lat\")\n        lon = ArrayCoordinates1d([0, 1, 2], \"lon\")\n        alt = ArrayCoordinates1d([0, 1, 2], name=\"alt\")\n\n        c = Coordinates([lat, lon], crs=\"proj=merc\")\n        assert c.alt_units is None\n\n        c = Coordinates([alt], crs=\"+proj=merc +vunits=us-ft\")\n\n        with pytest.warns(UserWarning):\n            assert c.alt_units in [\"us-ft\", \"US survey foot\"]  # pyproj < 3.0  # pyproj >= 3.0",
            "class TestCoordinatesSerialization(object):",
            "def test_definition(self):\n        # this tests array coordinates, uniform coordinates, and stacked coordinates\n        c = Coordinates(\n            [[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"], crange(0, 10, 0.5)],\n            dims=[\"lat_lon\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n        d = c.definition\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)\n        c2 = Coordinates.from_definition(d)\n        assert c2 == c",
            "def test_definition_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        c = Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        d = c.definition\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)\n        c2 = Coordinates.from_definition(d)\n        assert c2 == c",
            "def test_definition_affine(self):\n        latlon = AffineCoordinates(geotransform=(10.0, 2.0, 0.0, 20.0, 0.0, -3.0), shape=(3, 4))\n        c = Coordinates([latlon])\n        d = c.definition\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)\n        c2 = Coordinates.from_definition(d)\n        assert c2 == c",
            "def test_definition_properties(self):\n        lat = ArrayCoordinates1d([0, 1, 2], \"lat\")\n        lon = ArrayCoordinates1d([0, 1, 2], \"lon\")\n\n        # default\n        c = Coordinates([lat, lon], crs=\"EPSG:2193\")\n        d = c.definition\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)\n        c2 = Coordinates.from_definition(d)\n        assert c2 == c\n        assert c2.crs == \"EPSG:2193\"",
            "def test_from_definition(self):\n        d = {\n            \"coords\": [{\"name\": \"lat\", \"values\": [0, 1, 2]}, {\"name\": \"lon\", \"start\": 0, \"stop\": 10, \"size\": 6}],\n            \"crs\": \"EPSG:2193\",\n        }\n\n        c = Coordinates.from_definition(d)\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.crs == \"EPSG:2193\"\n        assert_equal(c[\"lat\"].coordinates, [0, 1, 2])\n        assert_equal(c[\"lon\"].coordinates, [0, 2, 4, 6, 8, 10])",
            "def test_invalid_definition(self):\n        with pytest.raises(TypeError, match=\"Could not parse coordinates definition\"):\n            Coordinates.from_definition([0, 1, 2])\n\n        with pytest.raises(ValueError, match=\"Could not parse coordinates definition\"):\n            Coordinates.from_definition({\"data\": [0, 1, 2]})\n\n        with pytest.raises(TypeError, match=\"Could not parse coordinates definition\"):\n            Coordinates.from_definition({\"coords\": {}})\n\n        with pytest.raises(ValueError, match=\"Could not parse coordinates definition item\"):\n            Coordinates.from_definition({\"coords\": [{}]})",
            "def test_json(self):\n        c = Coordinates(\n            [[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"], crange(0, 10, 0.5)],\n            dims=[\"lat_lon\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n\n        s = c.json\n\n        json.loads(s)\n\n        c2 = Coordinates.from_json(s)\n        assert c2 == c",
            "class TestCoordinatesProperties(object):",
            "def test_xarray_coords(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30, 40]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates(\n            [\n                ArrayCoordinates1d(lat, name=\"lat\"),\n                ArrayCoordinates1d(lon, name=\"lon\"),\n                ArrayCoordinates1d(dates, name=\"time\"),\n            ]\n        )\n\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n\n        assert x.dims == (\"lat\", \"lon\", \"time\")\n        np.testing.assert_equal(x[\"lat\"], np.array(lat, dtype=float))\n        np.testing.assert_equal(x[\"lon\"], np.array(lon, dtype=float))\n        np.testing.assert_equal(x[\"time\"], np.array(dates).astype(np.datetime64))",
            "def test_xarray_coords_stacked(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates(\n            [\n                StackedCoordinates([ArrayCoordinates1d(lat, name=\"lat\"), ArrayCoordinates1d(lon, name=\"lon\")]),\n                ArrayCoordinates1d(dates, name=\"time\"),\n            ]\n        )\n\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n\n        assert x.dims == (\"lat_lon\", \"time\")\n        np.testing.assert_equal(x[\"lat\"], np.array(lat, dtype=float))\n        np.testing.assert_equal(x[\"lon\"], np.array(lon, dtype=float))\n        np.testing.assert_equal(x[\"time\"], np.array(dates).astype(np.datetime64))",
            "def test_xarray_coords_stacked_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates([StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"]), ArrayCoordinates1d(dates, name=\"time\")])\n\n        x = xr.DataArray(np.empty(c.shape), dims=c.xdims, coords=c.xcoords)\n\n        assert len(x.dims) == 3\n        np.testing.assert_equal(x[\"lat\"], np.array(lat, dtype=float))\n        np.testing.assert_equal(x[\"lon\"], np.array(lon, dtype=float))\n        np.testing.assert_equal(x[\"time\"], np.array(dates).astype(np.datetime64))",
            "def test_bounds(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n\n        c = Coordinates([[lat, lon], dates], dims=[\"lat_lon\", \"time\"])\n        bounds = c.bounds\n        assert isinstance(bounds, dict)\n        assert set(bounds.keys()) == set(c.udims)\n        assert_equal(bounds[\"lat\"], c[\"lat\"].bounds)\n        assert_equal(bounds[\"lon\"], c[\"lon\"].bounds)\n        assert_equal(bounds[\"time\"], c[\"time\"].bounds)",
            "class TestCoordinatesDict(object):\n    coords = Coordinates([[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat_lon\", \"time\"])",
            "def test_keys(self):\n        assert set(self.coords.keys()) == {\"lat_lon\", \"time\"}",
            "def test_values(self):\n        values = list(self.coords.values())\n        assert len(values) == 2\n        assert self.coords[\"lat_lon\"] in values\n        assert self.coords[\"time\"] in values",
            "def test_items(self):\n        keys, values = zip(*self.coords.items())\n        assert set(keys) == {\"lat_lon\", \"time\"}\n        assert len(values) == 2\n        assert self.coords[\"lat_lon\"] in values\n        assert self.coords[\"time\"] in values",
            "def test_iter(self):\n        assert set(self.coords) == {\"lat_lon\", \"time\"}",
            "def test_getitem(self):\n        lat = ArrayCoordinates1d([0, 1, 2], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\"], name=\"time\")\n        lat_lon = StackedCoordinates([lat, lon])\n        coords = Coordinates([lat_lon, time])\n\n        assert coords[\"lat_lon\"] == lat_lon\n        assert coords[\"time\"] == time\n        assert coords[\"lat\"] == lat\n        assert coords[\"lon\"] == lon\n\n        with pytest.raises(KeyError, match=\"Dimension 'alt' not found in Coordinates\"):\n            coords[\"alt\"]",
            "def test_get(self):\n        assert self.coords.get(\"lat_lon\") is self.coords[\"lat_lon\"]\n        assert self.coords.get(\"lat\") is self.coords[\"lat\"]\n        assert self.coords.get(\"alt\") == None\n        assert self.coords.get(\"alt\", \"DEFAULT\") == \"DEFAULT\"",
            "def test_setitem(self):\n        coords = deepcopy(self.coords)\n\n        coords[\"time\"] = [1, 2, 3]\n        coords[\"time\"] = ArrayCoordinates1d([1, 2, 3])\n        coords[\"time\"] = ArrayCoordinates1d([1, 2, 3], name=\"time\")\n        coords[\"time\"] = Coordinates([[1, 2, 3]], dims=[\"time\"])\n\n        # coords['lat_lon'] = [np.linspace(0, 10, 5), np.linspace(0, 10, 5)]\n        coords[\"lat_lon\"] = clinspace((0, 1), (10, 20), 5)\n        coords[\"lat_lon\"] = (np.linspace(0, 10, 5), np.linspace(0, 10, 5))\n        coords[\"lat_lon\"] = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lat_lon\"])\n\n        # update a single stacked dimension\n        coords[\"lat\"] = np.linspace(5, 20, 5)\n        assert coords[\"lat\"] == ArrayCoordinates1d(np.linspace(5, 20, 5), name=\"lat\")\n\n        coords = deepcopy(self.coords)\n        coords[\"lat_lon\"][\"lat\"] = np.linspace(5, 20, 3)\n        assert coords[\"lat\"] == ArrayCoordinates1d(np.linspace(5, 20, 3), name=\"lat\")\n\n        with pytest.raises(KeyError, match=\"Cannot set dimension\"):\n            coords[\"alt\"] = ArrayCoordinates1d([1, 2, 3], name=\"alt\")\n\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            coords[\"alt\"] = ArrayCoordinates1d([1, 2, 3], name=\"lat\")\n\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            coords[\"time\"] = ArrayCoordinates1d([1, 2, 3], name=\"alt\")\n\n        with pytest.raises(KeyError, match=\"not found in Coordinates\"):\n            coords[\"lat_lon\"] = Coordinates([(np.linspace(0, 10, 5), np.linspace(0, 10, 5))], dims=[\"lon_lat\"])\n\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            coords[\"lat_lon\"] = clinspace((0, 1), (10, 20), 5, name=\"lon_lat\")\n\n        with pytest.raises(ValueError, match=\"Shape mismatch\"):\n            coords[\"lat\"] = np.linspace(5, 20, 5)\n\n        with pytest.raises(ValueError, match=\"Dimension mismatch\"):\n            coords[\"lat\"] = clinspace(0, 10, 3, name=\"lon\")",
            "def test_delitem(self):\n        # unstacked\n        coords = deepcopy(self.coords)\n        del coords[\"time\"]\n        assert coords.dims == (\"lat_lon\",)\n\n        # stacked\n        coords = deepcopy(self.coords)\n        del coords[\"lat_lon\"]\n        assert coords.dims == (\"time\",)\n\n        # missing\n        coords = deepcopy(self.coords)\n        with pytest.raises(KeyError, match=\"Cannot delete dimension 'alt' in Coordinates\"):\n            del coords[\"alt\"]\n\n        # part of stacked dimension\n        coords = deepcopy(self.coords)\n        with pytest.raises(KeyError, match=\"Cannot delete dimension 'lat' in Coordinates\"):\n            del coords[\"lat\"]",
            "def test_update(self):\n        # add a new dimension\n        coords = deepcopy(self.coords)\n        c = Coordinates([[100, 200, 300]], dims=[\"alt\"])\n        coords.update(c)\n        assert coords.dims == (\"lat_lon\", \"time\", \"alt\")\n        assert coords[\"lat_lon\"] == self.coords[\"lat_lon\"]\n        assert coords[\"time\"] == self.coords[\"time\"]\n        assert coords[\"alt\"] == c[\"alt\"]\n\n        # overwrite a dimension\n        coords = deepcopy(self.coords)\n        c = Coordinates([[100, 200, 300]], dims=[\"time\"])\n        coords.update(c)\n        assert coords.dims == (\"lat_lon\", \"time\")\n        assert coords[\"lat_lon\"] == self.coords[\"lat_lon\"]\n        assert coords[\"time\"] == c[\"time\"]\n\n        # overwrite a stacked dimension\n        coords = deepcopy(self.coords)\n        c = Coordinates([clinspace((0, 1), (10, 20), 5)], dims=[\"lat_lon\"])\n        coords.update(c)\n        assert coords.dims == (\"lat_lon\", \"time\")\n        assert coords[\"lat_lon\"] == c[\"lat_lon\"]\n        assert coords[\"time\"] == self.coords[\"time\"]\n\n        # mixed\n        coords = deepcopy(self.coords)\n        c = Coordinates([clinspace((0, 1), (10, 20), 5), [100, 200, 300]], dims=[\"lat_lon\", \"alt\"])\n        coords.update(c)\n        assert coords.dims == (\"lat_lon\", \"time\", \"alt\")\n        assert coords[\"lat_lon\"] == c[\"lat_lon\"]\n        assert coords[\"time\"] == self.coords[\"time\"]\n        assert coords[\"alt\"] == c[\"alt\"]\n\n        # invalid\n        coords = deepcopy(self.coords)\n        with pytest.raises(TypeError, match=\"Cannot update Coordinates with object of type\"):\n            coords.update({\"time\": [1, 2, 3]})\n\n        # duplicate dimension\n        coords = deepcopy(self.coords)\n        c = Coordinates([[0, 0.1, 0.2]], dims=[\"lat\"])\n        with pytest.raises(ValueError, match=\"Duplicate dimension 'lat'\"):\n            coords.update(c)",
            "def test_len(self):\n        assert len(self.coords) == 2",
            "class TestCoordinatesIndexing(object):",
            "def test_get_index(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = Coordinates([lat, lon, time])\n\n        I = [2, 1, 3]\n        J = slice(1, 3)\n        K = 1\n\n        # full\n        c2 = c[I, J, K]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 2, 1)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[J])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates[K])\n\n        # partial\n        c2 = c[I, J]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 2, 3)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[J])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates)\n\n        c2 = c[I]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 4, 3)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates)\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates)",
            "def test_get_index_stacked(self):\n        lat = [0, 1, 2, 3, 4]\n        lon = [10, 20, 30, 40, 50]\n        dates = [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"]\n\n        c = Coordinates(\n            [\n                StackedCoordinates([ArrayCoordinates1d(lat, name=\"lat\"), ArrayCoordinates1d(lon, name=\"lon\")]),\n                ArrayCoordinates1d(dates, name=\"time\"),\n            ]\n        )\n\n        I = [2, 1, 3]\n        J = slice(1, 3)\n\n        # full\n        c2 = c[I, J]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 2)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[I])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates[J])\n\n        # partial\n        c2 = c[I]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 4)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[I])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates)",
            "def test_get_index_stacked_shaped(self):\n        lat = np.linspace(0, 1, 20).reshape((5, 4))\n        lon = np.linspace(10, 20, 20).reshape((5, 4))\n        dates = [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"]\n\n        c = Coordinates([StackedCoordinates([lat, lon], dims=[\"lat\", \"lon\"]), ArrayCoordinates1d(dates, name=\"time\")])\n\n        I = [2, 1, 3]\n        J = slice(1, 3)\n        K = 1\n\n        # full\n        c2 = c[I, J, K]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 2, 1)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I, J])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[I, J])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates[K])\n\n        # partial\n        c2 = c[I, J]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 2, 4)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I, J])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[I, J])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates)\n\n        c2 = c[I]\n        assert isinstance(c2, Coordinates)\n        assert c2.shape == (3, 4, 4)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, c[\"lat\"].coordinates[I])\n        assert_equal(c2[\"lon\"].coordinates, c[\"lon\"].coordinates[I])\n        assert_equal(c2[\"time\"].coordinates, c[\"time\"].coordinates)",
            "def test_get_index_properties(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"], name=\"time\")\n        c = Coordinates([lat, lon, time], crs=\"EPSG:2193\")\n\n        I = [2, 1, 3]\n        J = slice(1, 3)\n        K = 1\n\n        c2 = c[I, J, K]\n        assert c2.crs == c.crs",
            "class TestCoordinatesMethods(object):\n    coords = Coordinates(\n        [[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"], 10],\n        dims=[\"lat_lon\", \"time\", \"alt\"],\n        crs=\"+proj=merc +vunits=us-ft\",\n    )",
            "def test_drop(self):\n        # drop one existing dimension\n        c1 = self.coords.drop(\"time\")\n        c2 = self.coords.udrop(\"time\")\n        assert c1.dims == (\"lat_lon\", \"alt\")\n        assert c2.dims == (\"lat_lon\", \"alt\")\n\n        # drop multiple existing dimensions\n        c1 = self.coords.drop([\"time\", \"alt\"])\n        c2 = self.coords.udrop([\"time\", \"alt\"])\n        assert c1.dims == (\"lat_lon\",)\n        assert c2.dims == (\"lat_lon\",)\n\n        # drop all dimensions\n        c1 = self.coords.drop(self.coords.dims)\n        c2 = self.coords.udrop(self.coords.udims)\n        assert c1.dims == ()\n        assert c2.dims == ()\n\n        # drop no dimensions\n        c1 = self.coords.drop([])\n        c2 = self.coords.udrop([])\n        assert c1.dims == (\"lat_lon\", \"time\", \"alt\")\n        assert c2.dims == (\"lat_lon\", \"time\", \"alt\")\n\n        # drop a missing dimension\n        c = self.coords.drop(\"alt\")\n        with pytest.raises(KeyError, match=\"Dimension 'alt' not found in Coordinates with dims\"):\n            c1 = c.drop(\"alt\")\n        with pytest.raises(KeyError, match=\"Dimension 'alt' not found in Coordinates with udims\"):\n            c2 = c.udrop(\"alt\")\n\n        c1 = c.drop(\"alt\", ignore_missing=True)\n        c2 = c.udrop(\"alt\", ignore_missing=True)\n        assert c1.dims == (\"lat_lon\", \"time\")\n        assert c2.dims == (\"lat_lon\", \"time\")\n\n        # drop a stacked dimension: drop works but udrop gives an exception\n        c1 = self.coords.drop(\"lat_lon\")\n        assert c1.dims == (\"time\", \"alt\")\n\n        with pytest.raises(KeyError, match=\"Dimension 'lat_lon' not found in Coordinates with udims\"):\n            c2 = self.coords.udrop(\"lat_lon\")\n\n        # drop part of a stacked dimension: drop gives exception but udrop does not\n        # note: two udrop cases: 'lat_lon' -> 'lon' and 'lat_lon_alt' -> 'lat_lon'\n        with pytest.raises(KeyError, match=\"Dimension 'lat' not found in Coordinates with dims\"):\n            c1 = self.coords.drop(\"lat\")\n\n        c2 = self.coords.udrop(\"lat\")\n        assert c2.dims == (\"lon\", \"time\", \"alt\")\n\n        coords = Coordinates([[[0, 1], [10, 20], [100, 300]]], dims=[\"lat_lon_alt\"], crs=\"+proj=merc +vunits=us-ft\")\n        c2 = coords.udrop(\"alt\")\n        assert c2.dims == (\"lat_lon\",)",
            "def test_drop_invalid(self):\n        with pytest.raises(TypeError, match=\"Invalid drop dimension type\"):\n            self.coords.drop(2)\n\n        with pytest.raises(TypeError, match=\"Invalid drop dimension type\"):\n            self.coords.udrop(2)\n\n        with pytest.raises(TypeError, match=\"Invalid drop dimension type\"):\n            self.coords.drop([2, 3])\n\n        with pytest.raises(TypeError, match=\"Invalid drop dimension type\"):\n            self.coords.udrop([2, 3])",
            "def test_drop_properties(self):\n        coords = Coordinates(\n            [[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"], 10],\n            dims=[\"lat_lon\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n\n        c1 = coords.drop(\"time\")\n        c2 = coords.udrop(\"time\")\n\n        # check properties\n        assert c1.crs == \"+proj=merc +vunits=us-ft\"\n        assert c2.crs == \"+proj=merc +vunits=us-ft\"",
            "def test_unique(self):\n        # unstacked (numerical, datetime, and empty)\n        c = Coordinates(\n            [[2, 1, 0, 1], [\"2018-01-01\", \"2018-01-02\", \"2018-01-01\"], []],\n            dims=[\"lat\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n        c2 = c.unique()\n        assert_equal(c2[\"lat\"].coordinates, [0, 1, 2])\n        assert_equal(c2[\"time\"].coordinates, [np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-02\")])\n        assert_equal(c2[\"alt\"].coordinates, [])\n\n        # return indices\n        c = Coordinates(\n            [[2, 1, 0, 1], [\"2018-01-01\", \"2018-01-02\", \"2018-01-01\"], []],\n            dims=[\"lat\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n        c2, I = c.unique(return_index=True)\n        assert_equal(c2[\"lat\"].coordinates, [0, 1, 2])\n        assert_equal(c2[\"time\"].coordinates, [np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-02\")])\n        assert_equal(c2[\"alt\"].coordinates, [])\n        assert c2 == c[I]\n\n        # stacked\n        lat_lon = [(0, 0), (0, 1), (0, 2), (0, 2), (1, 0), (1, 1), (1, 1)]\n        lat, lon = zip(*lat_lon)\n        c = Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        c2 = c.unique()\n        assert_equal(c2[\"lat\"].coordinates, [0.0, 0.0, 0.0, 1.0, 1.0])\n        assert_equal(c2[\"lon\"].coordinates, [0.0, 1.0, 2.0, 0.0, 1.0])\n\n        # empty\n        c = Coordinates([])\n        c2 = c.unique()\n        assert c2.size == 0\n\n        c2, I = c.unique(return_index=True)\n        assert c2.size == 0\n        assert c2 == c[I]",
            "def test_unique_properties(self):\n        c = Coordinates(\n            [[2, 1, 0, 1], [\"2018-01-01\", \"2018-01-02\", \"2018-01-01\"], []],\n            dims=[\"lat\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n        c2 = c.unique()\n\n        # check properties\n        assert c2.crs == \"+proj=merc +vunits=us-ft\"",
            "def test_unstack(self):\n        c1 = Coordinates([[[0, 1], [10, 20], [100, 300]]], dims=[\"lat_lon_alt\"], crs=\"+proj=merc +vunits=us-ft\")\n        c2 = c1.unstack()\n        assert c1.dims == (\"lat_lon_alt\",)\n        assert c2.dims == (\"lat\", \"lon\", \"alt\")\n        assert c1[\"lat\"] == c2[\"lat\"]\n        assert c1[\"lon\"] == c2[\"lon\"]\n        assert c1[\"alt\"] == c2[\"alt\"]\n\n        # mixed\n        c1 = Coordinates([[[0, 1], [10, 20]], [100, 200, 300]], dims=[\"lat_lon\", \"alt\"], crs=\"+proj=merc +vunits=us-ft\")\n        c2 = c1.unstack()\n        assert c1.dims == (\"lat_lon\", \"alt\")\n        assert c2.dims == (\"lat\", \"lon\", \"alt\")\n        assert c1[\"lat\"] == c2[\"lat\"]\n        assert c1[\"lon\"] == c2[\"lon\"]\n        assert c1[\"alt\"] == c2[\"alt\"]",
            "def test_unstack_properties(self):\n        c1 = Coordinates([[[0, 1], [10, 20], [100, 300]]], dims=[\"lat_lon_alt\"], crs=\"+proj=merc +vunits=us-ft\")\n        c2 = c1.unstack()\n\n        # check properties\n        assert c2.crs == \"+proj=merc +vunits=us-ft\"",
            "def test_iterchunks(self):\n        c = Coordinates(\n            [clinspace(0, 1, 100), clinspace(0, 1, 200), [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"]\n        )\n\n        for chunk in c.iterchunks(shape=(10, 10, 10)):\n            assert isinstance(chunk, Coordinates)\n            assert chunk.dims == c.dims\n            assert chunk.shape == (10, 10, 2)\n\n        for chunk, slices in c.iterchunks(shape=(10, 10, 10), return_slices=True):\n            assert isinstance(chunk, Coordinates)\n            assert chunk.dims == c.dims\n            assert chunk.shape == (10, 10, 2)\n\n            assert isinstance(slices, tuple)\n            assert len(slices) == 3\n            assert all(isinstance(slc, slice) for slc in slices)",
            "def test_iterchunks_properties(self):\n        c = Coordinates(\n            [clinspace(0, 1, 100), clinspace(0, 1, 200), [\"2018-01-01\", \"2018-01-02\"]],\n            dims=[\"lat\", \"lon\", \"time\"],\n            crs=\"EPSG:2193\",\n        )\n\n        for chunk in c.iterchunks(shape=(10, 10, 10)):\n            # check properties\n            assert chunk.crs == \"EPSG:2193\"",
            "def test_tranpose(self):\n        c = Coordinates([[0, 1], [10, 20], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"])\n\n        # transpose\n        t = c.transpose(\"lon\", \"lat\", \"time\")\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert t.dims == (\"lon\", \"lat\", \"time\")\n\n        # default: full transpose\n        t = c.transpose()\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert t.dims == (\"time\", \"lon\", \"lat\")\n\n        # in place\n        t = c.transpose(\"lon\", \"lat\", \"time\", in_place=False)\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n        assert t.dims == (\"lon\", \"lat\", \"time\")\n\n        c.transpose(\"lon\", \"lat\", \"time\", in_place=True)\n        assert c.dims == (\"lon\", \"lat\", \"time\")\n\n        with pytest.raises(ValueError, match=\"Invalid transpose dimensions\"):\n            c.transpose(\"lon\", \"lat\")\n\n        with pytest.raises(ValueError, match=\"Invalid transpose dimensions\"):\n            c.transpose(\"lat\", \"lon\", \"alt\")",
            "def test_transpose_stacked_shaped(self):\n        lat = np.linspace(0, 1, 12).reshape((3, 4))\n        lon = np.linspace(10, 20, 12).reshape((3, 4))\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n        c = Coordinates([[lat, lon], dates], dims=[\"lat_lon\", \"time\"])\n\n        t = c.transpose(\"time\", \"lon_lat\", in_place=False)\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert t.dims == (\"time\", \"lon_lat\")\n\n        c.transpose(\"time\", \"lon_lat\", in_place=True)\n        assert c.dims == (\"time\", \"lon_lat\")",
            "def test_transpose_stacked(self):\n        lat = np.linspace(0, 1, 12)\n        lon = np.linspace(10, 20, 12)\n        dates = [\"2018-01-01\", \"2018-01-02\"]\n        c = Coordinates([[lat, lon], dates], dims=[\"lat_lon\", \"time\"])\n\n        t = c.transpose(\"time\", \"lon_lat\", in_place=False)\n        assert c.dims == (\"lat_lon\", \"time\")\n        assert t.dims == (\"time\", \"lon_lat\")\n\n        c.transpose(\"time\", \"lon_lat\", in_place=True)\n        assert c.dims == (\"time\", \"lon_lat\")",
            "def test_select_single(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = Coordinates([lat, lon, time])\n\n        # single dimension\n        s = c.select({\"lat\": [0.5, 2.5]})\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"][1:3]\n        assert s[\"lon\"] == c[\"lon\"]\n        assert s[\"time\"] == c[\"time\"]\n\n        s, I = c.select({\"lat\": [0.5, 2.5]}, return_index=True)\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"][1:3]\n        assert s[\"lon\"] == c[\"lon\"]\n        assert s[\"time\"] == c[\"time\"]\n        assert s == c[I]\n\n        # a different single dimension\n        s = c.select({\"lon\": [5, 25]})\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"]\n        assert s[\"lon\"] == c[\"lon\"][0:2]\n        assert s[\"time\"] == c[\"time\"]\n\n        s, I = c.select({\"lon\": [5, 25]}, return_index=True)\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"]\n        assert s[\"lon\"] == c[\"lon\"][0:2]\n        assert s[\"time\"] == c[\"time\"]\n        assert s == c[I]\n\n        # outer\n        s = c.select({\"lat\": [0.5, 2.5]}, outer=True)\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"][0:4]\n        assert s[\"lon\"] == c[\"lon\"]\n        assert s[\"time\"] == c[\"time\"]\n\n        s, I = c.select({\"lat\": [0.5, 2.5]}, outer=True, return_index=True)\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"][0:4]\n        assert s[\"lon\"] == c[\"lon\"]\n        assert s[\"time\"] == c[\"time\"]\n        assert s == c[I]\n\n        # no matching dimension\n        s = c.select({\"alt\": [0, 10]})\n        assert s == c\n\n        s, I = c.select({\"alt\": [0, 10]}, return_index=True)\n        assert s == c[I]\n        assert s == c",
            "def test_select_multiple(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 4, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40, 50, 60], name=\"lon\")\n        c = Coordinates([lat, lon])\n\n        s = c.select({\"lat\": [0.5, 3.5], \"lon\": [25, 55]})\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"][1:4]\n        assert s[\"lon\"] == c[\"lon\"][2:5]\n\n        s, I = c.select({\"lat\": [0.5, 3.5], \"lon\": [25, 55]}, return_index=True)\n        assert isinstance(s, Coordinates)\n        assert s.dims == c.dims\n        assert s[\"lat\"] == c[\"lat\"][1:4]\n        assert s[\"lon\"] == c[\"lon\"][2:5]\n        assert s == c[I]",
            "def test_select_properties(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40], name=\"lon\")\n        time = ArrayCoordinates1d([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"], name=\"time\")\n        c = Coordinates([lat, lon, time], crs=\"EPSG:2193\")\n\n        s = c.select({\"lat\": [0.5, 2.5]})\n\n        # check properties\n        assert s.crs == \"EPSG:2193\"",
            "def test_intersect(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 4, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40, 50, 60], name=\"lon\")\n        c = Coordinates([lat, lon])\n\n        other_lat = ArrayCoordinates1d([0.5, 2.5, 3.5], name=\"lat\")\n        other_lon = ArrayCoordinates1d([25, 35, 55], name=\"lon\")\n\n        other = Coordinates([other_lat, other_lon])\n        c2 = c.intersect(other)\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][1:4]\n        assert c2[\"lon\"] == c[\"lon\"][2:5]\n\n        c2, I = c.intersect(other, return_index=True)\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][1:4]\n        assert c2[\"lon\"] == c[\"lon\"][2:5]\n        assert c2 == c[I]\n\n        other = Coordinates([other_lat, other_lon])\n        c2 = c.intersect(other, outer=True)\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][0:5]\n        assert c2[\"lon\"] == c[\"lon\"][1:6]\n\n        # missing dimension\n        other = Coordinates([other_lat])\n        c2 = c.intersect(other)\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][1:4]\n        assert c2[\"lon\"] == c[\"lon\"]\n\n        other = Coordinates([other_lon])\n        c2 = c.intersect(other)\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"]\n        assert c2[\"lon\"] == c[\"lon\"][2:5]\n\n        # extra dimension\n        other = Coordinates([other_lat, other_lon, [0, 1, 2]], dims=[\"lat\", \"lon\", \"time\"])\n        c2 = c.intersect(other)\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][1:4]\n        assert c2[\"lon\"] == c[\"lon\"][2:5]\n\n        # Confusing time intersection\n        ct = Coordinates([[\"2012-05-19T12:00:00\", \"2012-05-19T13:00:00\", \"2012-05-20T14:00:00\"]], [\"time\"])\n        cti = Coordinates([[\"2012-05-18\", \"2012-05-19\"]], [\"time\"])\n        ct2 = ct.intersect(cti, outer=True)\n        assert ct2.size == 3\n\n        ct2 = ct.intersect(cti, outer=False)\n        assert ct2.size == 0  # Is this behavior desired?",
            "def test_intersect_dims(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 4, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40, 50, 60], name=\"lon\")\n        c = Coordinates([lat, lon])\n\n        other_lat = ArrayCoordinates1d([0.5, 2.5, 3.5], name=\"lat\")\n        other_lon = ArrayCoordinates1d([25, 35, 55], name=\"lon\")\n        other = Coordinates([other_lat, other_lon])\n\n        c2 = c.intersect(other, dims=[\"lat\", \"lon\"])\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][1:4]\n        assert c2[\"lon\"] == c[\"lon\"][2:5]\n\n        c2 = c.intersect(other, dims=[\"lat\"])\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"][1:4]\n        assert c2[\"lon\"] == c[\"lon\"]\n\n        c2 = c.intersect(other, dims=[\"lon\"])\n        assert isinstance(c2, Coordinates)\n        assert c2.dims == c.dims\n        assert c2[\"lat\"] == c[\"lat\"]\n        assert c2[\"lon\"] == c[\"lon\"][2:5]",
            "def test_intersect_crs(self):\n        # should change the other coordinates crs into the coordinates crs for intersect\n        c = Coordinates(\n            [np.linspace(0, 10, 11), np.linspace(0, 10, 11), [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"]\n        )\n        o = Coordinates(\n            [np.linspace(28000000, 29500000, 20), np.linspace(-280000, 400000, 20), [\"2018-01-01\", \"2018-01-02\"]],\n            dims=[\"lat\", \"lon\", \"time\"],\n            crs=\"EPSG:2193\",\n        )\n\n        c_int = c.intersect(o)\n        assert c_int.crs == c.crs\n        assert o.crs == \"EPSG:2193\"  # didn't get changed\n        assert np.all(c_int[\"lat\"].bounds == np.array([5.0, 10.0]))\n        assert np.all(c_int[\"lon\"].bounds == np.array([4.0, 10.0]))\n        assert np.all(c_int[\"time\"] == c[\"time\"])",
            "def test_intersect_invalid(self):\n        lat = ArrayCoordinates1d([0, 1, 2, 3, 4, 5], name=\"lat\")\n        lon = ArrayCoordinates1d([10, 20, 30, 40, 50, 60], name=\"lon\")\n        c = Coordinates([lat, lon])\n\n        with pytest.raises(TypeError, match=\"Coordinates cannot be intersected with type\"):\n            c.intersect(lat)\n\n        with pytest.raises(TypeError, match=\"Coordinates cannot be intersected with type\"):\n            c.intersect({\"lat\": [0, 1]})",
            "def test_issubset(self):\n        c1 = Coordinates([[0, 1, 2, 3], [10, 20, 30, 40]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[1, 2, 3, 4], [10, 20, 30, 40]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([[1, 3], [40, 30, 20, 10]], dims=[\"lat\", \"lon\"])\n\n        # self\n        assert c1.issubset(c1)\n        assert c2.issubset(c2)\n        assert c3.issubset(c3)\n\n        # other\n        assert not c1.issubset(c2)\n        assert not c1.issubset(c3)\n        assert not c2.issubset(c1)\n        assert not c2.issubset(c3)\n        assert c3.issubset(c1)\n        assert c3.issubset(c2)\n\n        # missing dims\n        c4 = c1.drop(\"lat\")\n        assert not c1.issubset(c4)\n        assert not c4.issubset(c1)",
            "def test_issubset_stacked(self):\n        lat1, lon1 = [0, 1, 2, 3], [10, 20, 30, 40]\n        u1 = Coordinates([lat1, lon1], dims=[\"lat\", \"lon\"])\n        s1 = Coordinates([[lat1, lon1]], dims=[\"lat_lon\"])\n\n        lat2, lon2 = [1, 3], [20, 40]\n        u2 = Coordinates([lat2, lon2], dims=[\"lat\", \"lon\"])\n        s2 = Coordinates([[lat2, lon2]], dims=[\"lat_lon\"])\n\n        lat3, lon3 = [1, 3], [40, 20]\n        u3 = Coordinates([lat3, lon3], dims=[\"lat\", \"lon\"])\n        s3 = Coordinates([[lat3, lon3]], dims=[\"lat_lon\"])\n\n        # stacked issubset of stacked: must check stacked dims together\n        assert s1.issubset(s1)\n        assert s2.issubset(s1)\n        assert not s1.issubset(s2)\n        assert not s3.issubset(s1)  # this is an important case because the udims are all subsets\n\n        # stacked issubset of unstacked: check udims individually\n        assert s1.issubset(u1)\n        assert s2.issubset(u2)\n        assert s3.issubset(u3)\n\n        assert s2.issubset(u1)\n        assert s3.issubset(u1)\n\n        assert not s1.issubset(u2)\n\n        # unstacked issubset of stacked: must check other's stacked dims together\n        assert not u1.issubset(s1)\n        assert not u2.issubset(s2)\n        assert not u3.issubset(s3)\n\n        # unstacked issubset of stacked: sometimes it is a subset, not yet implemented\n        # lat, lon = np.meshgrid(lat1, lon1)\n        # s = Coordinates([[lat.flatten(), lon.flatten()]], dims=['lat_lon'])\n        # assert u1.issubset(s)\n        # assert u2.issubset(s)\n        # assert u3.issubset(s)",
            "def test_issubset_stacked_shaped(self):\n        lat1, lon1 = np.array([0, 1, 2, 3]), np.array([10, 20, 30, 40])\n        u1 = Coordinates([lat1, lon1], dims=[\"lat\", \"lon\"])\n        d1 = Coordinates([[lat1.reshape((2, 2)), lon1.reshape((2, 2))]], dims=[\"lat_lon\"])\n\n        lat2, lon2 = np.array([1, 3]), np.array([20, 40])\n        u2 = Coordinates([lat2, lon2], dims=[\"lat\", \"lon\"])\n        d2 = Coordinates([[lat2.reshape((2, 1)), lon2.reshape((2, 1))]], dims=[\"lat_lon\"])\n\n        lat3, lon3 = np.array([1, 3]), np.array([40, 20])\n        u3 = Coordinates([lat3, lon3], dims=[\"lat\", \"lon\"])\n        d3 = Coordinates([[lat3.reshape((2, 1)), lon3.reshape((2, 1))]], dims=[\"lat_lon\"])\n\n        # dependent issubset of dependent: must check dependent dims together\n        assert d1.issubset(d1)\n        assert d2.issubset(d1)\n        assert not d1.issubset(d2)\n        assert not d3.issubset(d1)  # this is an important case because the udims are all subsets\n\n        # dependent issubset of unstacked: check udims individually\n        assert d1.issubset(u1)\n        assert d2.issubset(u2)\n        assert d3.issubset(u3)\n\n        assert d2.issubset(u1)\n        assert d3.issubset(u1)\n\n        assert not d1.issubset(u2)\n\n        # unstacked issubset of dependent: must check other's dependent dims together\n        assert not u1.issubset(d1)\n        assert not u2.issubset(d2)\n        assert not u3.issubset(d3)\n\n        # unstacked issubset of dependent: sometimes it is a subset, not yet implemented\n        # lat, lon = np.meshgrid(lat1, lon1)\n        # d = Coordinates([[lat, lon]], dims=['lat_lon'])\n        # assert u1.issubset(d)\n        # assert u2.issubset(d)\n        # assert u3.issubset(d)",
            "def test_issubset_time(self):\n        c1 = Coordinates([[\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"]], dims=[\"time\"])\n        c2 = Coordinates([[\"2020-01-02\", \"2020-01-03\"]], dims=[\"time\"])\n        c3 = Coordinates([[\"2020-01-01T00:00:00\", \"2020-01-02T00:00:00\", \"2020-01-03T00:00:00\"]], dims=[\"time\"])\n\n        # self\n        assert c1.issubset(c1)\n        assert c2.issubset(c2)\n        assert c3.issubset(c3)\n\n        # other\n        assert not c1.issubset(c2)\n        assert c1.issubset(c3)\n        assert c2.issubset(c1)\n        assert c2.issubset(c3)\n        assert c3.issubset(c1)\n        assert not c3.issubset(c2)",
            "class TestCoordinatesSpecial(object):",
            "def test_repr(self):\n        repr(Coordinates([[0, 1], [10, 20], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"]))\n        repr(Coordinates([[[0, 1], [10, 20]], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat_lon\", \"time\"]))\n        repr(Coordinates([[[[0, 1]], [[10, 20]]], [[\"2018-01-01\", \"2018-01-02\"]]], dims=[\"lat_lon\", \"time\"]))\n        repr(Coordinates([0, 10, []], dims=[\"lat\", \"lon\", \"time\"]))\n        repr(Coordinates([crange(0, 10, 0.5)], dims=[\"alt\"], crs=\"+proj=merc +vunits=us-ft\"))\n        repr(Coordinates([]))",
            "def test_eq_ne_hash(self):\n        c1 = Coordinates([[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat_lon\", \"time\"])\n        c2 = Coordinates([[[0, 2, 1], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat_lon\", \"time\"])\n        c3 = Coordinates([[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lon_lat\", \"time\"])\n        c4 = Coordinates([[[0, 1, 2], [10, 20, 30]], [\"2018-01-01\"]], dims=[\"lat_lon\", \"time\"])\n        c5 = Coordinates([[0, 1, 2], [10, 20, 30], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"])\n\n        # eq\n        assert c1 == c1\n        assert c1 == deepcopy(c1)\n\n        assert not c1 == None\n        assert not c1 == c2\n        assert not c1 == c3\n        assert not c1 == c4\n        assert not c1 == c5\n\n        # ne (this only matters in python 2)\n        assert not c1 != c1\n        assert not c1 != deepcopy(c1)\n\n        assert c1 != None\n        assert c1 != c3\n        assert c1 != c2\n        assert c1 != c4\n        assert c1 != c5\n\n        # hash\n        assert c1.hash == c1.hash\n        assert c1.hash == deepcopy(c1).hash\n\n        assert c1.hash != c3.hash\n        assert c1.hash != c2.hash\n        assert c1.hash != c4.hash\n        assert c1.hash != c5.hash",
            "def test_eq_ne_hash_crs(self):\n        lat = [0, 1, 2]\n        lon = [10, 20, 30]\n        c1 = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([lat, lon], dims=[\"lat\", \"lon\"], crs=\"EPSG:2193\")\n\n        # eq\n        assert not c1 == c2\n        assert c2 == deepcopy(c2)\n\n        # ne (this only matters in python 2)\n        assert c1 != c2\n        assert not c2 != deepcopy(c2)\n\n        # hash\n        assert c1.hash != c2.hash\n        assert c2.hash == deepcopy(c2).hash",
            "class TestCoordinatesFunctions(object):",
            "def test_merge_dims(self):\n        ctime = Coordinates([[\"2018-01-01\", \"2018-01-02\"]], dims=[\"time\"])\n        clatlon = Coordinates([[2, 4, 5], [3, -1, 5]], dims=[\"lat\", \"lon\"])\n        clatlon_stacked = Coordinates([[[2, 4, 5], [3, -1, 5]]], dims=[\"lat_lon\"])\n        clat = Coordinates([[2, 4, 5]], dims=[\"lat\"])\n\n        c = merge_dims([clatlon, ctime])\n        assert c.dims == (\"lat\", \"lon\", \"time\")\n\n        c = merge_dims([ctime, clatlon])\n        assert c.dims == (\"time\", \"lat\", \"lon\")\n\n        c = merge_dims([clatlon_stacked, ctime])\n        assert c.dims == (\"lat_lon\", \"time\")\n\n        c = merge_dims([ctime, clatlon_stacked])\n        assert c.dims == (\"time\", \"lat_lon\")\n\n        c = merge_dims([])\n        assert c.dims == ()\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension 'lat'\"):\n            merge_dims([clatlon, clat])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension 'lat'\"):\n            merge_dims([clatlon_stacked, clat])\n\n        with pytest.raises(TypeError, match=\"Cannot merge\"):\n            merge_dims([clat, 0])",
            "def test_merge_dims_crs(self):\n        clat = Coordinates([[2, 4, 5]], dims=[\"lat\"], crs=\"EPSG:4326\")\n        clon = Coordinates([[3, -1, 5]], dims=[\"lon\"], crs=\"EPSG:2193\")\n\n        with pytest.raises(ValueError, match=\"Cannot merge Coordinates\"):\n            merge_dims([clat, clon])",
            "def test_concat_and_union(self):\n        c1 = Coordinates([[2, 4, 5], [3, -1, 5]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[2, 3], [3, 0], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"])\n        c3 = Coordinates([[[2, 3], [3, 0]]], dims=[\"lat_lon\"])\n\n        c = concat([c1, c2])\n        assert c.shape == (5, 5, 2)\n\n        c = union([c1, c2])\n        assert c.shape == (4, 4, 2)\n\n        c = concat([])\n        assert c.dims == ()\n\n        c = union([])\n        assert c.dims == ()\n\n        with pytest.raises(TypeError, match=\"Cannot concat\"):\n            concat([c1, [1, 2]])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension 'lat' in dims\"):\n            concat([c1, c3])",
            "def test_concat_stacked_datetimes(self):\n        c1 = Coordinates([[0, 0.5, \"2018-01-01\"]], dims=[\"lat_lon_time\"])\n        c2 = Coordinates([[1, 1.5, \"2018-01-02\"]], dims=[\"lat_lon_time\"])\n        c = concat([c1, c2])\n        np.testing.assert_array_equal(c[\"lat\"].coordinates, np.array([0.0, 1.0]))\n        np.testing.assert_array_equal(c[\"lon\"].coordinates, np.array([0.5, 1.5]))\n        np.testing.assert_array_equal(\n            c[\"time\"].coordinates, np.array([\"2018-01-01\", \"2018-01-02\"]).astype(np.datetime64)\n        )\n\n        c1 = Coordinates([[0, 0.5, \"2018-01-01T01:01:01\"]], dims=[\"lat_lon_time\"])\n        c2 = Coordinates([[1, 1.5, \"2018-01-01T01:01:02\"]], dims=[\"lat_lon_time\"])\n        c = concat([c1, c2])\n        np.testing.assert_array_equal(c[\"lat\"].coordinates, np.array([0.0, 1.0]))\n        np.testing.assert_array_equal(c[\"lon\"].coordinates, np.array([0.5, 1.5]))\n        np.testing.assert_array_equal(\n            c[\"time\"].coordinates, np.array([\"2018-01-01T01:01:01\", \"2018-01-01T01:01:02\"]).astype(np.datetime64)\n        )",
            "def test_concat_crs(self):\n        c1 = Coordinates([[0, 0.5, \"2018-01-01\"]], dims=[\"lat_lon_time\"], crs=\"EPSG:4326\")\n        c2 = Coordinates([[1, 1.5, \"2018-01-02\"]], dims=[\"lat_lon_time\"], crs=\"EPSG:2193\")\n\n        with pytest.raises(ValueError, match=\"Cannot concat Coordinates\"):\n            concat([c1, c2])",
            "class TestCoordinatesGeoTransform(object):",
            "def test_uniform_working(self):\n        # order: -lat, lon\n        c = Coordinates([clinspace(1.5, 0.5, 5, \"lat\"), clinspace(1, 2, 9, \"lon\")])\n        c2 = Coordinates.from_geotransform(c.geotransform, c.shape)\n        c3 = Coordinates.from_xarray(podpac.Node().create_output_array(c))\n        assert c == c2\n        assert c == c3\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [c[\"lon\"].bounds[0] - c[\"lon\"].step / 2, c[\"lon\"].step, 0],\n                    [c[\"lat\"].bounds[1] - c[\"lat\"].step / 2, 0, c[\"lat\"].step],\n                ]\n            ),\n        )\n        # order: lon, lat\n        c = Coordinates([clinspace(0.5, 1.5, 5, \"lon\"), clinspace(1, 2, 9, \"lat\")])\n        c2 = Coordinates.from_geotransform(c.geotransform, c.shape)\n        c3 = Coordinates.from_xarray(podpac.Node().create_output_array(c))\n        assert c == c2\n        assert c == c3\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [c[\"lon\"].bounds[0] - c[\"lon\"].step / 2, 0, c[\"lon\"].step],\n                    [c[\"lat\"].bounds[0] - c[\"lat\"].step / 2, c[\"lat\"].step, 0],\n                ]\n            ),\n        )\n\n        # order: lon, -lat, time\n        c = Coordinates([clinspace(0.5, 1.5, 5, \"lon\"), clinspace(2, 1, 9, \"lat\"), crange(10, 11, 2, \"time\")])\n        c2 = Coordinates.from_geotransform(c.geotransform, c.drop(\"time\").shape)\n        assert c.drop(\"time\") == c2\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [c[\"lon\"].bounds[0] - c[\"lon\"].step / 2, 0, c[\"lon\"].step],\n                    [c[\"lat\"].bounds[1] - c[\"lat\"].step / 2, c[\"lat\"].step, 0],\n                ]\n            ),\n        )\n        # order: -lon, -lat, time, alt\n        c = Coordinates(\n            [\n                clinspace(1.5, 0.5, 5, \"lon\"),\n                clinspace(2, 1, 9, \"lat\"),\n                crange(10, 11, 2, \"time\"),\n                crange(10, 11, 2, \"alt\"),\n            ]\n        )\n        c2 = Coordinates.from_geotransform(c.geotransform, c.drop([\"time\", \"alt\"]).shape)\n        assert c.drop([\"time\", \"alt\"]) == c2\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [c[\"lon\"].bounds[1] - c[\"lon\"].step / 2, 0, c[\"lon\"].step],\n                    [c[\"lat\"].bounds[1] - c[\"lat\"].step / 2, c[\"lat\"].step, 0],\n                ]\n            ),\n        )",
            "def test_error_time_alt_too_big(self):\n        # time\n        c = Coordinates(\n            [\n                clinspace(1.5, 0.5, 5, \"lon\"),\n                clinspace(2, 1, 9, \"lat\"),\n                crange(1, 11, 2, \"time\"),\n                crange(1, 11, 2, \"alt\"),\n            ]\n        )\n        with pytest.raises(\n            TypeError, match='Only 2-D coordinates have a GDAL transform. This array has a \"time\" dimension of'\n        ):\n            c.geotransform\n        # alt\n        c = Coordinates([clinspace(1.5, 0.5, 5, \"lon\"), clinspace(2, 1, 9, \"lat\"), crange(1, 11, 2, \"alt\")])\n        with pytest.raises(\n            TypeError, match='Only 2-D coordinates have a GDAL transform. This array has a \"alt\" dimension of'\n        ):\n            c.geotransform\n\n    @pytest.mark.skip(reason=\"obsolete\")",
            "def rot_coords_working(self):\n        # order -lat, lon\n        rc = RotatedCoordinates(shape=(4, 3), theta=np.pi / 8, origin=[10, 20], step=[-2.0, 1.0], dims=[\"lat\", \"lon\"])\n        c = Coordinates([rc], dims=[\"lat,lon\"])\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [rc.origin[1] - rc.step[1] / 2, rc.step[1] * np.cos(rc.theta), -rc.step[0] * np.sin(rc.theta)],\n                    [rc.origin[0] - rc.step[0] / 2, rc.step[1] * np.sin(rc.theta), rc.step[0] * np.cos(rc.theta)],\n                ]\n            ),\n        )\n        # order lon, lat\n        rc = RotatedCoordinates(shape=(4, 3), theta=np.pi / 8, origin=[10, 20], step=[2.0, 1.0], dims=[\"lon\", \"lat\"])\n        c = Coordinates([rc], dims=[\"lon,lat\"])\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [rc.origin[0] - rc.step[0] / 2, rc.step[1] * np.sin(rc.theta), rc.step[0] * np.cos(rc.theta)],\n                    [rc.origin[1] - rc.step[1] / 2, rc.step[1] * np.cos(rc.theta), -rc.step[0] * np.sin(rc.theta)],\n                ]\n            ),\n        )\n\n        # order -lon, lat\n        rc = RotatedCoordinates(shape=(4, 3), theta=np.pi / 8, origin=[10, 20], step=[-2.0, 1.0], dims=[\"lon\", \"lat\"])\n        c = Coordinates([rc], dims=[\"lon,lat\"])\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [rc.origin[0] - rc.step[0] / 2, rc.step[1] * np.sin(rc.theta), rc.step[0] * np.cos(rc.theta)],\n                    [rc.origin[1] - rc.step[1] / 2, rc.step[1] * np.cos(rc.theta), -rc.step[0] * np.sin(rc.theta)],\n                ]\n            ),\n        )\n        # order -lat, -lon\n        rc = RotatedCoordinates(shape=(4, 3), theta=np.pi / 8, origin=[10, 20], step=[-2.0, -1.0], dims=[\"lat\", \"lon\"])\n        c = Coordinates([rc], dims=[\"lat,lon\"])\n        tf = np.array(c.geotransform).reshape(2, 3)\n        np.testing.assert_almost_equal(\n            tf,\n            np.array(\n                [\n                    [rc.origin[1] - rc.step[1] / 2, rc.step[1] * np.cos(rc.theta), -rc.step[0] * np.sin(rc.theta)],\n                    [rc.origin[0] - rc.step[0] / 2, rc.step[1] * np.sin(rc.theta), rc.step[0] * np.cos(rc.theta)],\n                ]\n            ),\n        )",
            "class TestCoordinatesMethodTransform(object):",
            "def test_transform(self):\n        c = Coordinates(\n            [[0, 1], [10, 20, 30, 40], [\"2018-01-01\", \"2018-01-02\"]], dims=[\"lat\", \"lon\", \"time\"], crs=\"EPSG:4326\"\n        )\n\n        # transform\n        t = c.transform(\"EPSG:2193\")\n        assert c.crs == \"EPSG:4326\"\n        assert t.crs == \"EPSG:2193\"\n        assert round(t[\"lat\"].coordinates[0, 0]) == 29995930.0\n\n        # no transform needed\n        t = c.transform(\"EPSG:4326\")\n        assert c.crs == \"EPSG:4326\"\n        assert t.crs == \"EPSG:4326\"\n        assert t is not c\n        assert t == c\n\n        # support proj4 strings\n        proj = \"+proj=merc +lat_ts=56.5 +ellps=GRS80\"\n        t = c.transform(proj)\n        assert c.crs == \"EPSG:4326\"\n        assert t.crs == proj\n        assert round(t[\"lat\"].coordinates[0]) == 0.0",
            "def test_transform_stacked(self):\n        c = Coordinates(\n            [[[0, 1], [10, 20]], [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"]], dims=[\"lat_lon\", \"time\"], crs=\"EPSG:4326\"\n        )\n\n        proj = \"+proj=merc +lat_ts=56.5 +ellps=GRS80\"\n        t = c.transform(proj)\n        assert c.crs == \"EPSG:4326\"\n        assert t.crs == proj\n        assert round(t[\"lat\"].coordinates[0]) == 0.0",
            "def test_transform_alt(self):\n        c = Coordinates(\n            [[0, 1], [10, 20, 30, 40], [\"2018-01-01\", \"2018-01-02\"], [100, 200, 300]],\n            dims=[\"lat\", \"lon\", \"time\", \"alt\"],\n            crs=\"+proj=merc +vunits=us-ft\",\n        )\n\n        proj = \"+proj=merc +vunits=m\"\n        t = c.transform(proj)\n        assert c.crs == \"+proj=merc +vunits=us-ft\"\n        assert t.crs == \"+proj=merc +vunits=m\"\n        np.testing.assert_array_almost_equal(t[\"lat\"].coordinates, c[\"lat\"].coordinates)\n        np.testing.assert_array_almost_equal(t[\"lon\"].coordinates, c[\"lon\"].coordinates)\n        assert t[\"time\"] == c[\"time\"]\n        np.testing.assert_array_almost_equal(t[\"alt\"].coordinates, 0.30480061 * c[\"alt\"].coordinates)",
            "def test_transform_uniform_to_uniform(self):\n        c = Coordinates([clinspace(-90, 90, 5, \"lat\"), clinspace(-180, 180, 11, \"lon\"), clinspace(0, 1, 5, \"time\")])\n        t = c.transform(\"EPSG:4269\")  # NAD 1983 uses same ellipsoid\n\n        assert isinstance(t[\"lat\"], UniformCoordinates1d)\n        assert isinstance(t[\"lon\"], UniformCoordinates1d)\n        assert t.crs == \"EPSG:4269\"\n        assert t.dims == c.dims\n\n        # Same thing, change the order of the inputs\n        c = Coordinates(\n            [clinspace(90, -90, 5, \"lat\"), clinspace(180, -180, 11, \"lon\"), clinspace(0, 1, 5, \"time\")][::-1]\n        )\n        t = c.transform(\"EPSG:4269\")  # NAD 1983 uses same ellipsoid\n\n        assert isinstance(t[\"lat\"], UniformCoordinates1d)\n        assert isinstance(t[\"lon\"], UniformCoordinates1d)\n        assert t.crs == \"EPSG:4269\"\n\n        assert t.dims == c.dims\n        for d in [\"lat\", \"lon\"]:\n            for a in [\"start\", \"stop\", \"step\"]:\n                np.testing.assert_almost_equal(getattr(c[d], a), getattr(t[d], a))",
            "def test_transform_uniform_stacked(self):\n        # TODO: Fix this test\n        c = Coordinates(\n            [[clinspace(-90, 90, 11, \"lat\"), clinspace(-180, 180, 11, \"lon\")], clinspace(0, 1, 5, \"time\")],\n            [[\"lat\", \"lon\"], \"time\"],\n        )\n        t = c.transform(\"EPSG:4269\")  # NAD 1983 uses same ellipsoid\n\n        assert isinstance(t[\"lat\"], UniformCoordinates1d)\n        assert isinstance(t[\"lon\"], UniformCoordinates1d)\n        np.testing.assert_array_almost_equal(t[\"lat\"].coordinates, c[\"lat\"].coordinates)\n        np.testing.assert_array_almost_equal(t[\"lon\"].coordinates, c[\"lon\"].coordinates)",
            "def test_transform_uniform_to_array(self):\n        c = Coordinates([clinspace(-45, 45, 5, \"lat\"), clinspace(-180, 180, 11, \"lon\")])\n\n        # Ok for array coordinates\n        t = c.transform(\"EPSG:3395\")\n\n        assert isinstance(t[\"lat\"], ArrayCoordinates1d)\n        assert isinstance(t[\"lon\"], UniformCoordinates1d)\n        assert t[\"lon\"].is_descending == c[\"lon\"].is_descending\n        assert t[\"lat\"].is_descending == c[\"lat\"].is_descending\n\n        t2 = t.transform(c.crs)\n\n        for d in [\"lon\", \"lat\"]:\n            for a in [\"start\", \"stop\", \"step\"]:\n                np.testing.assert_almost_equal(getattr(c[d], a), getattr(t2[d], a))\n\n        # Reverse the order of the coordinates\n        c = Coordinates([clinspace(45, -45, 5, \"lat\"), clinspace(180, -180, 11, \"lon\")])\n\n        # Ok for array coordinates\n        t = c.transform(\"EPSG:3395\")\n\n        assert isinstance(t[\"lat\"], ArrayCoordinates1d)\n        assert isinstance(t[\"lon\"], UniformCoordinates1d)\n        assert t[\"lon\"].is_descending == c[\"lon\"].is_descending\n        assert t[\"lat\"].is_descending == c[\"lat\"].is_descending\n\n        t2 = t.transform(c.crs)\n\n        for d in [\"lon\", \"lat\"]:\n            for a in [\"start\", \"stop\", \"step\"]:\n                np.testing.assert_almost_equal(getattr(c[d], a), getattr(t2[d], a))",
            "def test_transform_uniform_to_stacked_to_uniform(self):\n        c = Coordinates([clinspace(50, 45, 7, \"lat\"), clinspace(70, 75, 11, \"lon\")])\n\n        # Ok for array coordinates\n        t = c.transform(\"EPSG:32629\")\n        assert \"lat_lon\" in t.dims\n\n        t2 = t.transform(c.crs)\n\n        np.testing.assert_allclose(t2[\"lat\"].start, c[\"lat\"].start)\n        np.testing.assert_allclose(t2[\"lat\"].stop, c[\"lat\"].stop)\n        np.testing.assert_allclose(t2[\"lat\"].step, c[\"lat\"].step)\n        np.testing.assert_allclose(t2[\"lon\"].start, c[\"lon\"].start)\n        np.testing.assert_allclose(t2[\"lon\"].stop, c[\"lon\"].stop)\n        np.testing.assert_allclose(t2[\"lon\"].step, c[\"lon\"].step)\n\n        # TODO JXM test this with time, alt, etc",
            "def test_transform_stacked_to_stacked(self):\n        c = Coordinates([[np.array([[1, 2, 3], [4, 5, 6]]), np.array([[7, 8, 9], [10, 11, 12]])]], [\"lat_lon\"])\n        c2 = Coordinates([[np.array([1, 2, 3, 4, 5, 6]), np.array([7, 8, 9, 10, 11, 12])]], [\"lat_lon\"])\n\n        # Ok for array coordinates\n        t = c.transform(\"EPSG:32629\")\n        assert \"lat_lon\" in t.dims\n        t_s = c2.transform(\"EPSG:32629\")\n        assert \"lat_lon\" in t_s.dims\n\n        for d in [\"lat\", \"lon\"]:\n            np.testing.assert_almost_equal(t[d].coordinates.ravel(), t_s[d].coordinates.ravel())\n\n        t2 = t.transform(c.crs)\n        t2_s = t_s.transform(c.crs)\n\n        for d in [\"lat\", \"lon\"]:\n            np.testing.assert_almost_equal(t2[d].coordinates, c[d].coordinates)\n            np.testing.assert_almost_equal(t2_s[d].coordinates, c2[d].coordinates)\n\n        # Reverse order\n        c = Coordinates([[np.array([[1, 2, 3], [4, 5, 6]]), np.array([[7, 8, 9], [10, 11, 12]])]], [\"lon_lat\"])\n        c2 = Coordinates([[np.array([1, 2, 3, 4, 5, 6]), np.array([7, 8, 9, 10, 11, 12])]], [\"lon_lat\"])\n\n        # Ok for array coordinates\n        t = c.transform(\"EPSG:32629\")\n        assert \"lon_lat\" in t.dims\n        t_s = c2.transform(\"EPSG:32629\")\n        assert \"lon_lat\" in t_s.dims\n\n        for d in [\"lat\", \"lon\"]:\n            np.testing.assert_almost_equal(t[d].coordinates.ravel(), t_s[d].coordinates.ravel())\n\n        t2 = t.transform(c.crs)\n        t2_s = t_s.transform(c.crs)\n\n        for d in [\"lat\", \"lon\"]:\n            np.testing.assert_almost_equal(t2[d].coordinates, c[d].coordinates)\n            np.testing.assert_almost_equal(t2_s[d].coordinates, c2[d].coordinates)",
            "def test_transform_missing_lat_lon(self):\n        with pytest.raises(ValueError, match=\"Cannot transform lat coordinates without lon coordinates\"):\n            grid_coords = Coordinates([np.linspace(-10, 10, 21)], dims=[\"lat\"])\n            grid_coords.transform(crs=\"EPSG:2193\")\n\n        with pytest.raises(ValueError, match=\"Cannot transform lon coordinates without lat coordinates\"):\n            stack_coords = Coordinates([(np.linspace(-10, 10, 21), np.linspace(-30, -10, 21))], dims=[\"lon_time\"])\n            stack_coords.transform(crs=\"EPSG:2193\")\n\n        with pytest.raises(ValueError, match=\"nonadjacent lat and lon\"):\n            grid_coords = Coordinates([np.linspace(-10, 10, 21), [1], [1, 2, 3]], dims=[\"lat\", \"time\", \"lon\"])\n            grid_coords.transform(crs=\"EPSG:2193\")",
            "def test_transform_same_crs_same_result(self):\n        c1 = Coordinates(\n            [[1, 2, 4], clinspace(0, 4, 4)], dims=[\"lat\", \"lon\"], crs=\"+proj=longlat +datum=WGS84 +no_defs +vunits=m\"\n        )\n        c2 = c1.transform(\"EPSG:4326\")\n\n        assert_array_equal(c2[\"lat\"].coordinates, c1[\"lat\"].coordinates)\n        assert_array_equal(c2[\"lon\"].coordinates, c1[\"lon\"].coordinates)",
            "def test_transform_size_1_lat(self):\n        c1 = Coordinates([ArrayCoordinates1d([1], name=\"lat\"), clinspace(0, 2, 5, \"lon\")], crs=\"EPSG:3857\")\n        c2 = c1.transform(\"EPSG:4326\")\n        assert c2.shape == c1.shape\n        c1 = Coordinates([clinspace(0, 2, 5, \"lat\"), ArrayCoordinates1d([1], name=\"lon\")], crs=\"EPSG:3857\")\n        c2 = c1.transform(\"EPSG:4326\")\n        assert c2.shape == c1.shape",
            "class TestCoordinatesMethodSimplify(object):",
            "def test_simplify_array_to_uniform(self):\n        c1 = Coordinates([[1, 2, 3, 4], [4, 6, 8]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[1, 2, 3, 5], [4, 6, 8]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([clinspace(1, 4, 4), clinspace(4, 8, 3)], dims=[\"lat\", \"lon\"])\n\n        # array -> uniform\n        assert c1.simplify() == c3.simplify()\n\n        # array -> array\n        assert c2.simplify() == c2.simplify()\n\n        # uniform -> uniform\n        assert c3.simplify() == c3.simplify()\n\n    @pytest.mark.skip(reason=\"not implemented, spec uncertain\")",
            "def test_simplify_stacked_to_unstacked_arrays(self):\n        stacked = Coordinates([np.meshgrid([1, 2, 3, 5], [4, 6, 8])], dims=[\"lat_lon\"])\n        unstacked = Coordinates([[1, 2, 3, 5], [4, 6, 8]], dims=[\"lat\", \"lon\"])\n\n        assert stacked.simplify() == unstacked\n        assert unstacked.simplify() == unstacked",
            "def test_stacked_to_unstacked_uniform(self):\n        stacked = Coordinates([np.meshgrid([4, 6, 8], [1, 2, 3, 4])[::-1]], dims=[\"lat_lon\"])\n        unstacked_uniform = Coordinates([clinspace(1, 4, 4), clinspace(4, 8, 3)], dims=[\"lat\", \"lon\"])\n\n        # stacked grid -> uniform\n        assert stacked.simplify() == unstacked_uniform\n\n        # uniform -> uniform\n        assert unstacked_uniform.simplify() == unstacked_uniform",
            "def test_stacked_to_affine(self):\n        geotransform_rotated = (10.0, 1.879, -1.026, 20.0, 0.684, 2.819)\n        affine = Coordinates([AffineCoordinates(geotransform=geotransform_rotated, shape=(4, 6))])\n        stacked = Coordinates([StackedCoordinates([affine[\"lat_lon\"][\"lat\"], affine[\"lat_lon\"][\"lon\"]])])\n\n        # stacked -> affine\n        assert stacked.simplify() == affine\n\n        # affine -> affine\n        assert affine.simplify() == affine",
            "def test_affine_to_uniform(self):\n        # NOTE: this assumes that podpac prefers unstacked UniformCoordinates to AffineCoordinates\n        geotransform_northup = (10.0, 2.0, 0.0, 20.0, 0.0, -3.0)\n        geotransform_rotated = (10.0, 1.879, -1.026, 20.0, 0.684, 2.819)\n\n        c1 = Coordinates([AffineCoordinates(geotransform=geotransform_northup, shape=(4, 6))])\n        c2 = Coordinates([AffineCoordinates(geotransform=geotransform_rotated, shape=(4, 6))])\n        c3 = Coordinates([clinspace(18.5, 9.5, 4, name=\"lat\"), clinspace(11, 21, 6, name=\"lon\")])\n\n        # unrotated affine -> unstacked uniform\n        assert c1.simplify() == c3\n\n        # rotated affine -> rotated affine\n        assert c2.simplify() == c2\n\n        # unstacked uniform -> unstacked uniform\n        assert c3.simplify() == c3",
            "class TestResolutions(object):",
            "def test_horizontal_resolution(self):\n        \"\"\"Test edge cases of resolutions, and that Resolutions are returned correctly in an OrderedDict.\n        StackedCoordinates and Coordiantes1d handle the resolution calculations, so correctness of the resolutions are tested there.\n        \"\"\"\n\n        # Dimensions\n        lat = podpac.clinspace(-80, 80, 5)\n        lon = podpac.clinspace(-180, 180, 5)\n        time = [\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\", \"2018-01-05\"]\n\n        # Invalid stacked dims check:\n        c = Coordinates([[lon, time]], dims=[\"lon_time\"])\n        with pytest.raises(ValueError):\n            c.horizontal_resolution()\n\n        # Require latitude\n        c = Coordinates([lon], dims=[\"lon\"])\n        with pytest.raises(ValueError):\n            c.horizontal_resolution()\n\n        # Valid dims check:\n        c = Coordinates([[lat, lon]], dims=[\"lat_lon\"])\n        c.horizontal_resolution()\n\n        c = Coordinates([[lon, lat]], dims=[\"lon_lat\"])\n        c.horizontal_resolution()\n\n        # Corect name for restype:\n        with pytest.raises(ValueError):\n            c.horizontal_resolution(restype=\"whateverIwant\")\n\n        # Unstacked\n        c = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        c.horizontal_resolution()\n\n        c = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        c.horizontal_resolution(restype=\"summary\")\n\n        # Mixed stacked, unstacked, but still valid\n        # Stacked and Unstacked valid Check:\n        c = Coordinates([[lat, time], lon], dims=[\"lat_time\", \"lon\"])\n        c2 = Coordinates([lat, lon], dims=[\"lat\", \"lon\"])\n        np.testing.assert_array_equal(c.horizontal_resolution(), c2.horizontal_resolution())\n        # Lat only\n        c = Coordinates([[lat, time]], dims=[\"lat_time\"])\n        c2 = Coordinates([lat], dims=[\"lat\"])\n        np.testing.assert_array_equal(c.horizontal_resolution(), c2.horizontal_resolution())"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_uniform_coordinates1d.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestUniformCoordinatesCreation(object):",
            "def test_numerical(self):\n        # ascending\n        c = UniformCoordinates1d(0, 50, 10)\n        a = np.array([0, 10, 20, 30, 40, 50], dtype=float)\n        assert c.start == 0\n        assert c.stop == 50\n        assert c.step == 10\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0, 50])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 6\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending\n        c = UniformCoordinates1d(50, 0, -10)\n        a = np.array([50, 40, 30, 20, 10, 0], dtype=float)\n        assert c.start == 50\n        assert c.stop == 0\n        assert c.step == -10\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0, 50])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 6\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_numerical_inexact(self):\n        # ascending\n        c = UniformCoordinates1d(0, 49, 10)\n        a = np.array([0, 10, 20, 30, 40], dtype=float)\n        assert c.start == 0\n        assert c.stop == 40\n        assert c.step == 10\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [0, 40])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 5\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending\n        c = UniformCoordinates1d(50, 1, -10)\n        a = np.array([50, 40, 30, 20, 10], dtype=float)\n        assert c.start == 50\n        assert c.stop == 10\n        assert c.step == -10\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [10, 50])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.dtype == float\n        assert c.size == a.size\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_datetime(self):\n        # ascending\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-04\", \"1,D\")\n        a = np.array([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\", \"2018-01-04\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-04\")\n        assert c.step == np.timedelta64(1, \"D\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending\n        c = UniformCoordinates1d(\"2018-01-04\", \"2018-01-01\", \"-1,D\")\n        a = np.array([\"2018-01-04\", \"2018-01-03\", \"2018-01-02\", \"2018-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-04\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert c.step == np.timedelta64(-1, \"D\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_datetime_inexact(self):\n        # ascending\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-06\", \"2,D\")\n        a = np.array([\"2018-01-01\", \"2018-01-03\", \"2018-01-05\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-05\")\n        assert c.step == np.timedelta64(2, \"D\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending\n        c = UniformCoordinates1d(\"2018-01-06\", \"2018-01-01\", \"-2,D\")\n        a = np.array([\"2018-01-06\", \"2018-01-04\", \"2018-01-02\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-06\")\n        assert c.stop == np.datetime64(\"2018-01-02\")\n        assert c.step == np.timedelta64(-2, \"D\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_datetime_month_step(self):\n        # ascending\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-04-01\", \"1,M\")\n        a = np.array([\"2018-01-01\", \"2018-02-01\", \"2018-03-01\", \"2018-04-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-04-01\")\n        assert c.step == np.timedelta64(1, \"M\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending\n        c = UniformCoordinates1d(\"2018-04-01\", \"2018-01-01\", \"-1,M\")\n        a = np.array([\"2018-04-01\", \"2018-03-01\", \"2018-02-01\", \"2018-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-04-01\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert c.step == np.timedelta64(-1, \"M\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_datetime_year_step(self):\n        # ascending, exact\n        c = UniformCoordinates1d(\"2018-01-01\", \"2021-01-01\", \"1,Y\")\n        a = np.array([\"2018-01-01\", \"2019-01-01\", \"2020-01-01\", \"2021-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2021-01-01\")\n        assert c.step == np.timedelta64(1, \"Y\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending, exact\n        c = UniformCoordinates1d(\"2021-01-01\", \"2018-01-01\", \"-1,Y\")\n        a = np.array([\"2021-01-01\", \"2020-01-01\", \"2019-01-01\", \"2018-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2021-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert c.step == np.timedelta64(-1, \"Y\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True\n\n        # ascending, inexact (two cases)\n        c = UniformCoordinates1d(\"2018-01-01\", \"2021-04-01\", \"1,Y\")\n        a = np.array([\"2018-01-01\", \"2019-01-01\", \"2020-01-01\", \"2021-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2021-01-01\")\n        assert c.step == np.timedelta64(1, \"Y\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        c = UniformCoordinates1d(\"2018-04-01\", \"2021-01-01\", \"1,Y\")\n        a = np.array([\"2018-04-01\", \"2019-04-01\", \"2020-04-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-04-01\")\n        assert c.stop == np.datetime64(\"2020-04-01\")\n        assert c.step == np.timedelta64(1, \"Y\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending, inexact (two cases)\n        c = UniformCoordinates1d(\"2021-01-01\", \"2018-04-01\", \"-1,Y\")\n        a = np.array([\"2021-01-01\", \"2020-01-01\", \"2019-01-01\", \"2018-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2021-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert c.step == np.timedelta64(-1, \"Y\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True\n\n        c = UniformCoordinates1d(\"2021-04-01\", \"2018-01-01\", \"-1,Y\")\n        a = np.array([\"2021-04-01\", \"2020-04-01\", \"2019-04-01\", \"2018-04-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2021-04-01\")\n        assert c.stop == np.datetime64(\"2018-04-01\")\n        assert c.step == np.timedelta64(-1, \"Y\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_numerical_size(self):\n        # ascending\n        c = UniformCoordinates1d(0, 10, size=20)\n        assert c.start == 0\n        assert c.stop == 10\n        assert c.step == 10 / 19.0\n        assert_equal(c.coordinates, np.linspace(0, 10, 20))\n        assert_equal(c.bounds, [0, 10])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 20\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == False\n        assert c.is_uniform == True\n\n        # descending\n        c = UniformCoordinates1d(10, 0, size=20)\n        assert c.start == 10\n        assert c.stop == 0\n        assert c.step == -10 / 19.0\n        assert_equal(c.coordinates, np.linspace(10, 0, 20))\n        assert_equal(c.bounds, [0, 10])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 20\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == True\n        assert c.is_uniform == True",
            "def test_datetime_size(self):\n        # ascending\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-10\", size=10)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-10\")\n        assert_equal(c.bounds, [np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-10\")])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 10\n        assert c.dtype == np.datetime64\n        assert c.is_descending == False\n\n        # descending\n        c = UniformCoordinates1d(\"2018-01-10\", \"2018-01-01\", size=10)\n        assert c.start == np.datetime64(\"2018-01-10\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert_equal(c.bounds, [np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-10\")])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 10\n        assert c.dtype == np.datetime64\n        assert c.is_descending == True\n\n        # increase resolution\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-10\", size=21)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-10\")\n        assert_equal(c.bounds, [np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-10\")])\n        assert c.coordinates[c.argbounds[0]] == c.bounds[0]\n        assert c.coordinates[c.argbounds[1]] == c.bounds[1]\n        assert c.size == 21\n        assert c.dtype == np.datetime64\n        assert c.is_descending == False",
            "def test_datetime_size_invalid(self):\n        with pytest.raises(ValueError, match=\"Cannot divide timedelta\"):\n            c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-10\", size=20)",
            "def test_numerical_size_floating_point_error(self):\n        c = UniformCoordinates1d(50.619, 50.62795, size=30)\n        assert c.size == 30",
            "def test_numerical_singleton(self):\n        # positive step\n        c = UniformCoordinates1d(1, 1, 10)\n        a = np.array([1], dtype=float)\n        assert c.start == 1\n        assert c.stop == 1\n        assert c.step == 10\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [1, 1])\n        assert c.size == 1\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == None\n        assert c.is_uniform == True\n\n        # negative step\n        c = UniformCoordinates1d(1, 1, -10)\n        a = np.array([1], dtype=float)\n        assert c.start == 1\n        assert c.stop == 1\n        assert c.step == -10\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, [1, 1])\n        assert c.size == 1\n        assert c.dtype == float\n        assert c.is_monotonic == True\n        assert c.is_descending == None\n        assert c.is_uniform == True",
            "def test_datetime_singleton(self):\n        # positive step\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-01\", \"1,D\")\n        a = np.array([\"2018-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert c.step == np.timedelta64(1, \"D\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[0, -1]])\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == None\n        assert c.is_uniform == True\n\n        # negative step\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-01\", \"-1,D\")\n        a = np.array([\"2018-01-01\"]).astype(np.datetime64)\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-01\")\n        assert c.step == np.timedelta64(-1, \"D\")\n        assert_equal(c.coordinates, a)\n        assert_equal(c.bounds, a[[-1, 0]])\n        assert c.size == a.size\n        assert c.dtype == np.datetime64\n        assert c.is_monotonic == True\n        assert c.is_descending == None\n        assert c.is_uniform == True",
            "def test_from_tuple(self):\n        # numerical, step\n        c = UniformCoordinates1d.from_tuple((0, 10, 0.5))\n        assert c.start == 0.0\n        assert c.stop == 10.0\n        assert c.step == 0.5\n\n        # numerical, size\n        c = UniformCoordinates1d.from_tuple((0, 10, 20))\n        assert c.start == 0.0\n        assert c.stop == 10.0\n        assert c.size == 20\n\n        # datetime, step\n        c = UniformCoordinates1d.from_tuple((\"2018-01-01\", \"2018-01-04\", \"1,D\"))\n        assert c.start == np.datetime64(\"2018-01-01\")\n        assert c.stop == np.datetime64(\"2018-01-04\")\n        assert c.step == np.timedelta64(1, \"D\")\n\n        # invalid\n        with pytest.raises(ValueError, match=\"UniformCoordinates1d.from_tuple expects a tuple\"):\n            UniformCoordinates1d.from_tuple((0, 10))\n\n        with pytest.raises(ValueError, match=\"UniformCoordinates1d.from_tuple expects a tuple\"):\n            UniformCoordinates1d.from_tuple(np.array([0, 10, 0.5]))",
            "def test_copy(self):\n        c = UniformCoordinates1d(0, 10, 50, name=\"lat\")\n        c2 = c.copy()\n        assert c is not c2\n        assert c == c2",
            "def test_invalid_init(self):\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(0, 0, 0)\n\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(0, 50, 0)\n\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(0, 50, -10)\n\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(50, 0, 10)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0, \"2018-01-01\", 10)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(\"2018-01-01\", 50, 10)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(\"2018-01-01\", \"2018-01-02\", 10)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0.0, \"2018-01-01\", \"1,D\")\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(\"2018-01-01\", 50, \"1,D\")\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0, 50, \"1,D\")\n\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(\"a\", 50, 10)\n\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(0, \"b\", 10)\n\n        with pytest.raises(ValueError):\n            UniformCoordinates1d(0, 50, \"a\")\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d()\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0, 50)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0, 50, 10, size=6)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0, 10, size=20.0)\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(0, 10, size=\"string\")\n\n        with pytest.raises(TypeError):\n            UniformCoordinates1d(\"2018-01-10\", \"2018-01-01\", size=\"1,D\")",
            "class TestUniformCoordinatesEq(object):",
            "def test_equal(self):\n        c1 = UniformCoordinates1d(0, 50, 10)\n        c2 = UniformCoordinates1d(0, 50, 10)\n        c3 = UniformCoordinates1d(0, 50, 10)\n        c4 = UniformCoordinates1d(5, 50, 10)\n        c5 = UniformCoordinates1d(0, 60, 10)\n        c6 = UniformCoordinates1d(0, 50, 5)\n        c7 = UniformCoordinates1d(50, 0, -10)\n\n        assert c1 == c2\n        assert c1 == c3\n        assert c1 != c4\n        assert c1 != c5\n        assert c1 != c6\n        assert c1 != c7",
            "def test_equal_array_coordinates(self):\n        c1 = UniformCoordinates1d(0, 50, 10)\n        c2 = ArrayCoordinates1d([0, 10, 20, 30, 40, 50])\n        c3 = ArrayCoordinates1d([10, 20, 30, 40, 50, 60])\n\n        assert c1 == c2\n        assert c1 != c3",
            "class TestUniformCoordinatesSerialization(object):",
            "def test_definition(self):\n        # numerical\n        c = UniformCoordinates1d(0, 50, 10, name=\"lat\")\n        d = c.definition\n        assert isinstance(d, dict)\n        assert set(d.keys()) == set([\"start\", \"stop\", \"step\", \"name\"])\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = UniformCoordinates1d.from_definition(d)  # test from_definition\n        assert c2 == c\n\n        # datetimes\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-03\", \"1,D\")\n        d = c.definition\n        assert isinstance(d, dict)\n        assert set(d.keys()) == set([\"start\", \"stop\", \"step\"])\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = UniformCoordinates1d.from_definition(d)  # test from_definition\n        assert c2 == c",
            "def test_invalid_definition(self):\n        # incorrect definition\n        d = {\"stop\": 50}\n        with pytest.raises(ValueError, match='UniformCoordinates1d definition requires \"start\"'):\n            UniformCoordinates1d.from_definition(d)\n\n        d = {\"start\": 0}\n        with pytest.raises(ValueError, match='UniformCoordinates1d definition requires \"stop\"'):\n            UniformCoordinates1d.from_definition(d)",
            "def test_from_definition_size(self):\n        # numerical\n        d = {\"start\": 0, \"stop\": 50, \"size\": 6}\n        c = UniformCoordinates1d.from_definition(d)\n        assert_equal(c.coordinates, [0, 10, 20, 30, 40, 50])\n\n        # datetime, size\n        d = {\"start\": \"2018-01-01\", \"stop\": \"2018-01-03\", \"size\": 3}\n        c = UniformCoordinates1d.from_definition(d)\n        assert_equal(c.coordinates, np.array([\"2018-01-01\", \"2018-01-02\", \"2018-01-03\"]).astype(np.datetime64))",
            "class TestUniformCoordinatesIndexing(object):",
            "def test_len(self):\n        c = UniformCoordinates1d(0, 50, 10)\n        assert len(c) == 6",
            "def test_index(self):\n        c = UniformCoordinates1d(0, 50, 10, name=\"lat\")\n\n        # int\n        c2 = c[2]\n        assert isinstance(c2, Coordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [20])\n\n        c2 = c[-2]\n        assert isinstance(c2, Coordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [40])\n\n        # slice\n        c2 = c[:2]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 0\n        assert c2.stop == 10\n        assert c2.step == 10\n\n        c2 = c[2:]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 20\n        assert c2.stop == 50\n        assert c2.step == 10\n\n        c2 = c[::2]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 0\n        assert c2.stop == 40\n        assert c2.step == 20\n\n        c2 = c[1:-1]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 10\n        assert c2.stop == 40\n        assert c2.step == 10\n\n        c2 = c[-3:5]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 30\n        assert c2.stop == 40\n        assert c2.step == 10\n\n        c2 = c[::-1]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 50\n        assert c2.stop == 0\n        assert c2.step == -10\n\n        # index array\n        c2 = c[[0, 1, 3]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [0, 10, 30])\n\n        c2 = c[[3, 1, 0]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [30, 10, 0])\n\n        c2 = c[[0, 3, 1]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [0, 30, 10])\n\n        c2 = c[[]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [])\n\n        c2 = c[0:0]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [])\n\n        c2 = c[[]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [])\n\n        # boolean array\n        c2 = c[[True, True, True, False, True, False]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [0, 10, 20, 40])\n\n        # invalid\n        with pytest.raises(IndexError):\n            c[0.3]\n\n        with pytest.raises(IndexError):\n            c[10]",
            "def test_index_descending(self):\n        c = UniformCoordinates1d(50, 0, -10, name=\"lat\")\n\n        # int\n        c2 = c[2]\n        assert isinstance(c2, Coordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [30])\n\n        c2 = c[-2]\n        assert isinstance(c2, Coordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [10])\n\n        # slice\n        c2 = c[:2]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 50\n        assert c2.stop == 40\n        assert c2.step == -10\n\n        c2 = c[2:]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 30\n        assert c2.stop == 0\n        assert c2.step == -10\n\n        c2 = c[::2]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 50\n        assert c2.stop == 10\n        assert c2.step == -20\n\n        c2 = c[1:-1]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 40\n        assert c2.stop == 10\n        assert c2.step == -10\n\n        c2 = c[-3:5]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 20\n        assert c2.stop == 10\n        assert c2.step == -10\n\n        c2 = c[::-1]\n        assert isinstance(c2, UniformCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert c2.start == 0\n        assert c2.stop == 50\n        assert c2.step == 10\n\n        # index array\n        c2 = c[[0, 1, 3]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [50, 40, 20])\n\n        c2 = c[[3, 1, 0]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [20, 40, 50])\n\n        c2 = c[[0, 3, 1]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [50, 20, 40])\n\n        # boolean array\n        c2 = c[[True, True, True, False, True, False]]\n        assert isinstance(c2, ArrayCoordinates1d)\n        assert c2.name == c.name\n        assert c2.properties == c.properties\n        assert_equal(c2.coordinates, [50, 40, 30, 10])",
            "def test_in(self):\n        c = UniformCoordinates1d(0, 50, 10, name=\"lat\")\n        assert 0 in c\n        assert 10 in c\n        assert 50 in c\n        assert -10 not in c\n        assert 60 not in c\n        assert 5 not in c\n        assert np.datetime64(\"2018\") not in c\n        assert \"a\" not in c\n\n        c = UniformCoordinates1d(50, 0, -10, name=\"lat\")\n        assert 0 in c\n        assert 10 in c\n        assert 50 in c\n        assert -10 not in c\n        assert 60 not in c\n        assert 5 not in c\n        assert np.datetime64(\"2018\") not in c\n        assert \"a\" not in c\n\n        c = UniformCoordinates1d(\"2020-01-01\", \"2020-01-09\", \"2,D\", name=\"time\")\n        assert np.datetime64(\"2020-01-01\") in c\n        assert np.datetime64(\"2020-01-03\") in c\n        assert np.datetime64(\"2020-01-09\") in c\n        assert np.datetime64(\"2020-01-11\") not in c\n        assert np.datetime64(\"2020-01-02\") not in c\n        assert 10 not in c\n        assert \"a\" not in c",
            "class TestArrayCoordinatesAreaBounds(object):",
            "def test_get_area_bounds_numerical(self):\n        c = UniformCoordinates1d(0, 50, 10)\n\n        # point\n        area_bounds = c.get_area_bounds(None)\n        assert_equal(area_bounds, [0.0, 50.0])\n\n        # uniform\n        area_bounds = c.get_area_bounds(0.5)\n        assert_equal(area_bounds, [-0.5, 50.5])\n\n        # segment\n        area_bounds = c.get_area_bounds([-0.2, 0.7])\n        assert_equal(area_bounds, [-0.2, 50.7])\n\n        # polygon (i.e. there would be corresponding offets for another dimension)\n        area_bounds = c.get_area_bounds([-0.2, -0.5, 0.7, 0.5])\n        assert_equal(area_bounds, [-0.5, 50.7])",
            "def test_get_area_bounds_datetime(self):\n        c = UniformCoordinates1d(\"2018-01-01\", \"2018-01-04\", \"1,D\")\n\n        # point\n        area_bounds = c.get_area_bounds(None)\n        assert_equal(area_bounds, make_coord_array([\"2018-01-01\", \"2018-01-04\"]))\n\n        # uniform\n        area_bounds = c.get_area_bounds(\"1,D\")\n        assert_equal(area_bounds, make_coord_array([\"2017-12-31\", \"2018-01-05\"]))\n\n        area_bounds = c.get_area_bounds(\"1,M\")\n        assert_equal(area_bounds, make_coord_array([\"2017-12-01\", \"2018-02-04\"]))\n\n        area_bounds = c.get_area_bounds(\"1,Y\")\n        assert_equal(area_bounds, make_coord_array([\"2017-01-01\", \"2019-01-04\"]))\n\n        # segment\n        area_bounds = c.get_area_bounds([\"0,h\", \"12,h\"])\n        assert_equal(area_bounds, make_coord_array([\"2018-01-01 00:00\", \"2018-01-04 12:00\"]))",
            "class TestUniformCoordinatesSelection(object):",
            "def test_select_all_shortcut(self):\n        c = UniformCoordinates1d(20.0, 70.0, 10.0)\n\n        s = c.select([0, 100])\n        assert s.start == 20.0\n        assert s.stop == 70.0\n        assert s.step == 10.0\n\n        s, I = c.select([0, 100], return_index=True)\n        assert s.start == 20.0\n        assert s.stop == 70.0\n        assert s.step == 10.0\n        assert_equal(c[I], s)",
            "def test_select_none_shortcut(self):\n        c = UniformCoordinates1d(20.0, 70.0, 10.0)\n\n        # above\n        s = c.select([100, 200])\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([100, 200], return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert c[I] == s\n\n        # below\n        s = c.select([0, 5])\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([0, 5], return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert c[I] == s",
            "def test_select_ascending(self):\n        c = UniformCoordinates1d(20.0, 70.0, 10.0)\n\n        # inner\n        s = c.select([35.0, 55.0])\n        assert s.start == 40.0\n        assert s.stop == 50.0\n        assert s.step == 10.0\n\n        s, I = c.select([35.0, 55.0], return_index=True)\n        assert s.start == 40.0\n        assert s.stop == 50.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # inner with aligned bounds\n        s = c.select([30.0, 60.0])\n        assert s.start == 30.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n\n        s, I = c.select([30.0, 60.0], return_index=True)\n        assert s.start == 30.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # above\n        s = c.select([45, 100])\n        assert s.start == 50.0\n        assert s.stop == 70.0\n        assert s.step == 10.0\n\n        s, I = c.select([45, 100], return_index=True)\n        assert s.start == 50.0\n        assert s.stop == 70.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # below\n        s = c.select([5, 55])\n        assert s.start == 20.0\n        assert s.stop == 50.0\n        assert s.step == 10.0\n\n        s, I = c.select([5, 55], return_index=True)\n        assert s.start == 20.0\n        assert s.stop == 50.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # between coordinates\n        s = c.select([52, 55])\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([52, 55], return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])\n\n        # backwards bounds\n        s = c.select([70, 30])\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_descending(self):\n        c = UniformCoordinates1d(70.0, 20.0, -10.0)\n\n        # inner\n        s = c.select([35.0, 55.0])\n        assert s.start == 50.0\n        assert s.stop == 40.0\n        assert s.step == -10.0\n\n        s, I = c.select([35.0, 55.0], return_index=True)\n        assert s.start == 50.0\n        assert s.stop == 40.0\n        assert s.step == -10.0\n        assert c[I] == s\n\n        # inner with aligned bounds\n        s = c.select([30.0, 60.0])\n        assert s.start == 60.0\n        assert s.stop == 30.0\n        assert s.step == -10.0\n\n        s, I = c.select([30.0, 60.0], return_index=True)\n        assert s.start == 60.0\n        assert s.stop == 30.0\n        assert s.step == -10.0\n        assert c[I] == s\n\n        # above\n        s = c.select([45, 100])\n        assert s.start == 70.0\n        assert s.stop == 50.0\n        assert s.step == -10.0\n\n        s, I = c.select([45, 100], return_index=True)\n        assert s.start == 70.0\n        assert s.stop == 50.0\n        assert s.step == -10.0\n        assert c[I] == s\n\n        # below\n        s = c.select([5, 55])\n        assert s.start == 50.0\n        assert s.stop == 20.0\n        assert s.step == -10.0\n\n        s, I = c.select([5, 55], return_index=True)\n        assert s.start == 50.0\n        assert s.stop == 20.0\n        assert s.step == -10.0\n        assert c[I] == s\n\n        # between coordinates\n        s = c.select([52, 55])\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([52, 55], return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])\n\n        # backwards bounds\n        s = c.select([70, 30])\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_outer(self):\n        c = UniformCoordinates1d(20.0, 70.0, 10.0)\n\n        # inner\n        s = c.select([35.0, 55.0], outer=True)\n        assert s.start == 30.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n\n        s, I = c.select([35.0, 55.0], outer=True, return_index=True)\n        assert s.start == 30.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # inner with aligned bounds\n        s = c.select([30.0, 60.0], outer=True)\n        assert s.start == 30.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n\n        s, I = c.select([30.0, 60.0], outer=True, return_index=True)\n        assert s.start == 30.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # above\n        s = c.select([45, 100], outer=True)\n        assert s.start == 40.0\n        assert s.stop == 70.0\n        assert s.step == 10.0\n\n        s, I = c.select([45, 100], outer=True, return_index=True)\n        assert s.start == 40.0\n        assert s.stop == 70.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # below\n        s = c.select([5, 55], outer=True)\n        assert s.start == 20.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n\n        s, I = c.select([5, 55], outer=True, return_index=True)\n        assert s.start == 20.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # between coordinates\n        s = c.select([52, 55], outer=True)\n        assert s.start == 50.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n\n        s, I = c.select([52, 55], outer=True, return_index=True)\n        assert s.start == 50.0\n        assert s.stop == 60.0\n        assert s.step == 10.0\n        assert c[I] == s\n\n        # backwards bounds\n        s = c.select([70, 30], outer=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n\n        s, I = c.select([70, 30], outer=True, return_index=True)\n        assert isinstance(s, ArrayCoordinates1d)\n        assert_equal(s.coordinates, [])\n        assert_equal(c.coordinates[I], [])",
            "def test_select_time_variable_precision(self):\n        c = UniformCoordinates1d(\"2012-05-19\", \"2012-05-20\", \"1,D\", name=\"time\")\n        c2 = UniformCoordinates1d(\"2012-05-20T12:00:00\", \"2012-05-21T12:00:00\", \"1,D\", name=\"time\")\n        s = c.select(c2.bounds, outer=True)\n        s1 = c.select(c2.bounds, outer=False)\n        s2 = c2.select(c.bounds)\n        assert s.size == 1\n        assert s1.size == 0\n        assert s2.size == 1",
            "class TestUniformCoordinatesMethods(object):",
            "def test_unique(self):\n        c = UniformCoordinates1d(1, 5, step=1)\n        c2 = c.unique()\n        assert c2 == c and c2 is not c\n\n        c2, I = c.unique(return_index=True)\n        assert c2 == c and c2 is not c\n        assert c2 == c[I]",
            "def test_simplify(self):\n        c = UniformCoordinates1d(1, 5, step=1)\n        c2 = c.simplify()\n        assert c2 == c and c2 is not c\n\n        # reversed, step -2\n        c = UniformCoordinates1d(4, 0, step=-2)\n        c2 = c.simplify()\n        assert c2 == c and c2 is not c\n\n        # time, convert to UniformCoordinates\n        c = UniformCoordinates1d(\"2020-01-01\", \"2020-01-05\", step=\"1,D\")\n        c2 = c.simplify()\n        assert c2 == c and c2 is not c\n\n        # time, reverse -2,h\n        c = UniformCoordinates1d(\"2020-01-01T12:00\", \"2020-01-01T08:00\", step=\"-3,h\")\n        c2 = c.simplify()\n        assert c2 == c and c2 is not c",
            "def test_flatten(self):\n        c = UniformCoordinates1d(1, 5, step=1)\n        c2 = c.flatten()\n        assert c2 == c and c2 is not c",
            "def test_reshape(self):\n        c = UniformCoordinates1d(1, 6, step=1, name=\"lat\")\n        c2 = c.reshape((2, 3))\n        assert c2 == ArrayCoordinates1d(c.coordinates.reshape((2, 3)), name=\"lat\")",
            "def test_issubset(self):\n        c1 = UniformCoordinates1d(2, 1, step=-1)\n        c2 = UniformCoordinates1d(1, 3, step=1)\n        c3 = UniformCoordinates1d(0, 2, step=1)\n        c4 = UniformCoordinates1d(1, 4, step=0.5)\n        c5 = UniformCoordinates1d(1.5, 2.5, step=0.5)\n        c6 = UniformCoordinates1d(1.4, 2.4, step=0.5)\n        c7 = UniformCoordinates1d(1.4, 2.4, step=10)\n\n        # self\n        assert c1.issubset(c1)\n\n        # subsets\n        assert c1.issubset(c2)\n        assert c1.issubset(c3)\n        assert c1.issubset(c4)\n        assert c5.issubset(c4)\n        assert c7.issubset(c6)\n\n        # not subsets\n        assert not c2.issubset(c1)\n        assert not c2.issubset(c3)\n        assert not c3.issubset(c1)\n        assert not c3.issubset(c2)\n        assert not c4.issubset(c1)\n        assert not c6.issubset(c4)",
            "def test_issubset_datetime(self):\n        c1 = UniformCoordinates1d(\"2020-01-01\", \"2020-01-03\", \"1,D\")\n        c2 = UniformCoordinates1d(\"2020-01-01\", \"2020-01-03\", \"2,D\")\n        c3 = UniformCoordinates1d(\"2020-01-01\", \"2020-01-05\", \"1,D\")\n        c4 = UniformCoordinates1d(\"2020-01-05\", \"2020-01-01\", \"-2,D\")\n\n        # self\n        assert c1.issubset(c1)\n\n        # same resolution\n        assert c1.issubset(c3)\n        assert c2.issubset(c1)\n        assert c2.issubset(c4)\n        assert not c1.issubset(c2)\n        assert not c1.issubset(c4)\n        assert not c3.issubset(c1)\n\n        # different resolution\n        c5 = UniformCoordinates1d(\"2020-01-01T00:00\", \"2020-01-03T00:00\", \"1,D\")\n        c6 = UniformCoordinates1d(\"2020-01-01T00:00\", \"2020-01-03T00:00\", \"6,h\")\n        assert c1.issubset(c5)\n        assert c5.issubset(c1)\n        assert c1.issubset(c6)\n        assert not c6.issubset(c1)",
            "def test_issubset_dtype(self):\n        c1 = UniformCoordinates1d(0, 10, step=1)\n        c2 = UniformCoordinates1d(\"2018\", \"2020\", step=\"1,Y\")\n        assert not c1.issubset(c2)\n        assert not c2.issubset(c1)",
            "def test_issubset_array_coordinates(self):\n        u = UniformCoordinates1d(start=1, stop=3, step=1)\n        a1 = ArrayCoordinates1d([1, 3, 2])\n        a2 = ArrayCoordinates1d([1, 2, 3])\n        a3 = ArrayCoordinates1d([1, 3, 4])\n        e = ArrayCoordinates1d([])\n\n        # self\n        assert u.issubset(a1)\n        assert u.issubset(a2)\n        assert not u.issubset(a3)\n        assert not u.issubset(e)",
            "def test_issubset_coordinates(self):\n        u = UniformCoordinates1d(1, 3, 1, name=\"lat\")\n        c1 = Coordinates([[1, 2, 3], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[1, 2, 4], [10, 20, 30]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([[10, 20, 30]], dims=[\"alt\"])\n\n        assert u.issubset(c1)\n        assert not u.issubset(c2)\n        assert not u.issubset(c3)",
            "def test_coordinates_floating_point_consistency(self):\n        c = podpac.Coordinates.from_url(\n            \"?VERSION=1.3.0&HEIGHT=512&WIDTH=512&CRS=EPSG%3A3857&BBOX=-8061966.247294108,5322463.153553393,-7983694.730330088,5400734.670517412\"\n        ).transform(\"EPSG:4326\")\n        u = c[\"lon\"]\n        u2 = podpac.coordinates.UniformCoordinates1d(\n            u.start - 2 * u.step, u.stop + 2 * u.step + 1e-14, step=u.step, fix_stop_val=True\n        )\n        u3 = podpac.coordinates.UniformCoordinates1d(\n            u.start - 2 * u.step, u.stop + 2 * u.step + 1e-14, size=u.size + 4, fix_stop_val=True\n        )\n\n        assert u2.start == u3.start\n        assert u2.stop == u3.stop\n        step = (u2.stop - u2.start) / (u2.size - 1)\n        assert u2.step == step\n        assert u3.step == step\n        assert_equal(u2.coordinates, u3.coordinates)\n\n        # Lat has a different order (i.e negative step)\n        u = c[\"lat\"]\n        step = (u.coordinates[-1] - u.coordinates[0]) / (u.size - 1)\n        start = u.coordinates[0]\n        stop = u.coordinates[-1]\n        u2 = podpac.coordinates.UniformCoordinates1d(\n            start - 2 * step, stop + 2 * step + 1e-14, step=step, fix_stop_val=True\n        )\n        u3 = podpac.coordinates.UniformCoordinates1d(\n            start - 2 * step, stop + 2 * step + 1e-14, size=u.size + 4, fix_stop_val=True\n        )\n\n        assert u2.start == u3.start\n        assert u2.stop == u3.stop\n        step = (u2.stop - u2.start) / (u2.size - 1)\n        assert u2.step == step\n        assert u3.step == step\n        assert_equal(u2.coordinates, u3.coordinates)\n\n        # Need to make sure time data still works\n        u2 = podpac.coordinates.UniformCoordinates1d(\"2000-01-01\", \"2000-01-31\", step=\"23,h\")\n        u3 = podpac.coordinates.UniformCoordinates1d(\n            \"2000-01-01T00\", \"2000-01-30T17\", size=u2.size\n        )  # Won't allow me to specify something inconsistent...\n\n        assert u2.start == u3.start\n        assert u2.stop == u3.stop\n        step = (u2.stop - u2.start) / (u2.size - 1)\n        assert u2.step == step\n        assert u3.step == step\n        assert_equal(u2.coordinates, u3.coordinates)\n\n        # Now check consistency without the `fix_stop_val` flag\n        u = c[\"lon\"]\n        u2 = podpac.coordinates.UniformCoordinates1d(u.start - 2 * u.step, u.stop + 2 * u.step + 1e-14, step=u.step)\n        c3 = podpac.Coordinates([u2], [\"lon\"])\n        n = podpac.Node().create_output_array(podpac.Coordinates([u2], [\"lon\"]))\n        u2b = podpac.Coordinates.from_xarray(n)\n        assert_equal(u2b[\"lon\"].coordinates, u2.coordinates)\n\n        u2 = podpac.coordinates.UniformCoordinates1d(u.start - 2 * u.step, u.stop + 2 * u.step + 1e-14, size=u.size + 4)\n        n = podpac.Node().create_output_array(podpac.Coordinates([u2], [\"lon\"]))\n        u2b = podpac.Coordinates.from_xarray(n)\n        assert_equal(u2b[\"lon\"].coordinates, u2.coordinates)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_group_coordinates.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestGroupCoordinates(object):",
            "def test_init(self):\n        # empty\n        g = GroupCoordinates([])\n\n        # same dims, unstacked\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        g = GroupCoordinates([c1, c2])\n\n        # same dims, stacked\n        c2 = Coordinates([[[0, 1], [0, 1]]], dims=[\"lat_lon\"])\n        c2 = Coordinates([[[10, 11], [10, 11]]], dims=[\"lat_lon\"])\n        g = GroupCoordinates([c1, c2])\n\n        # different order\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lon\", \"lat\"])\n        g = GroupCoordinates([c1, c2])\n\n        # different stacking\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[[10, 11], [10, 11]]], dims=[\"lat_lon\"])\n        g = GroupCoordinates([c1, c2])",
            "def test_init_mismatching_dims(self):\n        # mismatching dims\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11], \"2018-01-01\"], dims=[\"lat\", \"lon\", \"time\"])\n\n        with pytest.raises(ValueError, match=\"Mismatching dims\"):\n            GroupCoordinates([c1, c2])",
            "def test_properties(self):\n        g = GroupCoordinates([])\n        assert len(g) == 0\n        assert g.udims == set()\n\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[[10, 11], [10, 11]]], dims=[\"lat_lon\"])\n\n        g = GroupCoordinates([c1, c2])\n        assert len(g) == 2\n        assert g.udims == set([\"lat\", \"lon\"])",
            "def test_iter(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        g = GroupCoordinates([c1, c2])\n\n        for c in g:\n            assert isinstance(c, Coordinates)",
            "def test_append(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([\"2018-01-01\"], dims=[\"time\"])\n\n        g = GroupCoordinates([])\n        assert len(g) == 0\n\n        g.append(c1)\n        assert len(g) == 1\n\n        g.append(c2)\n        assert len(g) == 2\n\n        with pytest.raises(TypeError):\n            g.append(10)\n\n        with pytest.raises(ValueError):\n            g.append(c3)\n\n        assert g._items[0] is c1\n        assert g._items[1] is c2",
            "def test_add(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([\"2018-01-01\"], dims=[\"time\"])\n\n        g1 = GroupCoordinates([c1])\n        g2 = GroupCoordinates([c2])\n        g3 = GroupCoordinates([c3])\n\n        g = g1 + g2\n\n        assert len(g1) == 1\n        assert len(g2) == 1\n        assert len(g) == 2\n        assert g._items[0] is c1\n        assert g._items[1] is c2\n\n        with pytest.raises(ValueError):\n            g1 + g3\n\n        with pytest.raises(TypeError):\n            g1 + c1",
            "def test_iadd(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([\"2018-01-01\"], dims=[\"time\"])\n\n        g1 = GroupCoordinates([c1])\n        g2 = GroupCoordinates([c2])\n        g3 = GroupCoordinates([c3])\n\n        g1 += g2\n\n        with pytest.raises(ValueError):\n            g1 += g3\n\n        with pytest.raises(TypeError):\n            g1 += c1\n\n        assert len(g1) == 2\n        assert g1._items[0] is c1\n        assert g1._items[1] is c2\n\n        assert len(g2) == 1\n        assert g2._items[0] is c2",
            "def test_repr(self):\n        # empty\n        g = GroupCoordinates([])\n        repr(g)\n\n        # nonempty\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[[10, 11], [10, 11]]], dims=[\"lat_lon\"])\n        g = GroupCoordinates([c1, c2])\n        repr(g)",
            "def test_intersect(self):\n        c1 = Coordinates([[0, 1, 2], [0, 1, 2]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([[0.5, 1.5, 2.5]], dims=[\"lat\"])\n\n        g = GroupCoordinates([c1, c2])\n\n        g2 = g.intersect(c3)\n        g2 = g.intersect(c3, outer=True)\n        g2, I = g.intersect(c3, return_index=True)",
            "def test_definition(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        g = GroupCoordinates([c1, c2])\n\n        d = g.definition\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)\n        g2 = GroupCoordinates.from_definition(d)",
            "def test_json(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        g = GroupCoordinates([c1, c2])\n\n        s = g.json\n        g2 = GroupCoordinates.from_json(s)",
            "def test_hash(self):\n        c1 = Coordinates([[0, 1], [0, 1]], dims=[\"lat\", \"lon\"])\n        c2 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        c3 = Coordinates([[10, 11], [10, 11]], dims=[\"lat\", \"lon\"])\n        c4 = Coordinates([[10, 12], [10, 11]], dims=[\"lat\", \"lon\"])\n\n        g1 = GroupCoordinates([c1, c2])\n        g2 = GroupCoordinates([c1, c2])\n        g3 = GroupCoordinates([c1, c3])\n        g4 = GroupCoordinates([c1, c4])\n\n        assert g1.hash == g1.hash\n        assert g1.hash == g2.hash\n        assert g1.hash == g3.hash\n        assert g1.hash != g4.hash"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_coordinates_utils.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "def test_get_timedelta():\n    td64 = np.timedelta64\n    assert get_timedelta(\"2,ms\") == td64(2, \"ms\")\n    assert get_timedelta(\"2,s\") == td64(2, \"s\")\n    assert get_timedelta(\"2,Y\") == td64(2, \"Y\")\n    assert get_timedelta(\"-1,s\") == td64(-1, \"s\")\n\n    with pytest.raises(ValueError):\n        get_timedelta(\"1.5,s\")\n\n    with pytest.raises(ValueError):\n        get_timedelta(\"1\")\n\n    with pytest.raises(TypeError, match=\"Invalid datetime unit\"):\n        get_timedelta(\"1,x\")",
            "def test_get_timedelta_unit():\n    td64 = np.timedelta64\n    assert get_timedelta_unit(td64(2, \"ms\")) == \"ms\"\n    assert get_timedelta_unit(td64(2, \"s\")) == \"s\"\n    assert get_timedelta_unit(td64(2, \"Y\")) == \"Y\"\n\n    with pytest.raises(TypeError):\n        get_timedelta_unit(\"a string\")\n\n    with pytest.raises(TypeError):\n        get_timedelta_unit(np.array([1, 2]))",
            "def test_make_timedelta_string():\n    td64 = np.timedelta64\n    assert make_timedelta_string(td64(2, \"ms\")) == \"2,ms\"\n    assert make_timedelta_string(td64(2, \"s\")) == \"2,s\"\n    assert make_timedelta_string(td64(2, \"Y\")) == \"2,Y\"\n    assert make_timedelta_string(td64(-1, \"s\")) == \"-1,s\"\n\n    with pytest.raises(TypeError):\n        assert make_timedelta_string(1)",
            "def test_make_coord_value():\n    # numbers\n    assert make_coord_value(10.5) == 10.5\n    assert make_coord_value(10) == 10.0\n    assert make_coord_value(np.array(10.5)) == 10.5\n    assert make_coord_value(np.array([10.5])) == 10.5\n\n    assert type(make_coord_value(10.5)) is float\n    assert type(make_coord_value(10)) is float\n    assert type(make_coord_value(np.array(10.5))) is float\n    assert type(make_coord_value(np.array([10.5]))) is float\n\n    # datetimes\n    dt = np.datetime64(\"2018-01-01\")\n    assert make_coord_value(dt) == dt\n    assert make_coord_value(dt.item()) == dt\n    assert make_coord_value(\"2018-01-01\") == dt\n    assert make_coord_value(\"2018-01-01\") == dt\n    assert make_coord_value(np.array(dt)) == dt\n    assert make_coord_value(np.array([dt])) == dt\n    assert make_coord_value(np.array(\"2018-01-01\")) == dt\n    assert make_coord_value(np.array([\"2018-01-01\"])) == dt\n\n    # arrays and lists\n    with pytest.raises(TypeError, match=\"Invalid coordinate value\"):\n        make_coord_value(np.arange(5))\n\n    with pytest.raises(TypeError, match=\"Invalid coordinate value\"):\n        make_coord_value(range(5))\n\n    # invalid strings\n    with pytest.raises(ValueError, match=\"Error parsing datetime string\"):\n        make_coord_value(\"not a valid datetime\")",
            "def test_make_coord_delta():\n    # numbers\n    assert make_coord_delta(10.5) == 10.5\n    assert make_coord_delta(10) == 10.0\n    assert make_coord_delta(np.array(10.5)) == 10.5\n    assert make_coord_delta(np.array([10.5])) == 10.5\n\n    assert type(make_coord_delta(10.5)) is float\n    assert type(make_coord_delta(10)) is float\n    assert type(make_coord_delta(np.array(10.5))) is float\n    assert type(make_coord_delta(np.array([10.5]))) is float\n\n    # timedelta\n    td = np.timedelta64(2, \"D\")\n    assert make_coord_delta(td) == td\n    assert make_coord_delta(td.item()) == td\n    assert make_coord_delta(\"2,D\") == td\n    assert make_coord_delta(\"2,D\") == td\n    assert make_coord_delta(np.array(td)) == td\n    assert make_coord_delta(np.array([td])) == td\n    assert make_coord_delta(np.array(\"2,D\")) == td\n    assert make_coord_delta(np.array([\"2,D\"])) == td\n\n    # arrays and lists\n    with pytest.raises(TypeError, match=\"Invalid coordinate delta\"):\n        make_coord_delta(np.arange(5))\n\n    with pytest.raises(TypeError, match=\"Invalid coordinate delta\"):\n        make_coord_delta(range(5))\n\n    # invalid strings\n    with pytest.raises(ValueError):\n        make_coord_delta(\"not a valid timedelta\")",
            "class TestMakeCoordArray(object):",
            "def test_numerical_singleton(self):\n        a = np.array([5.0])\n        f = 5.0\n        i = 5\n\n        # float\n        np.testing.assert_array_equal(make_coord_array(f), a)\n        np.testing.assert_array_equal(make_coord_array([f]), a)\n\n        # float array\n        np.testing.assert_array_equal(make_coord_array(np.array(f)), a)\n        np.testing.assert_array_equal(make_coord_array(np.array([f])), a)\n\n        # int\n        np.testing.assert_array_equal(make_coord_array(i), a)\n        np.testing.assert_array_equal(make_coord_array([i]), a)\n\n        # int array\n        np.testing.assert_array_equal(make_coord_array(np.array(i)), a)\n        np.testing.assert_array_equal(make_coord_array(np.array([i])), a)",
            "def test_numerical_array(self):\n        a = np.array([5.0, 5.5])\n        l = [5, 5.5]\n\n        np.testing.assert_array_equal(make_coord_array(l), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(l)), a)",
            "def test_numerical_ndarray(self):\n        a = [[0, 1], [5, 6]]\n        np.testing.assert_array_equal(make_coord_array(a), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(a)), a)",
            "def test_date_singleton(self):\n        a = np.array([\"2018-01-01\"]).astype(np.datetime64)\n        s = \"2018-01-01\"\n        u = \"2018-01-01\"\n        dt64 = np.datetime64(\"2018-01-01\")\n        dt = np.datetime64(\"2018-01-01\").item()\n\n        # str\n        np.testing.assert_array_equal(make_coord_array(s), a)\n        np.testing.assert_array_equal(make_coord_array([s]), a)\n\n        # unicode\n        np.testing.assert_array_equal(make_coord_array(u), a)\n        np.testing.assert_array_equal(make_coord_array([u]), a)\n\n        # datetime64\n        np.testing.assert_array_equal(make_coord_array(dt64), a)\n        np.testing.assert_array_equal(make_coord_array([dt64]), a)\n\n        # python Datetime\n        np.testing.assert_array_equal(make_coord_array(dt), a)\n        np.testing.assert_array_equal(make_coord_array([dt]), a)\n\n        # pandas Timestamp\n        # not tested here because these always have h:m:s",
            "def test_datetime_singleton(self):\n        a = np.array([\"2018-01-01T01:01:01\"]).astype(np.datetime64)\n        s = \"2018-01-01T01:01:01\"\n        u = \"2018-01-01T01:01:01\"\n        dt64 = np.datetime64(\"2018-01-01T01:01:01\")\n        dt = np.datetime64(\"2018-01-01T01:01:01\").item()\n        ts = pd.Timestamp(\"2018-01-01T01:01:01\")\n\n        # str\n        np.testing.assert_array_equal(make_coord_array(s), a)\n        np.testing.assert_array_equal(make_coord_array([s]), a)\n\n        # unicode\n        np.testing.assert_array_equal(make_coord_array(u), a)\n        np.testing.assert_array_equal(make_coord_array([u]), a)\n\n        # datetime64\n        np.testing.assert_array_equal(make_coord_array(dt64), a)\n        np.testing.assert_array_equal(make_coord_array([dt64]), a)\n\n        # python Datetime\n        np.testing.assert_array_equal(make_coord_array(dt), a)\n        np.testing.assert_array_equal(make_coord_array([dt]), a)\n\n        # pandas Timestamp\n        np.testing.assert_array_equal(make_coord_array(ts), a)\n        np.testing.assert_array_equal(make_coord_array([ts]), a)",
            "def test_date_array(self):\n        a = np.array([\"2018-01-01\", \"2018-01-02\"]).astype(np.datetime64)\n        s = [\"2018-01-01\", \"2018-01-02\"]\n        u = [\"2018-01-01\", \"2018-01-02\"]\n        dt64 = [np.datetime64(\"2018-01-01\"), np.datetime64(\"2018-01-02\")]\n        dt = [np.datetime64(\"2018-01-01\").item(), np.datetime64(\"2018-01-02\").item()]\n\n        # str\n        np.testing.assert_array_equal(make_coord_array(s), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(s)), a)\n\n        # unicode\n        np.testing.assert_array_equal(make_coord_array(u), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(u)), a)\n\n        # datetime64\n        np.testing.assert_array_equal(make_coord_array(dt64), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(dt64)), a)\n\n        # python datetime\n        np.testing.assert_array_equal(make_coord_array(dt), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(dt)), a)\n\n        # pandas Timestamp\n        # not tested here because these always have h:m:s",
            "def test_datetime_array(self):\n        a = np.array([\"2018-01-01T01:01:01\", \"2018-01-01T01:01:02\"]).astype(np.datetime64)\n        s = [\"2018-01-01T01:01:01\", \"2018-01-01T01:01:02\"]\n        u = [\"2018-01-01T01:01:01\", \"2018-01-01T01:01:02\"]\n        dt64 = [np.datetime64(\"2018-01-01T01:01:01\"), np.datetime64(\"2018-01-01T01:01:02\")]\n        dt = [np.datetime64(\"2018-01-01T01:01:01\").item(), np.datetime64(\"2018-01-01T01:01:02\").item()]\n        ts = [pd.Timestamp(\"2018-01-01T01:01:01\"), pd.Timestamp(\"2018-01-01T01:01:02\")]\n\n        # str\n        np.testing.assert_array_equal(make_coord_array(s), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(s)), a)\n\n        # unicode\n        np.testing.assert_array_equal(make_coord_array(u), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(u)), a)\n\n        # datetime64\n        np.testing.assert_array_equal(make_coord_array(dt64), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(dt64)), a)\n\n        # python datetime\n        np.testing.assert_array_equal(make_coord_array(dt), a)\n        np.testing.assert_array_equal(make_coord_array(np.array(dt)), a)\n\n        # pandas Timestamp\n        np.testing.assert_array_equal(make_coord_array(ts), a)",
            "def test_invalid_type(self):\n        with pytest.raises(TypeError):\n            make_coord_array([{}])",
            "def test_mixed_type(self):\n        with pytest.raises(ValueError, match=\"Invalid coordinate values\"):\n            make_coord_array([5.0, \"2018-01-01\"])\n\n        with pytest.raises(ValueError, match=\"Invalid coordinate values\"):\n            make_coord_array([\"2018-01-01\", 5.0])\n\n        with pytest.raises(ValueError, match=\"Invalid coordinate values\"):\n            make_coord_array([5.0, np.datetime64(\"2018-01-01\")])\n\n        with pytest.raises(ValueError, match=\"Invalid coordinate values\"):\n            make_coord_array([np.datetime64(\"2018-01-01\"), 5.0])",
            "def test_invalid_time_string(self):\n        with pytest.raises(ValueError, match=\"Error parsing datetime string\"):\n            make_coord_array([\"invalid\"])",
            "class TestMakeCoordDeltaArray(object):",
            "def test_numerical_singleton(self):\n        a = np.array([5.0])\n        f = 5.0\n        i = 5\n\n        # float\n        np.testing.assert_array_equal(make_coord_delta_array(f), a)\n        np.testing.assert_array_equal(make_coord_delta_array([f]), a)\n\n        # float array\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(f)), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array([f])), a)\n\n        # int\n        np.testing.assert_array_equal(make_coord_delta_array(i), a)\n        np.testing.assert_array_equal(make_coord_delta_array([i]), a)\n\n        # int array\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(i)), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array([i])), a)",
            "def test_numerical_array(self):\n        a = np.array([5.0, 5.5])\n        l = [5, 5.5]\n\n        np.testing.assert_array_equal(make_coord_delta_array(l), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(l)), a)",
            "def test_timedelta_singleton(self):\n        a = np.array([np.timedelta64(1, \"D\")])\n        s = \"1,D\"\n        u = \"1,D\"\n        td64 = np.timedelta64(1, \"D\")\n        td = np.timedelta64(1, \"D\").item()\n\n        # str\n        np.testing.assert_array_equal(make_coord_delta_array(s), a)\n        np.testing.assert_array_equal(make_coord_delta_array([s]), a)\n\n        # unicode\n        np.testing.assert_array_equal(make_coord_delta_array(u), a)\n        np.testing.assert_array_equal(make_coord_delta_array([u]), a)\n\n        # timedelta64\n        np.testing.assert_array_equal(make_coord_delta_array(td64), a)\n        np.testing.assert_array_equal(make_coord_delta_array([td64]), a)\n\n        # python timedelta\n        np.testing.assert_array_equal(make_coord_delta_array(td), a)\n        np.testing.assert_array_equal(make_coord_delta_array([td]), a)",
            "def test_date_array(self):\n        a = np.array([np.timedelta64(1, \"D\"), np.timedelta64(2, \"D\")])\n        s = [\"1,D\", \"2,D\"]\n        u = [\"1,D\", \"2,D\"]\n        td64 = [np.timedelta64(1, \"D\"), np.timedelta64(2, \"D\")]\n        td = [np.timedelta64(1, \"D\").item(), np.timedelta64(2, \"D\").item()]\n\n        # str\n        np.testing.assert_array_equal(make_coord_delta_array(s), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(s)), a)\n\n        # unicode\n        np.testing.assert_array_equal(make_coord_delta_array(u), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(u)), a)\n\n        # timedelta64\n        np.testing.assert_array_equal(make_coord_delta_array(td64), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(td64)), a)\n\n        # python timedelta\n        np.testing.assert_array_equal(make_coord_delta_array(td), a)\n        np.testing.assert_array_equal(make_coord_delta_array(np.array(td)), a)",
            "def test_invalid_type(self):\n        with pytest.raises(TypeError):\n            make_coord_delta_array([{}])",
            "def test_mixed_type(self):\n        with pytest.raises(ValueError):\n            make_coord_delta_array([5.0, \"1,D\"])\n\n        with pytest.raises(ValueError):\n            make_coord_delta_array([\"1,D\", 5.0])\n\n        with pytest.raises(ValueError):\n            make_coord_delta_array([5.0, np.timedelta64(1, \"D\")])\n\n        with pytest.raises(ValueError):\n            make_coord_delta_array([np.timedelta64(1, \"D\"), 5.0])",
            "def test_invalid_time_string(self):\n        with pytest.raises(ValueError):\n            make_coord_delta_array([\"invalid\"])",
            "def test_invalid_shape(self):\n        with pytest.raises(ValueError):\n            make_coord_delta_array([[0, 1], [5, 6]])\n\n        with pytest.raises(ValueError):\n            make_coord_delta_array(np.array([[0, 1], [5, 6]]))",
            "def test_add_coord():\n    # numbers\n    assert add_coord(5, 1) == 6\n    assert add_coord(5, -1) == 4\n    assert np.allclose(add_coord(5, np.array([-1, 1])), [4, 6])\n\n    # simple timedeltas\n    td64 = np.timedelta64\n    dt64 = np.datetime64\n    assert add_coord(dt64(\"2018-01-30\"), td64(1, \"D\")) == dt64(\"2018-01-31\")\n    assert add_coord(dt64(\"2018-01-30\"), td64(2, \"D\")) == dt64(\"2018-02-01\")\n    assert add_coord(dt64(\"2018-01-30\"), td64(-1, \"D\")) == dt64(\"2018-01-29\")\n    assert add_coord(dt64(\"2018-01-01\"), td64(-1, \"D\")) == dt64(\"2017-12-31\")\n    assert np.all(\n        add_coord(dt64(\"2018-01-30\"), np.array([td64(1, \"D\"), td64(2, \"D\")]))\n        == np.array([dt64(\"2018-01-31\"), dt64(\"2018-02-01\")])\n    )\n\n    # year timedeltas\n    assert add_coord(dt64(\"2018-01-01\"), td64(1, \"Y\")) == dt64(\"2019-01-01\")\n    assert add_coord(dt64(\"2018-01-01T00:00:00.0000000\"), td64(1, \"Y\")) == dt64(\"2019-01-01\")\n    assert add_coord(dt64(\"2018-01-01\"), td64(-1, \"Y\")) == dt64(\"2017-01-01\")\n    assert add_coord(dt64(\"2020-02-29\"), td64(1, \"Y\")) == dt64(\"2021-02-28\")\n\n    # month timedeltas\n    assert add_coord(dt64(\"2018-01-01\"), td64(1, \"M\")) == dt64(\"2018-02-01\")\n    assert add_coord(dt64(\"2018-01-01\"), td64(-1, \"M\")) == dt64(\"2017-12-01\")\n    assert add_coord(dt64(\"2018-01-01\"), td64(24, \"M\")) == dt64(\"2020-01-01\")\n    assert add_coord(dt64(\"2018-01-31\"), td64(1, \"M\")) == dt64(\"2018-02-28\")\n    assert add_coord(dt64(\"2018-01-31\"), td64(2, \"M\")) == dt64(\"2018-03-31\")\n    assert add_coord(dt64(\"2018-01-31\"), td64(3, \"M\")) == dt64(\"2018-04-30\")\n    assert add_coord(dt64(\"2020-01-31\"), td64(1, \"M\")) == dt64(\"2020-02-29\")\n\n    # type error\n    with pytest.raises(TypeError):\n        add_coord(25.0, dt64(\"2020-01-31\"))\n\n    # this base case is generally not encountered\n    from podpac.core.coordinates.utils import _add_nominal_timedelta\n\n    assert _add_nominal_timedelta(dt64(\"2018-01-30\"), td64(1, \"D\")) == dt64(\"2018-01-31\")",
            "def test_divide_timedelta():\n    # simple\n    assert divide_timedelta(np.timedelta64(2, \"D\"), 2) == np.timedelta64(1, \"D\")\n    assert divide_timedelta(np.timedelta64(5, \"D\"), 2.5) == np.timedelta64(2, \"D\")\n\n    # increase resolution, if necessary\n    assert divide_timedelta(np.timedelta64(1, \"Y\"), 365) == np.timedelta64(1, \"D\")\n    assert divide_timedelta(np.timedelta64(1, \"D\"), 2) == np.timedelta64(12, \"h\")\n    assert divide_timedelta(np.timedelta64(1, \"h\"), 2) == np.timedelta64(30, \"m\")\n    assert divide_timedelta(np.timedelta64(1, \"m\"), 2) == np.timedelta64(30, \"s\")\n    assert divide_timedelta(np.timedelta64(1, \"s\"), 2) == np.timedelta64(500, \"ms\")\n\n    # increase resolution several times, if necessary\n    assert divide_timedelta(np.timedelta64(1, \"D\"), 40) == np.timedelta64(36, \"m\")\n\n    # sometimes, a time is not divisible since we need integer time units\n    with pytest.raises(ValueError, match=\"Cannot divide timedelta .* evenly\"):\n        divide_timedelta(np.timedelta64(1, \"ms\"), 3)\n\n    with pytest.raises(ValueError, match=\"Cannot divide timedelta .* evenly\"):\n        divide_timedelta(np.timedelta64(1, \"D\"), 17)",
            "def test_divide_delta():\n    # numerical\n    assert divide_delta(5.0, 2.0) == 2.5\n\n    # timedelta\n    assert divide_delta(np.timedelta64(2, \"D\"), 2) == np.timedelta64(1, \"D\")\n    assert divide_delta(np.timedelta64(1, \"D\"), 2) == np.timedelta64(12, \"h\")\n    with pytest.raises(ValueError, match=\"Cannot divide timedelta .* evenly\"):\n        divide_delta(np.timedelta64(1, \"D\"), 17)",
            "def test_timedelta_divisible():\n    assert timedelta_divisible(np.timedelta64(1, \"D\"), np.timedelta64(1, \"D\"))\n    assert timedelta_divisible(np.timedelta64(4, \"D\"), np.timedelta64(2, \"D\"))\n    assert timedelta_divisible(np.timedelta64(1, \"D\"), np.timedelta64(6, \"h\"))\n    assert timedelta_divisible(np.timedelta64(1, \"D\"), np.timedelta64(6, \"m\"))\n    assert timedelta_divisible(np.timedelta64(1, \"D\"), np.timedelta64(6, \"s\"))\n    assert timedelta_divisible(np.timedelta64(1, \"Y\"), np.timedelta64(2, \"M\"))\n\n    assert not timedelta_divisible(np.timedelta64(4, \"D\"), np.timedelta64(3, \"D\"))\n    assert not timedelta_divisible(np.timedelta64(1, \"D\"), np.timedelta64(2, \"D\"))\n    assert not timedelta_divisible(np.timedelta64(6, \"h\"), np.timedelta64(1, \"D\"))\n    assert not timedelta_divisible(np.timedelta64(1, \"D\"), np.timedelta64(5, \"h\"))\n\n    assert not timedelta_divisible(np.timedelta64(1, \"M\"), np.timedelta64(1, \"D\"))",
            "def test_has_alt_units():\n    assert has_alt_units(pyproj.CRS(\"+proj=merc\")) is False\n    assert has_alt_units(pyproj.CRS(\"+proj=merc +vunits=m\")) is True",
            "def test_lower_precision_time_bounds():\n    a = [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-02\")]\n    b = [np.datetime64(\"2020-01-01T12:00\"), np.datetime64(\"2020-01-01T14:00\")]\n\n    with pytest.raises(TypeError, match=\"Input bounds should be of type np.datetime64\"):\n        lower_precision_time_bounds(a, [10, 20], False)\n\n    with pytest.raises(TypeError, match=\"Native bounds should be of type np.datetime64\"):\n        lower_precision_time_bounds([10, 20], b, False)\n\n    # outer True\n    a1, b1 = lower_precision_time_bounds(a, b, True)\n    assert a1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-02\")]\n    assert a1[0].dtype == \"<M8[D]\"\n    assert a1[1].dtype == \"<M8[D]\"\n    assert b1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-01\")]\n    assert b1[0].dtype == \"<M8[D]\"\n    assert b1[1].dtype == \"<M8[D]\"\n\n    b1, a1 = lower_precision_time_bounds(b, a, True)\n    assert b1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-01\")]\n    assert b1[0].dtype == \"<M8[D]\"\n    assert b1[1].dtype == \"<M8[D]\"\n    assert a1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-02\")]\n    assert a1[0].dtype == \"<M8[D]\"\n    assert a1[1].dtype == \"<M8[D]\"\n\n    # outer False\n    a1, b1 = lower_precision_time_bounds(a, b, False)\n    assert a1 == [np.datetime64(\"2020-01-01T00:00\"), np.datetime64(\"2020-01-02T00:00\")]\n    assert a1[0].dtype == \"<M8[m]\"\n    assert a1[1].dtype == \"<M8[m]\"\n    assert b1 == [np.datetime64(\"2020-01-01T12:00\"), np.datetime64(\"2020-01-01T14:00\")]\n    assert b1[0].dtype == \"<M8[m]\"\n    assert b1[1].dtype == \"<M8[m]\"\n\n    b1, a1 = lower_precision_time_bounds(b, a, False)\n    assert b1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-01\")]\n    assert b1[0].dtype == \"<M8[D]\"\n    assert b1[1].dtype == \"<M8[D]\"\n    assert a1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-02\")]\n    assert a1[0].dtype == \"<M8[D]\"\n    assert a1[1].dtype == \"<M8[D]\"",
            "def test_higher_precision_time_bounds():\n    a = [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-02\")]\n    b = [np.datetime64(\"2020-01-01T12:00\"), np.datetime64(\"2020-01-01T14:00\")]\n\n    with pytest.raises(TypeError, match=\"Input bounds should be of type np.datetime64\"):\n        higher_precision_time_bounds(a, [10, 20], False)\n\n    with pytest.raises(TypeError, match=\"Native bounds should be of type np.datetime64\"):\n        higher_precision_time_bounds([10, 20], b, False)\n\n    # outer True\n    a1, b1 = higher_precision_time_bounds(a, b, True)\n    assert a1 == [np.datetime64(\"2020-01-01T00:00\"), np.datetime64(\"2020-01-02T00:00\")]\n    assert a1[0].dtype == \"<M8[m]\"\n    assert a1[1].dtype == \"<M8[m]\"\n    assert b1 == [np.datetime64(\"2020-01-01T12:00\"), np.datetime64(\"2020-01-01T14:00\")]\n    assert b1[0].dtype == \"<M8[m]\"\n    assert b1[1].dtype == \"<M8[m]\"\n\n    b1, a1 = higher_precision_time_bounds(b, a, True)\n    assert b1 == [np.datetime64(\"2020-01-01T12:00\"), np.datetime64(\"2020-01-01T14:00\")]\n    assert b1[0].dtype == \"<M8[m]\"\n    assert b1[1].dtype == \"<M8[m]\"\n    assert a1 == [np.datetime64(\"2020-01-01T00:00\"), np.datetime64(\"2020-01-02T23:59\")]\n    assert a1[0].dtype == \"<M8[m]\"\n    assert a1[1].dtype == \"<M8[m]\"\n\n    # outer False\n    a1, b1 = higher_precision_time_bounds(a, b, False)\n    assert a1 == [np.datetime64(\"2020-01-01T00:00\"), np.datetime64(\"2020-01-02T00:00\")]\n    assert a1[0].dtype == \"<M8[m]\"\n    assert a1[1].dtype == \"<M8[m]\"\n    assert b1 == [np.datetime64(\"2020-01-01T12:00\"), np.datetime64(\"2020-01-01T14:00\")]\n    assert b1[0].dtype == \"<M8[m]\"\n    assert b1[1].dtype == \"<M8[m]\"\n\n    b1, a1 = higher_precision_time_bounds(b, a, False)\n    assert b1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-01\")]\n    assert b1[0].dtype == \"<M8[D]\"\n    assert b1[1].dtype == \"<M8[D]\"\n    assert a1 == [np.datetime64(\"2020-01-01\"), np.datetime64(\"2020-01-02\")]\n    assert a1[0].dtype == \"<M8[D]\"\n    assert a1[1].dtype == \"<M8[D]\""
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_affine_coordinates.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestAffineCoordinatesCreation(object):",
            "def test_init(self):\n        c = AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(3, 4))\n\n        assert c.geotransform == GEOTRANSFORM_NORTHUP\n        assert c.shape == (3, 4)\n        assert c.is_affine\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert len(set(c.xdims)) == 2\n        assert c.name == \"lat_lon\"\n        repr(c)",
            "def test_rotated(self):\n        c = AffineCoordinates(geotransform=GEOTRANSFORM_ROTATED, shape=(3, 4))\n\n        assert c.geotransform == GEOTRANSFORM_ROTATED\n        assert c.shape == (3, 4)\n        assert c.is_affine\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert len(set(c.xdims)) == 2\n        assert c.name == \"lat_lon\"\n        repr(c)",
            "def test_invalid(self):\n        with pytest.raises(ValueError, match=\"Invalid shape\"):\n            AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(-3, 4))\n\n        with pytest.raises(ValueError, match=\"Invalid shape\"):\n            AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(3, 0))",
            "def test_size_one_simplify(self):\n        c = AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(1, 1))\n        c2 = c.simplify()\n        assert isinstance(c2[0], UniformCoordinates1d)\n        assert isinstance(c2[1], UniformCoordinates1d)\n        assert c2[0].shape == (1,)\n        assert c2[1].shape == (1,)\n        assert c2[0].step == GEOTRANSFORM_NORTHUP[-1]\n        assert c2[1].step == GEOTRANSFORM_NORTHUP[1]\n        assert c2[1].name == \"lon\"\n        assert c2[0].name == \"lat\"\n\n    #",
            "def test_copy(self):\n    #     c = AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(3, 4))\n    #     c2 = c.copy()\n    #     assert c2 is not c\n    #     assert c2 == c\n\n\n#",
            "class TestAffineCoordinatesStandardMethods(object):\n#",
            "def test_eq_type(self):\n#         c = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         assert c != []\n\n#",
            "def test_eq_shape(self):\n#         c1 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         c2 = RotatedCoordinates(shape=(4, 3), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         assert c1 != c2\n\n#",
            "def test_eq_affine(self):\n#         c1 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         c2 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         c3 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 3, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         c4 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[11, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         c5 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.1, 2.0], dims=[\"lat\", \"lon\"])\n\n#         assert c1 == c2\n#         assert c1 != c3\n#         assert c1 != c4\n#         assert c1 != c5\n\n#",
            "def test_eq_dims(self):\n#         c1 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         c2 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lon\", \"lat\"])\n#         assert c1 != c2\n\n\n#",
            "class TestRotatedCoordinatesSerialization(object):\n#",
            "def test_definition(self):\n#         c = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         d = c.definition\n\n#         assert isinstance(d, dict)\n#         json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n#         c2 = RotatedCoordinates.from_definition(d)\n#         assert c2 == c\n\n#",
            "def test_from_definition_corner(self):\n#         c1 = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], corner=[15, 17], dims=[\"lat\", \"lon\"])\n\n#         d = {\"shape\": (3, 4), \"theta\": np.pi / 4, \"origin\": [10, 20], \"corner\": [15, 17], \"dims\": [\"lat\", \"lon\"]}\n#         c2 = RotatedCoordinates.from_definition(d)\n\n#         assert c1 == c2\n\n#",
            "def test_invalid_definition(self):\n#         d = {\"theta\": np.pi / 4, \"origin\": [10, 20], \"step\": [1.0, 2.0], \"dims\": [\"lat\", \"lon\"]}\n#         with pytest.raises(ValueError, match='RotatedCoordinates definition requires \"shape\"'):\n#             RotatedCoordinates.from_definition(d)\n\n#         d = {\"shape\": (3, 4), \"origin\": [10, 20], \"step\": [1.0, 2.0], \"dims\": [\"lat\", \"lon\"]}\n#         with pytest.raises(ValueError, match='RotatedCoordinates definition requires \"theta\"'):\n#             RotatedCoordinates.from_definition(d)\n\n#         d = {\"shape\": (3, 4), \"theta\": np.pi / 4, \"step\": [1.0, 2.0], \"dims\": [\"lat\", \"lon\"]}\n#         with pytest.raises(ValueError, match='RotatedCoordinates definition requires \"origin\"'):\n#             RotatedCoordinates.from_definition(d)\n\n#         d = {\"shape\": (3, 4), \"theta\": np.pi / 4, \"origin\": [10, 20], \"dims\": [\"lat\", \"lon\"]}\n#         with pytest.raises(ValueError, match='RotatedCoordinates definition requires \"step\" or \"corner\"'):\n#             RotatedCoordinates.from_definition(d)\n\n#         d = {\"shape\": (3, 4), \"theta\": np.pi / 4, \"origin\": [10, 20], \"step\": [1.0, 2.0]}\n#         with pytest.raises(ValueError, match='RotatedCoordinates definition requires \"dims\"'):\n#             RotatedCoordinates.from_definition(d)\n\n#",
            "def test_full_definition(self):\n#         c = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         d = c.full_definition\n\n#         assert isinstance(d, dict)\n#         assert set(d.keys()) == {\"dims\", \"shape\", \"theta\", \"origin\", \"step\"}\n#         json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable",
            "class TestAffineCoordinatesProperties(object):",
            "def test_origin(self):\n        c = AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(3, 4))\n        np.testing.assert_array_equal(c.origin, [20.0, 10.0])",
            "def test_origin_rotated(self):\n        # lat, lon\n        c = AffineCoordinates(geotransform=GEOTRANSFORM_ROTATED, shape=(3, 4))\n        np.testing.assert_array_equal(c.origin, [20.0, 10.0])",
            "def test_coordinates(self):\n        c = AffineCoordinates(geotransform=GEOTRANSFORM_NORTHUP, shape=(3, 4))\n\n        assert c.coordinates.shape == (3, 4, 2)\n\n        lat = c.coordinates[:, :, 0]\n        lon = c.coordinates[:, :, 1]\n\n        np.testing.assert_allclose(\n            lat,\n            [\n                [18.5, 18.5, 18.5, 18.5],\n                [15.5, 15.5, 15.5, 15.5],\n                [12.5, 12.5, 12.5, 12.5],\n            ],\n        )\n\n        np.testing.assert_allclose(\n            lon,\n            [\n                [11.0, 13.0, 15.0, 17.0],\n                [11.0, 13.0, 15.0, 17.0],\n                [11.0, 13.0, 15.0, 17.0],\n            ],\n        )\n\n    #",
            "def test_coordinates_rotated(self):\n    #     c = AffineCoordinates(geotransform=GEOTRANSFORM_ROTATED, shape=(3, 4))\n\n    #     assert c.coordinates.shape == (3, 4, 2)\n\n    #     # lat\n    #     np.testing.assert_allclose(\n    #         c.coordinates[:, :, 0],\n    #         [\n    #             [20.000, 22.819, 25.638, 28.457],\n    #             [20.684, 23.503, 26.322, 29.141],\n    #             [21.368, 24.187, 27.006, 29.825]\n    #         ],\n    #     )\n\n    #     # lon\n    #     np.testing.assert_allclose(\n    #         c.coordinates[:, :, 1],\n    #         [\n    #             [10.000,  8.974,  7.948,  6.922],\n    #             [11.879, 10.853,  9.827,  8.801],\n    #             [13.758, 12.732, 11.706, 10.680],\n    #         ],\n    #     )\n\n\n#",
            "class TestRotatedCoordinatesIndexing(object):\n#",
            "def test_get_dim(self):\n#         c = RotatedCoordinates(shape=(3, 4), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n\n#         lat = c[\"lat\"]\n#         lon = c[\"lon\"]\n#         assert isinstance(lat, ArrayCoordinates1d)\n#         assert isinstance(lon, ArrayCoordinates1d)\n#         assert lat.name == \"lat\"\n#         assert lon.name == \"lon\"\n#         assert_equal(lat.coordinates, c.coordinates[0])\n#         assert_equal(lon.coordinates, c.coordinates[1])\n\n#         with pytest.raises(KeyError, match=\"Dimension .* not found\"):\n#             c[\"other\"]\n\n#",
            "def test_get_index_slices(self):\n#         c = RotatedCoordinates(shape=(5, 7), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n\n#         # full\n#         c2 = c[1:4, 2:4]\n#         assert isinstance(c2, RotatedCoordinates)\n#         assert c2.shape == (3, 2)\n#         assert c2.theta == c.theta\n#         np.testing.assert_allclose(c2.origin, [c.coordinates[0][1, 2], c.coordinates[1][1, 2]])\n#         np.testing.assert_allclose(c2.corner, [c.coordinates[0][3, 3], c.coordinates[1][3, 3]])\n#         assert c2.dims == c.dims\n#         np.testing.assert_allclose(c2.coordinates[0], c.coordinates[0][1:4, 2:4])\n#         np.testing.assert_allclose(c2.coordinates[1], c.coordinates[1][1:4, 2:4])\n\n#         # partial/implicit\n#         c2 = c[1:4]\n#         assert isinstance(c2, RotatedCoordinates)\n#         assert c2.shape == (3, 7)\n#         assert c2.theta == c.theta\n#         np.testing.assert_allclose(c2.origin, c.coordinates[0][1, 0], c.coordinates[1][1, 0])\n#         np.testing.assert_allclose(c2.corner, c.coordinates[0][3, -1], c.coordinates[1][3, -1])\n#         assert c2.dims == c.dims\n#         np.testing.assert_allclose(c2.coordinates[0], c.coordinates[0][1:4])\n#         np.testing.assert_allclose(c2.coordinates[1], c.coordinates[1][1:4])\n\n#         # stepped\n#         c2 = c[1:4:2, 2:4]\n#         assert isinstance(c2, RotatedCoordinates)\n#         assert c2.shape == (2, 2)\n#         assert c2.theta == c.theta\n#         np.testing.assert_allclose(c2.origin, [c.coordinates[0][1, 2], c.coordinates[1][1, 2]])\n#         np.testing.assert_allclose(c2.corner, [c.coordinates[0][3, 3], c.coordinates[1][3, 3]])\n#         assert c2.dims == c.dims\n#         np.testing.assert_allclose(c2.coordinates[0], c.coordinates[0][1:4:2, 2:4])\n#         np.testing.assert_allclose(c2.coordinates[1], c.coordinates[1][1:4:2, 2:4])\n\n#         # reversed\n#         c2 = c[4:1:-1, 2:4]\n#         assert isinstance(c2, RotatedCoordinates)\n#         assert c2.shape == (3, 2)\n#         assert c2.theta == c.theta\n#         np.testing.assert_allclose(c2.origin, c.coordinates[0][1, 2], c.coordinates[0][1, 2])\n#         np.testing.assert_allclose(c2.corner, c.coordinates[0][3, 3], c.coordinates[0][3, 3])\n#         assert c2.dims == c.dims\n#         np.testing.assert_allclose(c2.coordinates[0], c.coordinates[0][4:1:-1, 2:4])\n#         np.testing.assert_allclose(c2.coordinates[1], c.coordinates[1][4:1:-1, 2:4])\n\n#",
            "def test_get_index_fallback(self):\n#         c = RotatedCoordinates(shape=(5, 7), theta=np.pi / 4, origin=[10, 20], step=[1.0, 2.0], dims=[\"lat\", \"lon\"])\n#         lat, lon = c.coordinates\n\n#         I = [3, 1]\n#         J = slice(1, 4)\n#         B = lat > 6\n\n#         # int/slice/indices\n#         c2 = c[I, J]\n#         assert isinstance(c2, StackedCoordinates)\n#         assert c2.shape == (2, 3)\n#         assert c2.dims == c.dims\n#         assert_equal(c2[\"lat\"].coordinates, lat[I, J])\n#         assert_equal(c2[\"lon\"].coordinates, lon[I, J])\n\n#         # boolean\n#         c2 = c[B]\n#         assert isinstance(c2, StackedCoordinates)\n#         assert c2.shape == (21,)\n#         assert c2.dims == c.dims\n#         assert_equal(c2[\"lat\"].coordinates, lat[B])\n#         assert_equal(c2[\"lon\"].coordinates, lon[B])\n\n\n#",
            "class TestRotatedCoordinatesSelection(object):\n#",
            "def test_select_single(self):\n#         c = RotatedCoordinates([LAT, LON], dims=['lat', 'lon'])\n\n#         # single dimension\n#         bounds = {'lat': [0.25, .55]}\n#         E0, E1 = [0, 1, 1, 1], [3, 0, 1, 2] # expected\n\n#         s = c.select(bounds)\n#         assert isinstance(s, StackedCoordinates)\n#         assert s == c[E0, E1]\n\n#         s, I = c.select(bounds, return_index=True)\n#         assert isinstance(s, StackedCoordinates)\n#         assert s == c[I]\n#         assert_equal(I[0], E0)\n#         assert_equal(I[1], E1)\n\n#         # a different single dimension\n#         bounds = {'lon': [12.5, 17.5]}\n#         E0, E1 = [0, 1, 1, 1, 1, 2], [3, 0, 1, 2, 3, 0]\n\n#         s = c.select(bounds)\n#         assert isinstance(s, StackedCoordinates)\n#         assert s == c[E0, E1]\n\n#         s, I = c.select(bounds, return_index=True)\n#         assert isinstance(s, StackedCoordinates)\n#         assert s == c[I]\n#         assert_equal(I[0], E0)\n#         assert_equal(I[1], E1)\n\n#         # outer\n#         bounds = {'lat': [0.25, .75]}\n#         E0, E1 = [0, 0, 1, 1, 1, 1, 2, 2], [2, 3, 0, 1, 2, 3, 0, 1]\n\n#         s = c.select(bounds, outer=True)\n#         assert isinstance(s, StackedCoordinates)\n#         assert s == c[E0, E1]\n\n#         s, I = c.select(bounds, outer=True, return_index=True)\n#         assert isinstance(s, StackedCoordinates)\n#         assert s == c[E0, E1]\n#         assert_equal(I[0], E0)\n#         assert_equal(I[1], E1)\n\n#         # no matching dimension\n#         bounds = {'alt': [0, 10]}\n#         s = c.select(bounds)\n#         assert s == c\n\n#         s, I = c.select(bounds, return_index=True)\n#         assert s == c[I]\n#         assert s == c\n\n#",
            "def test_select_multiple(self):\n#         c = RotatedCoordinates([LAT, LON], dims=['lat', 'lon'])\n\n#         # this should be the AND of both intersections\n#         bounds = {'lat': [0.25, 0.95], 'lon': [10.5, 17.5]}\n#         E0, E1 = [0, 1, 1, 1, 1, 2], [3, 0, 1, 2, 3, 0]\n#         s = c.select(bounds)\n#         assert s == c[E0, E1]\n\n#         s, I = c.select(bounds, return_index=True)\n#         assert s == c[E0, E1]\n#         assert_equal(I[0], E0)\n#         assert_equal(I[1], E1)\n\n#",
            "def test_intersect(self):\n#         c = RotatedCoordinates([LAT, LON], dims=['lat', 'lon'])\n\n\n#         other_lat = ArrayCoordinates1d([0.25, 0.5, .95], name='lat')\n#         other_lon = ArrayCoordinates1d([10.5, 15, 17.5], name='lon')\n\n#         # single other\n#         E0, E1 = [0, 1, 1, 1, 1, 2, 2, 2], [3, 0, 1, 2, 3, 0, 1, 2]\n#         s = c.intersect(other_lat)\n#         assert s == c[E0, E1]\n\n#         s, I = c.intersect(other_lat, return_index=True)\n#         assert s == c[E0, E1]\n#         assert s == c[I]\n\n#         E0, E1 = [0, 0, 1, 1, 1, 1, 2, 2, 2, 2], [2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n#         s = c.intersect(other_lat, outer=True)\n#         assert s == c[E0, E1]\n\n#         E0, E1 = [0, 0, 0, 1, 1, 1, 1, 2], [1, 2, 3, 0, 1, 2, 3, 0]\n#         s = c.intersect(other_lon)\n#         assert s == c[E0, E1]\n\n#         # multiple, in various ways\n#         E0, E1 = [0, 1, 1, 1, 1, 2], [3, 0, 1, 2, 3, 0]\n\n#         other = StackedCoordinates([other_lat, other_lon])\n#         s = c.intersect(other)\n#         assert s == c[E0, E1]\n\n#         other = StackedCoordinates([other_lon, other_lat])\n#         s = c.intersect(other)\n#         assert s == c[E0, E1]\n\n#         from podpac.coordinates import Coordinates\n#         other = Coordinates([other_lat, other_lon])\n#         s = c.intersect(other)\n#         assert s == c[E0, E1]\n\n#         # full\n#         other = Coordinates(['2018-01-01'], dims=['time'])\n#         s = c.intersect(other)\n#         assert s == c\n\n#         s, I = c.intersect(other, return_index=True)\n#         assert s == c\n#         assert s == c[I]\n\n#",
            "def test_intersect_invalid(self):\n#         c = RotatedCoordinates([LAT, LON], dims=['lat', 'lon'])\n\n#         with pytest.raises(TypeError, match=\"Cannot intersect with type\"):\n#             c.intersect({})\n\n#         with pytest.raises(ValueError, match=\"Cannot intersect mismatched dtypes\"):\n#             c.intersect(ArrayCoordinates1d(['2018-01-01'], name='lat'))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_polar_coordinates.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestPolarCoordinatesCreation(object):",
            "def test_init(self):\n        theta = np.linspace(0, 2 * np.pi, 9)[:-1]\n\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta=theta, dims=[\"lat\", \"lon\"])\n        assert_equal(c.center, [1.5, 2.0])\n        assert_equal(c.theta.coordinates, theta)\n        assert_equal(c.radius.coordinates, [1, 2, 4, 5])\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"r\", \"t\")\n        assert c.name == \"lat_lon\"\n        assert c.shape == (4, 8)\n        repr(c)\n\n        # uniform theta\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        assert c.theta.start == 0\n        assert c.theta.size == 8\n        assert c.dims == (\"lat\", \"lon\")\n        assert c.udims == (\"lat\", \"lon\")\n        assert c.xdims == (\"r\", \"t\")\n        assert c.name == \"lat_lon\"\n        assert c.shape == (4, 8)\n        repr(c)",
            "def test_invalid(self):\n        with pytest.raises(TypeError, match=\"PolarCoordinates expected theta or theta_size, not both\"):\n            PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta=[0, 1, 2], theta_size=8, dims=[\"lat\", \"lon\"])\n\n        with pytest.raises(TypeError, match=\"PolarCoordinates requires theta or theta_size\"):\n            PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], dims=[\"lat\", \"lon\"])\n\n        with pytest.raises(ValueError, match=\"PolarCoordinates radius must all be positive\"):\n            PolarCoordinates(center=[1.5, 2.0], radius=[-1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n\n        with pytest.raises(ValueError, match=\"PolarCoordinates dims\"):\n            PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"time\"])\n\n        with pytest.raises(ValueError, match=\"Duplicate dimension\"):\n            PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lat\"])",
            "def test_copy(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        c2 = c.copy()\n        assert c2 is not c\n        assert c2 == c",
            "class TestPolarCoordinatesStandardMethods(object):",
            "def test_eq_type(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        assert c != []",
            "def test_eq_center(self):\n        c1 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        c2 = PolarCoordinates(center=[1.5, 2.5], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        assert c1 != c2",
            "def test_eq_radius(self):\n        c1 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        c2 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4], theta_size=8, dims=[\"lat\", \"lon\"])\n        assert c1 != c2",
            "def test_eq_theta(self):\n        c1 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        c2 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=7, dims=[\"lat\", \"lon\"])\n        assert c1 != c2",
            "def test_eq(self):\n        c1 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        c2 = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        assert c1 == c2",
            "class TestPolarCoordinatesSerialization(object):",
            "def test_definition(self):\n        # array radius and theta, plus other checks\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta=[0, 1, 2], dims=[\"lat\", \"lon\"])\n        d = c.definition\n\n        assert isinstance(d, dict)\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable\n        c2 = PolarCoordinates.from_definition(d)\n        assert c2 == c\n\n        # uniform radius and theta\n        c = PolarCoordinates(\n            center=[1.5, 2.0], radius=clinspace(1, 5, 4), theta=clinspace(0, np.pi, 5), dims=[\"lat\", \"lon\"]\n        )\n        d = c.definition\n        c2 = PolarCoordinates.from_definition(d)\n        assert c2 == c",
            "def test_from_definition(self):\n        # radius and theta lists\n        d = {\"center\": [1.5, 2.0], \"radius\": [1, 2, 4, 5], \"theta\": [0, 1, 2], \"dims\": [\"lat\", \"lon\"]}\n        c = PolarCoordinates.from_definition(d)\n        assert_allclose(c.center, [1.5, 2.0])\n        assert_allclose(c.radius.coordinates, [1, 2, 4, 5])\n        assert_allclose(c.theta.coordinates, [0, 1, 2])\n        assert c.dims == (\"lat\", \"lon\")\n\n        # theta size\n        d = {\"center\": [1.5, 2.0], \"radius\": [1, 2, 4, 5], \"theta_size\": 8, \"dims\": [\"lat\", \"lon\"]}\n        c = PolarCoordinates.from_definition(d)\n        assert_allclose(c.center, [1.5, 2.0])\n        assert_allclose(c.radius.coordinates, [1, 2, 4, 5])\n        assert_allclose(c.theta.coordinates, np.linspace(0, 2 * np.pi, 9)[:-1])\n        assert c.dims == (\"lat\", \"lon\")",
            "def test_invalid_definition(self):\n        d = {\"radius\": [1, 2, 4, 5], \"theta\": [0, 1, 2], \"dims\": [\"lat\", \"lon\"]}\n        with pytest.raises(ValueError, match='PolarCoordinates definition requires \"center\"'):\n            PolarCoordinates.from_definition(d)\n\n        d = {\"center\": [1.5, 2.0], \"theta\": [0, 1, 2], \"dims\": [\"lat\", \"lon\"]}\n        with pytest.raises(ValueError, match='PolarCoordinates definition requires \"radius\"'):\n            PolarCoordinates.from_definition(d)\n\n        d = {\"center\": [1.5, 2.0], \"radius\": [1, 2, 4, 5], \"dims\": [\"lat\", \"lon\"]}\n        with pytest.raises(ValueError, match='PolarCoordinates definition requires \"theta\" or \"theta_size\"'):\n            PolarCoordinates.from_definition(d)\n\n        d = {\"center\": [1.5, 2.0], \"radius\": [1, 2, 4, 5], \"theta\": [0, 1, 2]}\n        with pytest.raises(ValueError, match='PolarCoordinates definition requires \"dims\"'):\n            PolarCoordinates.from_definition(d)\n\n        d = {\"center\": [1.5, 2.0], \"radius\": {\"a\": 1}, \"theta\": [0, 1, 2], \"dims\": [\"lat\", \"lon\"]}\n        with pytest.raises(ValueError, match=\"Could not parse radius coordinates\"):\n            PolarCoordinates.from_definition(d)\n\n        d = {\"center\": [1.5, 2.0], \"radius\": [1, 2, 4, 5], \"theta\": {\"a\": 1}, \"dims\": [\"lat\", \"lon\"]}\n        with pytest.raises(ValueError, match=\"Could not parse theta coordinates\"):\n            PolarCoordinates.from_definition(d)",
            "def test_full_definition(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta=[0, 1, 2], dims=[\"lat\", \"lon\"])\n        d = c.full_definition\n\n        assert isinstance(d, dict)\n        assert set(d.keys()) == {\"dims\", \"radius\", \"center\", \"theta\"}\n        json.dumps(d, cls=podpac.core.utils.JSONEncoder)  # test serializable",
            "class TestPolarCoordinatesProperties(object):",
            "def test_coordinates(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4], theta_size=4, dims=[\"lat\", \"lon\"])\n        lat, lon = c.coordinates\n\n        assert_allclose(lat, [[1.5, 2.5, 1.5, 0.5], [1.5, 3.5, 1.5, -0.5], [1.5, 5.5, 1.5, -2.5]])\n\n        assert_allclose(lon, [[3.0, 2.0, 1.0, 2.0], [4.0, 2.0, 0.0, 2.0], [6.0, 2.0, -2.0, 2.0]])",
            "class TestPolarCoordinatesIndexing(object):",
            "def test_get_dim(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n\n        lat = c[\"lat\"]\n        lon = c[\"lon\"]\n        assert isinstance(lat, ArrayCoordinates1d)\n        assert isinstance(lon, ArrayCoordinates1d)\n        assert lat.name == \"lat\"\n        assert lon.name == \"lon\"\n        assert_equal(lat.coordinates, c.coordinates[0])\n        assert_equal(lon.coordinates, c.coordinates[1])\n\n        with pytest.raises(KeyError, match=\"Dimension .* not found\"):\n            c[\"other\"]",
            "def test_get_index_slices(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5, 6], theta_size=8, dims=[\"lat\", \"lon\"])\n\n        # full\n        c2 = c[1:4, 2:4]\n        assert isinstance(c2, PolarCoordinates)\n        assert c2.shape == (3, 2)\n        assert_allclose(c2.center, c.center)\n        assert c2.radius == c.radius[1:4]\n        assert c2.theta == c.theta[2:4]\n        assert c2.dims == c.dims\n        assert_allclose(c2.coordinates[0], c.coordinates[0][1:4, 2:4])\n        assert_allclose(c2.coordinates[1], c.coordinates[1][1:4, 2:4])\n\n        # partial/implicit\n        c2 = c[1:4]\n        assert isinstance(c2, PolarCoordinates)\n        assert c2.shape == (3, 8)\n        assert_allclose(c2.center, c.center)\n        assert c2.radius == c.radius[1:4]\n        assert c2.theta == c.theta\n        assert c2.dims == c.dims\n        assert_allclose(c2.coordinates[0], c.coordinates[0][1:4])\n        assert_allclose(c2.coordinates[1], c.coordinates[1][1:4])\n\n        # stepped\n        c2 = c[1:4:2, 2:4]\n        assert isinstance(c2, PolarCoordinates)\n        assert c2.shape == (2, 2)\n        assert_allclose(c2.center, c.center)\n        assert c2.radius == c.radius[1:4:2]\n        assert c2.theta == c.theta[2:4]\n        assert c2.dims == c.dims\n        assert_allclose(c2.coordinates[0], c.coordinates[0][1:4:2, 2:4])\n        assert_allclose(c2.coordinates[1], c.coordinates[1][1:4:2, 2:4])\n\n        # reversed\n        c2 = c[4:1:-1, 2:4]\n        assert isinstance(c2, PolarCoordinates)\n        assert c2.shape == (3, 2)\n        assert_allclose(c2.center, c.center)\n        assert c2.radius == c.radius[4:1:-1]\n        assert c2.theta == c.theta[2:4]\n        assert c2.dims == c.dims\n        assert_allclose(c2.coordinates[0], c.coordinates[0][4:1:-1, 2:4])\n        assert_allclose(c2.coordinates[1], c.coordinates[1][4:1:-1, 2:4])",
            "def test_get_index_fallback(self):\n        c = PolarCoordinates(center=[1.5, 2.0], radius=[1, 2, 4, 5], theta_size=8, dims=[\"lat\", \"lon\"])\n        lat, lon = c.coordinates\n\n        Ra = [3, 1]\n        Th = slice(1, 4)\n        B = lat > 0.5\n\n        # int/slice/indices\n        c2 = c[Ra, Th]\n        assert isinstance(c2, StackedCoordinates)\n        assert c2.shape == (2, 3)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, lat[Ra, Th])\n        assert_equal(c2[\"lon\"].coordinates, lon[Ra, Th])\n\n        # boolean\n        c2 = c[B]\n        assert isinstance(c2, StackedCoordinates)\n        assert c2.shape == (22,)\n        assert c2.dims == c.dims\n        assert_equal(c2[\"lat\"].coordinates, lat[B])\n        assert_equal(c2[\"lon\"].coordinates, lon[B])"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/coordinates/test/test_base_coordinates.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestBaseCoordinates(object):",
            "def test_common_api(self):\n        c = BaseCoordinates()\n\n        attrs = [\n            \"name\",\n            \"dims\",\n            \"xdims\",\n            \"udims\",\n            \"coordinates\",\n            \"xcoords\",\n            \"size\",\n            \"shape\",\n            \"definition\",\n            \"full_definition\",\n        ]\n        for attr in attrs:\n            try:\n                getattr(c, attr)\n            except NotImplementedError:\n                pass\n\n        for method_name in [\"_set_name\"]:\n            try:\n                method = getattr(c, method_name)\n                method(None)\n            except NotImplementedError:\n                pass\n\n        try:\n            c.from_definition({})\n        except NotImplementedError:\n            pass\n\n        try:\n            c.copy()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.unique()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.get_area_bounds(None)\n        except NotImplementedError:\n            pass\n\n        try:\n            c.select([0, 1])\n        except NotImplementedError:\n            pass\n\n        try:\n            c.select([0, 1], outer=True, return_index=True)\n        except NotImplementedError:\n            pass\n\n        try:\n            c[0]\n        except NotImplementedError:\n            pass\n\n        try:\n            repr(c)\n        except NotImplementedError:\n            pass\n\n        try:\n            c == c\n        except NotImplementedError:\n            pass\n\n        try:\n            c.simplify()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.flatten()\n        except NotImplementedError:\n            pass\n\n        try:\n            c.reshape((10, 10))\n        except NotImplementedError:\n            pass\n\n        try:\n            c.issubset(c)\n        except NotImplementedError:\n            pass"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/utility.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nUtility Algorithm Nodes.\nThese nodes are mainly used for testing.\n\"\"\"",
            "\"\"\"A simple test node that gives each value in the output a number.\"\"\"",
            "\"\"\"Uses np.arange to give each value in output a unique number\n\n        Arguments\n        ---------\n        inputs : dict\n            Unused, should be empty for this algorithm.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n\n        Returns\n        -------\n        UnitsDataArray\n            A row-majored numbered array of the requested size.\n        \"\"\"",
            "\"\"\"Extracts the coordinates from a request and makes it available as a data\n\n    Attributes\n    ----------\n    coord_name : str\n        Name of coordinate to extract (one of lat, lon, time, alt)\n    \"\"\"",
            "\"\"\"Extract coordinate from request and makes data available.\n\n        Arguments\n        ----------\n        inputs : dict\n            Unused, should be empty for this algorithm.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        UnitsDataArray\n            The coordinates as data for the requested coordinate.\n        \"\"\"",
            "\"\"\"A simple test node that creates a data based on coordinates and trigonometric (sin) functions.\"\"\"",
            "\"\"\"Computes sinusoids of all the coordinates.\n\n        Arguments\n        ----------\n        inputs : dict\n            Unused, should be empty for this algorithm.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n\n        Returns\n        -------\n        UnitsDataArray\n            Sinusoids of a certain period for all of the requested coordinates\n        \"\"\""
        ],
        "code_snippets": [
            "class Arange(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"Uses np.arange to give each value in output a unique number\n\n        Arguments\n        ---------\n        inputs : dict\n            Unused, should be empty for this algorithm.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n\n        Returns\n        -------\n        UnitsDataArray\n            A row-majored numbered array of the requested size.\n        \"\"\"\n        data = np.arange(coordinates.size).reshape(coordinates.shape)\n        return self.create_output_array(coordinates, data=data)",
            "class CoordData(Algorithm):\n    \"\"\"Extracts the coordinates from a request and makes it available as a data\n\n    Attributes\n    ----------\n    coord_name : str\n        Name of coordinate to extract (one of lat, lon, time, alt)\n    \"\"\"\n\n    coord_name = tl.Enum([\"time\", \"lat\", \"lon\", \"alt\"], default_value=\"none\", allow_none=False).tag(\n        attr=True, required=True\n    )",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"Extract coordinate from request and makes data available.\n\n        Arguments\n        ----------\n        inputs : dict\n            Unused, should be empty for this algorithm.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        UnitsDataArray\n            The coordinates as data for the requested coordinate.\n        \"\"\"\n\n        if self.coord_name not in coordinates.udims:\n            raise ValueError(\"Coordinate name not in evaluated coordinates\")\n\n        c = coordinates[self.coord_name]\n        coords = Coordinates([c], validate_crs=False)\n        return self.create_output_array(coords, data=c.coordinates)",
            "class SinCoords(Algorithm):",
            "def _default_style(self):\n        return Style(clim=[-1.0, 1.0], colormap=\"jet\")",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"Computes sinusoids of all the coordinates.\n\n        Arguments\n        ----------\n        inputs : dict\n            Unused, should be empty for this algorithm.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n\n        Returns\n        -------\n        UnitsDataArray\n            Sinusoids of a certain period for all of the requested coordinates\n        \"\"\"\n        out = self.create_output_array(coordinates, data=1.0)\n        crds = list(out.coords.values())\n        try:\n            i_time = list(out.coords.keys()).index(\"time\")\n            crds[i_time] = crds[i_time].astype(\"datetime64[h]\").astype(float)\n        except ValueError:\n            pass  # Value error indicates the source does not have time\n\n        crds = np.meshgrid(*crds, indexing=\"ij\")\n        for crd in crds:\n            out *= np.sin(np.pi * crd / 90.0)\n        return out"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/generic.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nGeneral-purpose Algorithm Nodes.\n\"\"\"",
            "\"\"\"Base class for Algorithms that accept generic named inputs.\"\"\"",
            "\"\"\"Create a simple point-by-point computation using named input nodes.\n\n    Examples\n    ----------\n    a = SinCoords()\n    b = Arange()\n    arith = Arithmetic(A=a, B=b, eqn = 'A * B + {offset}', params={'offset': 1})\n    \"\"\"",
            "\"\"\"Compute the algorithms equation\n\n        Attributes\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        result : UnitsDataArray\n            Algorithm result.\n        \"\"\"",
            "\"\"\"\n    Generic Algorithm Node that allows arbitrary Python code to be executed.\n\n    Attributes\n    ----------\n    code : str\n        The multi-line code that will be evaluated. This code should assign \"output\" to the desired result, and \"output\"\n        needs to be a \"numpy array\" or \"xarray DataArray\"\n    inputs : dict(str: podpac.Node)\n        A dictionary of PODPAC nodes that will serve as the input data for the Python script\n\n    Examples\n    ----------\n    a = SinCoords()\n    b = Arange()\n    code = '''import numpy as np\n    output = np.minimum(a, b)\n    '''\n    generic = Generic(code=code, a=a, b=b)\n    \"\"\"",
            "\"\"\"\n        Run the generic code.\n\n        Attributes\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        result : UnitsDataArray\n            Algorithm result.\n        \"\"\"",
            "\"\"\"\n    Masks the `source` based on a boolean expression involving the `mask`\n    (i.e. source[mask <bool_op> <bool_val> ] = <masked_val>).\n    For a normal boolean mask input, default values for `bool_op`, `bool_val` and `masked_val` can be used.\n\n    Attributes\n    ----------\n    source : podpac.Node\n        The source that will be masked\n    mask : podpac.Node\n        The data that will be used to compute the mask\n    masked_val : float, optional\n        Default value is np.nan. The value that will replace the masked items.\n    bool_val : float, optional\n        Default value is 1. The value used to compare the mask when creating the boolean expression\n    bool_op : enum, optional\n        Default value is '=='. One of ['==', '<', '<=', '>', '>=']\n    in_place : bool, optional\n        Default is False. If True, the source array will be changed in-place, which could affect the value of the source\n        in other parts of the pipeline.\n\n    Examples\n    ----------\n    # Mask data from a boolean data node using the default behavior.\n    # Create a boolean masked Node (as an example)\n    b = Arithmetic(A=SinCoords(), eqn='A>0)\n    # Create the source node\n    a = Arange()\n    masked = Mask(source=a, mask=b)\n\n    # Create a node that make the following substitution \"a[b > 0] = np.nan\"\n    a = Arange()\n    b = SinCoords()\n    masked = Mask(source=a, mask=b,\n                  masked_val=np.nan,\n                  bool_val=0, bool_op='>'\n                  in_place=True)\n\n    \"\"\"",
            "\"\"\"\n        Sets the values in inputs['source'] to self.masked_val using (inputs['mask'] <self.bool_op> <self.bool_val>)\n\n        Attributes\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        result : UnitsDataArray\n            Algorithm result.\n        \"\"\"",
            "\"\"\"Combine multiple nodes into a single node with multiple outputs.\n\n    If not output names are specified, the keyword argument names will be used.\n    \"\"\""
        ],
        "code_snippets": [
            "class PermissionError(OSError):\n        pass",
            "class GenericInputs(Algorithm):",
            "def _first_init(self, **kwargs):\n        trait_names = self.trait_names()\n        for key in kwargs:\n            if key in trait_names and isinstance(kwargs[key], Node):\n                raise RuntimeError(\"Trait '%s' is reserved and cannot be used as an Generic Algorithm input\" % key)\n        input_keys = [key for key in kwargs if key not in trait_names and isinstance(kwargs[key], Node)]\n        inputs = {key: kwargs.pop(key) for key in input_keys}\n        self.set_trait(\"inputs\", inputs)\n        return super(GenericInputs, self)._first_init(**kwargs)\n\n    @property",
            "def _base_definition(self):\n        d = super(GenericInputs, self)._base_definition\n        d[\"inputs\"] = self.inputs\n        return d",
            "class Arithmetic(GenericInputs):\n    \"\"\"Create a simple point-by-point computation using named input nodes.\n\n    Examples\n    ----------\n    a = SinCoords()\n    b = Arange()\n    arith = Arithmetic(A=a, B=b, eqn = 'A * B + {offset}', params={'offset': 1})\n    \"\"\"\n\n    eqn = tl.Unicode().tag(attr=True, required=True)\n    params = tl.Dict().tag(attr=True, required=True)\n\n    _repr_keys = [\"eqn\"]",
            "def init(self):\n        if not settings.allow_unsafe_eval:\n            warnings.warn(\n                \"Insecure evaluation of Python code using Arithmetic node has not been allowed. If \"\n                \"this is an error, use: `podpac.settings.allow_unrestricted_code_execution(True)`. \"\n                \"NOTE: Allowing unsafe evaluation enables arbitrary execution of Python code through PODPAC \"\n                \"Node definitions.\"\n            )\n\n        if self.eqn == \"\":\n            raise ValueError(\"Arithmetic eqn cannot be empty\")\n\n        super(Arithmetic, self).init()",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"Compute the algorithms equation\n\n        Attributes\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        result : UnitsDataArray\n            Algorithm result.\n        \"\"\"\n\n        if not settings.allow_unsafe_eval:\n            raise PermissionError(\n                \"Insecure evaluation of Python code using Arithmetic node has not been allowed. If \"\n                \"this is an error, use: `podpac.settings.allow_unrestricted_code_execution(True)`. \"\n                \"NOTE: Allowing unsafe evaluation enables arbitrary execution of Python code through PODPAC \"\n                \"Node definitions.\"\n            )\n\n        eqn = self.eqn.format(**self.params)\n\n        fields = self.inputs.keys()\n        res = xr.broadcast(*[inputs[f] for f in fields])\n        f_locals = dict(zip(fields, res))\n\n        try:\n            from numexpr import evaluate  # Needed for some systems to get around lazy_module issues\n\n            result = ne.evaluate(eqn, f_locals)\n        except (NotImplementedError, ImportError):\n            result = eval(eqn, f_locals)\n        res = res[0].copy()  # Make an xarray object with correct dimensions\n        res[:] = result\n        return res",
            "class Generic(GenericInputs):\n    \"\"\"\n    Generic Algorithm Node that allows arbitrary Python code to be executed.\n\n    Attributes\n    ----------\n    code : str\n        The multi-line code that will be evaluated. This code should assign \"output\" to the desired result, and \"output\"\n        needs to be a \"numpy array\" or \"xarray DataArray\"\n    inputs : dict(str: podpac.Node)\n        A dictionary of PODPAC nodes that will serve as the input data for the Python script\n\n    Examples\n    ----------\n    a = SinCoords()\n    b = Arange()\n    code = '''import numpy as np\n    output = np.minimum(a, b)\n    '''\n    generic = Generic(code=code, a=a, b=b)\n    \"\"\"\n\n    code = tl.Unicode().tag(attr=True, readonly=True, required=True)",
            "def init(self):\n        if not settings.allow_unsafe_eval:\n            warnings.warn(\n                \"Insecure evaluation of Python code using Generic node has not been allowed. If this \"\n                \"this is an error, use: `podpac.settings.allow_unrestricted_code_execution(True)`. \"\n                \"NOTE: Allowing unsafe evaluation enables arbitrary execution of Python code through PODPAC \"\n                \"Node definitions.\"\n            )\n        super(Generic, self).init()",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"\n        Run the generic code.\n\n        Attributes\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        result : UnitsDataArray\n            Algorithm result.\n        \"\"\"\n\n        if not settings.allow_unsafe_eval:\n            raise PermissionError(\n                \"Insecure evaluation of Python code using Generic node has not been allowed. If this \"\n                \"this is an error, use: `podpac.settings.allow_unrestricted_code_execution(True)`. \"\n                \"NOTE: Allowing unsafe evaluation enables arbitrary execution of Python code through PODPAC \"\n                \"Node definitions.\"\n            )\n        exec(self.code, inputs)\n        return inputs[\"output\"]",
            "class Mask(Algorithm):\n    \"\"\"\n    Masks the `source` based on a boolean expression involving the `mask`\n    (i.e. source[mask <bool_op> <bool_val> ] = <masked_val>).\n    For a normal boolean mask input, default values for `bool_op`, `bool_val` and `masked_val` can be used.\n\n    Attributes\n    ----------\n    source : podpac.Node\n        The source that will be masked\n    mask : podpac.Node\n        The data that will be used to compute the mask\n    masked_val : float, optional\n        Default value is np.nan. The value that will replace the masked items.\n    bool_val : float, optional\n        Default value is 1. The value used to compare the mask when creating the boolean expression\n    bool_op : enum, optional\n        Default value is '=='. One of ['==', '<', '<=', '>', '>=']\n    in_place : bool, optional\n        Default is False. If True, the source array will be changed in-place, which could affect the value of the source\n        in other parts of the pipeline.\n\n    Examples\n    ----------\n    # Mask data from a boolean data node using the default behavior.\n    # Create a boolean masked Node (as an example)\n    b = Arithmetic(A=SinCoords(), eqn='A>0)\n    # Create the source node\n    a = Arange()\n    masked = Mask(source=a, mask=b)\n\n    # Create a node that make the following substitution \"a[b > 0] = np.nan\"\n    a = Arange()\n    b = SinCoords()\n    masked = Mask(source=a, mask=b,\n                  masked_val=np.nan,\n                  bool_val=0, bool_op='>'\n                  in_place=True)\n\n    \"\"\"\n\n    source = NodeTrait().tag(attr=True, required=True)\n    mask = NodeTrait().tag(attr=True, required=True)\n    masked_val = tl.Float(allow_none=True, default_value=None).tag(attr=True)\n    bool_val = tl.Float(1).tag(attr=True)\n    bool_op = tl.Enum([\"==\", \"<\", \"<=\", \">\", \">=\"], default_value=\"==\").tag(attr=True)\n    in_place = tl.Bool(False).tag(attr=True)\n\n    _repr_keys = [\"source\", \"mask\"]",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"\n        Sets the values in inputs['source'] to self.masked_val using (inputs['mask'] <self.bool_op> <self.bool_val>)\n\n        Attributes\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain with different coordinates.\n\n        Returns\n        -------\n        result : UnitsDataArray\n            Algorithm result.\n        \"\"\"\n\n        # shorter names\n        mask = inputs[\"mask\"]\n        source = inputs[\"source\"]\n        op = self.bool_op\n        bv = self.bool_val\n\n        # Make a copy if we don't want to change the source in-place\n        if not self.in_place:\n            source = source.copy()\n\n        # Make the mask boolean\n        if op == \"==\":\n            mask = mask == bv\n        elif op == \"<\":\n            mask = mask < bv\n        elif op == \"<=\":\n            mask = mask <= bv\n        elif op == \">\":\n            mask = mask > bv\n        elif op == \">=\":\n            mask = mask >= bv\n\n        # Mask the values and return\n        if self.masked_val is None:\n            source.set(np.nan, mask)\n        else:\n            source.set(self.masked_val, mask)\n\n        return source",
            "class Combine(GenericInputs):\n    \"\"\"Combine multiple nodes into a single node with multiple outputs.\n\n    If not output names are specified, the keyword argument names will be used.\n    \"\"\"\n\n    @tl.default(\"outputs\")",
            "def _default_outputs(self):\n        input_keys = list(self.inputs.keys())\n        return input_keys",
            "def algorithm(self, inputs, coordinates):\n        cs = [Coordinates.from_xarray(x) for x in inputs.values()]\n        if any(c != cs[0] for c in cs):\n            raise NodeException(\"Cannot combine inputs with different coordinates\")\n\n        data = np.stack([inputs[key] for key in self.inputs], axis=-1)\n        return self.create_output_array(cs[0], data=data)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/algorithm.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nBase class for Algorithm Nodes\n\"\"\"",
            "\"\"\"Base class for algorithm nodes.\n\n    Note: developers should generally use one of the Algorithm or UnaryAlgorithm child classes.\n    \"\"\"",
            "\"\"\"\n        Get the available coordinates for the inputs to the Node.\n\n        Returns\n        -------\n        coords_list : list\n            list of available coordinates (Coordinate objects)\n        \"\"\"",
            "\"\"\"Base class for computation nodes with a custom algorithm.\n\n    Notes\n    ------\n    Developers of new Algorithm nodes need to implement the `algorithm` method.\n    \"\"\"",
            "\"\"\"\n        Arguments\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names. Each item is a `UnitsDataArray`.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain different coordinates than the requested coordinates\n        \"\"\"",
            "\"\"\"Evalutes this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"",
            "\"\"\"\n    Base class for computation nodes that take a single source and transform it.\n\n    Attributes\n    ----------\n    source : Node\n        The source node\n\n    Notes\n    ------\n    Developers of new Algorithm nodes need to implement the `eval` method.\n    \"\"\""
        ],
        "code_snippets": [
            "class BaseAlgorithm(Node):\n    \"\"\"Base class for algorithm nodes.\n\n    Note: developers should generally use one of the Algorithm or UnaryAlgorithm child classes.\n    \"\"\"\n\n    @property",
            "def inputs(self):\n        # gettattr(self, ref) can take a long time, so we inspect trait.klass instead\n        return {\n            ref: getattr(self, ref)\n            for ref, trait in self.traits().items()\n            if hasattr(trait, \"klass\") and Node in inspect.getmro(trait.klass) and getattr(self, ref) is not None\n        }",
            "def find_coordinates(self):\n        \"\"\"\n        Get the available coordinates for the inputs to the Node.\n\n        Returns\n        -------\n        coords_list : list\n            list of available coordinates (Coordinate objects)\n        \"\"\"\n\n        return [c for node in self.inputs.values() for c in node.find_coordinates()]",
            "class Algorithm(BaseAlgorithm):\n    \"\"\"Base class for computation nodes with a custom algorithm.\n\n    Notes\n    ------\n    Developers of new Algorithm nodes need to implement the `algorithm` method.\n    \"\"\"\n\n    # not the best solution... hard to check for these attrs\n    # abstract = tl.Bool(default_value=True, allow_none=True).tag(attr=True, required=False, hidden=True)",
            "def algorithm(self, inputs, coordinates):\n        \"\"\"\n        Arguments\n        ----------\n        inputs : dict\n            Evaluated outputs of the input nodes. The keys are the attribute names. Each item is a `UnitsDataArray`.\n        coordinates : podpac.Coordinates\n            Requested coordinates.\n            Note that the ``inputs`` may contain different coordinates than the requested coordinates\n        \"\"\"\n\n        raise NotImplementedError\n\n    @common_doc(COMMON_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evalutes this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"\n\n        self._requested_coordinates = coordinates\n\n        inputs = {}\n\n        if settings[\"MULTITHREADING\"]:\n            n_threads = thread_manager.request_n_threads(len(self.inputs))\n            if n_threads == 1:\n                thread_manager.release_n_threads(n_threads)\n        else:\n            n_threads = 0\n\n        if settings[\"MULTITHREADING\"] and n_threads > 1:\n            # Create a function for each thread to execute asynchronously",
            "def f(node):\n                return node.eval(coordinates, _selector=_selector)\n\n            # Create pool of size n_threads, note, this may be created from a sub-thread (i.e. not the main thread)\n            pool = thread_manager.get_thread_pool(processes=n_threads)\n\n            # Evaluate nodes in parallel/asynchronously\n            results = [pool.apply_async(f, [node]) for node in self.inputs.values()]\n\n            # Collect the results in dictionary\n            for key, res in zip(self.inputs.keys(), results):\n                inputs[key] = res.get()\n\n            # This prevents any more tasks from being submitted to the pool, and will close the workers once done\n            pool.close()\n\n            # Release these number of threads back to the thread pool\n            thread_manager.release_n_threads(n_threads)\n            self._multi_threaded = True\n        else:\n            # Evaluate nodes in serial\n            for key, node in self.inputs.items():\n                inputs[key] = node.eval(coordinates, output=output, _selector=_selector)\n            self._multi_threaded = False\n\n        result = self.algorithm(inputs, coordinates)\n\n        if not isinstance(result, xr.DataArray):\n            raise NodeException(\"algorithm returned unsupported type '%s'\" % type(result))\n\n        if \"output\" in result.dims and self.output is not None:\n            result = result.sel(output=self.output)\n\n        if output is not None:\n            missing = [dim for dim in result.dims if dim not in output.dims]\n            if any(missing):\n                raise NodeException(\"provided output is missing dims %s\" % missing)\n\n            output_dims = output.dims\n            output = output.transpose(..., *result.dims)\n            output[:] = result.data\n            output = output.transpose(*output_dims)\n        elif isinstance(result, UnitsDataArray):\n            output = result\n        else:\n            output_coordinates = Coordinates.from_xarray(result)\n            output = self.create_output_array(output_coordinates, data=result.data)\n\n        return output",
            "class UnaryAlgorithm(BaseAlgorithm):\n    \"\"\"\n    Base class for computation nodes that take a single source and transform it.\n\n    Attributes\n    ----------\n    source : Node\n        The source node\n\n    Notes\n    ------\n    Developers of new Algorithm nodes need to implement the `eval` method.\n    \"\"\"\n\n    source = NodeTrait().tag(attr=True, required=True)\n\n    # list of attribute names, used by __repr__ and __str__ to display minimal info about the node\n    _repr_keys = [\"source\"]\n\n    @tl.default(\"outputs\")",
            "def _default_outputs(self):\n        return self.source.outputs\n\n    @tl.default(\"style\")",
            "def _default_style(self):  # Pass through source style by default\n        return self.source.style"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/__init__.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": []
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/stats.py",
        "comments": [
            "// s",
            "// s",
            "//github.com/pydata/xarray/blob/eeb109d9181c84dfb93356c5f14045d839ee64cb/xarray/core/accessors.py#L61",
            "//github.com/pydata/xarray/blob/eeb109d9181c84dfb93356c5f14045d839ee64cb/xarray/core/accessors.py#L61",
            "// 2"
        ],
        "docstrings": [
            "\"\"\"\nStats Summary\n\"\"\"",
            "\"\"\"Base node for statistical algorithms\n\n    Attributes\n    ----------\n    dims : list\n        List of strings that give the dimensions which should be reduced\n    source : podpac.Node\n        The source node that will be reduced.\n    \"\"\"",
            "\"\"\"Finds the indices for the dimensions that will be reduced. This is passed to numpy.\n\n        Parameters\n        ----------\n        output : UnitsDataArray\n            The output array with the reduced dimensions\n\n        Returns\n        -------\n        list\n            List of integers for the dimensions that will be reduces\n        \"\"\"",
            "\"\"\"Size of chunks for parallel processing or large arrays that do not fit in memory\n\n        Returns\n        -------\n        int\n            Size of chunks\n        \"\"\"",
            "\"\"\"Shape of chunks for parallel processing or large arrays that do not fit in memory.\n\n        Returns\n        -------\n        list\n            List of integers giving the shape of each chunk.\n        \"\"\"",
            "\"\"\"\n        Transpose and reshape a DataArray to put the reduce dimensions together\n        as axis 0. This is useful for example for scipy.stats.skew and kurtosis\n        which only calculate over a single axis, by default 0.\n\n        Parameters\n        ----------\n        x : xr.DataArray\n            Input DataArray\n\n        Returns\n        -------\n        a : np.array\n            Transposed and reshaped array\n        \"\"\"",
            "\"\"\"Generator for the chunks of the output\n\n        Yields\n        ------\n        UnitsDataArray\n            Output for this chunk\n        \"\"\"",
            "\"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"",
            "\"\"\"\n        Reduce a full array, e.g. x.mean(dims).\n\n        Must be defined in each child.\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Array that needs to be reduced.\n\n        Raises\n        ------\n        NotImplementedError\n            Must be defined in each child.\n        \"\"\"",
            "\"\"\"\n        Reduce a list of xs with a memory-effecient iterative algorithm.\n\n        Optionally defined in each child.\n\n        Parameters\n        ----------\n        xs : list, generator\n            List of UnitsDataArray's that need to be reduced together.\n\n        Returns\n        -------\n        UnitsDataArray\n            Reduced output.\n        \"\"\"",
            "\"\"\"\n    Extended Reduce class that enables chunks that are smaller than the reduced\n    output array.\n\n    The base Reduce node ensures that each chunk is at least as big as the\n    reduced output, which works for statistics that can be calculated in O(1)\n    space. For statistics that require O(n) space, the node must iterate\n    through the Coordinates orthogonally to the reduce dimension, using chunks\n    that only cover a portion of the output array.\n    \"\"\"",
            "\"\"\"Shape of chunks for parallel processing or large arrays that do not fit in memory.\n\n        Returns\n        -------\n        list\n            List of integers giving the shape of each chunk.\n        \"\"\"",
            "\"\"\"Generator for the chunks of the output\n\n        Yields\n        ------\n        UnitsDataArray\n            Output for this chunk\n        \"\"\"",
            "\"\"\"\n        Reduce a list of xs with a memory-effecient iterative algorithm.\n\n        Optionally defined in each child.\n\n        Parameters\n        ----------\n        xs : list, generator\n            List of UnitsDataArray's that need to be reduced together.\n\n        Returns\n        -------\n        UnitsDataArray\n            Reduced output.\n        \"\"\"",
            "\"\"\"Computes the minimum across dimension(s)\"\"\"",
            "\"\"\"Computes the minimum across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Minimum of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the minimum across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Minimum of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the maximum across dimension(s)\"\"\"",
            "\"\"\"Computes the maximum across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Maximum of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the maximum across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Maximum of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the sum across dimension(s)\"\"\"",
            "\"\"\"Computes the sum across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Sum of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the sum across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Sum of the source data over dims\n        \"\"\"",
            "\"\"\"Counts the finite values across dimension(s)\"\"\"",
            "\"\"\"Counts the finite values across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Number of finite values of the source data over dims\n        \"\"\"",
            "\"\"\"Counts the finite values across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Number of finite values of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the mean across dimension(s)\"\"\"",
            "\"\"\"Computes the mean across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Mean of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the mean across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Mean of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the variance across dimension(s)\"\"\"",
            "\"\"\"Computes the variance across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Variance of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the variance across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Variance of the source data over dims\n        \"\"\"",
            "\"\"\"\n    Computes the skew across dimension(s)\n\n    TODO NaN behavior when there is NO data (currently different in reduce and reduce_chunked)\n    \"\"\"",
            "\"\"\"Computes the skew across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Skew of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the skew across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Skew of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the kurtosis across dimension(s)\n\n    TODO NaN behavior when there is NO data (currently different in reduce and reduce_chunked)\n    \"\"\"",
            "\"\"\"Computes the kurtosis across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Kurtosis of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the kurtosis across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Kurtosis of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the standard deviation across dimension(s)\"\"\"",
            "\"\"\"Computes the standard deviation across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Standard deviation of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the standard deviation across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Standard deviation of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the median across dimension(s)\n\n    Example\n    ---------\n    coords.dims == ['lat', 'lon', 'time']\n    median = Median(source=node, dims=['lat', 'lon'])\n    o = median.eval(coords)\n    o.dims == ['time']\n    \"\"\"",
            "\"\"\"Computes the median across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Median of the source data over dims\n        \"\"\"",
            "\"\"\"Computes the percentile across dimension(s)\n\n    Attributes\n    ----------\n    percentile : TYPE\n        Description\n    \"\"\"",
            "\"\"\"Computes the percentile across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Percentile of the source data over dims\n        \"\"\"",
            "\"\"\"\n    Group a time-dependent source node and then compute a statistic for each result.\n\n    Attributes\n    ----------\n    custom_reduce_fn : function\n        required if reduce_fn is 'custom'.\n    groupby : str\n        datetime sub-accessor. Currently 'dayofyear' is the enabled option.\n    reduce_fn : str\n        builtin xarray groupby reduce function, or 'custom'.\n    source : podpac.Node\n        Source node\n    \"\"\"",
            "\"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            If source it not time-depended (required by this node).\n        \"\"\"",
            "\"\"\"\n        Default node reference/name in node definitions\n\n        Returns\n        -------\n        str\n            Default node reference/name in node definitions\n        \"\"\"",
            "\"\"\"\n    Resample a time-dependent source node using a statistical operation to achieve the result.\n\n    Attributes\n    ----------\n    custom_reduce_fn : function\n        required if reduce_fn is 'custom'.\n    resample : str\n        datetime sub-accessor. Currently 'dayofyear' is the enabled option.\n    reduce_fn : str\n        builtin xarray groupby reduce function, or 'custom'.\n    source : podpac.Node\n        Source node\n    \"\"\"",
            "\"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            If source it not time-dependent (required by this node).\n        \"\"\"",
            "\"\"\"\n        Default node reference/name in node definitions\n\n        Returns\n        -------\n        str\n            Default node reference/name in node definitions\n        \"\"\"",
            "\"\"\"\n    Group a time-dependent source node by day of year and compute a statistic for each group.\n\n    Attributes\n    ----------\n    custom_reduce_fn : function\n        required if reduce_fn is 'custom'.\n    reduce_fn : str\n        builtin xarray groupby reduce function, or 'custom'.\n    source : podpac.Node\n        Source node\n    \"\"\"",
            "\"\"\"\n    This applies a function over a moving window around day-of-year in the requested coordinates.\n    It includes the ability to rescale the input/outputs. Note if, the input coordinates include multiple years, the\n    moving window will include all of the data inside the day-of-year window.\n\n    Users need to implement the 'function' method.\n\n    Attributes\n    -----------\n    source: podpac.Node\n        The source node from which the statistics will be computed\n    window: int, optional\n        Default is 0. The size of the window over which to compute the distrubtion. This is always centered about the\n        day-of-year. The total number of days is always an odd number. For example, window=2 and window=3 will compute\n        the beta distribution for [x-1, x, x + 1] and report it as the result for x, where x is a day of the year.\n    scale_max: podpac.Node, optional\n        Default is None. A source dataset that can be used to scale the maximum value of the source function so that it\n        will fall between [0, 1]. If None, uses self.scale_float[0].\n    scale_min: podpac.Node, optional\n        Default is None. A source dataset that can be used to scale the minimum value of the source function so that it\n        will fall between [0, 1]. If None, uses self.scale_float[1].\n    scale_float: list, optional\n        Default is []. Floating point numbers used to scale the max [0] and min [1] of the source so that it falls\n        between [0, 1]. If scale_max or scale_min are defined, this property is ignored. If these are defined, the data\n        will be rescaled only if rescale=True below.\n        If None and scale_max/scale_min are not defined, the data is not scaled in any way.\n    rescale: bool, optional\n        Rescales the output data after being scaled from scale_float or scale_min/max\n    \"\"\""
        ],
        "code_snippets": [
            "class Reduce(UnaryAlgorithm):\n    \"\"\"Base node for statistical algorithms\n\n    Attributes\n    ----------\n    dims : list\n        List of strings that give the dimensions which should be reduced\n    source : podpac.Node\n        The source node that will be reduced.\n    \"\"\"\n\n    from podpac.core.utils import DimsTrait\n\n    dims = DimsTrait(allow_none=True, default_value=None).tag(attr=True)\n\n    _reduced_coordinates = tl.Instance(Coordinates, allow_none=True)\n    _dims = tl.List(trait=tl.Unicode())",
            "def _first_init(self, **kwargs):\n        if \"dims\" in kwargs and isinstance(kwargs[\"dims\"], string_types):\n            kwargs[\"dims\"] = [kwargs[\"dims\"]]\n        return super(Reduce, self)._first_init(**kwargs)",
            "def dims_axes(self, output):\n        \"\"\"Finds the indices for the dimensions that will be reduced. This is passed to numpy.\n\n        Parameters\n        ----------\n        output : UnitsDataArray\n            The output array with the reduced dimensions\n\n        Returns\n        -------\n        list\n            List of integers for the dimensions that will be reduces\n        \"\"\"\n        axes = [i for i in range(len(output.dims)) if output.dims[i] in self._dims]\n        return axes\n\n    @property",
            "def chunk_size(self):\n        \"\"\"Size of chunks for parallel processing or large arrays that do not fit in memory\n\n        Returns\n        -------\n        int\n            Size of chunks\n        \"\"\"\n\n        chunk_size = podpac.settings[\"CHUNK_SIZE\"]\n        if chunk_size == \"auto\":\n            return 1024**2  # TODO\n        else:\n            return chunk_size",
            "def _get_chunk_shape(self, coords):\n        \"\"\"Shape of chunks for parallel processing or large arrays that do not fit in memory.\n\n        Returns\n        -------\n        list\n            List of integers giving the shape of each chunk.\n        \"\"\"\n        if self.chunk_size is None:\n            return None\n\n        chunk_size = self.chunk_size\n\n        d = {k: coords[k].size for k in coords.dims if k not in self._dims}\n        s = reduce(mul, d.values(), 1)\n        for dim in self._dims:\n            n = chunk_size",
            "def _reshape(self, x):\n        \"\"\"\n        Transpose and reshape a DataArray to put the reduce dimensions together\n        as axis 0. This is useful for example for scipy.stats.skew and kurtosis\n        which only calculate over a single axis, by default 0.\n\n        Parameters\n        ----------\n        x : xr.DataArray\n            Input DataArray\n\n        Returns\n        -------\n        a : np.array\n            Transposed and reshaped array\n        \"\"\"\n\n        if self._dims is None:\n            return x.data.flatten()\n\n        n = len(self._dims)\n        dims = list(self._dims) + [d for d in x.dims if d not in self._dims]\n        x = x.transpose(*dims)\n        a = x.data.reshape(-1, *x.shape[n:])\n        return a",
            "def iteroutputs(self, coordinates, _selector):\n        \"\"\"Generator for the chunks of the output\n\n        Yields\n        ------\n        UnitsDataArray\n            Output for this chunk\n        \"\"\"\n        chunk_shape = self._get_chunk_shape(coordinates)\n        for chunk in coordinates.iterchunks(chunk_shape):\n            yield self.source.eval(chunk, _selector=_selector)\n\n    @common_doc(COMMON_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"\n\n        self._requested_coordinates = coordinates\n\n        if self.dims:\n            self._dims = [dim for dim in self.dims if dim in coordinates.dims]\n        else:\n            self._dims = list(coordinates.dims)\n        self._reduced_coordinates = coordinates.drop(self._dims)\n\n        if output is None:\n            output = self.create_output_array(self._reduced_coordinates)\n\n        if self.chunk_size and self.chunk_size < reduce(mul, coordinates.shape, 1):\n            try:\n                result = self.reduce_chunked(self.iteroutputs(coordinates, _selector), output)\n            except NotImplementedError:\n                warnings.warn(\"No reduce_chunked method defined, using one-step reduce\")\n                source_output = self.source.eval(coordinates, _selector=_selector)\n                result = self.reduce(source_output)\n        else:\n            source_output = self.source.eval(coordinates, _selector=_selector)\n            result = self.reduce(source_output)\n\n        if output.shape == ():\n            output.data = result\n        else:\n            output[:] = result\n\n        return output",
            "def reduce(self, x):\n        \"\"\"\n        Reduce a full array, e.g. x.mean(dims).\n\n        Must be defined in each child.\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Array that needs to be reduced.\n\n        Raises\n        ------\n        NotImplementedError\n            Must be defined in each child.\n        \"\"\"\n\n        raise NotImplementedError",
            "def reduce_chunked(self, xs, output):\n        \"\"\"\n        Reduce a list of xs with a memory-effecient iterative algorithm.\n\n        Optionally defined in each child.\n\n        Parameters\n        ----------\n        xs : list, generator\n            List of UnitsDataArray's that need to be reduced together.\n\n        Returns\n        -------\n        UnitsDataArray\n            Reduced output.\n        \"\"\"\n\n        raise NotImplementedError",
            "class ReduceOrthogonal(Reduce):\n    \"\"\"\n    Extended Reduce class that enables chunks that are smaller than the reduced\n    output array.\n\n    The base Reduce node ensures that each chunk is at least as big as the\n    reduced output, which works for statistics that can be calculated in O(1)\n    space. For statistics that require O(n) space, the node must iterate\n    through the Coordinates orthogonally to the reduce dimension, using chunks\n    that only cover a portion of the output array.\n    \"\"\"",
            "def _get_chunk_shape(self, coords):\n        \"\"\"Shape of chunks for parallel processing or large arrays that do not fit in memory.\n\n        Returns\n        -------\n        list\n            List of integers giving the shape of each chunk.\n        \"\"\"\n        if self.chunk_size is None:\n            return None\n\n        chunk_size = self.chunk_size\n\n        # here, the minimum size is the reduce-dimensions size\n        d = {k: coords[k].size for k in self._dims}\n        s = reduce(mul, d.values(), 1)\n        for dim in coords.dims[::-1]:\n            if dim in self._dims:\n                continue\n            n = chunk_size",
            "def iteroutputs(self, coordinates, selector):\n        \"\"\"Generator for the chunks of the output\n\n        Yields\n        ------\n        UnitsDataArray\n            Output for this chunk\n        \"\"\"\n\n        chunk_shape = self._get_chunk_shape(coordinates)\n        for chunk, slices in coordinates.iterchunks(chunk_shape, return_slices=True):\n            yield self.source.eval(chunk, _selector=selector), slices",
            "def reduce_chunked(self, xs, output):\n        \"\"\"\n        Reduce a list of xs with a memory-effecient iterative algorithm.\n\n        Optionally defined in each child.\n\n        Parameters\n        ----------\n        xs : list, generator\n            List of UnitsDataArray's that need to be reduced together.\n\n        Returns\n        -------\n        UnitsDataArray\n            Reduced output.\n        \"\"\"\n        # special case for full reduce\n        if not self._reduced_coordinates.dims:\n            x, xslices = next(xs)\n            return self.reduce(x)\n\n        y = xr.full_like(output, np.nan)\n        for x, xslices in xs:\n            yslc = tuple(xslices[self._requested_coordinates.dims.index(dim)] for dim in self._reduced_coordinates.dims)\n            y.data[yslc] = self.reduce(x)\n        return y",
            "class Min(Reduce):",
            "def reduce(self, x):\n        \"\"\"Computes the minimum across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Minimum of the source data over dims\n        \"\"\"\n        return x.min(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the minimum across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Minimum of the source data over dims\n        \"\"\"\n        # note: np.fmin ignores NaNs, np.minimum propagates NaNs\n        y = xr.full_like(output, np.nan)\n        for x in xs:\n            y = np.fmin(y, x.min(dim=self._dims))\n        return y",
            "class Max(Reduce):",
            "def reduce(self, x):\n        \"\"\"Computes the maximum across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Maximum of the source data over dims\n        \"\"\"\n        return x.max(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the maximum across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Maximum of the source data over dims\n        \"\"\"\n        # note: np.fmax ignores NaNs, np.maximum propagates NaNs\n        y = xr.full_like(output, np.nan)\n        for x in xs:\n            y = np.fmax(y, x.max(dim=self._dims))\n        return y",
            "class Sum(Reduce):",
            "def reduce(self, x):\n        \"\"\"Computes the sum across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Sum of the source data over dims\n        \"\"\"\n        return x.sum(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the sum across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Sum of the source data over dims\n        \"\"\"\n        s = xr.zeros_like(output)\n        for x in xs:\n            s += x.sum(dim=self._dims)\n        return s",
            "class Count(Reduce):",
            "def reduce(self, x):\n        \"\"\"Counts the finite values across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Number of finite values of the source data over dims\n        \"\"\"\n        return np.isfinite(x).sum(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Counts the finite values across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Number of finite values of the source data over dims\n        \"\"\"\n        n = xr.zeros_like(output)\n        for x in xs:\n            n += np.isfinite(x).sum(dim=self._dims)\n        return n",
            "class Mean(Reduce):",
            "def reduce(self, x):\n        \"\"\"Computes the mean across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Mean of the source data over dims\n        \"\"\"\n        return x.mean(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the mean across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Mean of the source data over dims\n        \"\"\"\n        s = xr.zeros_like(output)\n        n = xr.zeros_like(output)\n        for x in xs:\n            # TODO efficency\n            s += x.sum(dim=self._dims)\n            n += np.isfinite(x).sum(dim=self._dims)\n        output = s / n\n        return output",
            "class Variance(Reduce):",
            "def reduce(self, x):\n        \"\"\"Computes the variance across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Variance of the source data over dims\n        \"\"\"\n        return x.var(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the variance across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Variance of the source data over dims\n        \"\"\"\n        n = xr.zeros_like(output)\n        m = xr.zeros_like(output)\n        m2 = xr.zeros_like(output)\n\n        # Welford, adapted to handle multiple data points in each iteration\n        for x in xs:\n            n += np.isfinite(x).sum(dim=self._dims)\n            d = x - m\n            m += (d / n).sum(dim=self._dims)\n            d2 = x - m\n            m2 += (d * d2).sum(dim=self._dims)\n\n        return m2 / n",
            "class Skew(Reduce):\n    \"\"\"\n    Computes the skew across dimension(s)\n\n    TODO NaN behavior when there is NO data (currently different in reduce and reduce_chunked)\n    \"\"\"",
            "def reduce(self, x):\n        \"\"\"Computes the skew across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Skew of the source data over dims\n        \"\"\"\n        # N = np.isfinite(x).sum(dim=self._dims)\n        # M1 = x.mean(dim=self._dims)\n        # E = x - M1\n        # E2 = E**2\n        # E3 = E2*E\n        # M2 = (E2).sum(dim=self._dims)\n        # M3 = (E3).sum(dim=self._dims)\n        # skew = self.skew(M3, M2, N)\n\n        a = self._reshape(x)\n        skew = scipy.stats.skew(a, nan_policy=\"omit\")\n        return skew",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the skew across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Skew of the source data over dims\n        \"\"\"\n        N = xr.zeros_like(output)\n        M1 = xr.zeros_like(output)\n        M2 = xr.zeros_like(output)\n        M3 = xr.zeros_like(output)\n        check_empty = True\n\n        for x in xs:\n            Nx = np.isfinite(x).sum(dim=self._dims)\n            M1x = x.mean(dim=self._dims)\n            Ex = x - M1x\n            Ex2 = Ex**2\n            Ex3 = Ex2 * Ex\n            M2x = (Ex2).sum(dim=self._dims)\n            M3x = (Ex3).sum(dim=self._dims)\n\n            # premask to omit NaNs\n            b = Nx.data > 0\n            Nx = Nx.data[b]\n            M1x = M1x.data[b]\n            M2x = M2x.data[b]\n            M3x = M3x.data[b]\n\n            Nb = N.data[b]\n            M1b = M1.data[b]\n            M2b = M2.data[b]\n\n            # merge\n            d = M1x - M1b\n            n = Nb + Nx\n            NNx = Nb * Nx\n\n            M3.data[b] += M3x + d**3 * NNx * (Nb - Nx) / n**2 + 3 * d * (Nb * M2x - Nx * M2b) / n\n            M2.data[b] += M2x + d**2 * NNx / n\n            M1.data[b] += d * Nx / n\n            N.data[b] = n\n\n        # calculate skew\n        skew = np.sqrt(N) * M3 / np.sqrt(M2**3)\n        return skew",
            "class Kurtosis(Reduce):\n    \"\"\"Computes the kurtosis across dimension(s)\n\n    TODO NaN behavior when there is NO data (currently different in reduce and reduce_chunked)\n    \"\"\"",
            "def reduce(self, x):\n        \"\"\"Computes the kurtosis across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Kurtosis of the source data over dims\n        \"\"\"\n        # N = np.isfinite(x).sum(dim=self._dims)\n        # M1 = x.mean(dim=self._dims)\n        # E = x - M1\n        # E2 = E**2\n        # E4 = E2**2\n        # M2 = (E2).sum(dim=self._dims)\n        # M4 = (E4).sum(dim=self._dims)\n        # kurtosis = N * M4 / M2**2 - 3\n\n        a = self._reshape(x)\n        kurtosis = scipy.stats.kurtosis(a, nan_policy=\"omit\")\n        return kurtosis",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the kurtosis across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Kurtosis of the source data over dims\n        \"\"\"\n        N = xr.zeros_like(output)\n        M1 = xr.zeros_like(output)\n        M2 = xr.zeros_like(output)\n        M3 = xr.zeros_like(output)\n        M4 = xr.zeros_like(output)\n\n        for x in xs:\n            Nx = np.isfinite(x).sum(dim=self._dims)\n            M1x = x.mean(dim=self._dims)\n            Ex = x - M1x\n            Ex2 = Ex**2\n            Ex3 = Ex2 * Ex\n            Ex4 = Ex2**2\n            M2x = (Ex2).sum(dim=self._dims)\n            M3x = (Ex3).sum(dim=self._dims)\n            M4x = (Ex4).sum(dim=self._dims)\n\n            # premask to omit NaNs\n            b = Nx.data > 0\n            Nx = Nx.data[b]\n            M1x = M1x.data[b]\n            M2x = M2x.data[b]\n            M3x = M3x.data[b]\n            M4x = M4x.data[b]\n\n            Nb = N.data[b]\n            M1b = M1.data[b]\n            M2b = M2.data[b]\n            M3b = M3.data[b]\n\n            # merge\n            d = M1x - M1b\n            n = Nb + Nx\n            NNx = Nb * Nx\n\n            M4.data[b] += (\n                M4x\n                + d**4 * NNx * (Nb**2 - NNx + Nx**2) / n**3\n                + 6 * d**2 * (Nb**2 * M2x + Nx**2 * M2b) / n**2\n                + 4 * d * (Nb * M3x - Nx * M3b) / n\n            )\n\n            M3.data[b] += M3x + d**3 * NNx * (Nb - Nx) / n**2 + 3 * d * (Nb * M2x - Nx * M2b) / n\n            M2.data[b] += M2x + d**2 * NNx / n\n            M1.data[b] += d * Nx / n\n            N.data[b] = n\n\n        # calculate kurtosis\n        kurtosis = N * M4 / M2**2 - 3\n        return kurtosis",
            "class StandardDeviation(Variance):",
            "def reduce(self, x):\n        \"\"\"Computes the standard deviation across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Standard deviation of the source data over dims\n        \"\"\"\n        return x.std(dim=self._dims)",
            "def reduce_chunked(self, xs, output):\n        \"\"\"Computes the standard deviation across a chunk\n\n        Parameters\n        ----------\n        xs : iterable\n            Iterable of sources\n\n        Returns\n        -------\n        UnitsDataArray\n            Standard deviation of the source data over dims\n        \"\"\"\n        var = super(StandardDeviation, self).reduce_chunked(xs, output)\n        return np.sqrt(var)",
            "class Median(ReduceOrthogonal):\n    \"\"\"Computes the median across dimension(s)\n\n    Example\n    ---------\n    coords.dims == ['lat', 'lon', 'time']\n    median = Median(source=node, dims=['lat', 'lon'])\n    o = median.eval(coords)\n    o.dims == ['time']\n    \"\"\"",
            "def reduce(self, x):\n        \"\"\"Computes the median across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Median of the source data over dims\n        \"\"\"\n        return x.median(dim=self._dims)",
            "class Percentile(ReduceOrthogonal):\n    \"\"\"Computes the percentile across dimension(s)\n\n    Attributes\n    ----------\n    percentile : TYPE\n        Description\n    \"\"\"\n\n    percentile = tl.Float(default_value=50.0).tag(attr=True)",
            "def reduce(self, x):\n        \"\"\"Computes the percentile across dimension(s)\n\n        Parameters\n        ----------\n        x : UnitsDataArray\n            Source data.\n\n        Returns\n        -------\n        UnitsDataArray\n            Percentile of the source data over dims\n        \"\"\"\n\n        return np.nanpercentile(x, self.percentile, self.dims_axes(x))\n\n\n# =============================================================================\n# Time-Grouped Reduce\n# =============================================================================\n\n_REDUCE_FUNCTIONS = [\"all\", \"any\", \"count\", \"max\", \"mean\", \"median\", \"min\", \"prod\", \"std\", \"sum\", \"var\", \"custom\"]",
            "class GroupReduce(UnaryAlgorithm):\n    \"\"\"\n    Group a time-dependent source node and then compute a statistic for each result.\n\n    Attributes\n    ----------\n    custom_reduce_fn : function\n        required if reduce_fn is 'custom'.\n    groupby : str\n        datetime sub-accessor. Currently 'dayofyear' is the enabled option.\n    reduce_fn : str\n        builtin xarray groupby reduce function, or 'custom'.\n    source : podpac.Node\n        Source node\n    \"\"\"\n\n    _repr_keys = [\"source\", \"groupby\", \"reduce_fn\"]\n    coordinates_source = NodeTrait(allow_none=True).tag(attr=True)\n\n    # see https:",
            "def _default_coordinates_source(self):\n        return self.source\n\n    @common_doc(COMMON_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            If source it not time-depended (required by this node).\n        \"\"\"\n\n        source_output = self.source.eval(coordinates)\n\n        # group\n        grouped = source_output.groupby(\"time.%s\" % self.groupby)\n\n        # reduce\n        if self.reduce_fn == \"custom\":\n            out = grouped.apply(self.custom_reduce_fn, \"time\")\n        else:\n            # standard, e.g. grouped.median('time')\n            out = getattr(grouped, self.reduce_fn)(\"time\")\n\n        out = out.rename({self.groupby: \"time\"})\n        if output is None:\n            coords = podpac.coordinates.merge_dims(\n                [coordinates.drop(\"time\"), Coordinates([out.coords[\"time\"]], [\"time\"])]\n            )\n            coords = coords.transpose(*out.dims)\n            output = self.create_output_array(coords, data=out.data)\n        else:\n            output.data[:] = out.data[:]\n\n        ## map\n        # eval_time = xr.DataArray(coordinates.coords[\"time\"])\n        # E = getattr(eval_time.dt, self.groupby)\n        # out = out.sel(**{self.groupby: E}).rename({self.groupby: \"time\"})\n        # output[:] = out.transpose(*output.dims).data\n\n        return output\n\n    @property",
            "def base_ref(self):\n        \"\"\"\n        Default node reference/name in node definitions\n\n        Returns\n        -------\n        str\n            Default node reference/name in node definitions\n        \"\"\"\n        return \"%s.%s.%s\" % (self.source.base_ref, self.groupby, self.reduce_fn)",
            "class ResampleReduce(UnaryAlgorithm):\n    \"\"\"\n    Resample a time-dependent source node using a statistical operation to achieve the result.\n\n    Attributes\n    ----------\n    custom_reduce_fn : function\n        required if reduce_fn is 'custom'.\n    resample : str\n        datetime sub-accessor. Currently 'dayofyear' is the enabled option.\n    reduce_fn : str\n        builtin xarray groupby reduce function, or 'custom'.\n    source : podpac.Node\n        Source node\n    \"\"\"\n\n    _repr_keys = [\"source\", \"resample\", \"reduce_fn\"]\n    coordinates_source = NodeTrait(allow_none=True).tag(attr=True)\n\n    # see https:",
            "def _default_coordinates_source(self):\n        return self.source\n\n    @common_doc(COMMON_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Raises\n        ------\n        ValueError\n            If source it not time-dependent (required by this node).\n        \"\"\"\n\n        source_output = self.source.eval(coordinates, _selector=_selector)\n\n        # group\n        grouped = source_output.resample(time=self.resample)\n\n        # reduce\n        if self.reduce_fn == \"custom\":\n            out = grouped.reduce(self.custom_reduce_fn)\n        else:\n            # standard, e.g. grouped.median('time')\n            out = getattr(grouped, self.reduce_fn)()\n\n        if output is None:\n            output = podpac.UnitsDataArray(out)\n            output.attrs = source_output.attrs\n        else:\n            output.data[:] = out.data[:]\n\n        ## map\n        # eval_time = xr.DataArray(coordinates.coords[\"time\"])\n        # E = getattr(eval_time.dt, self.groupby)\n        # out = out.sel(**{self.groupby: E}).rename({self.groupby: \"time\"})\n        # output[:] = out.transpose(*output.dims).data\n\n        return output\n\n    @property",
            "def base_ref(self):\n        \"\"\"\n        Default node reference/name in node definitions\n\n        Returns\n        -------\n        str\n            Default node reference/name in node definitions\n        \"\"\"\n        return \"%s.%s.%s\" % (self.source.base_ref, self.resample, self.reduce_fn)",
            "class DayOfYear(GroupReduce):\n    \"\"\"\n    Group a time-dependent source node by day of year and compute a statistic for each group.\n\n    Attributes\n    ----------\n    custom_reduce_fn : function\n        required if reduce_fn is 'custom'.\n    reduce_fn : str\n        builtin xarray groupby reduce function, or 'custom'.\n    source : podpac.Node\n        Source node\n    \"\"\"\n\n    groupby = \"dayofyear\"",
            "class DayOfYearWindow(Algorithm):\n    \"\"\"\n    This applies a function over a moving window around day-of-year in the requested coordinates.\n    It includes the ability to rescale the input/outputs. Note if, the input coordinates include multiple years, the\n    moving window will include all of the data inside the day-of-year window.\n\n    Users need to implement the 'function' method.\n\n    Attributes\n    -----------\n    source: podpac.Node\n        The source node from which the statistics will be computed\n    window: int, optional\n        Default is 0. The size of the window over which to compute the distrubtion. This is always centered about the\n        day-of-year. The total number of days is always an odd number. For example, window=2 and window=3 will compute\n        the beta distribution for [x-1, x, x + 1] and report it as the result for x, where x is a day of the year.\n    scale_max: podpac.Node, optional\n        Default is None. A source dataset that can be used to scale the maximum value of the source function so that it\n        will fall between [0, 1]. If None, uses self.scale_float[0].\n    scale_min: podpac.Node, optional\n        Default is None. A source dataset that can be used to scale the minimum value of the source function so that it\n        will fall between [0, 1]. If None, uses self.scale_float[1].\n    scale_float: list, optional\n        Default is []. Floating point numbers used to scale the max [0] and min [1] of the source so that it falls\n        between [0, 1]. If scale_max or scale_min are defined, this property is ignored. If these are defined, the data\n        will be rescaled only if rescale=True below.\n        If None and scale_max/scale_min are not defined, the data is not scaled in any way.\n    rescale: bool, optional\n        Rescales the output data after being scaled from scale_float or scale_min/max\n    \"\"\"\n\n    source = tl.Instance(podpac.Node).tag(attr=True)\n    window = tl.Int(0).tag(attr=True)\n    scale_max = tl.Instance(podpac.Node, default_value=None, allow_none=True).tag(attr=True)\n    scale_min = tl.Instance(podpac.Node, default_value=None, allow_none=True).tag(attr=True)\n    scale_float = tl.List(default_value=None, allow_none=True).tag(attr=True)\n    rescale = tl.Bool(False).tag(attr=True)\n\n    def algorithm(self, inputs, coordinates):\n        win = self.window",
            "def algorithm(self, inputs, coordinates):\n        win = self.window",
            "def function(self, data, output):\n        raise NotImplementedError(\n            \"Child classes need to implement this function. It is applied over the data and needs\"\n            \" to populate the output.\"\n        )",
            "def rescale_outputs(self, output, scale_max, scale_min):\n        output = (output * (scale_max - scale_min)) + scale_min\n        return output"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/signal.py",
        "comments": [
            "// 2",
            "// 2 - ((s + 1) % 2), 1)",
            "// 2), arr_coords[0], delta_start)",
            "// 2) + delta_end * 1e-14, delta_end"
        ],
        "docstrings": [
            "\"\"\"\nSignal Summary\n\"\"\"",
            "\"\"\"Kernel that contains all the dimensions of the input source, in the correct order.\n\n        Returns\n        -------\n        np.ndarray\n            The dimensionally full convolution kernel\"\"\"",
            "\"\"\"Checks to make sure the kernel is valid.\n\n        Parameters\n        ----------\n        proposal : np.ndarray\n            The proposed kernel\n\n        Returns\n        -------\n        np.ndarray\n            The valid kernel\n\n        Raises\n        ------\n        ValueError\n            If the kernel is not valid (i.e. incorrect dimensionality). \"\"\"",
            "\"\"\"Compute a general convolution over a source node.\n\n    This node automatically resizes the requested coordinates to avoid edge effects.\n\n    Attributes\n    ----------\n    source : podpac.Node\n        Source node on which convolution will be performed.\n    kernel : np.ndarray, optional\n        The convolution kernel. This kernel must include the dimensions of source node outputs. The dimensions for this\n        array are labelled by `kernel_dims`. Any dimensions not in the source nodes outputs will be summed over.\n    kernel_dims : list, optional\n        A list of the dimensions for the kernel axes. If the dimensions in this list do not match the\n        coordinates in the source, then any extra dimensions in the kernel are removed by adding all the values over that axis\n        dimensions in the source are not convolved with any kernel.\n\n    kernel_type : str, optional\n        If kernel is not defined, kernel_type will create a kernel based on the inputs, and it will have the\n        same number of axes as kernel_dims.\n        The format for the created  kernels is '<kernel_type>, <kernel_size>, <kernel_params>'.\n        Any kernel defined in `scipy.signal` as well as `mean` can be used. For example:\n        kernel_type = 'mean, 8' or kernel_type = 'gaussian,16,8' are both valid.\n        Note: These kernels are automatically normalized such that kernel.sum() == 1\n    \"\"\"",
            "\"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\""
        ],
        "code_snippets": [
            "class Convolution(UnaryAlgorithm):\n    \"\"\"Compute a general convolution over a source node.\n\n    This node automatically resizes the requested coordinates to avoid edge effects.\n\n    Attributes\n    ----------\n    source : podpac.Node\n        Source node on which convolution will be performed.\n    kernel : np.ndarray, optional\n        The convolution kernel. This kernel must include the dimensions of source node outputs. The dimensions for this\n        array are labelled by `kernel_dims`. Any dimensions not in the source nodes outputs will be summed over.\n    kernel_dims : list, optional\n        A list of the dimensions for the kernel axes. If the dimensions in this list do not match the\n        coordinates in the source, then any extra dimensions in the kernel are removed by adding all the values over that axis\n        dimensions in the source are not convolved with any kernel.\n\n    kernel_type : str, optional\n        If kernel is not defined, kernel_type will create a kernel based on the inputs, and it will have the\n        same number of axes as kernel_dims.\n        The format for the created  kernels is '<kernel_type>, <kernel_size>, <kernel_params>'.\n        Any kernel defined in `scipy.signal` as well as `mean` can be used. For example:\n        kernel_type = 'mean, 8' or kernel_type = 'gaussian,16,8' are both valid.\n        Note: These kernels are automatically normalized such that kernel.sum() == 1\n    \"\"\"\n\n    kernel = ArrayTrait(dtype=float).tag(attr=True)\n    kernel_dims = tl.List().tag(attr=True)\n    # Takes one or the other which is hard to implement in a GUI\n    kernel_type = tl.List().tag(attr=True)",
            "def _first_init(self, kernel=None, kernel_dims=None, kernel_type=None, kernel_ndim=None, **kwargs):\n        if kernel_dims is None:\n            raise TypeError(\"Convolution expected 'kernel_dims' to be specified when giving a 'kernel' array\")\n\n        if kernel is not None and kernel_type is not None:\n            raise TypeError(\"Convolution expected 'kernel' or 'kernel_type', not both\")\n\n        if kernel is None:\n            if kernel_type is None:\n                raise TypeError(\"Convolution requires 'kernel' array or 'kernel_type' string\")\n            kernel = self._make_kernel(kernel_type, len(kernel_dims))\n\n        if len(kernel_dims) != len(np.array(kernel).shape):\n            raise TypeError(\n                \"The kernel_dims should contain the same number of dimensions as the number of axes in 'kernel', but len(kernel_dims) {} != len(kernel.shape) {}\".format(\n                    len(kernel_dims), len(np.array(kernel).shape)\n                )\n            )\n\n        kwargs[\"kernel\"] = kernel\n        kwargs[\"kernel_dims\"] = kernel_dims\n        return super(Convolution, self)._first_init(**kwargs)\n\n    @common_doc(COMMON_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n        \"\"\"\n        # The size of this kernel is used to figure out the expanded size\n        full_kernel = self.kernel\n\n        # expand the coordinates\n        # The next line effectively drops extra coordinates, so we have to add those later in case the\n        # source is some sort of reduction Node.\n        kernel_dims = [kd for kd in coordinates.dims if kd in self.kernel_dims]\n        missing_dims = [kd for kd in coordinates.dims if kd not in self.kernel_dims]\n\n        exp_coords = []\n        exp_slice = []\n        for dim in kernel_dims:\n            coord = coordinates[dim]\n            s = full_kernel.shape[self.kernel_dims.index(dim)]\n            if s == 1 or not isinstance(coord, (UniformCoordinates1d, ArrayCoordinates1d)):\n                exp_coords.append(coord)\n                exp_slice.append(slice(None))\n                continue\n\n            if isinstance(coord, UniformCoordinates1d):\n                s_start = -s",
            "def _make_kernel(kernel_type, ndim):\n        ktype = kernel_type.split(\",\")[0]\n        size = int(kernel_type.split(\",\")[1])\n        if ktype == \"mean\":\n            k = np.ones([size] * ndim)\n        else:\n            args = [float(a) for a in kernel_type.split(\",\")[2:]]\n            f = getattr(scipy.signal, ktype)\n            k1d = f(size, *args)\n            k = k1d.copy()\n            for i in range(ndim - 1):\n                k = np.tensordot(k, k1d, 0)\n\n        return k / k.sum()"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/coord_select.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nCoordSelect Summary\n\"\"\"",
            "\"\"\"\n    Base class for nodes that modify the requested coordinates before evaluation.\n\n    Attributes\n    ----------\n    source : podpac.Node\n        Source node that will be evaluated with the modified coordinates.\n    coordinates_source : podpac.Node\n        Node that supplies the available coordinates when necessary, optional. The source node is used by default.\n    lat, lon, time, alt : List\n        Modification parameters for given dimension. Varies by node.\n    \"\"\"",
            "\"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Notes\n        -------\n        The input coordinates are modified and the passed to the base class implementation of eval.\n        \"\"\"",
            "\"\"\"Evaluate a source node with expanded coordinates.\n\n    This is normally used in conjunction with a reduce operation\n    to calculate, for example, the average temperature over the last month. While this is simple to do when evaluating\n    a single node (just provide the coordinates), this functionality is needed for nodes buried deeper in a pipeline.\n\n    lat, lon, time, alt : List\n        Expansion parameters for the given dimension: The options are::\n         * [start_offset, end_offset, step] to expand uniformly around each input coordinate.\n         * [start_offset, end_offset] to expand using the available source coordinates around each input coordinate.\n\n    bounds_only: bool\n        Default is False. If True, will only expand the bounds of the overall coordinates request. Otherwise, it will\n        expand around EACH coordinate in the request. For example, with bounds_only == True, and an expansion of 3\n        you may expand [5, 6, 8] to [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], whereas with bounds_only == False, it becomes\n        [[2, 5, 8], [3, 6, 9], [5, 8, 11]] (brackets added for clarity, they will be concatenated).\n    \"\"\"",
            "\"\"\"Returns the expanded coordinates for the requested dimension, depending on the expansion parameter for the\n        given dimension.\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension to expand\n\n        Returns\n        -------\n        expanded : :class:`podpac.coordinates.Coordinates1d`\n            Expanded coordinates\n        \"\"\"",
            "\"\"\"Evaluate a source node with select coordinates.\n\n    While this is simple to do when\n    evaluating a single node (just provide the coordinates), this functionality is needed for nodes buried deeper in a\n    pipeline. For example, if a single spatial reference point is used for a particular comparison, and this reference\n    point is different than the requested coordinates, we need to explicitly select those coordinates using this Node.\n\n    lat, lon, time, alt : List\n        Selection parameters for the given dimension: The options are::\n         * [value]: select this coordinate value\n         * [start, stop]: select the available source coordinates within the given bounds\n         * [start, stop, step]: select uniform coordinates defined by the given start, stop, and step\n    \"\"\"",
            "\"\"\"\n        Get the desired 1d coordinates for the given dimension, depending on the selection attr for the given\n        dimension::\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension for doing the selection\n\n        Returns\n        -------\n        coords1d : ArrayCoordinates1d\n            The selected coordinates for the given dimension.\n        \"\"\"",
            "\"\"\"\n        Get the desired 1d coordinates for the given dimension, depending on the selection attr for the given\n        dimension::\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension for doing the selection\n\n        Returns\n        -------\n        coords1d : ArrayCoordinates1d\n            The selected coordinates for the given dimension.\n        \"\"\"",
            "\"\"\"\n        Get the desired 1d coordinates for the given dimension, depending on the selection attr for the given\n        dimension::\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension for doing the selection\n\n        Returns\n        -------\n        coords1d : ArrayCoordinates1d\n            The selected coordinates for the given dimension.\n        \"\"\""
        ],
        "code_snippets": [
            "class ModifyCoordinates(UnaryAlgorithm):\n    \"\"\"\n    Base class for nodes that modify the requested coordinates before evaluation.\n\n    Attributes\n    ----------\n    source : podpac.Node\n        Source node that will be evaluated with the modified coordinates.\n    coordinates_source : podpac.Node\n        Node that supplies the available coordinates when necessary, optional. The source node is used by default.\n    lat, lon, time, alt : List\n        Modification parameters for given dimension. Varies by node.\n    \"\"\"\n\n    coordinates_source = NodeTrait().tag(attr=True)\n    lat = tl.List().tag(attr=True)\n    lon = tl.List().tag(attr=True)\n    time = tl.List().tag(attr=True)\n    alt = tl.List().tag(attr=True)\n    substitute_eval_coords = tl.Bool(False).tag(attr=True)\n\n    _modified_coordinates = tl.Instance(Coordinates, allow_none=True)\n\n    @tl.default(\"coordinates_source\")",
            "def _default_coordinates_source(self):\n        return self.source\n\n    @common_doc(COMMON_DOC)",
            "def _eval(self, coordinates, output=None, _selector=None):\n        \"\"\"Evaluates this nodes using the supplied coordinates.\n\n        Parameters\n        ----------\n        coordinates : podpac.Coordinates\n            {requested_coordinates}\n        output : podpac.UnitsDataArray, optional\n            {eval_output}\n        _selector: callable(coordinates, request_coordinates)\n            {eval_selector}\n\n        Returns\n        -------\n        {eval_return}\n\n        Notes\n        -------\n        The input coordinates are modified and the passed to the base class implementation of eval.\n        \"\"\"\n\n        self._requested_coordinates = coordinates\n        self._modified_coordinates = Coordinates(\n            [self.get_modified_coordinates1d(coordinates, dim) for dim in coordinates.dims],\n            crs=coordinates.crs,\n            validate_crs=False,\n        )\n\n        for dim in self._modified_coordinates.udims:\n            if self._modified_coordinates[dim].size == 0:\n                raise ValueError(\"Modified coordinates do not intersect with source data (dim '%s')\" % dim)\n\n        outputs = {}\n        outputs[\"source\"] = self.source.eval(self._modified_coordinates, output=output, _selector=_selector)\n\n        if self.substitute_eval_coords:\n            dims = outputs[\"source\"].dims\n            coords = self._requested_coordinates\n            extra_dims = [d for d in coords.dims if d not in dims]\n            coords = coords.drop(extra_dims)\n\n            outputs[\"source\"] = outputs[\"source\"].assign_coords(**coords.xcoords)\n\n        if output is None:\n            output = outputs[\"source\"]\n        else:\n            output[:] = outputs[\"source\"]\n\n        if settings[\"DEBUG\"]:\n            self._output = output\n        return output",
            "class ExpandCoordinates(ModifyCoordinates):\n    \"\"\"Evaluate a source node with expanded coordinates.\n\n    This is normally used in conjunction with a reduce operation\n    to calculate, for example, the average temperature over the last month. While this is simple to do when evaluating\n    a single node (just provide the coordinates), this functionality is needed for nodes buried deeper in a pipeline.\n\n    lat, lon, time, alt : List\n        Expansion parameters for the given dimension: The options are::\n         * [start_offset, end_offset, step] to expand uniformly around each input coordinate.\n         * [start_offset, end_offset] to expand using the available source coordinates around each input coordinate.\n\n    bounds_only: bool\n        Default is False. If True, will only expand the bounds of the overall coordinates request. Otherwise, it will\n        expand around EACH coordinate in the request. For example, with bounds_only == True, and an expansion of 3\n        you may expand [5, 6, 8] to [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], whereas with bounds_only == False, it becomes\n        [[2, 5, 8], [3, 6, 9], [5, 8, 11]] (brackets added for clarity, they will be concatenated).\n    \"\"\"\n\n    substitute_eval_coords = tl.Bool(False, read_only=True)\n    bounds_only = tl.Bool(False).tag(attr=True)",
            "def get_modified_coordinates1d(self, coords, dim):\n        \"\"\"Returns the expanded coordinates for the requested dimension, depending on the expansion parameter for the\n        given dimension.\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension to expand\n\n        Returns\n        -------\n        expanded : :class:`podpac.coordinates.Coordinates1d`\n            Expanded coordinates\n        \"\"\"\n\n        coords1d = coords[dim]\n        expansion = getattr(self, dim)\n\n        if not expansion:  # i.e. if list is empty\n            # no expansion in this dimension\n            return coords1d\n\n        if len(expansion) == 2:\n            # use available coordinates\n            dstart = make_coord_delta(expansion[0])\n            dstop = make_coord_delta(expansion[1])\n\n            available_coordinates = self.coordinates_source.find_coordinates()\n            if len(available_coordinates) != 1:\n                raise ValueError(\"Cannot implicity expand coordinates; too many available coordinates\")\n            acoords = available_coordinates[0][dim]\n            if self.bounds_only:\n                cs = [\n                    acoords.select(\n                        add_coord(coords1d.coordinates[0], dstart), add_coord(coords1d.coordinates[-1], dstop)\n                    )\n                ]\n            else:\n                cs = [acoords.select((add_coord(x, dstart), add_coord(x, dstop))) for x in coords1d.coordinates]\n\n        elif len(expansion) == 3:\n            # use a explicit step size\n            dstart = make_coord_delta(expansion[0])\n            dstop = make_coord_delta(expansion[1])\n            step = make_coord_delta(expansion[2])\n            if self.bounds_only:\n                cs = [\n                    UniformCoordinates1d(\n                        add_coord(coords1d.coordinates[0], dstart), add_coord(coords1d.coordinates[-1], dstop), step\n                    )\n                ]\n            else:\n                cs = [\n                    UniformCoordinates1d(add_coord(x, dstart), add_coord(x, dstop), step) for x in coords1d.coordinates\n                ]\n\n        else:\n            raise ValueError(\"Invalid expansion attrs for '%s'\" % dim)\n\n        return ArrayCoordinates1d(np.concatenate([c.coordinates for c in cs]), **coords1d.properties)",
            "class SelectCoordinates(ModifyCoordinates):\n    \"\"\"Evaluate a source node with select coordinates.\n\n    While this is simple to do when\n    evaluating a single node (just provide the coordinates), this functionality is needed for nodes buried deeper in a\n    pipeline. For example, if a single spatial reference point is used for a particular comparison, and this reference\n    point is different than the requested coordinates, we need to explicitly select those coordinates using this Node.\n\n    lat, lon, time, alt : List\n        Selection parameters for the given dimension: The options are::\n         * [value]: select this coordinate value\n         * [start, stop]: select the available source coordinates within the given bounds\n         * [start, stop, step]: select uniform coordinates defined by the given start, stop, and step\n    \"\"\"",
            "def get_modified_coordinates1d(self, coords, dim):\n        \"\"\"\n        Get the desired 1d coordinates for the given dimension, depending on the selection attr for the given\n        dimension::\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension for doing the selection\n\n        Returns\n        -------\n        coords1d : ArrayCoordinates1d\n            The selected coordinates for the given dimension.\n        \"\"\"\n\n        coords1d = coords[dim]\n        selection = getattr(self, dim)\n\n        if not selection:\n            # no selection in this dimension\n            return coords1d\n\n        if len(selection) == 1 or ((len(selection) == 2) and (selection[0] == selection[1])):\n            # a single value\n            coords1d = ArrayCoordinates1d(selection, **coords1d.properties)\n\n        elif len(selection) == 2:\n            # use available source coordinates within the selected bounds\n            available_coordinates = self.coordinates_source.find_coordinates()\n            if len(available_coordinates) != 1:\n                raise ValueError(\n                    \"SelectCoordinates Node cannot determine the step size between bounds for dimension\"\n                    + \"{} because source node (source.find_coordinates()) has {} different coordinates.\".format(\n                        dim, len(available_coordinates)\n                    )\n                    + \"Please specify step-size for this dimension.\"\n                )\n            coords1d = available_coordinates[0][dim].select(selection)\n\n        elif len(selection) == 3:\n            # uniform coordinates using start, stop, and step\n            coords1d = UniformCoordinates1d(*selection, **coords1d.properties)\n\n        else:\n            raise ValueError(\"Invalid selection attrs for '%s'\" % dim)\n\n        return coords1d",
            "class YearSubstituteCoordinates(ModifyCoordinates):\n    year = tl.Unicode().tag(attr=True)\n\n    # Remove tags from attributes\n    lat = tl.List()\n    lon = tl.List()\n    time = tl.List()\n    alt = tl.List()\n    coordinates_source = None",
            "def get_modified_coordinates1d(self, coord, dim):\n        \"\"\"\n        Get the desired 1d coordinates for the given dimension, depending on the selection attr for the given\n        dimension::\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension for doing the selection\n\n        Returns\n        -------\n        coords1d : ArrayCoordinates1d\n            The selected coordinates for the given dimension.\n        \"\"\"\n        if dim != \"time\":\n            return coord[dim]\n        times = coord[\"time\"]\n        delta = np.datetime64(self.year)\n        new_times = [add_coord(c, delta - c.astype(\"datetime64[Y]\")) for c in times.coordinates]\n\n        return ArrayCoordinates1d(new_times, name=\"time\")",
            "class TransformTimeUnits(ModifyCoordinates):\n    time_units = tl.Enum(\n        [\n            \"day\",\n            \"dayofweek\",\n            \"dayofyear\",\n            \"daysinmonth\",\n            \"microsecond\",\n            \"minute\",\n            \"month\",\n            \"nanosecond\",\n            \"quarter\",\n            \"season\",\n            \"second\",\n            \"time\",\n            \"week\",\n            \"weekday\",\n            \"weekday_name\",\n            \"weekofyear\",\n            \"year\",\n        ]\n    ).tag(attr=True)\n\n    # Remove tags from attributes\n    lat = tl.List()\n    lon = tl.List()\n    time = tl.List()\n    alt = tl.List()\n    coordinates_source = None",
            "def get_modified_coordinates1d(self, coords, dim):\n        \"\"\"\n        Get the desired 1d coordinates for the given dimension, depending on the selection attr for the given\n        dimension::\n\n        Parameters\n        ----------\n        coords : Coordinates\n            The requested input coordinates\n        dim : str\n            Dimension for doing the selection\n\n        Returns\n        -------\n        coords1d : ArrayCoordinates1d\n            The selected coordinates for the given dimension.\n        \"\"\"\n        if dim != \"time\":\n            return coords[dim]\n\n        return coords.transform_time(self.time_units)[\"time\"]"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/reprojection.py",
        "comments": [],
        "docstrings": [
            "\"\"\"\nReprojection Algorithm Node\n\"\"\"",
            "\"\"\"\n    Create a Algorithm that evalutes a Node with one set of coordinates, and then interpolates it.\n    This can be used to bilinearly interpolate an averaged dataset, for example.\n\n    Attributes\n    ----------\n    source : Node\n        The source node. This node will use it's own, specified interpolation scheme\n    interpolation : str\n        Type of interpolation method to use for the interpolation\n    coordinates : Coordinates, Node, str, dict\n        Coordinates used to evaluate the source. These can be specified as a dictionary, json-formatted string,\n        PODPAC Coordinates, or a PODPAC Node, where the node MUST implement the 'coordinates' attribute.\n    reproject_dims : list\n        Dimensions to reproject. The source will be evaluated with the reprojection coordinates in these dims\n        and the requested coordinates for any other dims.\n    \"\"\"",
            "\"\"\"Coordinates used to evaluate the source. These can be specified as a dictionary,\n                           json-formatted string, PODPAC Coordinates, or a PODPAC Node, where the node MUST implement\n                           the 'coordinates' attribute\"\"\""
        ],
        "code_snippets": [
            "class Reproject(Interpolate):\n    \"\"\"\n    Create a Algorithm that evalutes a Node with one set of coordinates, and then interpolates it.\n    This can be used to bilinearly interpolate an averaged dataset, for example.\n\n    Attributes\n    ----------\n    source : Node\n        The source node. This node will use it's own, specified interpolation scheme\n    interpolation : str\n        Type of interpolation method to use for the interpolation\n    coordinates : Coordinates, Node, str, dict\n        Coordinates used to evaluate the source. These can be specified as a dictionary, json-formatted string,\n        PODPAC Coordinates, or a PODPAC Node, where the node MUST implement the 'coordinates' attribute.\n    reproject_dims : list\n        Dimensions to reproject. The source will be evaluated with the reprojection coordinates in these dims\n        and the requested coordinates for any other dims.\n    \"\"\"\n\n    coordinates = tl.Union(\n        [NodeTrait(), tl.Dict(), tl.Unicode(), tl.Instance(Coordinates)],\n        help=\"\"\"Coordinates used to evaluate the source. These can be specified as a dictionary,\n                           json-formatted string, PODPAC Coordinates, or a PODPAC Node, where the node MUST implement\n                           the 'coordinates' attribute\"\"\",\n    ).tag(attr=True)\n\n    reproject_dims = tl.List(trait=tl.Unicode(), allow_none=True, default_value=None).tag(attr=True)\n\n    @tl.validate(\"coordinates\")",
            "def _validate_coordinates(self, d):\n        val = d[\"value\"]\n        if isinstance(val, Node):\n            if not hasattr(val, \"coordinates\"):\n                raise ValueError(\n                    \"When specifying the coordinates as a PODPAC Node, this Node must have a 'coordinates' attribute\"\n                )\n        elif isinstance(val, dict):\n            Coordinates.from_definition(self.coordinates)\n        elif isinstance(val, string_types):\n            Coordinates.from_json(self.coordinates)\n        return val\n\n    @cached_property",
            "def reprojection_coordinates(self):\n        # get coordinates\n        if isinstance(self.coordinates, Coordinates):\n            coordinates = self.coordinates\n        elif isinstance(self.coordinates, Node):\n            coordinates = self.coordinates.coordinates\n        elif isinstance(self.coordinates, dict):\n            coordinates = Coordinates.from_definition(self.coordinates)\n        elif isinstance(self.coordinates, string_types):\n            coordinates = Coordinates.from_json(self.coordinates)\n\n        # drop non-reprojection dims\n        if self.reproject_dims is not None:\n            coordinates = coordinates.drop([dim for dim in coordinates if dim not in self.reproject_dims])\n\n        return coordinates",
            "def _source_eval(self, coordinates, selector, output=None):\n        coords = self.reprojection_coordinates.intersect(coordinates, outer=True)\n        extra_eval_coords = coordinates.drop(self.reproject_dims or self.reprojection_coordinates.dims)\n        if coords.crs != coordinates.crs:\n            # Better to evaluate in reproject coordinate crs than eval crs for next step of interpolation\n            extra_eval_coords = extra_eval_coords.transform(coords.crs)\n        coords = merge_dims([coords, extra_eval_coords])\n        if settings[\"MULTITHREADING\"]:\n            # we have to do a new node here to avoid clashing with the source node.\n            # What happens is that the non-projected source gets evaluated\n            # at the projected source coordinates because we have to set\n            # self._requested_coordinates for the datasource to avoid floating point\n            # lat/lon disagreement issues\n            return Node.from_definition(self.source.definition).eval(coords, output=output, _selector=selector)\n        else:\n            return self.source.eval(coords, output=output, _selector=selector)\n\n    @property",
            "def base_ref(self):\n        return \"{}_reprojected\".format(self.source.base_ref)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_signal.py",
        "comments": [],
        "docstrings": [
            "\"\"\"When the kernel has more dimensions than the source, sum out the kernel for the missing dim\"\"\""
        ],
        "code_snippets": [
            "class TestConvolution(object):",
            "def test_init_kernel(self):\n        node = Convolution(source=Arange(), kernel=[1, 2, 1], kernel_dims=[\"lat\"])\n        assert_equal(node.kernel, [1, 2, 1])\n\n        node = Convolution(source=Arange(), kernel_type=\"mean, 5\", kernel_dims=[\"lat\", \"lon\"])\n        assert node.kernel.shape == (5, 5)\n        assert np.all(node.kernel == 0.04)\n\n        node = Convolution(source=Arange(), kernel_type=\"mean, 5\", kernel_dims=[\"lat\", \"lon\", \"time\"])\n        assert node.kernel.shape == (5, 5, 5)\n        assert np.all(node.kernel == 0.008)\n\n        node = Convolution(source=Arange(), kernel_type=\"gaussian, 3, 1\", kernel_dims=[\"lat\", \"lon\"])\n        assert node.kernel.shape == (3, 3)\n\n        # kernel and kernel_type invalid\n        with pytest.raises(TypeError, match=\"Convolution expected 'kernel' or 'kernel_type', not both\"):\n            Convolution(source=Arange(), kernel=[1, 2, 1], kernel_type=\"mean, 5\", kernel_dims=[\"lat\", \"lon\"])\n\n        # kernel or kernel_type required\n        with pytest.raises(TypeError, match=\"Convolution requires 'kernel' array or 'kernel_type' string\"):\n            Convolution(source=Arange(), kernel_dims=[\"lat\", \"lon\"])\n\n        # kernel_dims required\n        with pytest.raises(\n            TypeError, match=\"Convolution expected 'kernel_dims' to be specified when giving a 'kernel' array\"\n        ):\n            Convolution(source=Arange(), kernel_type=\"mean, 5\")\n\n        # kernel_dims correct number of entries\n        with pytest.raises(\n            TypeError,\n            match=\"The kernel_dims should contain the same number of dimensions as the number of axes in 'kernel', but \",\n        ):\n            Convolution(source=Arange(), kernel=[[[1, 2]]], kernel_dims=[\"lat\"])",
            "def test_eval(self):\n        lat = clinspace(45, 66, 30, name=\"lat\")\n        lon = clinspace(-80, 70, 40, name=\"lon\")\n        time = crange(\"2017-09-01\", \"2017-10-31\", \"1,D\", name=\"time\")\n\n        kernel1d = [1, 2, 1]\n        kernel2d = [[1, 2, 1]]\n        kernel3d = [[[1, 2, 1]]]\n\n        node1d = Convolution(source=Arange(), kernel=kernel1d, kernel_dims=[\"time\"])\n        node2d = Convolution(source=Arange(), kernel=kernel2d, kernel_dims=[\"lat\", \"lon\"])\n        node3d = Convolution(source=Arange(), kernel=kernel3d, kernel_dims=[\"lon\", \"lat\", \"time\"])\n\n        o = node1d.eval(Coordinates([time]))\n        o = node2d.eval(Coordinates([lat, lon]))\n        o = node3d.eval(Coordinates([lat, lon, time]))",
            "def test_eval_multiple_outputs(self):\n\n        lat = clinspace(45, 66, 30, name=\"lat\")\n        lon = clinspace(-80, 70, 40, name=\"lon\")\n        kernel = [[1, 2, 1]]\n        coords = Coordinates([lat, lon])\n        multi = Array(source=np.random.random(coords.shape + (2,)), coordinates=coords, outputs=[\"a\", \"b\"])\n        node = Convolution(source=multi, kernel=kernel, kernel_dims=[\"lat\", \"lon\"])\n        o1 = node.eval(Coordinates([lat, lon]))\n\n        kernel = [[[1, 2]]]\n        coords = Coordinates([lat, lon])\n        multi = Array(source=np.random.random(coords.shape + (2,)), coordinates=coords, outputs=[\"a\", \"b\"])\n        node1 = Convolution(source=multi, kernel=kernel, kernel_dims=[\"lat\", \"lon\", \"output\"], force_eval=True)\n        node2 = Convolution(source=multi, kernel=kernel[0], kernel_dims=[\"lat\", \"lon\"], force_eval=True)\n        o1 = node1.eval(Coordinates([lat, lon]))\n        o2 = node2.eval(Coordinates([lat, lon]))\n\n        assert np.any(o2.data != o1.data)",
            "def test_eval_nan(self):\n        lat = clinspace(45, 66, 30, name=\"lat\")\n        lon = clinspace(-80, 70, 40, name=\"lon\")\n        coords = Coordinates([lat, lon])\n\n        data = np.ones(coords.shape)\n        data[10, 10] = np.nan\n        source = Array(source=data, coordinates=coords)\n        node = Convolution(source=source, kernel=[[1, 2, 1]], kernel_dims=[\"lat\", \"lon\"])\n\n        o = node.eval(coords[8:12, 7:13])",
            "def test_eval_with_output_argument(self):\n        lat = clinspace(45, 66, 30, name=\"lat\")\n        lon = clinspace(-80, 70, 40, name=\"lon\")\n        coords = Coordinates([lat, lon])\n\n        node = Convolution(source=Arange(), kernel=[[1, 2, 1]], kernel_dims=[\"lat\", \"lon\"])\n\n        a = node.create_output_array(coords)\n        o = node.eval(coords, output=a)\n        assert_array_equal(a, o)",
            "def test_debuggable_source(self):\n        with podpac.settings:\n            podpac.settings[\"DEBUG\"] = False\n            lat = clinspace(45, 66, 30, name=\"lat\")\n            lon = clinspace(-80, 70, 40, name=\"lon\")\n            coords = Coordinates([lat, lon])\n\n            # normal version\n            a = Arange()\n            node = Convolution(source=a, kernel=[[1, 2, 1]], kernel_dims=[\"lat\", \"lon\"])\n            node.eval(coords)\n\n            assert node.source is a\n\n            # debuggable\n            podpac.settings[\"DEBUG\"] = True\n\n            a = Arange()\n            node = Convolution(source=a, kernel=[[1, 2, 1]], kernel_dims=[\"lat\", \"lon\"])\n            node.eval(coords)\n\n            assert node.source is not a\n            assert node._requested_coordinates == coords\n            assert node.source._requested_coordinates is not None\n            assert node.source._requested_coordinates != coords\n            assert a._requested_coordinates is None",
            "def test_extra_kernel_dims(self):\n        lat = clinspace(45, 66, 8, name=\"lat\")\n        lon = clinspace(-80, 70, 16, name=\"lon\")\n        coords = Coordinates([lat, lon])\n\n        node = Convolution(source=Arange(), kernel=[[[1, 2, 1]]], kernel_dims=[\"time\", \"lat\", \"lon\"])\n        o = node.eval(coords)",
            "def test_extra_coord_dims(self):\n        lat = clinspace(-0.25, 1.25, 7, name=\"lat\")\n        lon = clinspace(-0.125, 1.125, 11, name=\"lon\")\n        time = [\"2012-05-19\", \"2016-01-31\", \"2018-06-20\"]\n        coords = Coordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n\n        source = Array(source=np.random.random(coords.drop(\"time\").shape), coordinates=coords.drop(\"time\"))\n        node = Convolution(source=source, kernel=[[-1, 2, -1]], kernel_dims=[\"lat\", \"lon\"], force_eval=True)\n        o = node.eval(coords)\n        assert np.all([d in [\"lat\", \"lon\"] for d in o.dims])",
            "def test_coords_order(self):\n        lat = clinspace(-0.25, 1.25, 7, name=\"lat\")\n        lon = clinspace(-0.125, 1.125, 11, name=\"lon\")\n        coords = Coordinates([lat, lon])\n\n        lat = clinspace(0, 1, 5, name=\"lat\")\n        lon = clinspace(0, 1, 9, name=\"lon\")\n        coords1 = Coordinates([lat, lon])\n        coords2 = Coordinates([lon, lat])\n\n        source = Array(source=np.random.random(coords.shape), coordinates=coords)\n        node = Convolution(source=source, kernel=[[-1, 2, -1]], kernel_dims=[\"lat\", \"lon\"], force_eval=True)\n        o1 = node.eval(coords1)\n        o2 = node.eval(coords2)\n        assert np.all(o2.data == o1.data.T)",
            "def test_missing_source_dims(self):",
            "def test_partial_source_convolution(self):\n        lat = clinspace(-0.25, 1.25, 7, name=\"lat\")\n        lon = clinspace(-0.125, 1.125, 11, name=\"lon\")\n        time = [\"2012-05-19\", \"2016-01-31\", \"2018-06-20\"]\n        coords = Coordinates([lat, lon, time], dims=[\"lat\", \"lon\", \"time\"])\n\n        source = Array(source=np.random.random(coords.shape), coordinates=coords)\n        node = Convolution(source=source, kernel=[[-1, 2, -1]], kernel_dims=[\"lat\", \"lon\"], force_eval=True)\n        o = node.eval(coords[:, 1:-1, :])\n        expected = source.source[:, 1:-1] * 2 - source.source[:, 2:] - source.source[:, :-2]\n\n        assert np.abs(o.data - expected).max() < 1e-14"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_stats.py",
        "comments": [],
        "docstrings": [
            "\"\"\"Tests the Reduce class\"\"\"",
            "\"\"\"Common tests for Reduce subclasses\"\"\""
        ],
        "code_snippets": [
            "def setup_module():\n    global coords, source, data, multisource, bdata\n    coords = podpac.Coordinates(\n        [podpac.clinspace(0, 1, 10), podpac.clinspace(0, 1, 10), podpac.crange(\"2018-01-01\", \"2018-01-10\", \"1,D\")],\n        dims=[\"lat\", \"lon\", \"time\"],\n    )\n\n    a = np.random.random(coords.shape)\n    a[3, 0, 0] = np.nan\n    a[0, 3, 0] = np.nan\n    a[0, 0, 3] = np.nan\n    source = Array(source=a, coordinates=coords)\n    data = source.eval(coords)\n\n    ab = np.stack([a, 2 * a], -1)\n    multisource = Array(source=ab, coordinates=coords, outputs=[\"a\", \"b\"])\n    bdata = 2 * data",
            "class TestReduce(object):",
            "def test_auto_chunk(self):\n        # any reduce node would do here\n        node = Min(source=source)\n\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = \"auto\"\n            node.eval(coords)",
            "def test_chunked_fallback(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False",
            "class First(Reduce):",
            "def reduce(self, x):\n                    return x.isel(**{dim: 0 for dim in self.dims})\n\n            node = First(source=source, dims=\"time\")\n\n            # use reduce function\n            podpac.settings[\"CHUNK_SIZE\"] = None\n            output = node.eval(coords)\n\n            # fall back on reduce function with warning\n            with pytest.warns(UserWarning):\n                podpac.settings[\"CHUNK_SIZE\"] = 500\n                output_chunked = node.eval(coords)\n\n            # should be the same\n            xr.testing.assert_allclose(output, output_chunked)",
            "class BaseTests(object):",
            "def test_full(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = None\n\n            node = self.NodeClass(source=source)\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_full)\n            np.testing.assert_allclose(output.data, self.expected_full.data)\n\n            node = self.NodeClass(source=source, dims=coords.dims)\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_full)\n            np.testing.assert_allclose(output.data, self.expected_full.data)",
            "def test_full_chunked(self):\n        with podpac.settings:\n            node = self.NodeClass(source=source, dims=coords.dims)\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = 500\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_full)\n            np.testing.assert_allclose(output.data, self.expected_full.data)",
            "def test_lat_lon(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = None\n            node = self.NodeClass(source=source, dims=[\"lat\", \"lon\"])\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_latlon)\n            np.testing.assert_allclose(output.data, self.expected_latlon.data)",
            "def test_lat_lon_chunked(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = 500\n            node = self.NodeClass(source=source, dims=[\"lat\", \"lon\"])\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_latlon)\n            np.testing.assert_allclose(output.data, self.expected_latlon.data)",
            "def test_time(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = None\n            node = self.NodeClass(source=source, dims=\"time\")\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_time)\n            np.testing.assert_allclose(output.data, self.expected_time.data)",
            "def test_time_chunked(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = 500\n            node = self.NodeClass(source=source, dims=\"time\")\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_time)\n            np.testing.assert_allclose(output.data, self.expected_time.data)",
            "def test_multiple_outputs(self):\n        with podpac.settings:\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"CHUNK_SIZE\"] = None\n            node = self.NodeClass(source=multisource, dims=[\"lat\", \"lon\"])\n            output = node.eval(coords)\n            assert output.dims == (\"time\", \"output\")\n            np.testing.assert_array_equal(output[\"output\"], [\"a\", \"b\"])\n            np.testing.assert_allclose(output.sel(output=\"a\"), self.expected_latlon)\n            np.testing.assert_allclose(output.sel(output=\"b\"), self.expected_latlon_b)",
            "class TestMin(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Min\n        cls.expected_full = data.min()\n        cls.expected_latlon = data.min(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.min(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.min(dim=\"time\")",
            "class TestMax(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Max\n        cls.expected_full = data.max()\n        cls.expected_latlon = data.max(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.max(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.max(dim=\"time\")",
            "class TestSum(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Sum\n        cls.expected_full = data.sum()\n        cls.expected_latlon = data.sum(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.sum(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.sum(dim=\"time\")",
            "class TestCount(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Count\n        cls.expected_full = np.isfinite(data).sum()\n        cls.expected_latlon = np.isfinite(data).sum(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = np.isfinite(bdata).sum(dim=[\"lat\", \"lon\"])\n        cls.expected_time = np.isfinite(data).sum(dim=\"time\")",
            "class TestMean(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Mean\n        cls.expected_full = data.mean()\n        cls.expected_latlon = data.mean(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.mean(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.mean(dim=\"time\")",
            "def test_chunk_sizes(self):\n        for n in [20, 21, 1000, 1001]:\n            podpac.settings[\"CHUNK_SIZE\"] = n\n            node = self.NodeClass(source=source, dims=coords.dims)\n            output = node.eval(coords)\n            # xr.testing.assert_allclose(output, self.expected_full)\n            np.testing.assert_allclose(output.data, self.expected_full.data)",
            "class TestVariance(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Variance\n        cls.expected_full = data.var()\n        cls.expected_latlon = data.var(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.var(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.var(dim=\"time\")",
            "class TestStandardDeviation(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = StandardDeviation\n        cls.expected_full = data.std()\n        cls.expected_latlon = data.std(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.std(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.std(dim=\"time\")\n        cls.expected_latlon_b = 2 * cls.expected_latlon",
            "class TestSkew(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Skew\n        n, m, l = data.shape\n        cls.expected_full = xr.DataArray(scipy.stats.skew(data.data.reshape(n * m * l), nan_policy=\"omit\"))\n        cls.expected_latlon = scipy.stats.skew(data.data.reshape((n * m, l)), axis=0, nan_policy=\"omit\")\n        cls.expected_latlon_b = scipy.stats.skew(bdata.data.reshape((n * m, l)), axis=0, nan_policy=\"omit\")\n        cls.expected_time = scipy.stats.skew(data, axis=2, nan_policy=\"omit\")",
            "class TestKurtosis(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Kurtosis\n        n, m, l = data.shape\n        cls.expected_full = xr.DataArray(scipy.stats.kurtosis(data.data.reshape(n * m * l), nan_policy=\"omit\"))\n        cls.expected_latlon = scipy.stats.kurtosis(data.data.reshape((n * m, l)), axis=0, nan_policy=\"omit\")\n        cls.expected_latlon_b = scipy.stats.kurtosis(bdata.data.reshape((n * m, l)), axis=0, nan_policy=\"omit\")\n        cls.expected_time = scipy.stats.kurtosis(data, axis=2, nan_policy=\"omit\")",
            "class TestMedian(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.NodeClass = Median\n        cls.expected_full = data.median()\n        cls.expected_latlon = data.median(dim=[\"lat\", \"lon\"])\n        cls.expected_latlon_b = bdata.median(dim=[\"lat\", \"lon\"])\n        cls.expected_time = data.median(dim=\"time\")\n\n\n@pytest.mark.skip(\"TODO\")",
            "class TestPercentile(BaseTests):\n    @classmethod",
            "def setup_class(cls):\n        cls.node = Percentile(source=source)\n        # TODO can we replace dims_axes with reshape (or vice versa)",
            "class TestGroupReduce(object):\n    pass",
            "class TestResampleReduce(object):\n    pass",
            "class TestDayOfYear(object):\n    pass",
            "class F(DayOfYearWindow):\n    cache_output = tl.Bool(False)\n    force_eval = tl.Bool(True)",
            "def function(self, data, output):\n        return len(data)",
            "class FM(DayOfYearWindow):\n    cache_output = tl.Bool(False)\n    force_eval = tl.Bool(True)",
            "def function(self, data, output):\n        return np.mean(data)",
            "class TestDayOfYearWindow(object):",
            "def test_doy_window1(self):\n        coords = podpac.coordinates.concat(\n            [\n                podpac.Coordinates([podpac.crange(\"1999-12-29\", \"2000-01-02\", \"1,D\", \"time\")]),\n                podpac.Coordinates([podpac.crange(\"2001-12-30\", \"2002-01-03\", \"1,D\", \"time\")]),\n            ]\n        )\n\n        node = Arange()\n        nodedoywindow = F(source=node, window=1, cache_output=False, force_eval=True)\n        o = nodedoywindow.eval(coords)\n\n        np.testing.assert_array_equal(o, [2, 2, 1, 1, 2, 2])",
            "def test_doy_window2(self):\n        coords = podpac.coordinates.concat(\n            [\n                podpac.Coordinates([podpac.crange(\"1999-12-29\", \"2000-01-03\", \"1,D\", \"time\")]),\n                podpac.Coordinates([podpac.crange(\"2001-12-30\", \"2002-01-02\", \"1,D\", \"time\")]),\n            ]\n        )\n\n        node = Arange()\n        nodedoywindow = F(source=node, window=2, cache_output=False, force_eval=True)\n        o = nodedoywindow.eval(coords)\n\n        np.testing.assert_array_equal(o, [6, 5, 3, 3, 5, 6])",
            "def test_doy_window2_mean_rescale_float(self):\n        coords = podpac.coordinates.concat(\n            [\n                podpac.Coordinates([podpac.crange(\"1999-12-29\", \"2000-01-03\", \"1,D\", \"time\")]),\n                podpac.Coordinates([podpac.crange(\"2001-12-30\", \"2002-01-02\", \"1,D\", \"time\")]),\n            ]\n        )\n\n        node = Arange()\n        nodedoywindow = FM(source=node, window=2, cache_output=False, force_eval=True)\n        o = nodedoywindow.eval(coords)\n\n        nodedoywindow_s = FM(\n            source=node, window=2, cache_output=False, force_eval=True, scale_float=[0, coords.size], rescale=True\n        )\n        o_s = nodedoywindow_s.eval(coords)\n\n        np.testing.assert_array_almost_equal(o, o_s)",
            "def test_doy_window2_mean_rescale_max_min(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n\n            coords = podpac.coordinates.concat(\n                [\n                    podpac.Coordinates([podpac.crange(\"1999-12-29\", \"2000-01-03\", \"1,D\", \"time\")]),\n                    podpac.Coordinates([podpac.crange(\"2001-12-30\", \"2002-01-02\", \"1,D\", \"time\")]),\n                ]\n            )\n\n            node = Arange()\n            node_max = Arithmetic(source=node, eqn=\"(source < 5) + source\")\n            node_min = Arithmetic(source=node, eqn=\"-1*(source < 5) + source\")\n\n            nodedoywindow_s = FM(\n                source=node,\n                window=2,\n                cache_output=False,\n                force_eval=True,\n                scale_max=node_max,\n                scale_min=node_min,\n                rescale=False,\n            )\n            o_s = nodedoywindow_s.eval(coords)\n\n            np.testing.assert_array_almost_equal([0.5] * o_s.size, o_s)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_coord_select.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "def setup_module(module):\n    global COORDS\n    COORDS = podpac.Coordinates(\n        [\"2017-09-01\", podpac.clinspace(45, 66, 4), podpac.clinspace(-80, -70, 5)], dims=[\"time\", \"lat\", \"lon\"]\n    )",
            "class MyDataSource(DataSource):\n    coordinates = podpac.Coordinates(\n        [\n            podpac.crange(\"2010-01-01\", \"2018-01-01\", \"4,h\"),\n            podpac.clinspace(-180, 180, 6),\n            podpac.clinspace(-80, -70, 6),\n        ],\n        dims=[\"time\", \"lat\", \"lon\"],\n    )",
            "def get_data(self, coordinates, slc):\n        node = Arange()\n        return node.eval(coordinates)\n\n\n# TODO add assertions to tests",
            "class TestExpandCoordinates(object):",
            "def test_no_expansion(self):\n        node = ExpandCoordinates(source=Arange())\n        o = node.eval(COORDS)",
            "def test_time_expansion(self):\n        node = ExpandCoordinates(source=Arange(), time=(\"-5,D\", \"0,D\", \"1,D\"))\n        o = node.eval(COORDS)",
            "def test_spatial_expansion(self):\n        node = ExpandCoordinates(source=Arange(), lat=(-1, 1, 0.1))\n        o = node.eval(COORDS)",
            "def test_time_expansion_implicit_coordinates(self):\n        node = ExpandCoordinates(source=MyDataSource(), time=(\"-15,D\", \"0,D\"))\n        o = node.eval(COORDS)\n\n        node = ExpandCoordinates(source=MyDataSource(), time=(\"-15,Y\", \"0,D\", \"1,Y\"))\n        o = node.eval(COORDS)\n\n        node = ExpandCoordinates(source=MyDataSource(), time=(\"-5,M\", \"0,D\", \"1,M\"))\n        o = node.eval(COORDS)\n\n        # Behaviour a little strange on these?\n        node = ExpandCoordinates(source=MyDataSource(), time=(\"-15,Y\", \"0,D\", \"4,Y\"))\n        o = node.eval(COORDS)\n\n        node = ExpandCoordinates(source=MyDataSource(), time=(\"-15,Y\", \"0,D\", \"13,M\"))\n        o = node.eval(COORDS)\n\n        node = ExpandCoordinates(source=MyDataSource(), time=(\"-144,M\", \"0,D\", \"13,M\"))\n        o = node.eval(COORDS)",
            "def test_spatial_expansion_ultiple_outputs(self):\n        multi = Array(source=np.random.random(COORDS.shape + (2,)), coordinates=COORDS, outputs=[\"a\", \"b\"])\n        node = ExpandCoordinates(source=multi, lat=(-1, 1, 0.1))\n        o = node.eval(COORDS)",
            "class TestSelectCoordinates(object):",
            "def test_no_expansion(self):\n        node = SelectCoordinates(source=Arange())\n        o = node.eval(COORDS)",
            "def test_time_selection(self):\n        node = SelectCoordinates(source=Arange(), time=(\"2017-08-01\", \"2017-09-30\", \"1,D\"))\n        o = node.eval(COORDS)",
            "def test_spatial_selection(self):\n        node = SelectCoordinates(source=Arange(), lat=(46, 56, 1))\n        o = node.eval(COORDS)",
            "def test_time_selection_implicit_coordinates(self):\n        node = SelectCoordinates(source=MyDataSource(), time=(\"2011-01-01\", \"2011-02-01\"))\n        o = node.eval(COORDS)\n\n        node = SelectCoordinates(source=MyDataSource(), time=(\"2011-01-01\", \"2017-01-01\", \"1,Y\"))\n        o = node.eval(COORDS)",
            "def test_spatial_selection_multiple_outputs(self):\n        multi = Array(source=np.random.random(COORDS.shape + (2,)), coordinates=COORDS, outputs=[\"a\", \"b\"])\n        node = SelectCoordinates(source=multi, lat=(46, 56, 1))\n        o = node.eval(COORDS)",
            "class TestYearSubstituteCoordinates(object):",
            "def test_year_substitution(self):\n        node = YearSubstituteCoordinates(source=Arange(), year=\"2018\")\n        o = node.eval(COORDS)\n        assert o.time.dt.year.data[0] == 2018\n        assert not np.array_equal(o[\"time\"], COORDS[\"time\"].coordinates)",
            "def test_year_substitution_orig_coords(self):\n        node = YearSubstituteCoordinates(source=Arange(), year=\"2018\", substitute_eval_coords=True)\n        o = node.eval(COORDS)\n        assert o.time.dt.year.data[0] == xr.DataArray(COORDS[\"time\"].coordinates).dt.year.data[0]\n        np.testing.assert_array_equal(o[\"time\"], COORDS[\"time\"].coordinates)",
            "def test_year_substitution_missing_coords(self):\n        source = Array(\n            source=[[1, 2, 3], [4, 5, 6]],\n            coordinates=podpac.Coordinates(\n                [podpac.crange(\"2018-01-01\", \"2018-01-02\", \"1,D\"), podpac.clinspace(45, 66, 3)], dims=[\"time\", \"lat\"]\n            ),\n        )\n        node = YearSubstituteCoordinates(source=source, year=\"2018\")\n        o = node.eval(COORDS)\n        assert o.time.dt.year.data[0] == 2018\n        assert o[\"time\"].data != xr.DataArray(COORDS[\"time\"].coordinates).data",
            "def test_year_substitution_missing_coords_orig_coords(self):\n        source = Array(\n            source=[[1, 2, 3], [4, 5, 6]],\n            coordinates=podpac.Coordinates(\n                [podpac.crange(\"2018-01-01\", \"2018-01-02\", \"1,D\"), podpac.clinspace(45, 66, 3)], dims=[\"time\", \"lat\"]\n            ),\n        )\n        node = YearSubstituteCoordinates(source=source, year=\"2018\", substitute_eval_coords=True)\n        o = node.eval(COORDS)\n        assert o.time.dt.year.data[0] == 2017\n        np.testing.assert_array_equal(o[\"time\"], COORDS[\"time\"].coordinates)",
            "def test_year_substitution_multiple_outputs(self):\n        multi = Array(source=np.random.random(COORDS.shape + (2,)), coordinates=COORDS, outputs=[\"a\", \"b\"])\n        node = YearSubstituteCoordinates(source=multi, year=\"2018\")\n        o = node.eval(COORDS)\n        assert o.time.dt.year.data[0] == 2018\n        assert not np.array_equal(o[\"time\"], COORDS[\"time\"].coordinates)"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_algorithm.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestBaseAlgorithm(object):",
            "def test_eval_not_implemented(self):\n        node = BaseAlgorithm()\n        c = podpac.Coordinates([])\n        with pytest.raises(NotImplementedError):\n            node.eval(c)",
            "def test_inputs(self):",
            "class MyAlgorithm(BaseAlgorithm):\n            x = tl.Instance(podpac.Node).tag(attr=True)\n            y = tl.Instance(podpac.Node).tag(attr=True)\n\n        node = MyAlgorithm(x=podpac.Node(), y=podpac.Node())\n        assert \"x\" in node.inputs\n        assert \"y\" in node.inputs",
            "def test_find_coordinates(self):",
            "class MyAlgorithm(BaseAlgorithm):\n            x = tl.Instance(podpac.Node).tag(attr=True)\n            y = tl.Instance(podpac.Node).tag(attr=True)\n\n        node = MyAlgorithm(\n            x=podpac.data.Array(coordinates=podpac.Coordinates([[0, 1, 2]], dims=[\"lat\"])),\n            y=podpac.data.Array(coordinates=podpac.Coordinates([[10, 11, 12], [100, 200]], dims=[\"lat\", \"lon\"])),\n        )\n\n        l = node.find_coordinates()\n        assert isinstance(l, list)\n        assert len(l) == 2\n        assert node.x.coordinates in l\n        assert node.y.coordinates in l",
            "class TestAlgorithm(object):",
            "def test_algorithm_not_implemented(self):\n        node = Algorithm()\n        c = podpac.Coordinates([])\n        with pytest.raises(NotImplementedError):\n            node.eval(c)",
            "def test_eval(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coordinates.size).reshape(coordinates.shape)\n                return self.create_output_array(coordinates, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm()\n        output = node.eval(coords)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(3, 2))",
            "def test_eval_algorithm_returns_xarray(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coordinates.size).reshape(coordinates.shape)\n                return xr.DataArray(\n                    data, coords=coordinates.xcoords, dims=coordinates.dims, attrs={\"crs\": coordinates.crs}\n                )\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm()\n        output = node.eval(coords)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(3, 2))",
            "def test_eval_algorithm_returns_numpy_array(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coordinates.size).reshape(coordinates.shape)\n                return data\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm()\n        with pytest.raises(podpac.NodeException, match=\"algorithm returned unsupported type\"):\n            output = node.eval(coords)",
            "def test_eval_algorithm_drops_dimension(self):",
            "class MyAlgorithm(Algorithm):\n            drop = tl.Unicode()",
            "def algorithm(self, inputs, coordinates):\n                c = coordinates.drop(self.drop)\n                data = np.arange(c.size).reshape(c.shape)\n                return self.create_output_array(c, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm(drop=\"lat\")\n        output = node.eval(coords)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lon\",)\n        np.testing.assert_array_equal(output, np.arange(2))\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm(drop=\"lon\")\n        output = node.eval(coords)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\",)\n        np.testing.assert_array_equal(output, np.arange(3))",
            "def test_eval_algorithm_adds_dimension(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                c = podpac.Coordinates(\n                    [[\"2020-01-01\", \"2020-01-02\"], coordinates[\"lat\"], coordinates[\"lon\"]], dims=[\"time\", \"lat\", \"lon\"]\n                )\n                data = np.arange(c.size).reshape(c.shape)\n                return self.create_output_array(c, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm()\n        output = node.eval(coords)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\", \"time\")\n        np.testing.assert_array_equal(output, np.arange(12).reshape(2, 3, 2).transpose(1, 2, 0))",
            "def test_eval_algorithm_transposes(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                c = coordinates.transpose(\"lon\", \"lat\")\n                data = np.arange(c.size).reshape(c.shape)\n                return self.create_output_array(c, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = MyAlgorithm()\n        output = node.eval(coords)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(2, 3).T)",
            "def test_eval_with_output(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coordinates.size).reshape(coordinates.shape)\n                return self.create_output_array(coordinates, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        output = podpac.UnitsDataArray.create(coords)\n\n        node = MyAlgorithm()\n        node.eval(podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"]), output=output)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(3, 2))",
            "def test_eval_with_output_missing_dims(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coords.size).reshape(coords.shape)\n                return self.create_output_array(coords, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        output = podpac.UnitsDataArray.create(coords.drop(\"lat\"))\n\n        node = MyAlgorithm()\n        with pytest.raises(podpac.NodeException, match=\"provided output is missing dims\"):\n            node.eval(coords, output=output)",
            "def test_eval_with_output_transposed(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coordinates.size).reshape(coordinates.shape)\n                return self.create_output_array(coordinates, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        output = podpac.UnitsDataArray.create(coords).transpose(\"lon\", \"lat\")\n\n        node = MyAlgorithm()\n        node.eval(coords, output=output)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lon\", \"lat\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(3, 2).T)",
            "def test_eval_with_output_algorithm_returns_xarray(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                data = np.arange(coordinates.size).reshape(coordinates.shape)\n                return xr.DataArray(\n                    data, coords=coordinates.xcoords, dims=coordinates.dims, attrs={\"crs\": coordinates.crs}\n                )\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        output = podpac.UnitsDataArray.create(coords)\n\n        node = MyAlgorithm()\n        node.eval(coords, output=output)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(3, 2))",
            "def test_eval_with_output_algorithm_drops_dimension(self):",
            "class MyAlgorithm(Algorithm):\n            drop = tl.Unicode().tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                c = coordinates.drop(self.drop)\n                data = np.arange(c.size).reshape(c.shape)\n                return self.create_output_array(c, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n\n        output = podpac.UnitsDataArray.create(coords)\n        node = MyAlgorithm(drop=\"lat\")\n        node.eval(coords, output=output)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, [np.arange(2), np.arange(2), np.arange(2)])\n\n        output = podpac.UnitsDataArray.create(coords)\n        node = MyAlgorithm(drop=\"lon\")\n        node.eval(coords, output=output)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.array([np.arange(3), np.arange(3)]).T)",
            "def test_eval_with_output_algorithm_adds_dimension(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                c = podpac.Coordinates(\n                    [[\"2020-01-01\", \"2020-01-02\"], coordinates[\"lat\"], coordinates[\"lon\"]], dims=[\"time\", \"lat\", \"lon\"]\n                )\n                data = np.arange(c.size).reshape(c.shape)\n                return self.create_output_array(c, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n\n        output = podpac.UnitsDataArray.create(coords)\n        node = MyAlgorithm()\n        with pytest.raises(podpac.NodeException, match=\"provided output is missing dims\"):\n            node.eval(coords, output=output)",
            "def test_eval_with_output_algorithm_transposes(self):",
            "class MyAlgorithm(Algorithm):",
            "def algorithm(self, inputs, coordinates):\n                c = coordinates.transpose(\"lon\", \"lat\")\n                data = np.arange(c.size).reshape(c.shape)\n                return self.create_output_array(c, data=data)\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        output = podpac.UnitsDataArray.create(coords)\n\n        node = MyAlgorithm()\n        node.eval(coords, output=output)\n        assert isinstance(output, podpac.UnitsDataArray)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output, np.arange(6).reshape(2, 3).T)",
            "def test_eval_algorithm_inputs(self):",
            "class MyAlgorithm(Algorithm):\n            x = tl.Instance(podpac.Node).tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                return inputs[\"x\"]\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n\n        a = podpac.data.Array(source=np.arange(6).reshape(coords.shape), coordinates=coords)\n        node = MyAlgorithm(x=a)\n        output = node.eval(coords)\n        assert output.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(output.data, a.source)",
            "def test_eval_multiple_outputs(self):",
            "class MyAlgorithm(Algorithm):\n            x = tl.Instance(podpac.Node).tag(attr=True)\n            y = tl.Instance(podpac.Node).tag(attr=True)\n            outputs = [\"sum\", \"prod\", \"diff\"]",
            "def algorithm(self, inputs, coordinates):\n                sum_ = inputs[\"x\"] + inputs[\"y\"]\n                prod = inputs[\"x\"] * inputs[\"y\"]\n                diff = inputs[\"x\"] - inputs[\"y\"]\n                coords = podpac.Coordinates.from_xarray(prod, crs=coordinates.crs)\n                return self.create_output_array(coords, data=np.stack([sum_, prod, diff], -1))\n\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        x = podpac.data.Array(source=np.arange(coords.size).reshape(coords.shape), coordinates=coords)\n        y = podpac.data.Array(source=np.full(coords.shape, 2), coordinates=coords)\n\n        # all outputs\n        node = MyAlgorithm(x=x, y=y)\n        result = node.eval(coords)\n        assert result.dims == (\"lat\", \"lon\", \"output\")\n        np.testing.assert_array_equal(result[\"output\"], [\"sum\", \"prod\", \"diff\"])\n        np.testing.assert_array_equal(result.sel(output=\"sum\"), x.source + y.source)\n        np.testing.assert_array_equal(result.sel(output=\"prod\"), x.source * y.source)\n        np.testing.assert_array_equal(result.sel(output=\"diff\"), x.source - y.source)\n\n        # extract an output\n        node = MyAlgorithm(x=x, y=y, output=\"prod\")\n        result = node.eval(coords)\n        assert result.dims == (\"lat\", \"lon\")\n        np.testing.assert_array_equal(result, x.source * y.source)",
            "def test_eval_multi_threading(self):",
            "class MySum(Algorithm):\n            A = tl.Instance(podpac.Node).tag(attr=True)\n            B = tl.Instance(podpac.Node).tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                return sum(o for o in inputs.values() if o is not None)\n\n        coords = podpac.Coordinates([[1, 2, 3]], [\"lat\"])\n        array_node = podpac.data.Array(source=np.ones(coords.shape), coordinates=coords)\n\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"DEFAULT_CACHE\"] = []\n            podpac.settings[\"RAM_CACHE_ENABLED\"] = False\n\n            node1 = MySum(A=array_node, B=array_node)\n            node2 = MySum(A=node1, B=array_node)\n\n            # multithreaded\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 8\n            omt = node2.eval(coords)\n\n            # single threaded\n            podpac.settings[\"MULTITHREADING\"] = False\n            ost = node2.eval(coords)\n\n            np.testing.assert_array_equal(omt, ost)",
            "def test_eval_multi_threading_cache_race(self):",
            "class MyPow(Algorithm):\n            source = tl.Instance(podpac.Node).tag(attr=True)\n            exponent = tl.Float().tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                return inputs[\"source\"] ** self.exponent",
            "class MySum(Algorithm):\n            A = tl.Instance(podpac.Node).tag(attr=True)\n            B = tl.Instance(podpac.Node).tag(attr=True)\n            C = tl.Instance(podpac.Node).tag(attr=True)\n            D = tl.Instance(podpac.Node).tag(attr=True)\n            E = tl.Instance(podpac.Node).tag(attr=True)\n            F = tl.Instance(podpac.Node).tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                return sum(o for o in inputs.values() if o is not None)\n\n        coords = podpac.Coordinates([np.linspace(0, 1, 1024)], [\"lat\"])\n        array_node = podpac.data.Array(source=np.ones(coords.shape), coordinates=coords)\n\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 3\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = True\n            podpac.settings[\"DEFAULT_CACHE\"] = [\"ram\"]\n            podpac.settings[\"RAM_CACHE_ENABLED\"] = True\n            podpac.settings.set_unsafe_eval(True)\n            A = MyPow(source=array_node, exponent=2)\n            B = MyPow(source=array_node, exponent=2)\n            C = MyPow(source=array_node, exponent=2)\n            D = MyPow(source=array_node, exponent=2)\n            E = MyPow(source=array_node, exponent=2)\n            F = MyPow(source=array_node, exponent=2)\n\n            node2 = MySum(A=A, B=B, C=C, D=D, E=E, F=F)\n            om = node2.eval(coords)\n            assert sum(n._from_cache for n in node2.inputs.values()) > 0",
            "def test_eval_multi_threading_stress_nthreads(self):",
            "class MyPow(Algorithm):\n            source = tl.Instance(podpac.Node).tag(attr=True)\n            exponent = tl.Float().tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                return inputs[\"source\"] ** self.exponent",
            "class MySum(Algorithm):\n            A = tl.Instance(podpac.Node).tag(attr=True)\n            B = tl.Instance(podpac.Node).tag(attr=True)\n            C = tl.Instance(podpac.Node).tag(attr=True)\n            D = tl.Instance(podpac.Node).tag(attr=True)\n            E = tl.Instance(podpac.Node).tag(attr=True)\n            F = tl.Instance(podpac.Node).tag(attr=True)\n            G = tl.Instance(podpac.Node, allow_none=True).tag(attr=True)",
            "def algorithm(self, inputs, coordinates):\n                return sum(o for o in inputs.values() if o is not None)\n\n        coords = podpac.Coordinates([np.linspace(0, 1, 4)], [\"lat\"])\n        array_node = podpac.data.Array(source=np.ones(coords.shape), coordinates=coords)\n\n        A = MyPow(source=array_node, exponent=0)\n        B = MyPow(source=array_node, exponent=1)\n        C = MyPow(source=array_node, exponent=2)\n        D = MyPow(source=array_node, exponent=3)\n        E = MyPow(source=array_node, exponent=4)\n        F = MyPow(source=array_node, exponent=5)\n\n        node2 = MySum(A=A, B=B, C=C, D=D, E=E, F=F)\n        node3 = MySum(A=A, B=B, C=C, D=D, E=E, F=F, G=node2)\n\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 8\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"DEFAULT_CACHE\"] = []\n            podpac.settings[\"RAM_CACHE_ENABLED\"] = False\n            podpac.settings.set_unsafe_eval(True)\n\n            omt = node3.eval(coords)\n\n        assert node3._multi_threaded\n        assert not node2._multi_threaded\n\n        with podpac.settings:\n            podpac.settings[\"MULTITHREADING\"] = True\n            podpac.settings[\"N_THREADS\"] = 9  # 2 threads available after first 7\n            podpac.settings[\"CACHE_NODE_OUTPUT_DEFAULT\"] = False\n            podpac.settings[\"DEFAULT_CACHE\"] = []\n            podpac.settings[\"RAM_CACHE_ENABLED\"] = False\n            podpac.settings.set_unsafe_eval(True)\n\n            omt = node3.eval(coords)\n\n        assert node3._multi_threaded\n        assert node2._multi_threaded",
            "class TestUnaryAlgorithm(object):",
            "def test_outputs(self):\n        node = UnaryAlgorithm(source=podpac.data.Array())\n        assert node.outputs == None\n\n        node = UnaryAlgorithm(source=podpac.data.Array(outputs=[\"a\", \"b\"]))\n        assert node.outputs == [\"a\", \"b\"]"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_generic.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestGenericInputs(object):",
            "def test_init(self):\n        node = GenericInputs(a=Arange(), b=SinCoords())\n        assert node.inputs[\"a\"] == Arange()\n        assert node.inputs[\"b\"] == SinCoords()",
            "def test_base_definition(self):\n        node = GenericInputs(a=Arange(), b=SinCoords())\n        d = node._base_definition\n        assert \"inputs\" in d\n        assert \"a\" in d[\"inputs\"]\n        assert \"b\" in d[\"inputs\"]",
            "def test_reserved_name(self):\n        with pytest.raises(RuntimeError, match=\"Trait .* is reserved\"):\n            GenericInputs(style=SinCoords())",
            "class TestArithmetic(object):",
            "def test_init(self):\n        sine_node = SinCoords()\n\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n            node = Arithmetic(A=sine_node, B=sine_node, eqn=\"2*abs(A) - B + {offset}\", params={\"offset\": 1})\n\n            podpac.settings.set_unsafe_eval(False)\n            with pytest.warns(UserWarning, match=\"Insecure evaluation\"):\n                node = Arithmetic(A=sine_node, B=sine_node, eqn=\"2*abs(A) - B + {offset}\", params={\"offset\": 1})",
            "def test_evaluate(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n\n            coords = podpac.Coordinates(\n                [podpac.crange(-90, 90, 1.0), podpac.crange(-180, 180, 1.0)], dims=[\"lat\", \"lon\"]\n            )\n            sine_node = SinCoords()\n            node = Arithmetic(A=sine_node, B=sine_node, eqn=\"2*abs(A) - B + {offset}\", params={\"offset\": 1})\n            output = node.eval(coords)\n\n            a = sine_node.eval(coords)\n            b = sine_node.eval(coords)\n            np.testing.assert_allclose(output, 2 * abs(a) - b + 1)",
            "def test_evaluate_not_allowed(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(False)\n\n            coords = podpac.Coordinates(\n                [podpac.crange(-90, 90, 1.0), podpac.crange(-180, 180, 1.0)], dims=[\"lat\", \"lon\"]\n            )\n            sine_node = SinCoords()\n\n            with pytest.warns(UserWarning, match=\"Insecure evaluation\"):\n                node = Arithmetic(A=sine_node, B=sine_node, eqn=\"2*abs(A) - B + {offset}\", params={\"offset\": 1})\n\n            with pytest.raises(PermissionError):\n                node.eval(coords)",
            "def test_missing_equation(self):\n        sine_node = SinCoords()\n        with pytest.raises(ValueError), warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"Insecure evaluation.*\")\n            Arithmetic(A=sine_node, B=sine_node)",
            "class TestGeneric(object):",
            "def test_init(self):\n        a = SinCoords()\n        b = Arange()\n\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n            node = Generic(code=\"import numpy as np\\noutput = np.minimum(a,b)\", a=a, b=b)\n\n            podpac.settings.set_unsafe_eval(False)\n            with pytest.warns(UserWarning, match=\"Insecure evaluation\"):\n                node = Generic(code=\"import numpy as np\\noutput = np.minimum(a,b)\", a=a, b=b)",
            "def test_evaluate(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(True)\n\n            coords = podpac.Coordinates(\n                [podpac.crange(-90, 90, 1.0), podpac.crange(-180, 180, 1.0)], dims=[\"lat\", \"lon\"]\n            )\n            a = SinCoords()\n            b = Arange()\n            node = Generic(code=\"import numpy as np\\noutput = np.minimum(a,b)\", a=a, b=b)\n            output = node.eval(coords)\n\n            a = node.eval(coords)\n            b = node.eval(coords)\n            np.testing.assert_allclose(output, np.minimum(a, b))",
            "def test_evaluate_not_allowed(self):\n        with podpac.settings:\n            podpac.settings.set_unsafe_eval(False)\n\n            coords = podpac.Coordinates(\n                [podpac.crange(-90, 90, 1.0), podpac.crange(-180, 180, 1.0)], dims=[\"lat\", \"lon\"]\n            )\n            a = SinCoords()\n            b = Arange()\n\n            with pytest.warns(UserWarning, match=\"Insecure evaluation\"):\n                node = Generic(code=\"import numpy as np\\noutput = np.minimum(a,b)\", a=a, b=b)\n\n            with pytest.raises(PermissionError):\n                node.eval(coords)",
            "class TestMask(object):",
            "def test_mask_defaults(self):\n        coords = podpac.Coordinates([podpac.crange(-90, 90, 1.0), podpac.crange(-180, 180, 1.0)], dims=[\"lat\", \"lon\"])\n        sine_node = Arange()\n        a = sine_node.eval(coords).copy()\n        a.data[a.data == 1] = np.nan\n\n        node = Mask(source=sine_node, mask=sine_node)\n        output = node.eval(coords)\n\n        np.testing.assert_allclose(output, a)",
            "def test_mask_defaults_bool_op(self):\n        coords = podpac.Coordinates([podpac.clinspace(0, 1, 4), podpac.clinspace(0, 1, 3)], dims=[\"lat\", \"lon\"])\n        sine_node = Arange()\n        a = sine_node.eval(coords).copy()\n\n        # Less than\n        node = Mask(source=sine_node, mask=sine_node, bool_op=\"<\")\n        output = node.eval(coords)\n        b = a.copy()\n        b.data[a.data < 1] = np.nan\n        np.testing.assert_allclose(output, b)\n\n        # Less than equal\n        node = Mask(source=sine_node, mask=sine_node, bool_op=\"<=\")\n        output = node.eval(coords)\n        b = a.copy()\n        b.data[a.data <= 1] = np.nan\n        np.testing.assert_allclose(output, b)\n\n        # Greater than\n        node = Mask(source=sine_node, mask=sine_node, bool_op=\">\")\n        output = node.eval(coords)\n        b = a.copy()\n        b.data[a.data > 1] = np.nan\n        np.testing.assert_allclose(output, b)\n\n        # Greater than equal\n        node = Mask(source=sine_node, mask=sine_node, bool_op=\">=\")\n        output = node.eval(coords)\n        b = a.copy()\n        b.data[a.data >= 1] = np.nan\n        np.testing.assert_allclose(output, b)",
            "def test_bool_val(self):\n        coords = podpac.Coordinates([podpac.clinspace(0, 1, 4), podpac.clinspace(0, 1, 3)], dims=[\"lat\", \"lon\"])\n        sine_node = Arange()\n        a = sine_node.eval(coords).copy()\n        a.data[a.data == 2] = np.nan\n\n        node = Mask(source=sine_node, mask=sine_node, bool_val=2)\n        output = node.eval(coords)\n\n        np.testing.assert_allclose(output, a)",
            "def test_masked_val(self):\n        coords = podpac.Coordinates([podpac.clinspace(0, 1, 4), podpac.clinspace(0, 1, 3)], dims=[\"lat\", \"lon\"])\n        sine_node = Arange()\n        a = sine_node.eval(coords).copy()\n        a.data[a.data == 1] = -9999\n\n        node = Mask(source=sine_node, mask=sine_node, masked_val=-9999)\n        output = node.eval(coords)\n\n        np.testing.assert_allclose(output, a)",
            "def test_in_place(self):\n        coords = podpac.Coordinates([podpac.clinspace(0, 1, 4), podpac.clinspace(0, 1, 3)], dims=[\"lat\", \"lon\"])\n        sine_node = Arange()\n\n        node = Mask(source=sine_node, mask=sine_node, in_place=True)\n        output = node.eval(coords)\n        a = sine_node.eval(coords)\n\n        # In-place editing doesn't seem to work here\n        # np.testing.assert_allclose(output, node.source._output)\n\n        coords = podpac.Coordinates([podpac.clinspace(0, 1, 4), podpac.clinspace(0, 2, 3)], dims=[\"lat\", \"lon\"])\n        sine_node = Arange()\n        node = Mask(source=sine_node, mask=sine_node, in_place=False)\n        output = node.eval(coords)\n        a = sine_node.eval(coords)\n\n        assert not np.all(a == output)",
            "class TestCombine(object):",
            "def test_outputs(self):\n        node = Combine(a=Arange(), b=Arange(), c=Arange())\n        assert set(node.outputs) == set([\"a\", \"b\", \"c\"])\n\n        node = Combine(a=Arange(), b=Arange(), c=Arange(), outputs=[\"o1\", \"o2\", \"o3\"])\n        assert set(node.outputs) == set([\"o1\", \"o2\", \"o3\"])",
            "def test_eval(self):\n        coords = podpac.Coordinates([[0, 1, 2], [10, 20]], dims=[\"lat\", \"lon\"])\n        node = Combine(a=Arange(), b=Arange(), c=Arange())\n        output = node.eval(coords)\n        assert output.dims == (\"lat\", \"lon\", \"output\")\n        assert set(output[\"output\"].data) == set([\"a\", \"b\", \"c\"])\n        np.testing.assert_array_equal(output.sel(output=\"a\"), Arange().eval(coords))\n        np.testing.assert_array_equal(output.sel(output=\"b\"), Arange().eval(coords))\n        np.testing.assert_array_equal(output.sel(output=\"c\"), Arange().eval(coords))"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_utility.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestArange(object):",
            "def test_Arange(self):\n        coords = podpac.Coordinates([[0, 1, 2], [0, 1, 2, 3, 4]], dims=[\"lat\", \"lon\"])\n        node = Arange()\n        output = node.eval(coords)\n        assert output.shape == coords.shape",
            "class TestCoordData(object):",
            "def test_CoordData(self):\n        coords = podpac.Coordinates([[0, 1, 2], [0, 1, 2, 3, 4]], dims=[\"lat\", \"lon\"])\n\n        node = CoordData(coord_name=\"lat\")\n        np.testing.assert_array_equal(node.eval(coords), coords[\"lat\"].coordinates)\n\n        node = CoordData(coord_name=\"lon\")\n        np.testing.assert_array_equal(node.eval(coords), coords[\"lon\"].coordinates)",
            "def test_invalid_dimension(self):\n        coords = podpac.Coordinates([[0, 1, 2], [0, 1, 2, 3, 4]], dims=[\"lat\", \"lon\"])\n        node = CoordData(coord_name=\"time\")\n        with pytest.raises(ValueError):\n            node.eval(coords)",
            "class TestSinCoords(object):",
            "def test_SinCoords(self):\n        coords = podpac.Coordinates(\n            [podpac.crange(-90, 90, 1.0), podpac.crange(\"2018-01-01\", \"2018-01-30\", \"1,D\")], dims=[\"lat\", \"time\"]\n        )\n        node = SinCoords()\n        output = node.eval(coords)\n        assert output.shape == coords.shape"
        ]
    },
    {
        "file": "/home/cfoye/podpac/podpac/core/algorithm/test/test_reprojection.py",
        "comments": [],
        "docstrings": [],
        "code_snippets": [
            "class TestReprojection(object):\n    source_coords = Coordinates([clinspace(0, 8, 9, \"lat\"), clinspace(0, 8, 9, \"lon\")])\n    coarse_coords = Coordinates([clinspace(0, 8, 3, \"lat\"), clinspace(0, 8, 3, \"lon\")])\n    source = Array(source=np.arange(81).reshape(9, 9), coordinates=source_coords, interpolation=\"nearest\")\n    source_coarse = Array(\n        source=[[0, 4, 8], [36, 40, 44], [72, 76, 80]], coordinates=coarse_coords, interpolation=\"bilinear\"\n    )\n    source_coarse2 = Array(\n        source=[[0, 4, 8], [36, 40, 44], [72, 76, 80]],\n        coordinates=coarse_coords.transform(\"EPSG:3857\"),\n        interpolation=\"bilinear\",\n    )",
            "def test_reprojection_Coordinates(self):\n        reproject = Reproject(source=self.source, interpolation=\"bilinear\", coordinates=self.coarse_coords)\n        o1 = reproject.eval(self.source_coords)\n        o2 = self.source_coarse.eval(self.source_coords)\n\n        assert_array_equal(o1.data, o2.data)\n\n        node = podpac.Node.from_json(reproject.json)\n        o3 = node.eval(self.source_coords)\n        assert_array_equal(o1.data, o3.data)",
            "def test_reprojection_source_coords(self):\n        reproject = Reproject(source=self.source, interpolation=\"bilinear\", coordinates=self.source_coarse)\n        o1 = reproject.eval(self.coarse_coords)\n        o2 = self.source_coarse.eval(self.coarse_coords)\n\n        assert_array_equal(o1.data, o2.data)\n\n        node = podpac.Node.from_json(reproject.json)\n        o3 = node.eval(self.coarse_coords)\n        assert_array_equal(o1.data, o3.data)",
            "def test_reprojection_source_dict(self):\n        reproject = Reproject(source=self.source, interpolation=\"bilinear\", coordinates=self.coarse_coords.definition)\n        o1 = reproject.eval(self.coarse_coords)\n        o2 = self.source_coarse.eval(self.coarse_coords)\n\n        assert_array_equal(o1.data, o2.data)\n\n        node = podpac.Node.from_json(reproject.json)\n        o3 = node.eval(self.coarse_coords)\n        assert_array_equal(o1.data, o3.data)",
            "def test_reprojection_source_str(self):\n        reproject = Reproject(source=self.source, interpolation=\"bilinear\", coordinates=self.coarse_coords.json)\n        o1 = reproject.eval(self.coarse_coords)\n        o2 = self.source_coarse.eval(self.coarse_coords)\n\n        assert_array_equal(o1.data, o2.data)\n\n        node = podpac.Node.from_json(reproject.json)\n        o3 = node.eval(self.coarse_coords)\n        assert_array_equal(o1.data, o3.data)",
            "def test_reprojection_Coordinates_crs(self):\n        # same eval and source but different reproject\n        reproject = Reproject(\n            source=self.source,\n            interpolation={\"method\": \"bilinear\", \"params\": {\"fill_value\": \"extrapolate\"}},\n            coordinates=self.coarse_coords.transform(\"EPSG:3857\"),\n        )\n        o1 = reproject.eval(self.source_coords)\n        # We have to use a second source here because the reprojected source\n        # gets interpreted as having it's source coordinates in EPSG:3857\n        # and when being subsampled, there's a warping effect...\n        o2 = self.source_coarse2.eval(self.source_coords)\n        assert_almost_equal(o1.data, o2.data, decimal=13)\n\n        node = podpac.Node.from_json(reproject.json)\n        o3 = node.eval(self.source_coords)\n        assert_array_equal(o1.data, o3.data)\n\n        # same eval and reproject but different source\n        o1 = reproject.eval(self.source_coords.transform(\"EPSG:3857\"))\n        o2 = self.source_coarse2.eval(self.source_coords.transform(\"EPSG:3857\"))\n        assert_almost_equal(o1.data, o2.data, decimal=13)\n\n        # same source and reproject but different eval\n        reproject = Reproject(source=self.source, interpolation=\"bilinear\", coordinates=self.coarse_coords)\n        o1 = reproject.eval(self.source_coords.transform(\"EPSG:3857\"))\n        o2 = self.source_coarse.eval(self.source_coords.transform(\"EPSG:3857\"))\n        assert_almost_equal(o1.data, o2.data, decimal=13)"
        ]
    }
]